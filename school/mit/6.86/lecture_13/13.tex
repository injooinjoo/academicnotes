\documentclass[a4paper,12pt]{article}
\usepackage{kotex}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{bm}

% í˜ì´ì§€ ì„¤ì •
\geometry{left=25mm, right=25mm, top=30mm, bottom=30mm}
\pagestyle{fancy}
\fancyhead[L]{MIT 6.86x Unit 5}
\fancyhead[R]{Reinforcement Learning Basics}

% ìƒ‰ìƒ ì •ì˜
\definecolor{mainblue}{RGB}{0, 51, 102}
\definecolor{subblue}{RGB}{230, 240, 255}
\definecolor{warningred}{RGB}{204, 0, 0}
\definecolor{conceptgreen}{RGB}{0, 102, 51}
\definecolor{storypurple}{RGB}{102, 0, 102}

% ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜
\newtcolorbox{summarybox}[1]{
  colback=subblue, colframe=mainblue, 
  title=\textbf{#1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=2mm
}

\newtcolorbox{warningbox}[1]{
  colback=white, colframe=warningred, 
  title=\textbf{âš ï¸ #1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=0mm,
  coltitle=white
}

\newtcolorbox{conceptbox}[1]{
  colback=white, colframe=conceptgreen, 
  title=\textbf{ğŸ’¡ #1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=2mm,
  coltitle=white
}

\newtcolorbox{storybox}[1]{
  colback=white, colframe=storypurple, 
  title=\textbf{ğŸ¬ #1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=2mm,
  coltitle=white
}

\title{\textbf{MIT 6.86x: ì‹œí–‰ì°©ì˜¤ì˜ ê³¼í•™}}
\author{Unit 5 (Part A): Reinforcement Learning Fundamentals}
\date{}

\begin{document}

\maketitle

% 1. ì „ì²´ ëª©ì°¨ (TOC)
\tableofcontents
\vspace{1cm}
\hrule
\vspace{1cm}

\section*{Course Structure \& Current Focus}
\begin{itemize}
    \item Unit 1~4: Supervised/Unsupervised Learning (ì •ì  ë°ì´í„° í•™ìŠµ)
    \item \textbf{\textcolor{mainblue}{Unit 5: Reinforcement Learning (í˜„ì¬ ë‹¨ì›: ë™ì  ìƒí˜¸ì‘ìš© í•™ìŠµ)}}
    \begin{itemize}
        \item 13.1 MDP Framework ($S, A, P, R, \gamma$)
        \item 13.2 Bellman Equations (ê°€ì¹˜ì˜ ì •ì˜)
        \item 13.3 Solving MDPs: Policy vs Value Iteration
    \end{itemize}
    \item Unit 5 (Part B): Q-Learning (í™˜ê²½ì„ ëª¨ë¥¼ ë•Œ)
\end{itemize}

\newpage

% 2. í˜„ì¬ ë‹¨ì› ì œëª©
\section{Unit 5 (Part A). ê°•í™” í•™ìŠµ ê¸°ì´ˆ (RL Fundamentals)}

% 3. ì´ì „ ë‹¨ì›ê³¼ì˜ ì—°ê²°
\begin{quote}
\textit{ì§€ê¸ˆê¹Œì§€ëŠ” "ì´ë¯¸ì§€ $\to$ ê³ ì–‘ì´"ì²˜ëŸ¼ ì •í•´ì§„ ë°ì´í„°ì…‹ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìì „ê±° íƒ€ê¸°ë¥¼ ë°°ìš¸ ë•Œ ë°ì´í„°ë¥¼ ë³´ê³  ë°°ìš°ë‚˜ìš”? ì•„ë‹™ë‹ˆë‹¤. ì§ì ‘ íƒ€ë³´ê³  ë„˜ì–´ì§€ë©´ì„œ(Trial and Error) ë°°ì›ë‹ˆë‹¤. ì´ë²ˆ ë‹¨ì›ì—ì„œëŠ” ì—ì´ì „íŠ¸ê°€ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©í•˜ë©° \textbf{ë³´ìƒ(Reward)}ì„ í†µí•´ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ í–‰ë™ì„ ê¹¨ìš°ì¹˜ëŠ” ê³¼ì •ì„ ë°°ì›ë‹ˆë‹¤.}
\end{quote}

% 4. ê°œìš”
\subsection*{ğŸ“Œ ê°œìš” (Overview)}
ê°•í™” í•™ìŠµì€ \textbf{MDP(Markov Decision Process)}ë¼ëŠ” ìˆ˜í•™ì  ë¬´ëŒ€ ìœ„ì—ì„œ í¼ì³ì§‘ë‹ˆë‹¤. ì´ ë‹¨ì›ì—ì„œëŠ” MDPì˜ 5ê°€ì§€ ìš”ì†Œì™€, í˜„ì¬ì˜ ì„ íƒì´ ë¯¸ë˜ì— ë¯¸ì¹  ì˜í–¥ì„ ê³„ì‚°í•˜ëŠ” \textbf{ë²¨ë§Œ ë°©ì •ì‹(Bellman Equation)}ì„ ë°°ì›ë‹ˆë‹¤. ë˜í•œ í™˜ê²½ì˜ ê·œì¹™ì„ ì™„ë²½íˆ ì•Œ ë•Œ(Model-based), ìµœì ì˜ ì •ì±…ì„ ì°¾ëŠ” ë‘ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜(\textbf{Policy Iteration, Value Iteration})ì„ ë¹„êµí•©ë‹ˆë‹¤.

% 5. ìš©ì–´ ì •ë¦¬ í‘œ
\subsection*{ğŸ“ í•µì‹¬ ìš©ì–´ ì‚¬ì „}
\begin{table}[h]
\centering
\begin{tabular}{|p{0.25\textwidth}|p{0.7\textwidth}|}
\hline
\textbf{ìš©ì–´ (Term)} & \textbf{ì§ê´€ì  ì˜ë¯¸ (Meaning)} \\
\hline
\textbf{Agent & Environment} & í”Œë ˆì´ì–´(ë‚˜)ì™€ ê²Œì„ ì„¸ìƒ(ì›”ë“œ). \\
\hline
\textbf{State ($S$)} & í˜„ì¬ ë‚´ ìƒí™©. (ì˜ˆ: HP 50, ë³´ìŠ¤ ì•) \\
\hline
\textbf{Action ($A$)} & ë‚´ê°€ í•  ìˆ˜ ìˆëŠ” ì„ íƒ. (ì˜ˆ: ê³µê²©, íšŒí”¼, ë¬¼ì•½) \\
\hline
\textbf{Reward ($R$)} & í–‰ë™ì— ëŒ€í•œ ì¦‰ê°ì ì¸ ì ìˆ˜. (ì˜ˆ: ë³´ìŠ¤ ì²˜ì¹˜ +100) \\
\hline
\textbf{Policy ($\pi$)} & ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ í• ì§€ ì ì–´ë‘” 'ê³µëµì§‘'. \\
\hline
\textbf{Discount Factor ($\gamma$)} & ì¸ë‚´ì‹¬. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë¨¼ ë¯¸ë˜ì˜ ë³´ìƒì„ ì¤‘ìš”ì‹œí•¨. \\
\hline
\end{tabular}
\end{table}

\vspace{0.5cm}\hrule\vspace{0.5cm}

% 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª…
\subsection{1. ê°•í™” í•™ìŠµì˜ ê¸°ë³¸ ë¼ˆëŒ€: MDP}


[Image of reinforcement learning agent environment loop]


\begin{conceptbox}{ê°œë… 1: ì„¸ìƒì„ ì •ì˜í•˜ëŠ” 5ê°€ì§€ ìš”ì†Œ}
\textbf{í•œ ì¤„ ìš”ì•½:} ì„¸ìƒì€ ìƒíƒœ($S$), í–‰ë™($A$), ë¬¼ë¦¬ ë²•ì¹™($P$), ë³´ìƒ($R$), ê·¸ë¦¬ê³  ì‹œê°„ ê´€ë…($\gamma$)ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
\end{conceptbox}

\subsubsection*{1) êµ¬ì„± ìš”ì†Œ (Tuple)}
$$ \text{MDP} = (S, A, P, R, \gamma) $$
\begin{itemize}
    \item \textbf{Transition Probability ($P$):} ë‚´ê°€ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ê°€ë ¤ í•´ë„($A$), ë¹™íŒê¸¸ì´ë¼ ë¯¸ë„ëŸ¬ì ¸ì„œ ìœ„ë¡œ ê°ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤($S'$). ë‚´ ì˜ë„ëŒ€ë¡œ ì„¸ìƒì´ ì›€ì§ì´ì§€ ì•Šì„ í™•ë¥ ì…ë‹ˆë‹¤.
    \item \textbf{Discount Factor ($\gamma \in [0, 1]$):}
    \begin{itemize}
        \item $\gamma \approx 0$ (ìšœë¡œì¡±): ì§€ê¸ˆ ë‹¹ì¥ì˜ ë³´ìƒë§Œ ë´…ë‹ˆë‹¤.
        \item $\gamma \approx 1$ (íˆ¬ìì): 10ë…„ ë’¤ì˜ ëŒ€ë°•ì„ ìœ„í•´ ì§€ê¸ˆì˜ ê³ í†µì„ ì°¸ìŠµë‹ˆë‹¤.
    \end{itemize}
\end{itemize}

\subsubsection*{2) ë§ˆë¥´ì½”í”„ ì„±ì§ˆ (Markov Property)}
\textbf{"ê³¼ê±°ëŠ” ë¬»ì§€ ë§ˆì„¸ìš”."}
í˜„ì¬ ìƒíƒœ($S_t$)ë§Œ ì•Œë©´ ë¯¸ë˜($S_{t+1}$)ë¥¼ ì˜ˆì¸¡í•˜ê¸°ì— ì¶©ë¶„í•©ë‹ˆë‹¤. ë‚´ê°€ ì´ ë¯¸ë¡œì˜ ë§‰ë‹¤ë¥¸ ê¸¸ì— ì–´ë–»ê²Œ ë“¤ì–´ì™”ëŠ”ì§€ëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë§‰ë‹¤ë¥¸ ê¸¸ì— ìˆë‹¤ëŠ” ì‚¬ì‹¤ë§Œì´ íƒˆì¶œ ê²½ë¡œë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{2. ë²¨ë§Œ ë°©ì •ì‹ (Bellman Equations)}

\begin{conceptbox}{ê°œë… 2: ê°€ì¹˜(Value)ì˜ ì¬ê·€ì  ì •ì˜}
\textbf{í•œ ì¤„ ìš”ì•½:} ì–´ë–¤ ìƒíƒœì˜ ê°€ì¹˜ëŠ” "ì§€ê¸ˆ ë°›ëŠ” í˜„ì°°" + "ë¯¸ë˜ì— ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ” ëˆì˜ í˜„ì¬ ê°€ì¹˜"ì…ë‹ˆë‹¤.
\end{conceptbox}

\subsubsection*{1) ê°€ì¹˜ í•¨ìˆ˜ (Value Function $V(s)$)}
ìƒíƒœ $s$ì— ìˆëŠ” ê²ƒì´ ì–¼ë§ˆë‚˜ ì¢‹ì€ê°€?
$$ V(s) = \mathbb{E} [ R_{t+1} + \gamma V(S_{t+1}) | S_t = s ] $$
ì´ ì‹ì€ \textbf{ì¬ê·€ì (Recursive)}ì…ë‹ˆë‹¤. ë‚´ ê°€ì¹˜ë¥¼ ì•Œë ¤ë©´ ë‹¤ìŒ ìƒíƒœì˜ ê°€ì¹˜ë¥¼ ì•Œì•„ì•¼ í•˜ê³ , ë‹¤ìŒ ìƒíƒœëŠ” ë‹¤ë‹¤ìŒ ìƒíƒœë¥¼... ì´ë ‡ê²Œ ê¼¬ë¦¬ë¥¼ ë­…ë‹ˆë‹¤.

\subsubsection*{2) ë²¨ë§Œ ìµœì  ë°©ì •ì‹ (Bellman Optimality Equation)}
ìš°ë¦¬ì˜ ëª©í‘œëŠ” ìµœê³ ì˜ ì„ íƒ(Max)ì„ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
$$ V^*(s) = \max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^*(s') \right] $$
"ë‚´ê°€ í•  ìˆ˜ ìˆëŠ” í–‰ë™ $a$ë“¤ ì¤‘ì—ì„œ, (ì¦‰ì‹œ ë³´ìƒ + ë¯¸ë˜ ê°€ì¹˜ ê¸°ëŒ“ê°’)ì´ ìµœëŒ€ê°€ ë˜ëŠ” ê²ƒì„ ê³ ë¥´ê² ë‹¤."

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{3. ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•: ê°€ì¹˜ ë°˜ë³µ vs ì •ì±… ë°˜ë³µ}


ê°€ì •: ìš°ë¦¬ëŠ” ê²Œì„ì˜ ë£°(ì§€ë„, í™•ë¥  $P$)ì„ ëª¨ë‘ ì•Œê³  ìˆìŠµë‹ˆë‹¤. (Model-based)

\subsubsection*{A. ì •ì±… ë°˜ë³µ (Policy Iteration)}
\begin{itemize}
    \item \textbf{ë¹„ìœ :} "ì¼ë‹¨ ê³„íš(ì •ì±…)ì„ ì„¸ìš°ê³ , ê·¸ ê³„íšëŒ€ë¡œ ì­‰ ì‚´ì•„ë³¸ ë’¤(í‰ê°€), ë³„ë¡œë©´ ê³„íšì„ ìˆ˜ì •(ë°œì „)í•˜ì."
    \item \textbf{ê³¼ì •:}
    \begin{enumerate}
        \item \textbf{Evaluation:} í˜„ì¬ ì •ì±… $\pi$ë¡œ í–ˆì„ ë•Œì˜ ê°€ì¹˜ $V^\pi$ë¥¼ ëê¹Œì§€ ê³„ì‚° (ì—°ë¦½ë°©ì •ì‹ í’€ì´).
        \item \textbf{Improvement:} ê³„ì‚°ëœ $V$ë¥¼ ë³´ê³  ë” ë‚˜ì€ í–‰ë™ì´ ìˆìœ¼ë©´ ì •ì±… $\pi$ë¥¼ ì—…ë°ì´íŠ¸.
    \end{enumerate}
    \item \textbf{íŠ¹ì§•:} ì •í™•í•˜ì§€ë§Œ í•œ í„´ ê³„ì‚°ì´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤.
\end{itemize}

\subsubsection*{B. ê°€ì¹˜ ë°˜ë³µ (Value Iteration)}
\begin{itemize}
    \item \textbf{ë¹„ìœ :} "ë©€ë¦¬ ë³¼ ê²ƒ ì—†ì´, ì¼ë‹¨ ë§¤ ìˆœê°„ ê°€ì¥ ì´ë“ì´ ë˜ëŠ” ìª½(Greedy)ìœ¼ë¡œ ê°€ì¹˜ë¥¼ ê°±ì‹ í•˜ë‹¤ ë³´ë©´ ì •ë‹µì´ ë‚˜ì˜¤ê² ì§€."
    \item \textbf{ê³¼ì •:} ì •ì±… í‰ê°€ë¥¼ ëê¹Œì§€ í•˜ì§€ ì•Šê³ , ê·¸ëƒ¥ ìµœëŒ“ê°’(max)ì„ í•œ ë²ˆ ì·¨í•´ì„œ ê°€ì¹˜ë¥¼ ê°±ì‹ í•´ë²„ë¦½ë‹ˆë‹¤.
    $$ V_{k+1}(s) \leftarrow \max_a (R + \gamma \sum P V_k(s')) $$
    \item \textbf{íŠ¹ì§•:} ê³„ì‚°ì´ í›¨ì”¬ ë¹ ë¥´ê³  ê°„í¸í•˜ì—¬ ë„ë¦¬ ì“°ì…ë‹ˆë‹¤.
\end{itemize}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤: ë„¥ìŠ¨ ê²Œì„ ì˜¤í†  í”Œë ˆì´ AI}

\begin{storybox}{Scenario: ëª¨ë°”ì¼ RPG ìë™ ì‚¬ëƒ¥}
ë‹¹ì‹ ì€ ë„¥ìŠ¨ì˜ AI ê°œë°œìì…ë‹ˆë‹¤. ì‹ ê·œ RPG ê²Œì„ì˜ 'ìë™ ì‚¬ëƒ¥' ê¸°ëŠ¥ì„ ê°•í™” í•™ìŠµìœ¼ë¡œ êµ¬í˜„í•˜ë ¤ í•©ë‹ˆë‹¤.
\end{storybox}

\begin{enumerate}
    \item \textbf{MDP ì •ì˜:}
    \begin{itemize}
        \item \textbf{ìƒíƒœ($S$):} [ë‚´ HP, ë§ˆë‚˜, ë³´ìŠ¤ ê±°ë¦¬, ë³´ìŠ¤ ìŠ¤í‚¬ ì¿¨íƒ€ì„]
        \item \textbf{í–‰ë™($A$):} [ê³µê²©, íšŒí”¼, ë¬¼ì•½ ì‚¬ìš©]
        \item \textbf{ë³´ìƒ($R$):} ë³´ìŠ¤ ì²˜ì¹˜(+100), ì‚¬ë§(-100), ë¬¼ì•½ ë‚­ë¹„(-1), ì‹œê°„ ì§€ì²´(-0.1)
    \end{itemize}
    
    \item \textbf{í• ì¸ìœ¨($\gamma$) ì„¤ì •:}
    $\gamma=0.99$ë¡œ ì„¤ì •í•©ë‹ˆë‹¤. ë‹¹ì¥ ë¬¼ì•½ ì•„ë¼ëŠ” ê²ƒ(-1 íšŒí”¼)ë³´ë‹¤, ê²°êµ­ ì‚´ì•„ì„œ ë³´ìŠ¤ë¥¼ ì¡ëŠ” ê²ƒ(+100)ì´ ì¤‘ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
    
    \item \textbf{í•™ìŠµ (Value Iteration):}
    AIëŠ” ìˆ˜ë§Œ ë²ˆì˜ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ "ë³´ìŠ¤ê°€ ê´‘ì—­ê¸°ë¥¼ ì“¸ ë•Œ(ìƒíƒœ)ëŠ” ê³µê²©ë³´ë‹¤ íšŒí”¼(í–‰ë™)í•˜ëŠ” ê²ƒì´ ê¸°ëŒ€ ê°€ì¹˜($V$)ê°€ ë†’ë‹¤"ëŠ” ê²ƒì„ ìŠ¤ìŠ¤ë¡œ ê¹¨ë‹«ê²Œ ë©ë‹ˆë‹¤.
    
    \item \textbf{ê²°ê³¼:} í•˜ë“œì½”ë”©ëœ ê·œì¹™("ì²´ë ¥ 30\%ë©´ ë¬¼ì•½ ì¨")ë³´ë‹¤ í›¨ì”¬ ìœ ì—°í•˜ê³  ê³ ìˆ˜ ê°™ì€ í”Œë ˆì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.
\end{enumerate}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)}

\begin{description}
    \item[Q1. í• ì¸ìœ¨ $\gamma$ëŠ” ì™œ ê¼­ 1ë³´ë‹¤ ì‘ì•„ì•¼ í•˜ë‚˜ìš”?]
    \textbf{A.} ìˆ˜í•™ì  ì´ìœ ì™€ í˜„ì‹¤ì  ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤.
    \begin{itemize}
        \item \textbf{ìˆ˜í•™:} $\gamma=1$ì´ê³  ê²Œì„ì´ ëë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ë³´ìƒì˜ í•©ì´ ë¬´í•œëŒ€($\infty$)ê°€ ë˜ì–´ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥(ë°œì‚°)í•´ì§‘ë‹ˆë‹¤. ìˆ˜ë ´ì„ ìœ„í•´ 1ë³´ë‹¤ ì‘ì•„ì•¼ í•©ë‹ˆë‹¤.
        \item \textbf{í˜„ì‹¤:} ë¯¸ë˜ëŠ” ë¶ˆí™•ì‹¤í•©ë‹ˆë‹¤. 10ë…„ ë’¤ 100ë§Œ ì›ë³´ë‹¤ ì§€ê¸ˆ 100ë§Œ ì›ì´ ë” ê°€ì¹˜ ìˆëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    \end{itemize}
    
    \item[Q2. í™˜ê²½($P$)ì„ ëª¨ë¥´ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?]
    \textbf{A.} ì´ê²ƒì´ ì§„ì§œ ê°•í™” í•™ìŠµì˜ ì‹œì‘ì…ë‹ˆë‹¤.
    ì§€ê¸ˆ ë°°ìš´ MDP í’€ì´ë²•(Dynamic Programming)ì€ $P$ë¥¼ ì•ˆë‹¤ê³  ê°€ì •í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„  ë‚´ê°€ ì´ í–‰ë™ì„ í–ˆì„ ë•Œ ì„¸ìƒì´ ì–´ë–»ê²Œ ë³€í• ì§€ ëª¨ë¦…ë‹ˆë‹¤.
    ì´ë•ŒëŠ” ì§ì ‘ ê²ªì–´ë³´ë©° $P$ë¥¼ ì¶”ì •í•˜ê±°ë‚˜ ê°€ì¹˜ë¥¼ ë°°ìš°ëŠ” \textbf{Q-Learning} ê°™ì€ ë°©ë²•ì„ ì”ë‹ˆë‹¤. (ë‹¤ìŒ íŒŒíŠ¸ ë‚´ìš©)
\end{description}

% 10. ë‹¤ìŒ ë‹¨ì› ì—°ê²°
\vspace{1cm}
\begin{quote}
\textbf{Next Step:} ì§€ê¸ˆê¹Œì§€ëŠ” "ê²Œì„ì˜ ë£°(í™•ë¥  $P$)"ì„ ë‹¤ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•˜ê³  ìµœì ì˜ ìˆ˜ë¥¼ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤(Dynamic Programming). í•˜ì§€ë§Œ í˜„ì‹¤ ì„¸ê³„ëŠ” ë£°ì„ ì•Œë ¤ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒ \textbf{Unit 5 (Part B)}ì—ì„œëŠ” ë§¨ë•…ì— í—¤ë”©í•˜ë©° ê²½í—˜ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ì§„ì§œ ê°•í™” í•™ìŠµ, \textbf{Q-Learning}ì„ ë°°ì›ë‹ˆë‹¤.
\end{quote}

% 11. ë‹¨ì› ìš”ì•½ ë°•ìŠ¤
\begin{summarybox}{Unit 5 (Part A) í•µì‹¬ ìš”ì•½}
\begin{itemize}
    \item \textbf{MDP:} ê°•í™” í•™ìŠµì„ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” í‹€ ($S, A, P, R, \gamma$).
    \item \textbf{ë²¨ë§Œ ë°©ì •ì‹:} $V(s) = R + \gamma V(s')$. ê°€ì¹˜ëŠ” ì¬ê·€ì ìœ¼ë¡œ ì •ì˜ëœë‹¤.
    \item \textbf{Dynamic Programming:} í™˜ê²½($P$)ì„ ì™„ë²½íˆ ì•Œ ë•Œ ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•.
    \item \textbf{Policy vs Value Iteration:} 'ê³„íš ìˆ˜ì •' vs 'ë§¤ ìˆœê°„ ìµœì„  ì„ íƒ'. ë‘˜ ë‹¤ ìµœì í•´ì— ë„ë‹¬í•œë‹¤.
\end{itemize}
\end{summarybox}

\end{document}