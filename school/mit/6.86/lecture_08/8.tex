\documentclass[a4paper,12pt]{article}
\usepackage{kotex}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{fancyhdr}
\usepackage{bm}

% í˜ì´ì§€ ì„¤ì •
\geometry{left=25mm, right=25mm, top=30mm, bottom=30mm}
\pagestyle{fancy}
\fancyhead[L]{MIT 6.86x Unit 4}
\fancyhead[R]{Backpropagation}

% ìƒ‰ìƒ ì •ì˜
\definecolor{mainblue}{RGB}{0, 51, 102}
\definecolor{subblue}{RGB}{230, 240, 255}
\definecolor{warningred}{RGB}{204, 0, 0}
\definecolor{conceptgreen}{RGB}{0, 102, 51}
\definecolor{storypurple}{RGB}{102, 0, 102}

% ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜
\newtcolorbox{summarybox}[1]{
  colback=subblue, colframe=mainblue, 
  title=\textbf{#1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=2mm
}

\newtcolorbox{warningbox}[1]{
  colback=white, colframe=warningred, 
  title=\textbf{âš ï¸ #1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=0mm,
  coltitle=white
}

\newtcolorbox{conceptbox}[1]{
  colback=white, colframe=conceptgreen, 
  title=\textbf{ğŸ’¡ #1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=2mm,
  coltitle=white
}

\newtcolorbox{storybox}[1]{
  colback=white, colframe=storypurple, 
  title=\textbf{ğŸ¬ #1}, fonttitle=\bfseries,
  boxrule=0.5mm, arc=2mm,
  coltitle=white
}

\title{\textbf{MIT 6.86x: ì˜¤ì°¨ë¥¼ í†µí•´ ë°°ìš°ë‹¤}}
\author{Unit 4: Backpropagation \& Optimization Details}
\date{}

\begin{document}

\maketitle

% 1. ì „ì²´ ëª©ì°¨ (TOC)
\tableofcontents
\vspace{1cm}
\hrule
\vspace{1cm}

\section*{Course Structure \& Current Focus}
\begin{itemize}
    \item Unit 3: Neural Networks Basics (êµ¬ì¡° ì„¤ê³„)
    \item \textbf{\textcolor{mainblue}{Unit 4: Backpropagation (í˜„ì¬ ë‹¨ì›: í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜)}}
    \begin{itemize}
        \item 8.1 The Engine: Backpropagation Algorithm
        \item 8.2 The Scorecard: Loss Functions (MSE vs Cross-Entropy)
        \item 8.3 Setting the Stage: Initialization \& Learning Rate
        \item 8.4 Stabilization: Batch Norm \& Dropout
    \end{itemize}
    \item Unit 5: Recurrent Neural Networks (ìˆœì°¨ ë°ì´í„°)
\end{itemize}

\newpage

% 2. í˜„ì¬ ë‹¨ì› ì œëª©
\section{Unit 4. ì—­ì „íŒŒì™€ í•™ìŠµì˜ ì›ë¦¬ (Backpropagation)}

% 3. ì´ì „ ë‹¨ì›ê³¼ì˜ ì—°ê²°
\begin{quote}
\textit{Unit 3ì—ì„œ ìš°ë¦¬ëŠ” ìˆ˜ì²œ ê°œì˜ ë‰´ëŸ°ì´ ì—°ê²°ëœ ë‡Œ(FNN)ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê°“ íƒœì–´ë‚œ ë‡ŒëŠ” ì•„ëŠ” ê²ƒì´ ì—†ìŠµë‹ˆë‹¤(ê°€ì¤‘ì¹˜ $W$ê°€ ëœë¤ì„). ì´ì œ ì´ ë‡Œì—ê²Œ ì •ë‹µì„ ë³´ì—¬ì£¼ê³ , í‹€ë¦´ ë•Œë§ˆë‹¤ \textbf{"ëˆ„ê°€ ì˜ëª»í–ˆëŠ”ì§€"}ë¥¼ ë”°ì ¸ì„œ ì¡°ê¸ˆì”© ë˜‘ë˜‘í•´ì§€ê²Œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì´ ë°”ë¡œ í•™ìŠµ(Training)ì…ë‹ˆë‹¤.}
\end{quote}

% 4. ê°œìš”
\subsection*{ğŸ“Œ ê°œìš” (Overview)}
ì´ ë‹¨ì›ì—ì„œëŠ” ì‹ ê²½ë§ í•™ìŠµì˜ í•µì‹¬ ì—”ì§„ì¸ \textbf{ì—­ì „íŒŒ(Backpropagation)} ì•Œê³ ë¦¬ì¦˜ì„ ë°°ì›ë‹ˆë‹¤. ë¯¸ë¶„ì˜ \textbf{ì—°ì‡„ ë²•ì¹™(Chain Rule)}ì„ ì´ìš©í•´ ì˜¤ì°¨ì˜ ì›ì¸ì„ ì°¾ì•„ë‚´ê³ , \textbf{ì†ì‹¤ í•¨ìˆ˜(Loss Function)}ë¥¼ í†µí•´ ì„±ì ì„ ë§¤ê¸°ë©°, \textbf{ì´ˆê¸°í™”(Initialization)}ì™€ \textbf{ì •ê·œí™”(Normalization)} ê¸°ë²•ìœ¼ë¡œ í•™ìŠµì„ ì•ˆì •í™”ì‹œí‚¤ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë‹¤ë£¹ë‹ˆë‹¤.

% 5. ìš©ì–´ ì •ë¦¬ í‘œ
\subsection*{ğŸ“ í•µì‹¬ ìš©ì–´ ì‚¬ì „}
\begin{table}[h]
\centering
\begin{tabularx}{\textwidth}{|p{0.28\textwidth}|X|}
\hline
\textbf{ìš©ì–´ (Term)} & \textbf{ì§ê´€ì  ì˜ë¯¸ (Meaning)} \\
\hline
\textbf{Forward Pass} & ì…ë ¥ $\to$ ì¶œë ¥. ì˜ˆì¸¡ê°’ì„ ë½‘ì•„ë‚´ëŠ” ê³¼ì •. \\
\hline
\textbf{Backward Pass} & ì¶œë ¥ $\to$ ì…ë ¥. í‹€ë¦° ì´ìœ (ë¯¸ë¶„ê°’)ë¥¼ ë°°ë‹¬í•˜ëŠ” ê³¼ì •. \\
\hline
\textbf{Chain Rule} & ê¼¬ë¦¬ì— ê¼¬ë¦¬ë¥¼ ë¬´ëŠ” ë¯¸ë¶„. (ë‚˜ë¹„íš¨ê³¼ ì¶”ì ) \\
\hline
\textbf{Epoch} & ì „ì²´ ë¬¸ì œì§‘ì„ í•œ ë²ˆ ë‹¤ í’€ì–´ë³´ëŠ” ê²ƒ. \\
\hline
\textbf{Dropout} & ì¼ë¶€ ë‰´ëŸ°ì„ ëœë¤í•˜ê²Œ êº¼ì„œ, íŠ¹ì • ë‰´ëŸ° í¸ì• ë¥¼ ë§‰ëŠ” ê¸°ìˆ . \\
\hline
\end{tabularx}
\end{table}

\vspace{0.5cm}\hrule\vspace{0.5cm}

% 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª…
\subsection{1. ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ (Backpropagation)}

\begin{conceptbox}{ê°œë… 1: ì±…ì„ ì†Œì¬ ë”°ì§€ê¸° (The Blame Game)}
\textbf{í•œ ì¤„ ìš”ì•½:} ê²°ê³¼ê°€ í‹€ë ¸ì„ ë•Œ, ì¶œë ¥ì¸µì—ì„œë¶€í„° ê±°ê¾¸ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° ê° ë‰´ëŸ°ì´ ì˜¤ì°¨ì— ê¸°ì—¬í•œ ë§Œí¼(ê¸°ìš¸ê¸°) ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•©ë‹ˆë‹¤.
\end{conceptbox}

\subsubsection*{1) ì‘ë™ ì›ë¦¬: ì—°ì‡„ ë²•ì¹™ (Chain Rule)}
ìš°ë¦¬ëŠ” ìµœì¢… ì˜¤ì°¨ $L$ì„ ì¤„ì´ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ $w$ë¥¼ ì¡°ì ˆí•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì¦‰, $\frac{\partial L}{\partial w}$ê°€ í•„ìš”í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ $L$ê³¼ $w$ëŠ” ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
$$ \frac{\partial L}{\partial w} = \underbrace{\frac{\partial L}{\partial y}}_{\text{ì˜¤ì°¨ ë³€í™”}} \cdot \underbrace{\frac{\partial y}{\partial h}}_{\text{ì¶œë ¥ ë³€í™”}} \cdot \underbrace{\frac{\partial h}{\partial w}}_{\text{ì€ë‹‰ì¸µ ë³€í™”}} $$
\begin{itemize}
    \item ë§ˆì¹˜ í†µì—­ì„ ê±°ì¹˜ë“¯, ë¯¸ë¶„ê°’ì„ ì¸µì¸µì´ ê³±í•´ì„œ ì „ë‹¬í•©ë‹ˆë‹¤.
    \item **** (ìœ„ ì´ë¯¸ì§€ ì°¸ê³ ): $\delta$ê°’(ì˜¤ì°¨ ì‹ í˜¸)ì´ ì˜¤ë¥¸ìª½($a^4$)ì—ì„œ ì™¼ìª½($a^0$)ìœ¼ë¡œ ì „íŒŒë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{itemize}

\subsubsection*{2) ìˆ«ì ì˜ˆì‹œ}
ê°„ë‹¨í•œ ì‹ $L = (y - wx)^2$ ì—ì„œ, $y=10, x=2, w=3$ì´ë¼ê³  í•©ì‹œë‹¤.
\begin{enumerate}
    \item \textbf{ìˆœì „íŒŒ:} ì˜ˆì¸¡ $\hat{y} = 3 \times 2 = 6$.
    \item \textbf{ì˜¤ì°¨:} $L = (10 - 6)^2 = 16$.
    \item \textbf{ì—­ì „íŒŒ:} $w$ë¥¼ ì¡°ì ˆí•˜ë©´ $L$ì´ ì–¼ë§ˆë‚˜ ë³€í• ê¹Œ?
    $$ \frac{\partial L}{\partial w} = 2(y - \hat{y}) \cdot (-x) = 2(4) \cdot (-2) = -16 $$
    \item \textbf{ì˜ë¯¸:} ê¸°ìš¸ê¸°ê°€ ìŒìˆ˜(-16)ì´ë¯€ë¡œ, $w$ë¥¼ í‚¤ì›Œì•¼ ì˜¤ì°¨ê°€ ì¤„ì–´ë“­ë‹ˆë‹¤.
\end{enumerate}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{2. ì†ì‹¤ í•¨ìˆ˜ (Loss Function)}

\begin{conceptbox}{ê°œë… 2: ì±„ì  ê¸°ì¤€í‘œ}
\textbf{í•œ ì¤„ ìš”ì•½:} ë¬¸ì œì˜ ìœ í˜•(íšŒê·€ vs ë¶„ë¥˜)ì— ë”°ë¼ "ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€"ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤.
\end{conceptbox}

\begin{table}[h]
\centering
\begin{tabular}{l|l|l}
\toprule
\textbf{êµ¬ë¶„} & \textbf{MSE (Mean Squared Error)} & \textbf{Cross-Entropy} \\
\midrule
\textbf{ìš©ë„} & \textbf{íšŒê·€} (ì§‘ê°’, ì˜¨ë„ ì˜ˆì¸¡) & \textbf{ë¶„ë¥˜} (ìŠ¤íŒ¸, ì•„ì´í…œ ë“±ê¸‰) \\
\textbf{ìˆ˜ì‹} & $\frac{1}{n}\sum (y - \hat{y})^2$ & $-\sum y \log(\hat{y})$ \\
\textbf{íŠ¹ì§•} & ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ ì œê³±ìœ¼ë¡œ í˜¼ëƒ„. & í™•ë¥ ì´ í‹€ë¦´ìˆ˜ë¡ ê¸‰ê²©íˆ í˜¼ëƒ„. \\
\bottomrule
\end{tabular}
\end{table}

\begin{warningbox}{ì™œ ë¶„ë¥˜ ë¬¸ì œì— MSEë¥¼ ì•ˆ ì“°ë‚˜ìš”?}
Sigmoid + MSE ì¡°í•©ì„ ì“°ë©´, ì˜¤ì°¨ê°€ ì—„ì²­ í°ë°ë„ ë¯¸ë¶„ê°’(Gradient)ì´ 0ì— ê°€ê¹Œì›Œì ¸ì„œ í•™ìŠµì´ ì•ˆ ë˜ëŠ” í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤. ë°˜ë©´ \textbf{Cross-Entropy}ëŠ” ë¡œê·¸ í•¨ìˆ˜ íŠ¹ì„±ìƒ í‹€ë¦¬ë©´ ë¯¸ë¶„ê°’ì´ í­ë°œì ìœ¼ë¡œ ì»¤ì ¸ì„œ "ì •ì‹  ì°¨ë ¤!"ë¼ê³  ê°•í•˜ê²Œ í”¼ë“œë°±ì„ ì¤ë‹ˆë‹¤.
\end{warningbox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{3. ì´ˆê¸°í™”ì™€ í•™ìŠµë¥  (Initialization \& Learning Rate)}

\begin{conceptbox}{ê°œë… 3: ì‹œì‘ì´ ë°˜ì´ë‹¤}
\textbf{í•œ ì¤„ ìš”ì•½:} ì¶œë°œì (ì´ˆê¸°í™”)ì„ ì˜ ì¡ê³ , ë³´í­(í•™ìŠµë¥ )ì„ ì ì ˆíˆ ì¡°ì ˆí•´ì•¼ ì‚° ì•„ë˜ë¡œ ì˜ ë‚´ë ¤ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{conceptbox}

\subsubsection*{1) ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Initialization)}
\begin{itemize}
    \item \textbf{0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´? (The Trap of Symmetry):} ëª¨ë“  ë‰´ëŸ°ì´ ë˜‘ê°™ì€ ê°’ì„ ë‚´ë±‰ê³ , ë˜‘ê°™ì´ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤. 100ê°œë¥¼ ì¨ë„ 1ê°œ ì“°ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
    \item \textbf{í•´ê²°ì±…:}
    \begin{itemize}
        \item \textbf{He Initialization:} ReLU í•¨ìˆ˜ë¥¼ ì“¸ ë•Œ í‘œì¤€. (ë¶„ì‚°ì„ $2/n$ë¡œ ë§ì¶¤)
        \item \textbf{Xavier Initialization:} Sigmoid/Tanhë¥¼ ì“¸ ë•Œ í‘œì¤€.
    \end{itemize}
\end{itemize}

\subsubsection*{2) í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§ (Learning Rate Decay)}
ì²˜ìŒì—” ì„±í¼ì„±í¼(í° $\eta$) ê°€ë‹¤ê°€, ì •ë‹µ ê·¼ì²˜ì—ì„œëŠ” ì¡°ì‹¬ì¡°ì‹¬(ì‘ì€ $\eta$) ê°€ì•¼ í•©ë‹ˆë‹¤.
$$ \eta_t = \eta_0 / (1 + kt) \quad (\text{ì‹œê°„ì´ ê°ˆìˆ˜ë¡ ì¤„ì–´ë“¦}) $$

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{4. í•™ìŠµ ì•ˆì •í™” ê¸°ë²•: ë°°ì¹˜ ì •ê·œí™” \& ë“œë¡­ì•„ì›ƒ}

\begin{conceptbox}{ê°œë… 4: í”ë“¤ë¦¬ì§€ ì•ŠëŠ” í¸ì•ˆí•¨}
\textbf{í•œ ì¤„ ìš”ì•½:} ë”¥ëŸ¬ë‹ì€ ê¹Šì–´ì§ˆìˆ˜ë¡ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤. ì´ë¥¼ ë§‰ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ê°•ì œë¡œ ì •ë ¬í•˜ê±°ë‚˜(Batch Norm), ì¼ë¶€ëŸ¬ í•¸ë””ìº¡ì„ ì¤ë‹ˆë‹¤(Dropout).
\end{conceptbox}

\subsubsection*{A. ë°°ì¹˜ ì •ê·œí™” (Batch Normalization)}
\begin{itemize}
    \item \textbf{ë¬¸ì œ:} ì•ë‹¨ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ê°€ ë°”ë€Œë©´, ë’·ë‹¨ ë ˆì´ì–´ ì…ì¥ì—ì„œëŠ” ë“¤ì–´ì˜¤ëŠ” ë°ì´í„° ë¶„í¬ê°€ ê³„ì† ë°”ë€ë‹ˆë‹¤ (Internal Covariate Shift).
    \item \textbf{í•´ê²°:} ê° ì¸µë§ˆë‹¤ ë“¤ì–´ì˜¤ëŠ” ë°ì´í„°ë¥¼ \textbf{í‰ê·  0, ë¶„ì‚° 1}ë¡œ ê°•ì œ ì¡°ì •í•´ ë²„ë¦½ë‹ˆë‹¤.
    \item \textbf{íš¨ê³¼:} í•™ìŠµ ì†ë„ê°€ ë¹„ì•½ì ìœ¼ë¡œ ë¹¨ë¼ì§€ê³ , ì´ˆê¸°í™”ì— ëœ ë¯¼ê°í•´ì§‘ë‹ˆë‹¤.
\end{itemize}

\subsubsection*{B. ë“œë¡­ì•„ì›ƒ (Dropout)}
\begin{itemize}
    \item \textbf{ê°œë…:} í•™ìŠµí•  ë•Œë§ˆë‹¤ ì€ë‹‰ì¸µ ë‰´ëŸ°ì˜ 50\%ë¥¼ ëœë¤í•˜ê²Œ êº¼ë²„ë¦½ë‹ˆë‹¤.
    \item \textbf{ë¹„ìœ :} ì¶•êµ¬íŒ€ì—ì„œ ì—ì´ìŠ¤(íŠ¹ì • ë‰´ëŸ°) í•œ ëª…ë§Œ ë¯¿ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ì—°ìŠµ ë•Œ ì—ì´ìŠ¤ë¥¼ ë²¤ì¹˜ì— ì•‰í˜€ë‘ê³  ë‚˜ë¨¸ì§€ ì„ ìˆ˜ë“¤ë¼ë¦¬ í›ˆë ¨ì‹œí‚µë‹ˆë‹¤. íŒ€ ì „ì²´ ì „ë ¥ì´ ì˜¬ë¼ê°‘ë‹ˆë‹¤(ê³¼ì í•© ë°©ì§€).
\end{itemize}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤: ë„¥ìŠ¨ ê²Œì„ ì´íƒˆ(Churn) ë°©ì§€ ëª¨ë¸}

\begin{storybox}{Scenario: VIP ìœ ì €ê°€ ë– ë‚˜ëŠ” ì´ìœ  ì°¾ê¸°}
ë‹¹ì‹ ì€ ë„¥ìŠ¨ì˜ PMì…ë‹ˆë‹¤. "ì´ë²ˆ ë‹¬ì— ì ‘ì†í•  ê²ƒ"ì´ë¼ê³  ì˜ˆì¸¡(0.9)í–ˆë˜ VIP ìœ ì €ê°€ ì‹¤ì œë¡œëŠ” ì ‘ì†í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤(Target 0.0). ì˜¤ì°¨ê°€ í½ë‹ˆë‹¤.
\end{storybox}

\begin{enumerate}
    \item \textbf{Forward:} ìœ ì € ë°ì´í„°(ê¸¸ë“œ í™œë™, ê²°ì œì•¡ ë“±) $\to$ FNN $\to$ ì˜ˆì¸¡í™•ë¥  90\%.
    \item \textbf{Loss:} Cross-Entropyë¡œ ê³„ì‚°í•˜ë‹ˆ ì†ì‹¤ì´ ë§¤ìš° í½ë‹ˆë‹¤. ("í™•ì‹ í–ˆëŠ”ë° í‹€ë ¸êµ°!")
    \item \textbf{Backward:}
    \begin{itemize}
        \item ì—­ì „íŒŒê°€ ì‹œì‘ë©ë‹ˆë‹¤. ì¶œë ¥ì¸µì—ì„œ ì€ë‹‰ì¸µìœ¼ë¡œ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
        \item ë²”ì¸ ìƒ‰ì¶œ: "ê¸¸ë“œ í™œë™ ì ìˆ˜"ì— ì—°ê²°ëœ ê°€ì¤‘ì¹˜ê°€ ë²”ì¸ì´ì—ˆìŠµë‹ˆë‹¤. "ê¸¸ë“œ í™œë™ì´ ë§ìœ¼ë©´ ë¬´ì¡°ê±´ ë‚¨ëŠ”ë‹¤"ê³  ê³¼ëŒ€í‰ê°€í•˜ê³  ìˆì—ˆë„¤ìš”.
    \end{itemize}
    \item \textbf{Update:} í•´ë‹¹ ê°€ì¤‘ì¹˜ë¥¼ ëŒ€í­ ì‚­ê°í•©ë‹ˆë‹¤.
    \item \textbf{Result:} ëª¨ë¸ì€ ì´ì œ "ê¸¸ë“œ í™œë™ì´ ë§ì•„ë„ ìµœê·¼ ì ‘ì†ì´ ëœ¸í•˜ë©´ ì´íƒˆí•  ìˆ˜ ìˆë‹¤"ëŠ” ë¯¸ë¬˜í•œ íŒ¨í„´ì„ ë°°ìš°ê²Œ ë©ë‹ˆë‹¤.
\end{enumerate}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)}

\begin{description}
    \item[Q1. ì—­ì „íŒŒë¥¼ ì†ìœ¼ë¡œ ê³„ì‚°í•  ì¤„ ì•Œì•„ì•¼ í•˜ë‚˜ìš”?]
    \textbf{A.} ê°œë…ë§Œ ì•Œë©´ ë©ë‹ˆë‹¤. PyTorchë‚˜ TensorFlow ê°™ì€ í”„ë ˆì„ì›Œí¬ê°€ `loss.backward()` í•œ ì¤„ì´ë©´ ìë™ìœ¼ë¡œ ë¯¸ë¶„ì„ ë‹¤ í•´ì¤ë‹ˆë‹¤. (ì´ê±¸ \textbf{Autograd}ë¼ê³  í•©ë‹ˆë‹¤.)
    
    \item[Q2. Dropoutì€ í…ŒìŠ¤íŠ¸(ì‹¤ì „) ë•Œë„ ì“°ë‚˜ìš”?]
    \textbf{A.} \textbf{ì ˆëŒ€ ì•„ë‹™ë‹ˆë‹¤!} í•™ìŠµ(Training) ë•ŒëŠ” ë‰´ëŸ°ì„ ë„ì§€ë§Œ, í…ŒìŠ¤íŠ¸(Inference) ë•ŒëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì¼œì„œ ì „ë ¥ì„ ë‹¤í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€ì‹  ì¶œë ¥ê°’ì— ë¹„ìœ¨(0.5)ì„ ê³±í•´ì„œ ë³´ì •í•´ ì¤ë‹ˆë‹¤.
\end{description}

% 10. ë‹¤ìŒ ë‹¨ì› ì—°ê²°
\vspace{1cm}
\begin{quote}
\textbf{Next Step:} ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ì‹ ê²½ë§ì€ ì •ì§€ëœ ì´ë¯¸ì§€ë‚˜ í…Œì´ë¸” ë°ì´í„°ì—ëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, "ë¬¸ì¥"ì´ë‚˜ "ì£¼ê°€"ì²˜ëŸ¼ \textbf{ìˆœì„œê°€ ìˆëŠ”(Sequential) ë°ì´í„°}ì—ëŠ” ì•½í•©ë‹ˆë‹¤. ë‹¤ìŒ \textbf{Unit 5}ì—ì„œëŠ” ê¸°ì–µë ¥(Memory)ì„ ê°€ì§„ ì‹ ê²½ë§, \textbf{RNN(Recurrent Neural Networks)}ê³¼ LSTMì„ ë°°ì›ë‹ˆë‹¤.
\end{quote}

% 11. ë‹¨ì› ìš”ì•½ ë°•ìŠ¤
\begin{summarybox}{Unit 4 í•µì‹¬ ìš”ì•½}
\begin{itemize}
    \item \textbf{ì—­ì „íŒŒ (Backprop):} Chain Ruleì„ ì´ìš©í•´ ì˜¤ì°¨ì˜ ì±…ì„(Gradient)ì„ ê° ê°€ì¤‘ì¹˜ì— ë°°ë¶„í•œë‹¤.
    \item \textbf{ì†ì‹¤ í•¨ìˆ˜:} íšŒê·€ëŠ” MSE, ë¶„ë¥˜ëŠ” Cross-Entropyê°€ êµ­ë£°ì´ë‹¤.
    \item \textbf{He Initialization:} ReLUë¥¼ ì“¸ ë•ŒëŠ” ì´ˆê¸°í™”ë¥¼ ì˜í•´ì•¼ í•™ìŠµì´ ì‹œì‘ëœë‹¤.
    \item \textbf{Batch Norm \& Dropout:} í•™ìŠµì„ ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê³  ê³¼ì í•©ì„ ë§‰ëŠ” í•„ìˆ˜ í…Œí¬ë‹‰.
\end{itemize}
\end{summarybox}

\end{document}