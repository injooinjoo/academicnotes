\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì •ì˜)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Recurrent Neural Networks (RNN)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item \textbf{10.1 Recurrent Neural Networks (RNN)}
        \begin{itemize}
            \item Why Sequence Models? (Time \& Order)
            \item RNN Architecture (Unrolled View)
            \item Forward Propagation Formulas
            \item BPTT (Backpropagation Through Time)
        \end{itemize}
        \item 10.2 GRU \& LSTM (Gated Units) \textit{- Upcoming}
        \item 10.3 NLP \& Word Embeddings \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ê³ ì •ëœ í¬ê¸°ì˜ ì´ë¯¸ì§€($H \times W$)ë¥¼ ì²˜ë¦¬í•˜ëŠ” CNNì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„¸ìƒì˜ ë§ì€ ë°ì´í„°ëŠ” \textbf{'ìˆœì„œ(Sequence)'}ì™€ \textbf{'ì‹œê°„(Time)'}ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
"ë‚˜ëŠ” í”„ë‘ìŠ¤ì— ê°€ì„œ..."ë¼ëŠ” ë§ì„ ë“¤ìœ¼ë©´, ë’¤ì— "í”„ë‘ìŠ¤ì–´"ë¼ëŠ” ë§ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ìš°ë¦¬ëŠ” ì••ë‹ˆë‹¤. ì´ëŠ” ì•ì„  ë‹¨ì–´ë“¤ì˜ \textbf{ë¬¸ë§¥(Context)}ì„ ê¸°ì–µí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
ê¸°ì¡´ ì‹ ê²½ë§ì€ ì´ 'ê¸°ì–µ' ëŠ¥ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ê¸°ì–µì„ ê°€ì§„ ì‹ ê²½ë§, \textbf{RNN}ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” RNNì˜ ê¸°ë³¸ ì›ë¦¬ì™€ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ì¡°:} ì€ë‹‰ ìƒíƒœ(Hidden State)ë¥¼ í†µí•´ ê³¼ê±° ì •ë³´ë¥¼ í˜„ì¬ë¡œ ì „ë‹¬í•˜ëŠ” ë£¨í”„ êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜ì‹:} $a^{\langle t \rangle} = g(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle})$ ê³µì‹ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.
    \item \textbf{ê³µìœ :} ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì—ì„œ \textbf{íŒŒë¼ë¯¸í„°($W$)ë¥¼ ê³µìœ }í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ì›ë¦¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{í•™ìŠµ:} ì‹œê°„ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ëŠ” ì—­ì „íŒŒ, \textbf{BPTT}ì˜ ê°œë…ì„ ìµí™ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{ê¸°í˜¸} & \textbf{ìš©ì–´} & \textbf{ì„¤ëª…} \\ \hline
$x^{\langle t \rangle}$ & Input & ì‹œê°„ $t$ì—ì„œì˜ ì…ë ¥ (ì˜ˆ: $t$ë²ˆì§¸ ë‹¨ì–´). \\ \hline
$a^{\langle t \rangle}$ & Hidden State & ì‹œê°„ $t$ì—ì„œì˜ ì€ë‹‰ ìƒíƒœ. \textbf{(ê¸°ì–µ)} \\ \hline
$y^{\langle t \rangle}$ & Output & ì‹œê°„ $t$ì—ì„œì˜ ì¶œë ¥ (ì˜ˆ: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡). \\ \hline
$W_{aa}$ & Weight (Hidden) & ê³¼ê±° ê¸°ì–µì„ í˜„ì¬ë¡œ ê°€ì ¸ì˜¤ëŠ” ê°€ì¤‘ì¹˜. \\ \hline
$W_{ax}$ & Weight (Input) & í˜„ì¬ ì…ë ¥ì„ ë°›ì•„ë“¤ì´ëŠ” ê°€ì¤‘ì¹˜. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìˆœí™˜ì˜ ë§ˆë²•}

\subsection{1. RNN Architecture (Unrolled)}
RNNì€ ìì‹ ì„ ê°€ë¦¬í‚¤ëŠ” í™”ì‚´í‘œ(Loop)ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ë¥¼ ì‹œê°„ì¶•ìœ¼ë¡œ í¼ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.



\begin{itemize}
    \item \textbf{ì…ë ¥:} ë§¤ ì‹œì  $t$ë§ˆë‹¤ $x^{\langle t \rangle}$ê°€ ë“¤ì–´ì˜µë‹ˆë‹¤.
    \item \textbf{ì „ë‹¬:} ì´ì „ ì‹œì ì˜ ê¸°ì–µ $a^{\langle t-1 \rangle}$ì´ í˜„ì¬ ì‹œì  $t$ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
    \item \textbf{ì¶œë ¥:} ë‘ ì •ë³´ë¥¼ í•©ì³ì„œ $y^{\langle t \rangle}$ë¥¼ ì¶œë ¥í•˜ê³ , ë‹¤ìŒ ì‹œì  $t+1$ë¡œ ê¸°ì–µ $a^{\langle t \rangle}$ë¥¼ ë„˜ê¹ë‹ˆë‹¤.
\end{itemize}

\subsection{2. Forward Propagation Formulas}
RNNì˜ í•µì‹¬ì€ \textbf{"í˜„ì¬ ì…ë ¥ê³¼ ê³¼ê±° ê¸°ì–µì„ ì„ì–´ì„œ ìƒˆë¡œìš´ ê¸°ì–µì„ ë§Œë“ ë‹¤"}ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{formulabox}{ì€ë‹‰ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì–µ ê°±ì‹ )}
$$ a^{\langle t \rangle} = \tanh(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle} + b_a) $$
\begin{itemize}
    \item $W_{aa}a^{\langle t-1 \rangle}$: ê³¼ê±°ì˜ ê¸°ì–µ ë°˜ì˜.
    \item $W_{ax}x^{\langle t \rangle}$: í˜„ì¬ì˜ ì •ë³´ ë°˜ì˜.
    \item $\tanh$: ê°’ì„ -1 ~ 1 ì‚¬ì´ë¡œ ì••ì¶•í•˜ì—¬ í­ë°œ ë°©ì§€ (ì£¼ë¡œ ì‚¬ìš©).
\end{itemize}
\end{formulabox}

\begin{formulabox}{ì¶œë ¥ ê³„ì‚°}
$$ \hat{y}^{\langle t \rangle} = \text{softmax}(W_{ya}a^{\langle t \rangle} + b_y) $$
\begin{itemize}
    \item í˜„ì¬ì˜ ê¸°ì–µ($a^{\langle t \rangle}$)ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
\end{itemize}
\end{formulabox}

\textbf{Key Point (Parameter Sharing):}
$t=1$ì´ë“  $t=100$ì´ë“ , $W_{aa}, W_{ax}, W_{ya}$ëŠ” \textbf{ëª¨ë‘ ë˜‘ê°™ì€ í–‰ë ¬}ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ê²ƒì´ RNNì´ ê¸¸ì´ ì œí•œ ì—†ì´ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¹„ê²°ì…ë‹ˆë‹¤.

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Deep Dive: Backpropagation Through Time (BPTT)}

RNNì˜ í•™ìŠµì€ ì‹œê°„ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°‘ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Loss:} ì „ì²´ ì†ì‹¤ $L$ì€ ê° ì‹œê°„ ë‹¨ê³„ë³„ ì†ì‹¤ì˜ í•©ì…ë‹ˆë‹¤. $L = \sum L^{\langle t \rangle}$.
    \item \textbf{Gradient:} $t=100$ ì‹œì ì˜ ì˜¤ì°¨ë¥¼ ìˆ˜ì •í•˜ë ¤ë©´, $t=99, 98, \dots, 1$ ì‹œì ì˜ ìƒíƒœê¹Œì§€ ì˜í–¥ì„ ë¯¸ì³ì•¼ í•©ë‹ˆë‹¤.
    \item \textbf{Problem:} ë¯¸ë¶„ê°’ì´ ê³„ì† ê³±í•´ì§€ë©´ì„œ($W_{aa}^{100}$), ê°’ì´ 0ìœ¼ë¡œ ì‚¬ë¼ì§€ê±°ë‚˜(Vanishing) ë¬´í•œëŒ€ë¡œ ì»¤ì§€ëŠ”(Exploding) ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ê¸°ë³¸ RNNì€ \textbf{ì¥ê¸° ì˜ì¡´ì„±(Long-term Dependency)}ì„ í•™ìŠµí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: RNN Step \& Loop}

NumPyë¡œ RNNì˜ ë‚´ë¶€ ë™ì‘ì„ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=RNN Forward Pass Implementation, breaklines=true]
import numpy as np

def rnn_cell_forward(xt, a_prev, parameters):
    """
    ë‹¨ì¼ íƒ€ì„ ìŠ¤í… (t) ì²˜ë¦¬
    """
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    # 1. ë‹¤ìŒ ì€ë‹‰ ìƒíƒœ ê³„ì‚° (Tanh ì‚¬ìš©)
    # a_next = tanh(Waa*a_prev + Wax*xt + ba)
    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)
    
    # 2. í˜„ì¬ ì¶œë ¥ ì˜ˆì¸¡ (Softmax ê°€ì •)
    yt_pred = softmax(np.dot(Wya, a_next) + by)
    
    return a_next, yt_pred

def rnn_forward(x, a0, parameters):
    """
    ì „ì²´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (Time Loop)
    """
    n_x, m, T_x = x.shape  # T_x: ì‹œê°„ ê¸¸ì´ (Timesteps)
    n_y, n_a = parameters["Wya"].shape
    
    a = np.zeros((n_a, m, T_x))      # ëª¨ë“  ê¸°ì–µ ì €ì¥ìš©
    y_pred = np.zeros((n_y, m, T_x)) # ëª¨ë“  ì¶œë ¥ ì €ì¥ìš©
    
    a_next = a0 # ì´ˆê¸° ê¸°ì–µ
    
    # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë£¨í”„ (RNNì˜ í•µì‹¬)
    for t in range(T_x):
        xt = x[:, :, t] # të²ˆì§¸ ì…ë ¥
        
        # ì…€ ì—…ë°ì´íŠ¸
        a_next, yt_pred = rnn_cell_forward(xt, a_next, parameters)
        
        # ì €ì¥
        a[:, :, t] = a_next
        y_pred[:, :, t] = yt_pred
        
    return a, y_pred

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\textbf{Q. ì™œ ReLU ëŒ€ì‹  Tanhë¥¼ ì“°ë‚˜ìš”?} \\
\textbf{A.} RNNì€ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì‹­, ìˆ˜ë°± ë²ˆ ë°˜ë³µí•´ì„œ ê³±í•©ë‹ˆë‹¤. ReLUë¥¼ ì“°ë©´ ê°’ì´ ê³„ì† ì»¤ì ¸ì„œ ë°œì‚°(Exploding)í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. TanhëŠ” ê°’ì„ -1~1 ì‚¬ì´ë¡œ ë¬¶ì–´ë‘ì–´(Bounding) ì•ˆì •ì ì¸ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤.

\textbf{Q. RNNì€ ë³‘ë ¬ ì²˜ë¦¬ê°€ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} ë„¤, êµ¬ì¡°ì ìœ¼ë¡œ ì–´ë µìŠµë‹ˆë‹¤. $t$ ì‹œì ì˜ ê³„ì‚°ì„ í•˜ë ¤ë©´ ë°˜ë“œì‹œ $t-1$ ì‹œì ì˜ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤(Sequential). ì´ê²ƒì´ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ê°€ ë“±ì¥í•˜ê²Œ ëœ ë°°ê²½ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ê¸°ë³¸ RNNì€ ì´ë¡ ì ìœ¼ë¡œ í›Œë¥­í•˜ì§€ë§Œ, ë¬¸ì¥ì´ ì¡°ê¸ˆë§Œ ê¸¸ì–´ì ¸ë„(10ë‹¨ì–´ ì´ìƒ) ì•ë¶€ë¶„ ë‚´ìš©ì„ ê¹Œë¨¹ëŠ” \textbf{ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ}ê°€ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë”¥ëŸ¬ë‹ ì—°êµ¬ìë“¤ì€ \textbf{"ê¸°ì–µì„ ì–¼ë§ˆë‚˜ ì˜¤ë˜ ìœ ì§€í• ì§€ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ëŠ” ê²Œì´íŠ¸(Gate)"}ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” í˜„ëŒ€ NLPì˜ ê·¼ê°„ì´ ëœ \textbf{[GRU (Gated Recurrent Unit)]}ì™€ \textbf{[LSTM (Long Short-Term Memory)]}ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Structure:} ì…ë ¥($x$) + ì´ì „ ê¸°ì–µ($a$) $\rightarrow$ ìƒˆ ê¸°ì–µ $\rightarrow$ ì¶œë ¥($y$).
    \item \textbf{Sharing:} ëª¨ë“  ì‹œì ì—ì„œ ë™ì¼í•œ íŒŒë¼ë¯¸í„°($W_{ax}, W_{aa}$)ë¥¼ ì“´ë‹¤.
    \item \textbf{Formula:} $a^{\langle t \rangle} = \tanh(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle} + b_a)$.
    \item \textbf{Limit:} ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ì–´(Vanishing Gradient) ì´ˆê¸° ê¸°ì–µì„ ìƒëŠ”ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}