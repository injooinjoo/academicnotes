\documentclass[a4paper, 11pt]{article}

% --- 패키지 설정 ---
\usepackage{kotex} % 한글 지원
\usepackage{geometry} % 여백 설정
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % 수식 패키지
\usepackage{graphicx}
\usepackage{adjustbox}  % 표/박스 크기 조절 % 이미지 삽입
\usepackage{hyperref} % 하이퍼링크
\usepackage{xcolor} % 색상 지원
\usepackage{listings} % 코드 블록
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % 박스 디자인
\usepackage{enumitem} % 리스트 스타일
\usepackage{booktabs} % 표 디자인
\usepackage{array} % 표 정렬

% --- 색상 정의 ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- 코드 스타일 설정 ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- 박스 스타일 정의 ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=📌 #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=💡 #1 (직관적 비유)
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=⚠️ #1 (오해 방지 가이드)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=🧮 #1 (수학적 증명)
}

% --- 문서 정보 ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ Classic Networks (LeNet, AlexNet, VGG)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. 전체 목차 (TOC) ---
\section*{📚 Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.2 CNN Foundations \& Pooling \textit{- Completed}
        \item \textbf{9.3 Classic Networks}
        \begin{itemize}
            \item LeNet-5: The Pioneer
            \item AlexNet: The Game Changer
            \item VGG-16: The Standardizer ($3 \times 3$ Philosophy)
        \end{itemize}
        \item 9.4 ResNet (Residual Networks) \textit{- Upcoming}
        \item 9.5 Inception Network \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. 이전 단원 연결 ---
\section*{🔗 지난 시간 복습 및 연결}
우리는 CNN의 레고 블록(Convolution, Pooling, Activation)을 마스터했습니다. 이제 이 블록들을 조립하여 \textbf{거대한 성(Model)}을 쌓을 시간입니다.
딥러닝 역사에는 "이 모델 이전과 이후로 세상이 나뉘었다"고 평가받는 전설적인 아키텍처들이 있습니다. 오늘은 그 역사의 시작점인 \textbf{LeNet}, 딥러닝 붐을 일으킨 \textbf{AlexNet}, 그리고 깊은 신경망의 표준을 제시한 \textbf{VGG}를 해부합니다.

% --- 4. 개요 ---
\section{Unit Overview}
\begin{summarybox}{핵심 목표}
이 단원은 현대 컴퓨터 비전 모델의 조상이 되는 세 가지 고전 모델을 다룹니다.
\begin{itemize}
    \item \textbf{LeNet-5 (1998):} CNN의 기본 구조(Conv-Pool 반복)를 정립한 선구자.
    \item \textbf{AlexNet (2012):} ReLU, Dropout 등을 도입하여 딥러닝 붐을 일으킨 주인공.
    \item \textbf{VGG-16 (2014):} $3 \times 3$ 필터만으로 깊이를 쌓는 '단순함의 미학'을 증명한 표준 모델.
    \item \textbf{패턴:} 채널은 늘리고($\uparrow$), 크기는 줄이는($\downarrow$) 공통적인 설계 패턴을 익힙니다.
\end{itemize}
\end{summarybox}

% --- 5. 모델 비교 요약 ---
\section{Model Summary Table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{특징} & \textbf{LeNet-5 (1998)} & \textbf{AlexNet (2012)} & \textbf{VGG-16 (2014)} \\ \hline
\textbf{입력} & $32 \times 32$ (Gray) & $227 \times 227$ (RGB) & $224 \times 224$ (RGB) \\ \hline
\textbf{필터} & $5 \times 5$ & $11 \times 11, 5 \times 5$ & \textbf{Only $3 \times 3$} \\ \hline
\textbf{활성화} & Sigmoid / Tanh & \textbf{ReLU} & ReLU \\ \hline
\textbf{풀링} & Average Pooling & Max Pooling & Max Pooling \\ \hline
\textbf{파라미터} & 약 6만 개 & 약 6,000만 개 & \textbf{약 1억 3,800만 개} \\ \hline
\end{tabular}
\end{center}

% --- 6. 핵심 개념 상세 설명 ---
\section{Core Concepts: 전설들의 계보}

\subsection{1. LeNet-5: The Pioneer}

얀 르쿤(Yann LeCun) 교수가 은행 수표의 손글씨 숫자(MNIST) 인식을 위해 개발했습니다.
\begin{itemize}
    \item \textbf{구조:} Conv $\to$ Pool $\to$ Conv $\to$ Pool $\to$ FC ...
    \item \textbf{의의:} 이미지의 크기는 줄이고($32 \to 28 \to 14 \dots$), 채널 수는 늘리는($1 \to 6 \to 16$) 패턴을 처음 정립했습니다.
\end{itemize}

\subsection{2. AlexNet: The Game Changer}

2012년 ImageNet 대회 우승작. 딥러닝 시대를 연 장본인입니다.
\begin{itemize}
    \item \textbf{ReLU:} Sigmoid의 기울기 소실 문제를 해결했습니다.
    \item \textbf{Dropout:} 과대적합을 막기 위해 FC 층에 드롭아웃을 적용했습니다.
    \item \textbf{Multi-GPU:} 당시 GPU 성능 한계로 모델을 두 개로 쪼개 학습했습니다.
\end{itemize}

\subsection{3. VGG-16: The Standardizer}

옥스퍼드 대학 VGG 팀이 개발했습니다. \textbf{"화려한 기교(11x11 필터 등)는 필요 없다. 깊이(Depth)만이 정답이다"}를 증명했습니다.

\begin{mathbox}{Why $3 \times 3$ filters?}
VGG는 모든 층에서 $3 \times 3$ 필터만 씁니다. 왜 큰 필터 한 번 대신 작은 필터를 여러 번 쓸까요?

\textbf{1. 파라미터 효율 (Parameter Efficiency)}
\begin{itemize}
    \item $5 \times 5$ 필터 1개: $25 \times C^2$ 파라미터.
    \item $3 \times 3$ 필터 2개: $2 \times (9 \times C^2) = 18 \times C^2$ 파라미터.
    \item \textbf{결론:} 같은 영역(Receptive Field)을 보면서도 파라미터 수를 \textbf{28\% 절약}합니다.
\end{itemize}

\textbf{2. 비선형성 (Non-linearity)}
\begin{itemize}
    \item 층이 두 개라는 것은 ReLU를 두 번 통과한다는 뜻입니다.
    \item 더 복잡하고 정교한 함수를 학습할 수 있습니다.
\end{itemize}
\end{mathbox}

% --- 7. 구현 코드 ---
\section{Implementation: Load VGG16 with Keras}

최신 프레임워크에서는 이 거대한 모델을 단 한 줄로 불러올 수 있습니다. 전이 학습의 기초가 됩니다.

\begin{lstlisting}[language=Python, caption=Loading VGG16, breaklines=true]
from tensorflow.keras.applications import VGG16

# ImageNet 가중치를 가진 VGG16 모델 로드
# include_top=True: 마지막 FC 분류기(1000개 클래스)까지 포함
model = VGG16(weights='imagenet', include_top=True)

# 모델 구조 출력
model.summary()

# 출력 예시 (패턴 확인):
# Block 1: Conv(3x3) -> Conv(3x3) -> MaxPool
# Block 2: Conv(3x3) -> Conv(3x3) -> MaxPool
# ... (채널 수: 64 -> 128 -> 256 -> 512)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\textbf{Q. VGG-16의 단점은 없나요?} \\
\textbf{A.} \textbf{너무 무겁습니다.} 파라미터 수가 1억 3,800만 개나 되어 메모리를 엄청나게 잡아먹습니다. 또한 16층, 19층까지는 괜찮았지만, 그 이상 쌓으면 다시 기울기 소실 문제로 학습이 안 됩니다.

\textbf{Q. $1 \times 1$ Convolution은 뭔가요? (Inception 예고)} \\
\textbf{A.} 필터 크기가 $1 \times 1$인 합성곱입니다. 공간적 정보($H, W$)는 건드리지 않고, \textbf{채널 수($C$)를 줄이거나 늘리는 역할}을 합니다. 연산량을 줄이는 '병목(Bottleneck)' 기법의 핵심입니다.

% --- 9. 다음 단원 연결 ---
\section*{🔗 다음 단계 (Next Step)}
VGG는 훌륭했지만, 층이 20개를 넘어가면 성능이 오히려 떨어지는 현상이 발견되었습니다. (Degradation Problem)
인간의 뇌는 수백 층의 깊이도 처리합니다. 딥러닝도 100층, 1000층을 쌓을 수 없을까요?

다음 시간에는 딥러닝 역사상 가장 중요한 발명 중 하나인 \textbf{'잔차 연결(Residual Connection)'}을 도입하여 152층을 쌓은 괴물, \textbf{[ResNet (Residual Networks)]}을 분석하겠습니다.

\vspace{0.5cm}

\begin{summarybox}{단원 요약 (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{LeNet-5:} CNN의 조상. Conv-Pool 패턴의 시초.
    \item \textbf{AlexNet:} ReLU와 Dropout으로 딥러닝 성능을 입증함.
    \item \textbf{VGG-16:} $3 \times 3$ 필터만 사용하여 깊이를 쌓음. (단순함, 파라미터 효율)
    \item \textbf{Limit:} VGG조차도 너무 깊어지면 학습이 안 되는 한계가 있음.
\end{enumerate}
\end{summarybox}

\end{document}