\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Optimization Algorithms: \\ Batch Normalization}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.5 Regularization \& Data Setup \textit{- Completed}
        \item 5.6-5.10 Optimization (Adam, Tuning) \textit{- Completed}
        \item \textbf{5.11 Batch Normalization}
        \begin{itemize}
            \item Concept: Internal Covariate Shift
            \item The Algorithm: Norm, Scale, Shift
            \item Train Mode vs Test Mode (Running Average)
            \item Implementation
        \end{itemize}
        \item 5.12 Softmax Regression \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì—¬ì „íˆ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ê¸ˆë§Œ ë†’ì—¬ë„ ë°œì‚°í•´ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.
ì´ìœ ëŠ” \textbf{ì•ë‹¨ ì¸µì˜ íŒŒë¼ë¯¸í„°ê°€ ë°”ë€Œë©´, ë’·ë‹¨ ì¸µìœ¼ë¡œ ë„˜ì–´ì˜¤ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ê³„ì† ë°”ë€Œê¸° ë•Œë¬¸}ì…ë‹ˆë‹¤. ë’·ë‹¨ ì¸µ ì…ì¥ì—ì„œëŠ” ê³„ì† í”ë“¤ë¦¬ëŠ” ë•… ìœ„ì—ì„œ ê· í˜•ì„ ì¡ìœ¼ë ¤ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 2015ë…„, \textbf{"ë°ì´í„° ë¶„í¬ë¥¼ ê°•ì œë¡œ ê³ ì •ì‹œí‚¤ì"}ëŠ” í˜ëª…ì ì¸ ì•„ì´ë””ì–´ê°€ ë“±ì¥í•©ë‹ˆë‹¤. ë°”ë¡œ \textbf{ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)}ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ ì—­ì‚¬ìƒ ê°€ì¥ ìœ„ëŒ€í•œ ë°œëª… ì¤‘ í•˜ë‚˜ì¸ \textbf{ë°°ì¹˜ ì •ê·œí™”}ì˜ ì›ë¦¬ì™€ êµ¬í˜„ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì›ì¸:} í•™ìŠµì„ ë°©í•´í•˜ëŠ” \textbf{ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”(Internal Covariate Shift)} í˜„ìƒì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì•Œê³ ë¦¬ì¦˜:} ë¯¸ë‹ˆ ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê· /ë¶„ì‚°ì„ ì •ê·œí™”í•˜ê³ , \textbf{Scale($\gamma$) \& Shift($\beta$)} íŒŒë¼ë¯¸í„°ë¡œ ë³µì›í•˜ëŠ” ê³¼ì •ì„ ìœ ë„í•©ë‹ˆë‹¤.
    \item \textbf{ì°¨ì´:} í•™ìŠµ(Train) ë•ŒëŠ” ë°°ì¹˜ í†µê³„ëŸ‰ì„, ì¶”ë¡ (Test) ë•ŒëŠ” \textbf{ì´ë™ í‰ê· (Running Average)}ì„ ì¨ì•¼ í•¨ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•˜ëŠ” BN í´ë˜ìŠ¤ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´/ê¸°í˜¸} & \textbf{ì˜ë¯¸} & \textbf{ì—­í• } \\ \hline
\textbf{Batch Norm} & ë°°ì¹˜ ì •ê·œí™” & ì€ë‹‰ì¸µì˜ í™œì„±í™” ê°’ì„ ì •ê·œ ë¶„í¬ë¡œ ë§Œë“¦. \\ \hline
\textbf{Gamma ($\gamma$)} & ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° & ì •ê·œí™”ëœ ê°’ì˜ \textbf{ë¶„ì‚°}ì„ ì¡°ì ˆ (í•™ìŠµ ê°€ëŠ¥). \\ \hline
\textbf{Beta ($\beta$)} & ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„° & ì •ê·œí™”ëœ ê°’ì˜ \textbf{í‰ê· }ì„ ì¡°ì ˆ (í•™ìŠµ ê°€ëŠ¥). \\ \hline
\textbf{Running Stats} & ì´ë™ í‰ê·  í†µê³„ëŸ‰ & í…ŒìŠ¤íŠ¸ ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•™ìŠµ ì¤‘ ëˆ„ì í•´ë‘” í‰ê· /ë¶„ì‚°. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í”ë“¤ë¦¬ëŠ” ë•… ê³ ì •í•˜ê¸°}

\subsection{1. Internal Covariate Shift (ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”)}


\begin{analogybox}{í”ë“¤ë¦¬ëŠ” ë‹¤ë¦¬ ìœ„ì—ì„œ ê±·ê¸°}
\begin{itemize}
    \item \textbf{Without BN:} ì•ì‚¬ëŒ(Layer 1)ì´ ë°œì„ êµ¬ë¥¼ ë•Œë§ˆë‹¤ ë‹¤ë¦¬ê°€ í”ë“¤ë¦½ë‹ˆë‹¤. ë’·ì‚¬ëŒ(Layer 2)ì€ ì¤‘ì‹¬ ì¡ëŠë¼ ì•ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. (í•™ìŠµ ì†ë„ ì €í•˜)
    \item \textbf{With BN:} ê° ì¸µë§ˆë‹¤ \textbf{"ë°œíŒì„ ìˆ˜í‰ìœ¼ë¡œ ê³ ì •(Normalize)"}í•´ì¤ë‹ˆë‹¤. ë’·ì‚¬ëŒì€ ì•ì‚¬ëŒì˜ ì›€ì§ì„ì— ìƒê´€ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ ë‹¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•™ìŠµ ì†ë„ ë¹„ì•½ì  í–¥ìƒ)
\end{itemize}
\end{analogybox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{2. The Algorithm: Norm, Scale, Shift}
ì€ë‹‰ì¸µì˜ ê°’ $Z$ì— ëŒ€í•´ ë¯¸ë‹ˆ ë°°ì¹˜ ë‹¨ìœ„ë¡œ 4ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item \textbf{Mean:} $\mu = \frac{1}{m} \sum z^{(i)}$
    \item \textbf{Variance:} $\sigma^2 = \frac{1}{m} \sum (z^{(i)} - \mu)^2$
    \item \textbf{Normalize:} $z_{norm} = \frac{z - \mu}{\sqrt{\sigma^2 + \epsilon}}$ \quad ($\epsilon$: 0 ë‚˜ëˆ„ê¸° ë°©ì§€)
    \item \textbf{Scale \& Shift (í•µì‹¬):} $\tilde{z} = \gamma z_{norm} + \beta$
\end{enumerate}

\begin{warningbox}{ì™œ ë‹¤ì‹œ $\gamma, \beta$ë¡œ ë§ê°€ëœ¨ë¦¬ë‚˜ìš”?}
ë¬´ì¡°ê±´ í‰ê·  0, ë¶„ì‚° 1ë¡œ ê³ ì •í•˜ë©´, ë°ì´í„°ê°€ Sigmoidì˜ ì„ í˜• êµ¬ê°„(ê°€ìš´ë°)ì—ë§Œ ëª°ë¦¬ê²Œ ë˜ì–´ \textbf{ë¹„ì„ í˜•ì„±(í‘œí˜„ë ¥)ì„ ìƒê²Œ ë©ë‹ˆë‹¤.}
$\gamma$ì™€ $\beta$ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë‘ì–´, "í•„ìš”í•˜ë‹¤ë©´ ì›ë˜ ë¶„í¬ë¡œ ë˜ëŒë¦´ ìˆ˜ ìˆëŠ” ììœ "ë¥¼ ëª¨ë¸ì—ê²Œ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
\end{warningbox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Deep Dive: Train vs Test Mode}

ë°°ì¹˜ ì •ê·œí™” êµ¬í˜„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Training Mode:} í˜„ì¬ ë“¤ì–´ì˜¨ \textbf{ë¯¸ë‹ˆ ë°°ì¹˜ì˜ í‰ê· /ë¶„ì‚°}ì„ ê³„ì‚°í•´ì„œ ì”ë‹ˆë‹¤. ë™ì‹œì— ì´ ê°’ë“¤ì„ `running_mean`, `running_var`ì— ëˆ„ì (ì—…ë°ì´íŠ¸)í•´ë‘¡ë‹ˆë‹¤.
    \item \textbf{Test Mode:} í…ŒìŠ¤íŠ¸ ë•ŒëŠ” ë°ì´í„°ê°€ 1ê°œë§Œ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤(ë¶„ì‚° ê³„ì‚° ë¶ˆê°€). ë”°ë¼ì„œ í•™ìŠµ ë•Œ ë¯¸ë¦¬ ì €ì¥í•´ë‘” \textbf{`running_mean`, `running_var`}ë¥¼ ê°€ì ¸ì™€ì„œ ì •ê·œí™”í•©ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Batch Norm Class}

\begin{lstlisting}[language=Python, caption=Batch Normalization Implementation, breaklines=true]
import numpy as np

class BatchNorm:
    def __init__(self, n_features, momentum=0.9):
        self.gamma = np.ones((n_features, 1)) # ì´ˆê¸°ê°’ 1 (ë³€í™” ì—†ìŒ)
        self.beta = np.zeros((n_features, 1)) # ì´ˆê¸°ê°’ 0 (ë³€í™” ì—†ìŒ)
        
        # í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ (Running Stats)
        self.running_mean = np.zeros((n_features, 1))
        self.running_var = np.ones((n_features, 1))
        self.momentum = momentum
        self.epsilon = 1e-8

    def forward(self, Z, mode='train'):
        if mode == 'train':
            # 1. ë¯¸ë‹ˆ ë°°ì¹˜ í†µê³„ëŸ‰ ê³„ì‚°
            mu = np.mean(Z, axis=1, keepdims=True)
            var = np.var(Z, axis=1, keepdims=True)
            
            # 2. ì •ê·œí™”
            Z_norm = (Z - mu) / np.sqrt(var + self.epsilon)
            
            # 3. Running Stats ì—…ë°ì´íŠ¸ (ì§€ìˆ˜ ê°€ì¤‘ í‰ê· )
            # ì—­ì „íŒŒì™€ ë¬´ê´€í•˜ê²Œ ë³„ë„ë¡œ ê¸°ë¡í•´ë‘ 
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
            
        elif mode == 'test':
            # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì €ì¥í•´ë‘” í†µê³„ëŸ‰ ì‚¬ìš©
            Z_norm = (Z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)
            
        # 4. Scale and Shift (ê³µí†µ)
        out = self.gamma * Z_norm + self.beta
        return out
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\textbf{Q. BNì„ ì“°ë©´ ì™œ $b$(Bias)ë¥¼ ì—†ì• ë„ ë˜ë‚˜ìš”?} \\
\textbf{A.} $Z = WX + b$ì—ì„œ í‰ê·  $\mu$ë¥¼ ë¹¼ëŠ” ê³¼ì •($Z - \mu$) ë•Œë¬¸ì— ìƒìˆ˜ $b$ëŠ” ì–´ì°¨í”¼ ìƒì‡„ë˜ì–´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ëŒ€ì‹  BNì˜ $\beta$ê°€ í¸í–¥ ì—­í• ì„ ëŒ€ì‹ í•©ë‹ˆë‹¤.

\textbf{Q. BNì€ ì–´ë””ì— ë„£ë‚˜ìš”? Activation ì „? í›„?} \\
\textbf{A.} ì›ë˜ ë…¼ë¬¸(Andrew Ng ìŠ¤íƒ€ì¼)ì€ \textbf{Activation ì „}($Z \to BN \to A$)ì„ ê¶Œì¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìµœê·¼ì—ëŠ” í›„($Z \to A \to BN$)ì— ë„£ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ ì˜ ë™ì‘í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹í•œìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” ëª¨ë“  ë„êµ¬(ì´ˆê¸°í™”, ì •ê·œí™”, ìµœì í™”, ë°°ì¹˜ ì •ê·œí™”)ë¥¼ ì†ì— ë„£ì—ˆìŠµë‹ˆë‹¤.

ì§€ê¸ˆê¹Œì§€ëŠ” 'ê³ ì–‘ì´ vs ê°œ'ì²˜ëŸ¼ ë‹µì´ ë‘ ê°œì¸ ì´ì§„ ë¶„ë¥˜ë§Œ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„¸ìƒì—ëŠ” ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ë¬¸ì œê°€ ë” ë§ìŠµë‹ˆë‹¤. (ìˆ«ì 0~9, ì˜· ì¢…ë¥˜ ë“±)
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë™ì‹œì— ë¶„ë¥˜í•˜ëŠ” \textbf{[Softmax Regression]}ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Batch Norm:} ê° ì¸µì˜ ì…ë ¥ì„ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì„ ì•ˆì •í™”í•˜ê³  ê°€ì†í•œë‹¤.
    \item \textbf{Gamma/Beta:} ì •ê·œí™”ë¡œ ìƒì–´ë²„ë¦° í‘œí˜„ë ¥ì„ ë³µêµ¬í•˜ê¸° ìœ„í•œ í•™ìŠµ íŒŒë¼ë¯¸í„°.
    \item \textbf{Train/Test:} í•™ìŠµ ì‹œì—” ë°°ì¹˜ í†µê³„ëŸ‰, í…ŒìŠ¤íŠ¸ ì‹œì—” ì´ë™ í‰ê· (Running Avg)ì„ ì“´ë‹¤.
    \item \textbf{Effect:} ì´ˆê¸°í™”ì— ëœ ë¯¼ê°í•´ì§€ê³ , ë†’ì€ í•™ìŠµë¥ ì„ ì“¸ ìˆ˜ ìˆë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}