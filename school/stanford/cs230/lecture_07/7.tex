\documentclass[a4paper, 11pt]{article}

% --- 패키지 설정 ---
\usepackage{kotex} % 한글 지원
\usepackage{geometry} % 여백 설정
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % 수식 패키지
\usepackage{graphicx}
\usepackage{adjustbox}  % 표/박스 크기 조절 % 이미지 삽입
\usepackage{hyperref} % 하이퍼링크
\usepackage{xcolor} % 색상 지원
\usepackage{listings} % 코드 블록
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % 박스 디자인
\usepackage{enumitem} % 리스트 스타일
\usepackage{booktabs} % 표 디자인
\usepackage{multirow} % 표 병합

% --- 색상 정의 ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- 코드 스타일 설정 ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- 박스 스타일 정의 ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=📌 #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=💡 #1 (직관적 비유)
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=⚠️ #1 (오해 방지 가이드)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=🧮 #1 (수학적 증명 \& 시나리오)
}

% --- 문서 정보 ---
\title{\textbf{[CS230] Shallow Neural Networks: \\ Activation Functions}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. 전체 목차 (TOC) ---
\section*{📚 Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network \textit{- Completed}
    \item[\textbf{Chapter 3.}] \textbf{Shallow Neural Networks (Current Unit)}
    \begin{itemize}
        \item 3.1 Concept of Hidden Layer \& Architecture \textit{- Completed}
        \item \textbf{3.2 Activation Functions (Sigmoid, Tanh, ReLU)}
        \begin{itemize}
            \item The Big 4 Functions
            \item Why Sigmoid Failed? (Vanishing Gradient)
            \item Professor's Choice (Best Practice)
            \item Implementation
        \end{itemize}
        \item 3.3 Backpropagation Intuition
    \end{itemize}
    \item[Chapter 4.] Deep Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. 이전 단원 연결 ---
\section*{🔗 지난 시간 복습 및 연결}
지난 시간에 우리는 은닉층(Hidden Layer)을 추가하여 신경망의 깊이를 더했습니다. 그때 제가 "은닉층에는 Sigmoid보다 Tanh나 ReLU가 좋다"고 스쳐 지나가듯 말했습니다.
"왜요? Sigmoid가 가장 유명하지 않나요?"
이 질문에 답하지 못하면 여러분은 매번 모델을 설계할 때마다 '선택 장애'에 시달릴 것입니다. 활성화 함수는 단순한 스위치가 아닙니다. 학습 신호(Gradient)를 살릴 수도, 죽일 수도 있는 \textbf{생명 유지 장치}입니다.

% --- 4. 개요 ---
\section{Unit Overview}
\begin{summarybox}{핵심 목표}
이 단원은 딥러닝 모델의 성능을 결정짓는 \textbf{'4대 활성화 함수'}를 완벽하게 해부합니다.
\begin{itemize}
    \item \textbf{비교:} Sigmoid, Tanh, ReLU, Leaky ReLU의 수식과 그래프 특징을 비교합니다.
    \item \textbf{원리:} 깊은 신경망에서 Sigmoid를 쓰면 학습이 멈추는 \textbf{기울기 소실(Vanishing Gradient)} 문제를 수학적으로 증명합니다.
    \item \textbf{전략:} 출력층과 은닉층에 각각 어떤 함수를 써야 하는지 \textbf{Best Practice}를 확립합니다.
    \item \textbf{구현:} NumPy를 사용하여 각 함수와 그 도함수(Derivative)를 효율적으로 코딩합니다.
  \end{itemize}
\end{summarybox}

% --- 5. 용어 정리 ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{함수명} & \textbf{범위} & \textbf{한 줄 특징} \\ \hline
\textbf{Sigmoid} & $(0, 1)$ & 확률 표현에 최적. 하지만 깊어지면 학습 불가. \\ \hline
\textbf{Tanh} & $(-1, 1)$ & Sigmoid의 확장판. 0 중심(Zero-centered)이라 학습이 더 빠름. \\ \hline
\textbf{ReLU} & $[0, \infty)$ & \textbf{딥러닝의 표준.} 양수는 그대로, 음수는 차단. 연산 빠름. \\ \hline
\textbf{Leaky ReLU} & $(-\infty, \infty)$ & ReLU의 변형. 음수일 때도 아주 약간의 기울기를 줌. \\ \hline
\end{tabular}
\end{center}

% --- 6. 핵심 개념 상세 설명 ---
\section{Core Concepts: The Big 4 Functions}

\subsection{1. Sigmoid Function ($\sigma$)}


[Image of sigmoid function graph with equation]

\begin{itemize}
    \item \textbf{수식:} $a = \frac{1}{1 + e^{-z}}$
    \item \textbf{특징:} 0과 1 사이의 값으로 압축합니다. '확률' 개념과 잘 맞습니다.
    \item \textbf{치명적 단점:} 입력값($z$)이 아주 크거나 작으면 기울기(미분값)가 0에 가까워집니다. 학습이 멈춥니다.
    \item \textbf{용도:} \textbf{이진 분류의 출력층(Output Layer)}에만 씁니다. 은닉층엔 절대 쓰지 마세요.
\end{itemize}

\subsection{2. Tanh (Hyperbolic Tangent)}

\begin{itemize}
    \item \textbf{수식:} $a = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
    \item \textbf{특징:} Sigmoid를 위아래로 늘려 -1에서 1 사이 값을 갖게 했습니다. \textbf{평균이 0(Zero-centered)}이므로 데이터의 중심을 잘 잡아주어 Sigmoid보다 학습 수렴이 빠릅니다.
    \item \textbf{용도:} 은닉층에서 Sigmoid보다 무조건 좋습니다. 하지만 여전히 기울기 소실 문제는 있습니다.
\end{itemize}

\subsection{3. ReLU (Rectified Linear Unit) - The King}

\begin{itemize}
    \item \textbf{수식:} $a = \max(0, z)$
    \item \textbf{특징:} 단순 무식해 보이지만 가장 강력합니다.
    \begin{itemize}
        \item $z > 0$: 기울기가 항상 \textbf{1}입니다. (신호가 약해지지 않음)
        \item $z \le 0$: 값을 0으로 차단합니다. (불필요한 신호 제거)
    \end{itemize}
    \item \textbf{용도:} \textbf{모든 은닉층의 기본값(Default)}입니다. 고민될 땐 무조건 ReLU를 쓰세요.
\end{itemize}

\begin{analogybox}{전등 스위치 비유}
\begin{itemize}
    \item \textbf{Sigmoid:} 조광기(Dimmer). 밝기를 0\%에서 100\%까지 미세하게 조절하지만, 너무 복잡합니다.
    \item \textbf{ReLU:} 똑딱 스위치. 켜지면 확실하게 켜지고(그대로 통과), 꺼지면 확실하게 꺼집니다(0). 단순함이 속도의 비결입니다.
\end{itemize}
\end{analogybox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Deep Dive: 왜 Sigmoid는 퇴출당했나?}

이 부분은 딥러닝 역사에서 가장 중요한 전환점 중 하나인 \textbf{기울기 소실 문제(Vanishing Gradient Problem)}를 다룹니다.

\subsection{The Vanishing Gradient Problem}
역전파(Backpropagation)는 출력층의 오차를 입력층까지 전달하기 위해 \textbf{미분값(기울기)을 계속 곱하는(Chain Rule)} 과정입니다.

\begin{examplebox}{수학적 증명: $0.25$ vs $1.0$}
Sigmoid 함수의 미분 최댓값은 $z=0$일 때 \textbf{0.25}입니다.
만약 은닉층이 10개라고 가정해봅시다.

\textbf{Case 1: Sigmoid 사용}
$$ Gradient \approx 0.25 \times 0.25 \times \dots \times 0.25 = (0.25)^{10} \approx 0.0000009 $$
$\rightarrow$ 입력층에 도달할 때쯤 기울기는 0이 되어 사라집니다. 앞단은 학습이 전혀 안 됩니다.

\textbf{Case 2: ReLU 사용} (양수 구간)
$$ Gradient = 1 \times 1 \times \dots \times 1 = 1^{10} = 1 $$
$\rightarrow$ 기울기가 줄어들지 않고 생생하게 입력층까지 전달됩니다. 이것이 100층짜리 딥러닝이 가능한 이유입니다.
\end{examplebox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Professor's Cheat Sheet (Best Practice)}
실전에서 무엇을 쓸지 고민하지 마십시오. 이 규칙을 따르면 상위 10\%입니다.

\begin{tcolorbox}[colback=white, colframe=black, title=활성화 함수 선택 가이드]
\begin{itemize}
    \item \textbf{출력층 (Output Layer):}
    \begin{itemize}
        \item 이진 분류 (0 or 1): \textbf{Sigmoid}
        \item 다중 분류 (Cat, Dog, Bird...): \textbf{Softmax}
        \item 회귀 (집값 예측): \textbf{Linear} (활성화 함수 없음)
    \end{itemize}
    
    \item \textbf{은닉층 (Hidden Layer):}
    \begin{itemize}
        \item \textbf{기본 (Default):} \textbf{ReLU}
        \item ReLU 성능이 아쉽거나 뉴런이 죽는 경우: \textbf{Leaky ReLU}
        \item 데이터가 매우 적고 모델이 얕을 때: \textbf{Tanh}
        \item \textbf{금지:} \textbf{Sigmoid} (절대 사용 금지)
    \end{itemize}
\end{itemize}
\end{tcolorbox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Implementation (Python with NumPy)}

함수값뿐만 아니라 역전파에 필요한 \textbf{도함수(Derivative)}까지 구현합니다.

\begin{lstlisting}[language=Python, caption=Activation Functions \& Derivatives, breaklines=true]
import numpy as np

class Activations:
    @staticmethod
    def sigmoid(z):
        """출력층용: 0 ~ 1"""
        return 1 / (1 + np.exp(-z))
    
    @staticmethod
    def sigmoid_derivative(z):
        """Sigmoid 미분: a * (1-a)"""
        s = 1 / (1 + np.exp(-z))
        return s * (1 - s)

    @staticmethod
    def relu(z):
        """은닉층용: max(0, z)"""
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z):
        """
        ReLU 미분:
        z > 0 이면 1, z <= 0 이면 0
        """
        dZ = np.array(z, copy=True) # 원본 보존
        dZ[z <= 0] = 0
        dZ[z > 0] = 1
        return dZ

    @staticmethod
    def tanh(z):
        """은닉층용: -1 ~ 1"""
        return np.tanh(z) # NumPy 최적화 함수 사용

    @staticmethod
    def tanh_derivative(z):
        """Tanh 미분: 1 - a^2"""
        return 1 - np.power(np.tanh(z), 2)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\begin{warningbox}{ReLU의 미분 불가능 점 ($z=0$)}
\textbf{Q. 수학적으로 $z=0$에서 ReLU는 미분이 불가능한데(뾰족점), 코딩은 어떻게 하나요?} \\
\textbf{A.} 맞습니다. 하지만 컴퓨터 공학에서는 실용적으로 접근합니다. $z=0$일 때 기울기를 그냥 \textbf{0}이나 \textbf{1} 중 하나로 정해버립니다. (보통 0으로 둠). $z$가 정확히 0.0000...이 될 확률은 매우 낮으므로 학습에 아무런 지장이 없습니다.
\end{warningbox}

\textbf{Q. Leaky ReLU는 언제 쓰나요?} \\
\textbf{A.} ReLU를 썼는데 학습 중에 뉴런의 출력이 계속 0만 나와서 죽어버리는 현상(\textbf{Dying ReLU})이 발생할 때 씁니다. 음수일 때 0.01 같은 작은 기울기를 주어 뉴런을 소생시킵니다.

% --- 9. 다음 단원 연결 ---
\section*{🔗 다음 단계 (Next Step)}
이제 우리는 뉴런의 구조(Layer)와 신호를 조절하는 스위치(Activation)까지 모두 갖췄습니다. 자동차로 치면 엔진과 변속기를 조립한 상태입니다.

다음 시간에는 이 자동차를 실제로 달리게 만드는 엔진 점화 과정, 즉 오차를 줄이기 위해 미분을 사용하는 \textbf{'역전파(Backpropagation)'}의 수식적 유도 과정을 아주 깊이 있게 파헤쳐 보겠습니다. 긴장하십시오. 이제 진짜 미분의 숲으로 들어갑니다.

\vspace{0.5cm}

\begin{summarybox}{단원 요약 (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Sigmoid:} 출력층(이진 분류)에만 사용. 은닉층 사용 시 기울기 소실 발생.
    \item \textbf{ReLU:} 은닉층의 \textbf{Default}. 양수는 그대로(기울기 1), 음수는 0. 연산 빠름.
    \item \textbf{Tanh:} Sigmoid보다 좋음(Zero-centered). 얕은 모델에 적합.
    \item \textbf{Vanishing Gradient:} Sigmoid 미분값이 1보다 작아($\le 0.25$), 층이 깊어지면 학습 신호가 사라지는 현상.
\end{enumerate}
\end{summarybox}

\end{document}