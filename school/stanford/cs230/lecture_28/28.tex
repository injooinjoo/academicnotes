\documentclass[a4paper, 11pt]{article}

% --- 패키지 설정 ---
\usepackage{kotex} % 한글 지원
\usepackage{geometry} % 여백 설정
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % 수식 패키지
\usepackage{graphicx}
\usepackage{adjustbox}  % 표/박스 크기 조절 % 이미지 삽입
\usepackage{hyperref} % 하이퍼링크
\usepackage{xcolor} % 색상 지원
\usepackage{listings} % 코드 블록
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % 박스 디자인
\usepackage{enumitem} % 리스트 스타일
\usepackage{booktabs} % 표 디자인
\usepackage{array} % 표 정렬

% --- 색상 정의 ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- 코드 스타일 설정 ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- 박스 스타일 정의 ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=📌 #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=💡 #1 (직관적 비유)
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=⚠️ #1 (오해 방지 가이드)
}

\newtcolorbox{strategybox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=🧭 #1 (전략 가이드)
}

% --- 문서 정보 ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ End-to-End Deep Learning}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. 전체 목차 (TOC) ---
\section*{📚 Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-7.] ML Strategy \& Advanced Learning \textit{- Completed}
    \item[\textbf{Chapter 8.}] \textbf{End-to-End Deep Learning (Current Unit)}
    \begin{itemize}
        \item \textbf{8.1 Concept: Pipeline vs E2E}
        \begin{itemize}
            \item Definition: Direct Highway
            \item Pros: Let the Data Speak
            \item Cons: Data Hungry \& Black Box
            \item Decision Checklist
        \end{itemize}
        \item 8.2 Application Examples (Speech, Vision)
    \end{itemize}
    \item[Chapter 9.] Convolutional Neural Networks (CNN) \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. 이전 단원 연결 ---
\section*{🔗 지난 시간 복습 및 연결}
지난 시간까지 우리는 전이 학습과 다중 작업 학습을 통해 기존 지식을 활용하는 법을 배웠습니다.
이제 딥러닝이 가져온 가장 거대한 패러다임의 변화, \textbf{엔드투엔드(End-to-End) 딥러닝}에 대해 이야기할 시간입니다.
과거에는 음성 인식을 위해 음향학, 음성학 등 수많은 파이프라인을 조립했습니다. 하지만 딥러닝은 이 모든 단계를 건너뛰고, \textbf{"입력에서 출력으로 직행하는 고속도로"}를 뚫어버렸습니다. 이것이 언제나 정답일까요?

% --- 4. 개요 ---
\section{Unit Overview}
\begin{summarybox}{핵심 목표}
이 단원은 중간 단계를 생략하고 데이터만으로 학습하는 E2E 딥러닝의 장단점을 분석합니다.
\begin{itemize}
    \item \textbf{정의:} 전통적인 \textbf{파이프라인(Pipeline)} 방식과 \textbf{E2E} 방식의 구조적 차이를 구분합니다.
    \item \textbf{장점:} 인간의 편향(Hand-designed components)을 제거하여 최적의 성능을 내는 원리를 이해합니다.
    \item \textbf{단점:} 왜 막대한 양의 데이터가 필요한지(Data Hungry), 왜 디버깅이 어려운지 파악합니다.
    \item \textbf{결정:} 언제 E2E를 쓰고, 언제 파이프라인을 써야 하는지 결정 기준을 세웁니다.
\end{itemize}
\end{summarybox}

% --- 5. 용어 정리 ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{용어} & \textbf{의미} & \textbf{비유} \\ \hline
\textbf{End-to-End (E2E)} & 입력 $\to$ 출력으로 직행하는 단일 모델 & 직항 비행기 (중간 경유지 없음) \\ \hline
\textbf{Pipeline} & 여러 모듈을 순차적으로 연결한 시스템 & 경유 비행기 (A $\to$ B $\to$ C $\to$ D) \\ \hline
\textbf{Hand-engineered} & 사람이 직접 설계한 특징/규칙 & 수제작 부품 (장인의 손길) \\ \hline
\end{tabular}
\end{center}

% --- 6. 핵심 개념 상세 설명 ---
\section{Core Concepts: 직행 고속도로}

\subsection{1. Pipeline vs End-to-End}


\textbf{예시: 음성 인식 (Speech Recognition)}
\begin{itemize}
    \item \textbf{Traditional Pipeline:}
    $$ \text{Audio} \to \text{MFCC(특징 추출)} \to \text{Phoneme(음소)} \to \text{Word} \to \text{Text} $$
    각 단계마다 전문가의 지식(음성학 등)이 필요합니다.
    
    \item \textbf{End-to-End Deep Learning:}
    $$ \text{Audio} \to [\text{Deep Neural Network}] \to \text{Text} $$
    중간 단계(음소 등)를 명시적으로 가르치지 않습니다. 데이터만 충분하면 신경망이 알아서 최적의 내부 표현을 찾아냅니다.
\end{itemize}

\subsection{2. The Key Idea: Let the Data Speak}
전통적 방식에는 "음소를 먼저 찾아야 해"라는 인간의 가정(Bias)이 들어갑니다. 하지만 데이터가 충분하다면, 신경망은 음소보다 더 효율적인 자신만의 방식을 찾아낼 수 있습니다. E2E는 \textbf{데이터가 스스로 최적의 처리 과정을 설계하도록 허용}하는 것입니다.

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Deep Dive: Pros \& Cons}

무조건 E2E가 좋은 것은 아닙니다. 명확한 트레이드오프가 존재합니다.

\begin{strategybox}{End-to-End 장단점 분석}
\textbf{장점 (Pros):}
\begin{itemize}
    \item \textbf{Data-driven:} 인간의 선입견에 갇히지 않고 데이터 패턴 자체를 학습하므로, 데이터가 많을수록 성능 상한선이 높습니다.
    \item \textbf{Simplicity:} 복잡한 파이프라인을 설계하고 유지보수할 필요가 없습니다. 신경망 하나만 관리하면 됩니다.
\end{itemize}

\textbf{단점 (Cons):}
\begin{itemize}
    \item \textbf{Data Hungry:} $(x, y)$ 쌍 데이터가 엄청나게 많이 필요합니다. (파이프라인은 각 모듈별로 적은 데이터로 학습 가능).
    \item \textbf{Black Box:} 왜 틀렸는지 설명하기 어렵습니다. (파이프라인은 "음소 인식에서 틀렸군" 하고 알 수 있음).
\end{itemize}
\end{strategybox}

% --- 7. 구현 예시 ---
\section{Example: Date Formatting}

날짜 형식을 변환하는 간단한 예제로 E2E의 철학을 봅니다.
Input: "20th Jan. 2023" $\to$ Output: "2023-01-20"

\begin{lstlisting}[language=Python, caption=E2E Logic Overview, breaklines=true]
# Traditional Approach (Rule-based)
def parse_date(date_str):
    # 수많은 규칙(Regex) 작성 필요
    # "Jan" -> 01, "February" -> 02 ...
    # 오타 처리, 예외 처리 등 복잡함
    pass

# End-to-End Approach
# 규칙을 짜는 게 아니라, 데이터(x, y)를 들이붓는다.
x_data = ["25th Dec 2022", "December 25, 2022", "12/25/22"]
y_data = ["2022-12-25",    "2022-12-25",        "2022-12-25"]

# 데이터가 10만 개쯤 있다면, 
# 모델(Seq2Seq 등)은 "Dec"가 "12"라는 것을 스스로 깨우칩니다.
# 코드는 모델 아키텍처 정의뿐, 비즈니스 로직은 없습니다.
model.fit(x_data, y_data)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\textbf{Q. 자율주행도 E2E로 하나요?} \\
\textbf{A.} 초기에는 시도했지만(NVIDIA 등), 현재는 \textbf{안전} 때문에 파이프라인을 선호합니다. "이미지 $\to$ 핸들 조향"으로 바로 가면, 사고가 났을 때 왜 핸들을 꺾었는지 알 수 없어 디버깅과 책임 소재 파악이 불가능하기 때문입니다. (최근엔 다시 E2E 비중이 늘어나는 추세이긴 합니다.)

\textbf{Q. 데이터가 적을 때 E2E를 쓰면 안 되나요?} \\
\textbf{A.} 네, 성능이 처참할 수 있습니다. 데이터가 적을 때는 인간의 지식(Hand-engineered features)을 주입해주는 파이프라인 방식이 훨씬 효율적입니다.

% --- 9. 다음 단원 연결 ---
\section*{🔗 다음 단계 (Next Step)}
이것으로 앤드류 응 교수의 \textbf{'머신러닝 프로젝트 구조화 전략(Structuring ML Projects)'} 파트(Part 3)를 모두 마칩니다.
이제 여러분은 딥러닝의 기초 이론부터 성능 향상 기법, 그리고 프로젝트를 지휘하는 전략까지 모두 갖췄습니다.

다음 시간부터는 딥러닝의 꽃이자 가장 널리 쓰이는 분야인 \textbf{[Part 4. Convolutional Neural Networks (CNN)]}의 세계로 들어갑니다. 이미지를 처리하는 컴퓨터의 시각을 정복해보겠습니다. 기대하십시오.

\vspace{0.5cm}

\begin{summarybox}{단원 요약 (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{E2E:} 중간 단계 없이 입력에서 출력으로 바로 매핑하는 딥러닝 방식.
    \item \textbf{Pros:} 데이터가 충분하면 인간보다 더 효율적인 특징을 찾아낸다.
    \item \textbf{Cons:} 막대한 양의 라벨링 데이터가 필요하다. 설명력이 부족하다.
    \item \textbf{Decision:} 데이터 양이 적거나 안전/설명이 중요한 분야는 파이프라인을 쓴다.
\end{enumerate}
\end{summarybox}

\end{document}