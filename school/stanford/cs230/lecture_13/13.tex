\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Regularization (L2 / Weight Decay)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1 Train / Dev / Test Sets Strategy \textit{- Completed}
        \item 5.2 Bias vs Variance Analysis \textit{- Completed}
        \item \textbf{5.3 Regularization (L1/L2)}
        \begin{itemize}
            \item Why Regularize? (Penalizing Complexity)
            \item L2 Regularization (Ridge) Formula
            \item Math: Why is it called "Weight Decay"?
            \item Implementation
        \end{itemize}
        \item 5.4 Dropout Regularization \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ëª¨ë¸ì´ \textbf{High Variance(ê³¼ëŒ€ì í•©)}ë¼ëŠ” ë³‘ì— ê±¸ë ¸ìŒì„ ì§„ë‹¨í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ë„ˆë¬´ ì§‘ì°©í•´ì„œ(ì•”ê¸°í•´ì„œ), ì‹¤ì „ ë¬¸ì œ(Dev Set)ë¥¼ ëª» í‘¸ëŠ” ìƒí™©ì…ë‹ˆë‹¤.
ì´ì œ ì²˜ë°©ì „ì„ ì“¸ ì°¨ë¡€ì…ë‹ˆë‹¤. ê³¼ëŒ€ì í•©ì„ ì¹˜ë£Œí•˜ëŠ” ê°€ì¥ ì „í†µì ì´ê³  ê°•ë ¥í•œ í•­ìƒì œëŠ” ë°”ë¡œ \textbf{'ì •ê·œí™”(Regularization)'}ì…ë‹ˆë‹¤. ì •ê·œí™”ëŠ” ëª¨ë¸ì—ê²Œ \textbf{"ì •ë‹µì„ ë§ì¶”ë˜, ë„ˆë¬´ ê¼¼ìˆ˜(í° ê°€ì¤‘ì¹˜)ëŠ” ì“°ì§€ ë§ˆë¼"}ë¼ê³  ì œì•½(Penalty)ì„ ê±°ëŠ” ê²ƒì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì–µì œí•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” \textbf{L2 ì •ê·œí™”}ë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} ë¹„ìš© í•¨ìˆ˜ $J$ì— ê°€ì¤‘ì¹˜ í¬ê¸°($||W||^2$)ì— ë¹„ë¡€í•˜ëŠ” ë²Œì ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜í•™:} ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ê°€ ìŠ¤ìŠ¤ë¡œ ì¤„ì–´ë“œëŠ” \textbf{Weight Decay(ê°€ì¤‘ì¹˜ ê°ì‡ )} í˜„ìƒì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ì •ê·œí™” í•­ì´ í¬í•¨ëœ Forward ë° Backward ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    \item \textbf{ë¹„êµ:} L1 ì •ê·œí™”(Lasso)ì™€ì˜ ì°¨ì´ì (í¬ì†Œì„±)ì„ ì´í•´í•©ë‹ˆë‹¤.
  \end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{í•µì‹¬ ì˜ë¯¸} \\ \hline
\textbf{L2 Regularization} & Ridge & ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©ì„ ë²Œì ìœ¼ë¡œ ì‚¬ìš©. $W \to 0$ (ì‘ì•„ì§). \\ \hline
\textbf{L1 Regularization} & Lasso & ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©ì„ ë²Œì ìœ¼ë¡œ ì‚¬ìš©. $W = 0$ (ì‚¬ë¼ì§/í¬ì†Œì„±). \\ \hline
\textbf{Lambda} & $\lambda$ & ì •ê·œí™” ê°•ë„. í´ìˆ˜ë¡ ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§(Underfitting ìœ„í—˜). \\ \hline
\textbf{Weight Decay} & - & ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ê°€ì¤‘ì¹˜ê°€ ì¼ì • ë¹„ìœ¨ì”© ê°ì†Œí•˜ëŠ” í˜„ìƒ. \\ \hline
\textbf{Frobenius Norm} & $||W||_F^2$ & í–‰ë ¬ì˜ ëª¨ë“  ì›ì†Œë¥¼ ì œê³±í•´ì„œ ë”í•œ ê°’. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë²Œì  ì‹œìŠ¤í…œ}

\subsection{1. The Idea of Penalty}
ìš°ë¦¬ì˜ ëª©í‘œëŠ” ë¹„ìš© $J$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì— \textbf{"ê°€ì¤‘ì¹˜ $W$ê°€ ì»¤ì§€ë©´ ë²Œì ì„ ì£¼ê² ë‹¤"}ëŠ” ìƒˆë¡œìš´ ê·œì¹™ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

$$ J_{regularized}(W, b) = \underbrace{J_{original}(W, b)}_{\text{ì˜¤ì°¨ (Cross Entropy)}} + \underbrace{\frac{\lambda}{2m} \sum_{l} ||W^{[l]}||_F^2}_{\text{ë²Œì  (L2 Penalty)}} $$

\begin{itemize}
    \item $\lambda$ (Lambda): ë²Œì ì˜ ê°•ë„ì…ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.
    \item $m$: ë°ì´í„° ê°œìˆ˜.
    \item $2m$: ë¯¸ë¶„í•  ë•Œ ì œê³±($^2$)ì´ ë‚´ë ¤ì™€ì„œ 2ì™€ ì•½ë¶„ë˜ë¼ê³  ë¯¸ë¦¬ 2ë¡œ ë‚˜ëˆ ë‘¡ë‹ˆë‹¤. (ìˆ˜í•™ì  í¸ì˜)
\end{itemize}

\subsection{2. L1 vs L2 (Which one to use?)}
\begin{itemize}
    \item \textbf{L2 (Standard):} ê°€ì¤‘ì¹˜ë¥¼ 0ì— ê°€ê¹ê²Œ ë§Œë“­ë‹ˆë‹¤. ëª¨ë“  íŠ¹ì„±ì„ ê³¨ê³ ë£¨ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤. \textbf{ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ê°’(Default)}ì…ë‹ˆë‹¤.
    \item \textbf{L1 (Sparse):} ê°€ì¤‘ì¹˜ë¥¼ ì™„ì „íˆ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ì œê±°(Feature Selection)í•˜ê³  ì‹¶ì„ ë•Œ ì“°ì§€ë§Œ, ë¯¸ë¶„ì´ ê¹Œë‹¤ë¡œì›Œ ì˜ ì•ˆ ì”ë‹ˆë‹¤.
\end{itemize}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Deep Dive: Why "Weight Decay"?}

ì´ ì„¹ì…˜ì€ L2 ì •ê·œí™”ë¥¼ ì™œ \textbf{'ê°€ì¤‘ì¹˜ ê°ì‡ '}ë¼ê³  ë¶€ë¥´ëŠ”ì§€ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.

\begin{mathbox}{ì—­ì „íŒŒ ìˆ˜ì‹ ìœ ë„}
ë¹„ìš© í•¨ìˆ˜ $J_{reg}$ë¥¼ $W$ì— ëŒ€í•´ ë¯¸ë¶„í•´ ë´…ì‹œë‹¤.

$$ \frac{\partial J_{reg}}{\partial W} = \frac{\partial J_{orig}}{\partial W} + \frac{\partial}{\partial W} \left( \frac{\lambda}{2m} W^2 \right) $$
$$ dW_{reg} = dW_{orig} + \frac{\lambda}{m} W $$
(ë¶„ëª¨ì˜ 2ê°€ ë¯¸ë¶„ë˜ë©´ì„œ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤!)

ì´ì œ ê²½ì‚¬ í•˜ê°•ë²• ì—…ë°ì´íŠ¸ ì‹ì— ëŒ€ì…í•©ë‹ˆë‹¤.
$$ W_{new} = W - \alpha \cdot dW_{reg} $$
$$ W_{new} = W - \alpha \left( dW_{orig} + \frac{\lambda}{m} W \right) $$

ì´ ì‹ì„ $W$ë¡œ ë¬¶ìœ¼ë©´ ë†€ë¼ìš´ ê²°ê³¼ê°€ ë‚˜ì˜µë‹ˆë‹¤:
$$ W_{new} = \underbrace{\left( 1 - \frac{\alpha \lambda}{m} \right)}_{\text{Decay Factor } (< 1)} W - \alpha \cdot dW_{orig} $$
\end{mathbox}

\textbf{ê²°ë¡ :} ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ê°€ì¤‘ì¹˜ $W$ëŠ” ì›ë˜ í•™ìŠµ ë°©í–¥($-\alpha dW$)ìœ¼ë¡œ ê°€ê¸° ì „ì—, ìê¸° ìì‹ ì˜ í¬ê¸°ë¥¼ $(1 - \frac{\alpha \lambda}{m})$ ë¹„ìœ¨ë§Œí¼ ì¤„ì…ë‹ˆë‹¤. ì¦‰, ê°€ë§Œíˆ ìˆì–´ë„ \textbf{ìŠ¤ìŠ¤ë¡œ ê°ì†Œ(Decay)}í•©ë‹ˆë‹¤.

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: L2 Regularization}

ì •ê·œí™”ëŠ” Forward(ë¹„ìš© ê³„ì‚°)ì™€ Backward(ê¸°ìš¸ê¸° ê³„ì‚°) ì–‘ìª½ì— ëª¨ë‘ ì½”ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=L2 Regularization Implementation, breaklines=true]
import numpy as np

class L2Regularizer:
    def __init__(self, lambd):
        self.lambd = lambd

    def compute_cost(self, cost_cross_entropy, parameters, m):
        """
        J_total = J_cross_entropy + (lambda / 2m) * sum(W^2)
        """
        L = len(parameters) // 2
        L2_cost = 0
        
        for l in range(1, L + 1):
            W = parameters['W' + str(l)]
            # Frobenius Norm ì œê³± ê³„ì‚°
            L2_cost += np.sum(np.square(W))
            
        L2_cost *= (self.lambd / (2 * m)) # 2mìœ¼ë¡œ ë‚˜ëˆ” ì£¼ì˜!
        
        return cost_cross_entropy + L2_cost

    def backward(self, dW_orig, W, m):
        """
        dW_reg = dW_orig + (lambda / m) * W
        """
        # ì •ê·œí™” í•­ì˜ ê¸°ìš¸ê¸° ì¶”ê°€ (ì—¬ê¸°ì„œëŠ” mìœ¼ë¡œ ë‚˜ëˆ”!)
        dW_reg = dW_orig + ((self.lambd / m) * W)
        
        return dW_reg

# --- ì‹¤í–‰ ì˜ˆì œ ---
if __name__ == "__main__":
    m = 1000
    lambd = 0.7
    reg = L2Regularizer(lambd)
    
    # ê°€ìƒì˜ W (Weight)
    W = np.array([[0.5, -0.2], [0.1, 0.8]])
    dW_orig = np.array([[0.01, 0.02], [-0.01, 0.05]]) # ì›ë˜ ê¸°ìš¸ê¸°
    
    # ì—­ì „íŒŒ ì ìš©
    dW_final = reg.backward(dW_orig, W, m)
    
    print("Original dW:\n", dW_orig)
    print("Regularized dW:\n", dW_final)
    # dW ê°’ì´ W ë¶€í˜¸ ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆ ë” ì»¤ì§ -> Wë¥¼ 0ìª½ìœ¼ë¡œ ë” ì„¸ê²Œ ë°ˆ
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\begin{warningbox}{í¸í–¥(Bias) $b$ëŠ” ì •ê·œí™” ì•ˆ í•˜ë‚˜ìš”?}
\textbf{ë³´í†µ ì•ˆ í•©ë‹ˆë‹¤.}
$b$ëŠ” í•¨ìˆ˜ì˜ ëª¨ì–‘(ê³¡ë¥ )ì´ ì•„ë‹ˆë¼ ìœ„ì¹˜ë§Œ ì´ë™ì‹œí‚µë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì˜ ë³µì¡ë„ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. $W$ë§Œ ì •ê·œí™”í•´ë„ ì¶©ë¶„í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Lambda($\lambda$) ê°’ì€ ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ê°’ì„ ì‹œë„í•´ë³´ê³  Dev Setì˜ ì˜¤ì°¨ê°€ ê°€ì¥ ë‚®ì€ ê°’ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ë³´í†µ 0.01, 0.001 ì²˜ëŸ¼ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” L2 ì •ê·œí™”ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ì•„ ê³¼ëŒ€ì í•©ì„ ì–µì œí–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ë•Œë¡œëŠ” ë” ê³¼ê²©í•œ ë°©ë²•ì´ í•„ìš”í•  ë•Œê°€ ìˆìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì´ëŠ” ê²Œ ì•„ë‹ˆë¼, ì•„ì˜ˆ \textbf{ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ êº¼ë²„ë¦¬ëŠ”(Shutdown)} ë°©ë²•ì…ë‹ˆë‹¤. "ì–´ë–»ê²Œ ë‡Œì„¸í¬ë¥¼ ì£½ì´ëŠ”ë° í•™ìŠµì´ ë” ì˜ ë˜ë‚˜ìš”?"
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë…íŠ¹í•˜ê³  ê°•ë ¥í•œ ì •ê·œí™” ê¸°ë²•ì¸ \textbf{[Regularization] Dropout (ë“œë¡­ì•„ì›ƒ)}ì„ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{L2 Regularization:} ê°€ì¤‘ì¹˜ ì œê³±í•©($W^2$)ì„ ë¹„ìš© í•¨ìˆ˜ì— ì¶”ê°€í•˜ì—¬ í° ê°€ì¤‘ì¹˜ì— ë²Œì ì„ ì¤€ë‹¤.
    \item \textbf{Weight Decay:} ì—­ì „íŒŒ ì‹œ $W$ê°€ ë§¤ë²ˆ ì¡°ê¸ˆì”© 0ì„ í–¥í•´ ì¤„ì–´ë“ ë‹¤.
    \item \textbf{Effect:} $W$ê°€ ì‘ì•„ì§€ë©´ ëª¨ë¸ì´ ì„ í˜•(Linear)ì— ê°€ê¹Œì›Œì ¸ ë³µì¡ë„ê°€ ì¤„ì–´ë“ ë‹¤. (ê³¼ëŒ€ì í•© í•´ê²°)
    \item \textbf{Tip:} Cost ê³„ì‚° ì‹œì—” $2m$, Gradient ê³„ì‚° ì‹œì—” $m$ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}