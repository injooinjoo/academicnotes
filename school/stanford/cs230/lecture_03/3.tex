\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{orange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (í•µì‹¬ ì£¼ì˜ì‚¬í•­)
}

\newtcolorbox{examplebox}[1]{
    colback=orange!5!white,
    colframe=orange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Foundations of Neural Networks: \\ Cost Function \& Gradient Descent}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network
    \begin{itemize}
        \item 2.1 Architecture \& Forward Propagation \textit{- Completed}
        \item \textbf{2.2 Cost Function \& Gradient Descent (Current Unit)}
        \begin{itemize}
            \item Overview: Loss vs. Cost
            \item The Engine: Gradient Descent
            \item Why Log Loss? (Convexity)
            \item Implementation
        \end{itemize}
    \end{itemize}
    \item[Chapter 3.] Shallow Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ë‚œ ì‹œê°„ì— ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ 'ë‡Œ êµ¬ì¡°(Architecture)'ë¥¼ ë§Œë“¤ê³ , ì…ë ¥ ì‹ í˜¸ë¥¼ í˜ë ¤ë³´ë‚´ëŠ” 'ìˆœì „íŒŒ(Forward Propagation)'ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆ ì´ ì‹ ê²½ë§ì€ ê°“ íƒœì–´ë‚œ ì•„ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ì„¸ìƒì— ëŒ€í•´ ì•„ë¬´ê²ƒë„ ëª¨ë¥´ì£ (íŒŒë¼ë¯¸í„°ê°€ ì´ˆê¸°í™”ëœ ìƒíƒœ). ì´ì œ ì´ ì•„ì´ë¥¼ ê°€ë¥´ì¹  ì‹œê°„ì…ë‹ˆë‹¤. í•™ìŠµì´ë€ \textbf{"ë‚´ê°€ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ í™•ì¸í•˜ê³ (Cost), ê³ ì³ ë‚˜ê°€ëŠ”(Gradient Descent) ê³¼ì •"}ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ìœ ë‹›ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ \textbf{'ì—”ì§„(Engine)'}ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì°¨ì²´(ëª¨ë¸ êµ¬ì¡°)ê°€ ì¢‹ì•„ë„ ì—”ì§„(í•™ìŠµ ì•Œê³ ë¦¬ì¦˜)ì´ ì—†ìœ¼ë©´ ì›€ì§ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ë¶„:} ë°ì´í„° í•˜ë‚˜ì— ëŒ€í•œ ì˜¤ì°¨(Loss)ì™€ ì „ì²´ ì„±ì í‘œ(Cost)ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    \item \textbf{ì´ìœ :} ì™œ MSE ëŒ€ì‹  \textbf{Log Loss(Binary Cross-Entropy)}ë¥¼ ì¨ì•¼ í•˜ëŠ”ì§€ 'ì§€í˜•(Topology)' ê´€ì ì—ì„œ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ì‚°ì—ì„œ ë‚´ë ¤ì˜¤ëŠ” ë°©ë²•ì¸ \textbf{ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)}ì˜ ì›ë¦¬ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì¡°ì ˆ:} í•™ìŠµë¥ (Learning Rate)ì´ í•™ìŠµ ì†ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{í•œ ì¤„ í•µì‹¬ ìš”ì•½} \\ \hline
\textbf{ì†ì‹¤ í•¨ìˆ˜ (Loss)} & $L(\hat{y}, y)$ & ë°ì´í„° \textbf{ìƒ˜í”Œ 1ê°œ}ì— ëŒ€í•œ ì˜¤ì°¨ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ) \\ \hline
\textbf{ë¹„ìš© í•¨ìˆ˜ (Cost)} & $J(w, b)$ & ì „ì²´ í•™ìŠµ ë°ì´í„°($m$ê°œ)ì— ëŒ€í•œ \textbf{Lossì˜ í‰ê· } \\ \hline
\textbf{ë³¼ë¡ì„± (Convexity)} & - & ë°¥ê·¸ë¦‡ì²˜ëŸ¼ ë§¤ë„ëŸ¬ìš´ ëª¨ì–‘ (ìµœì†Œì ì´ í•˜ë‚˜ë¿ì¸ ì•ˆì „í•œ ì§€í˜•) \\ \hline
\textbf{ê¸°ìš¸ê¸° (Gradient)} & $dw, db$ & í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€íŒŒë¥¸ ê²½ì‚¬ì˜ ë°©í–¥ \\ \hline
\textbf{í•™ìŠµë¥  (Learning Rate)} & $\alpha$ & í•œ ë²ˆ ì—…ë°ì´íŠ¸í•  ë•Œ ì´ë™í•˜ëŠ” ë³´í­ì˜ í¬ê¸° \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í•™ìŠµì˜ ë§¤ì»¤ë‹ˆì¦˜}

\subsection{1. Loss vs. Cost (ì˜¤ì°¨ì˜ ì •ì˜)}
\textbf{í•œ ì¤„ ìš”ì•½:} LossëŠ” 'ìª½ì§€ì‹œí—˜ ì ìˆ˜', CostëŠ” 'í•™ê¸°ë§ í‰ê·  ì„±ì 'ì…ë‹ˆë‹¤.

\begin{analogybox}{ì‹œí—˜ ì ìˆ˜ ë¹„ìœ }
\begin{itemize}
    \item \textbf{Loss Function ($L$):} 1ë²ˆ í•™ìƒì´ ë¬¸ì œë¥¼ í‹€ë ¸ìŠµë‹ˆë‹¤. ì´ í•™ìƒ í•˜ë‚˜ì˜ ì˜¤ì°¨ì…ë‹ˆë‹¤.
    \item \textbf{Cost Function ($J$):} ìš°ë¦¬ ë°˜ 30ëª… ì „ì²´ì˜ í‰ê·  ì˜¤ì°¨ì…ë‹ˆë‹¤. ì„ ìƒë‹˜(ëª¨ë¸)ì˜ ëª©í‘œëŠ” íŠ¹ì • í•™ìƒë§Œ ì˜ ê°€ë¥´ì¹˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ë°˜ ì „ì²´ì˜ í‰ê·  ì„±ì ($J$)ì„ ì¢‹ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

$$ J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) $$

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{2. Why Log Loss? (MSEì˜ í•¨ì •)}
\textbf{í•œ ì¤„ ìš”ì•½:} ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ MSEë¥¼ ì“°ë©´ í•¨ì •ì´ ë§ì€ ì‚°ì´ ë˜ì§€ë§Œ, Log Lossë¥¼ ì“°ë©´ ë§¤ë„ëŸ¬ìš´ ë°¥ê·¸ë¦‡ì´ ë©ë‹ˆë‹¤.

\textbf{ê¸°ìˆ ì  ì •ì˜:}
ì„ í˜• íšŒê·€ì™€ ë‹¬ë¦¬ Sigmoid í•¨ìˆ˜ê°€ í¬í•¨ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€ì— MSE($\frac{1}{2}(\hat{y}-y)^2$)ë¥¼ ì ìš©í•˜ë©´ ë¹„ìš© í•¨ìˆ˜ê°€ \textbf{ë¹„ë³¼ë¡(Non-Convex)} í˜•íƒœê°€ ë©ë‹ˆë‹¤. ì´ëŠ” ìˆ˜ë§ì€ \textbf{êµ­ì†Œ ìµœì í•´(Local Optima)}ë¥¼ ë§Œë“­ë‹ˆë‹¤.



\begin{warningbox}{MSEë¥¼ ì“°ë©´ ì•ˆ ë˜ëŠ” ì´ìœ }
ìœ„ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½(Non-Convex)ì„ ë³´ì„¸ìš”. ìš¸í‰ë¶ˆí‰í•œ ì§€í˜•ì—ì„œëŠ” êµ¬ìŠ¬ì„ êµ´ë ¸ì„ ë•Œ ê°€ì¥ ê¹Šì€ ë°”ë‹¥(Global Minimum)ì´ ì•„ë‹ˆë¼, ì¤‘ê°„ì— ìˆëŠ” ì‘ì€ ì›…ë©ì´(Local Minimum)ì— ê°‡í˜€ë²„ë¦½ë‹ˆë‹¤. í•™ìŠµì´ ë§í–ˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
ë°˜ë©´, \textbf{ë¡œê·¸ ì†ì‹¤(Log Loss)}ì„ ì‚¬ìš©í•˜ë©´ ì™¼ìª½(Convex)ì²˜ëŸ¼ ë§¤ë„ëŸ¬ìš´ ê·¸ë¦‡ ëª¨ì–‘ì´ ë˜ì–´, ì–´ë””ì„œ ì‹œì‘í•˜ë“  ë°”ë‹¥ìœ¼ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{ìš°ë¦¬ê°€ ì‚¬ìš©í•  ê³µì‹ (Binary Cross-Entropy):}
$$ L(\hat{y}, y) = -(y \log(\hat{y}) + (1-y) \log(1-\hat{y})) $$
\begin{itemize}
    \item ì •ë‹µ($y$)ì´ 1ì¼ ë•Œ: ì˜ˆì¸¡($\hat{y}$)ì´ 1ì´ë©´ ë¹„ìš© 0, 0ì´ë©´ ë¹„ìš© $\infty$.
    \item í‹€ë ¸ì„ ë•Œ ë¬´í•œëŒ€ì˜ ë²Œì ì„ ì£¼ì–´ ë¹ ë¥´ê²Œ ê³ ì¹˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
\end{itemize}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{3. Gradient Descent (ê²½ì‚¬ í•˜ê°•ë²•)}
\textbf{í•œ ì¤„ ìš”ì•½:} ëˆˆì„ ê°€ë¦° ì±„ ì‚°ì—ì„œ ê°€ì¥ ë‚®ì€ ê³¨ì§œê¸°ë¡œ ë‚´ë ¤ê°€ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.



\begin{analogybox}{ì•ˆê°œ ë‚€ ì‚° í•˜ì‚°í•˜ê¸°}
ë‹¹ì‹ ì€ ì§™ì€ ì•ˆê°œê°€ ë‚€ ì‚° ì •ìƒì— ì„œ ìˆìŠµë‹ˆë‹¤. ì•ì´ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
ê°€ì¥ ë‚®ì€ ê³³(ë¹„ìš© ìµœì†Œí™” ì§€ì )ìœ¼ë¡œ ê°€ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
\begin{enumerate}
    \item ë°œë¡œ ë•…ì„ ë”ë“¬ì–´ ê²½ì‚¬ê°€ ê°€ì¥ ê¸‰í•˜ê²Œ ë‚´ë ¤ê°€ëŠ” ë°©í–¥ì„ ì°¾ìŠµë‹ˆë‹¤. (\textbf{Gradient ê³„ì‚°})
    \item ê·¸ ë°©í–¥ìœ¼ë¡œ í•œ ë°œìêµ­ ë‚´ë”›ìŠµë‹ˆë‹¤. (\textbf{Update})
    \item ë°”ë‹¥ì— ë„ì°©í•  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
\end{enumerate}
\end{analogybox}

\textbf{ì—…ë°ì´íŠ¸ ê³µì‹:}
$$ w := w - \alpha \frac{\partial J}{\partial w} $$
$$ b := b - \alpha \frac{\partial J}{\partial b} $$
\begin{itemize}
    \item \textbf{ë¹¼ê¸°($-$)ì˜ ì˜ë¯¸:} ê¸°ìš¸ê¸°ê°€ ì–‘ìˆ˜(ì˜¤ë¥´ë§‰)ë¼ë©´ $w$ë¥¼ ì¤„ì—¬ì•¼(ì™¼ìª½ìœ¼ë¡œ ê°€ì•¼) ë‚´ë ¤ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°€ì•¼ í•˜ë¯€ë¡œ ëºë‹ˆë‹¤.
    \item \textbf{$\alpha$ (Learning Rate):} í•œ ë°œìêµ­ì˜ í¬ê¸°ì…ë‹ˆë‹¤.
\end{itemize}

% --- 7. ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤ ---
\section{Practical Scenario: í•™ìŠµë¥  $\alpha$ì˜ ì¤‘ìš”ì„±}

í•™ìŠµë¥ (Learning Rate) $\alpha$ëŠ” ëª¨ë¸ì˜ ìš´ëª…ì„ ê²°ì •í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ìˆ«ì(Hyperparameter)ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Case A: $\alpha$ê°€ ë„ˆë¬´ ì‘ì„ ë•Œ (0.00001)} \\
    ê°œë¯¸ì²˜ëŸ¼ ê¸°ì–´ê°‘ë‹ˆë‹¤. í•´ê°€ ì§ˆ ë•Œê¹Œì§€(í•™ìŠµ ì¢…ë£Œê¹Œì§€) ì‚° ì¤‘í„±ì—ë„ ëª» ê°‘ë‹ˆë‹¤. (ìˆ˜ë ´ ì†ë„ ë§¤ìš° ëŠë¦¼)
    
    \item \textbf{Case B: $\alpha$ê°€ ë„ˆë¬´ í´ ë•Œ (10.0)} \\
    ê±°ì¸ì˜ ì í”„ì…ë‹ˆë‹¤. ê³¨ì§œê¸°ë¥¼ í–¥í•´ ë›°ì—ˆëŠ”ë° ë„ˆë¬´ ë©€ë¦¬ ë›°ì–´ì„œ ë°˜ëŒ€í¸ ì‚°ë“±ì„±ì´ì— ì²˜ë°•í™ë‹ˆë‹¤. ì˜¤íˆë ¤ ë” ë†’ì€ ê³³ìœ¼ë¡œ ì˜¬ë¼ê°ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. (\textbf{Overshooting / Divergence})
\end{itemize}

% --- 8. ê³µì‹/ì ˆì°¨ + ì˜ˆì‹œ ê³„ì‚° ---
\section{Numerical Example: ë¹„ìš© ê³„ì‚° í•´ë³´ê¸°}

\begin{examplebox}{ë¹„ìš© í•¨ìˆ˜ ê³„ì‚° ì‹¤ìŠµ}
\textbf{ìƒí™©:} ê³ ì–‘ì´ ì‚¬ì§„($y=1$)ì„ ë³´ì—¬ì¤¬ëŠ”ë°, ëª¨ë¸ì´ 0.8(80\%)ë¡œ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.
$$ y = 1, \quad \hat{y} = 0.8 $$

\textbf{1. ì†ì‹¤(Loss) ê³„ì‚°:}
ê³µì‹: $L = -(1 \cdot \log(0.8) + 0 \cdot \log(0.2))$
$$ L = -\log(0.8) \approx -(-0.223) = 0.223 $$

\textbf{ìƒí™© ë³€ê²½:} ë§Œì•½ ëª¨ë¸ì´ 0.1(10\%)ë¡œ ì˜ëª» ì˜ˆì¸¡í–ˆë‹¤ë©´?
$$ L = -\log(0.1) \approx -(-2.30) = 2.30 $$
$\rightarrow$ ì˜ˆì¸¡ì´ í‹€ë¦´ìˆ˜ë¡ ë²Œì (Loss)ì´ 0.223ì—ì„œ 2.30ìœ¼ë¡œ 10ë°° ë„˜ê²Œ ì»¤ì¡ŒìŠµë‹ˆë‹¤! ì´ê²ƒì´ Log Lossì˜ ìœ„ë ¥ì…ë‹ˆë‹¤.
\end{examplebox}

% --- 9. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation (Python)}

ì´ë¡ ì„ `numpy` ì½”ë“œë¡œ ì˜®ê²¨ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=Cost Function and Optimization, breaklines=true]
import numpy as np

def compute_cost(A, Y):
    """
    A: ì˜ˆì¸¡ê°’ (1, m), Y: ì‹¤ì œê°’ (1, m)
    """
    m = Y.shape[1]
    
    # log(0) ë°©ì§€ë¥¼ ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’(epsilon)ì„ ë”í•´ì£¼ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
    epsilon = 1e-5 
    
    # Binary Cross-Entropy ìˆ˜ì‹ (Element-wise multiplication)
    cost = -1/m * np.sum(Y * np.log(A + epsilon) + (1-Y) * np.log(1-A + epsilon))
    
    return float(np.squeeze(cost)) # ë°°ì—´ì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜

def update_parameters(w, b, dw, db, learning_rate):
    """
    ê²½ì‚¬ í•˜ê°•ë²• ì—…ë°ì´íŠ¸ ë‹¨ê³„
    """
    # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ê¸°(dw, db)ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ alphaë§Œí¼ ì´ë™
    w = w - learning_rate * dw
    b = b - learning_rate * db
    
    return w, b
\end{lstlisting}

% --- 10. FAQ ---
\section{FAQ: ì´ˆì‹¬ìê°€ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸}
\begin{itemize}
    \item \textbf{Q1. ê²½ì‚¬ í•˜ê°•ë²• ê³µì‹ì—ì„œ ì™œ ë”í•˜ì§€ ì•Šê³  ë¹¼ë‚˜ìš”?} \\
    \textbf{A.} ê¸°ìš¸ê¸°(Gradient)ëŠ” í•¨ìˆ˜ê°€ 'ì¦ê°€í•˜ëŠ”' ë°©í–¥ì„ ê°€ë¦¬í‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¹„ìš©ì„ 'ì¤„ì—¬ì•¼' í•˜ë¯€ë¡œ ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°€ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ëºë‹ˆë‹¤.
    
    \item \textbf{Q2. ë¹„ìš© í•¨ìˆ˜ê°€ 0ì´ ë˜ë©´ ì¢‹ì€ ê±´ê°€ìš”?} \\
    \textbf{A.} ì´ë¡ ì ìœ¼ë¡œëŠ” ì™„ë²½í•˜ì§€ë§Œ, í˜„ì‹¤ì—ì„œëŠ” \textbf{ê³¼ì í•©(Overfitting)}ì„ ì˜ì‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì œì§‘ ë‹µì„ ë‹¬ë‹¬ ì™¸ìš´ ìƒíƒœì¼ ìˆ˜ ìˆì–´ì„œ, ìƒˆë¡œìš´ ë¬¸ì œ(Test Set)ëŠ” ëª» í’€ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
\end{itemize}

% --- 11. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” \textbf{êµ¬ì¡°(Architecture)}ë¥¼ ë§Œë“¤ì—ˆê³ , \textbf{í•™ìŠµ ë°©ë²•(Optimizer)}ê¹Œì§€ ì¥ì°©í–ˆìŠµë‹ˆë‹¤. 
ë‹¤ìŒ ì¥ì—ì„œëŠ” ì´ ëª¨ë“  ë¶€í’ˆì„ ì¡°ë¦½í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” \textbf{ì „ì²´ ëª¨ë¸(Full Model)}ì„ ì™„ì„±í•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Cost Function:} ì „ì²´ ë°ì´í„°ì˜ ì˜¤ì°¨ í‰ê· ($J$)ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
    \item \textbf{Log Loss:} ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ëŠ” MSE ëŒ€ì‹  Log Lossë¥¼ ì¨ì•¼ Convex(ë³¼ë¡)í•´ì§„ë‹¤.
    \item \textbf{Gradient Descent:} $w_{new} = w_{old} - \alpha \cdot dw$. ê²½ì‚¬ë¥¼ íƒ€ê³  ë‚´ë ¤ê°€ëŠ” ì•Œê³ ë¦¬ì¦˜.
    \item \textbf{Learning Rate:} ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¬ë‹¤. ì ì ˆí•œ íŠœë‹ì´ í•„ìš”í•˜ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}