\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx}
\usepackage{adjustbox}  % í‘œ/ë°•ìŠ¤ í¬ê¸° ì¡°ì ˆ % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬
\usepackage{colortbl} % í‘œ ìƒ‰ìƒ

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{diagnosisbox}[1]{
    colback=purple!5!white,
    colframe=purple!80!black,
    fonttitle=\bfseries,
    title=ğŸ©º #1 (ë‹¥í„° ë”¥ëŸ¬ë‹ì˜ ì²˜ë°©ì „)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Bias vs Variance Trade-off Analysis}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1 Train / Dev / Test Sets Strategy \textit{- Completed}
        \item \textbf{5.2 Bias vs Variance Analysis (Diagnosis)}
        \begin{itemize}
            \item The Bullseye Analogy
            \item Diagnosis Recipe (The Gap Analysis)
            \item Modern Trade-off in Deep Learning
            \item Implementation: Auto-Diagnosis Class
        \end{itemize}
        \item 5.3 Regularization (L2, Dropout) \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ Train, Dev, Testë¡œ ë‚˜ëˆ„ëŠ” ì „ëµì„ ì„¸ì› ìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ê³  ì„±ì í‘œ(Error Rate)ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì„±ì ì´ ê¸°ëŒ€ ì´í•˜ì…ë‹ˆë‹¤.
ì´ë•Œ "ì™œ ì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¤ì§€?"ë¼ê³  ë§‰ì—°í•´í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ê°€ ë‚´ë¦´ ìˆ˜ ìˆëŠ” ì§„ë‹¨ì€ ë”± ë‘ ê°€ì§€ì…ë‹ˆë‹¤. \textbf{"ê³µë¶€ë¥¼ ëœ í–ˆê±°ë‚˜(High Bias)"} ì•„ë‹ˆë©´ \textbf{"ë¬¸ì œì§‘ë§Œ ë‹¬ë‹¬ ì™¸ì› ê±°ë‚˜(High Variance)"}. ì´ ë‘ ê°€ì§€ ë³‘ëª…ì„ ì •í™•íˆ ì§„ë‹¨í•´ì•¼ ì˜¬ë°”ë¥¸ ì•½(Solution)ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ ì›ì¸ì„ ê·œëª…í•˜ëŠ” \textbf{'ì§„ë‹¨(Diagnosis)'} ê¸°ìˆ ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} í¸í–¥(Bias)ê³¼ ë¶„ì‚°(Variance)ì„ ê°ê° ê³¼ì†Œì í•©(Underfitting)ê³¼ ê³¼ëŒ€ì í•©(Overfitting)ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ê¸°ì¤€:} \textbf{Bayes Error(ìµœì  ì˜¤ì°¨)}ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Train Errorì™€ Dev Errorì˜ ê²©ì°¨(Gap)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    \item \textbf{ë³€í™”:} ë”¥ëŸ¬ë‹ ì‹œëŒ€ì— Biasì™€ Varianceë¥¼ ë™ì‹œì— ì¤„ì´ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ ì´ìœ ë¥¼ ì•Œì•„ë´…ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} í•™ìŠµ ê³¡ì„ (Learning Curve)ì„ ê·¸ë¦¬ê³  ìë™ìœ¼ë¡œ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ëŠ” Python ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ  (í•™ìƒ)} \\ \hline
\textbf{High Bias} & ê³¼ì†Œì í•© (Underfitting) & ê³µë¶€ë¥¼ ëŒ€ì¶© í•´ì„œ êµê³¼ì„œ(Train) ë‚´ìš©ë„ ëª¨ë¦„. \\ \hline
\textbf{High Variance} & ê³¼ëŒ€ì í•© (Overfitting) & êµê³¼ì„œ ë‹µë§Œ ë‹¬ë‹¬ ì™¸ì›Œì„œ ì‘ìš© ë¬¸ì œ(Dev)ëŠ” ë‹¤ í‹€ë¦¼. \\ \hline
\textbf{Bayes Error} & ì´ë¡ ì  ìµœì†Œ ì˜¤ì°¨ & ì¸ê°„ë„ í‹€ë¦´ ìˆ˜ë°–ì— ì—†ëŠ” ë¬¸ì œì˜ ë‚œì´ë„ (í•œê³„ì¹˜). \\ \hline
\textbf{Avoidable Bias} & Train Error - Bayes Error & ìš°ë¦¬ê°€ ë…¸ë ¥ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” í¸í–¥. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ê³¼ë… ë§ì¶”ê¸° (Bullseye Analogy)}



\subsection{1. High Bias (Underfitting)}
\begin{itemize}
    \item \textbf{í˜„ìƒ:} í™”ì‚´ë“¤ì´ ì •ì¤‘ì•™ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆê³ , ìê¸°ë“¤ë¼ë¦¬ëŠ” ë­‰ì³ ìˆìŠµë‹ˆë‹¤.
    \item \textbf{ì›ì¸:} ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ(ì˜ˆ: ì§ì„ ) ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ì „í˜€ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
\end{itemize}

\subsection{2. High Variance (Overfitting)}
\begin{itemize}
    \item \textbf{í˜„ìƒ:} í™”ì‚´ë“¤ì˜ í‰ê·  ìœ„ì¹˜ëŠ” ì •ì¤‘ì•™ì´ì§€ë§Œ, ì‚¬ë°©ìœ¼ë¡œ í©ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    \item \textbf{ì›ì¸:} í›ˆë ¨ ë°ì´í„°ì˜ ì‚¬ì†Œí•œ ë…¸ì´ì¦ˆê¹Œì§€ ê³¼ë„í•˜ê²Œ í•™ìŠµí•´ì„œ, ì¡°ê¸ˆë§Œ ë‹¤ë¥¸ ë°ì´í„°ê°€ ì˜¤ë©´ ì˜ˆì¸¡ì´ ë„ëœë‹ˆë‹¤.
\end{itemize}

\subsection{3. The Diagnostic Recipe (ì§„ë‹¨ ë ˆì‹œí”¼)}
ìˆ«ìë¥¼ ë³´ê³  ì§„ë‹¨í•˜ëŠ” ë²•ì…ë‹ˆë‹¤. Bayes Error(ì¸ê°„ ìˆ˜ì¤€ ì˜¤ì°¨)ê°€ 0\%ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.

\begin{diagnosisbox}{ì¦ìƒë³„ ì²˜ë°©ì „}
\begin{center}
\begin{tabular}{c|c|l|l}
\hline
\textbf{Train Error} & \textbf{Dev Error} & \textbf{ì§„ë‹¨ (Diagnosis)} & \textbf{ì²˜ë°© (Prescription)} \\ \hline
1\% & 11\% & \textbf{High Variance} & ë°ì´í„° ì¶”ê°€, ì •ê·œí™”(L2/Dropout) \\ \hline
15\% & 16\% & \textbf{High Bias} & ë” í° ëª¨ë¸(ì€ë‹‰ì¸µ ì¶”ê°€), ì˜¤ë˜ í•™ìŠµ \\ \hline
15\% & 30\% & \textbf{High Bias \& Variance} & ëª¨ë¸ êµ¬ì¡° ë³€ê²½, ë°ì´í„° ì •ì œ \\ \hline
0.5\% & 1\% & \textbf{Good Fit} & í˜„ì¬ ìƒíƒœ ìœ ì§€ \\ \hline
\end{tabular}
\end{center}
\end{diagnosisbox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Deep Dive: ë”¥ëŸ¬ë‹ ì‹œëŒ€ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„}

\begin{itemize}
    \item \textbf{ê³¼ê±° (Traditional ML):} Biasë¥¼ ì¤„ì´ë©´ Varianceê°€ ëŠ˜ì–´ë‚˜ëŠ” 'ì‹œì†Œ' ê´€ê³„ì˜€ìŠµë‹ˆë‹¤.
    \item \textbf{í˜„ì¬ (Deep Learning):}
    \begin{itemize}
        \item \textbf{Bias ì¤„ì´ê¸°:} ë„¤íŠ¸ì›Œí¬ë¥¼ ë” í¬ê²Œ ë§Œë“­ë‹ˆë‹¤. (ë°ì´í„°ê°€ ë§ë‹¤ë©´ Varianceë¥¼ ê±°ì˜ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
        \item \textbf{Variance ì¤„ì´ê¸°:} ë°ì´í„°ë¥¼ ë” ë§ì´ ëª¨ìë‹ˆë‹¤. (Biasë¥¼ ê±°ì˜ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    \end{itemize}
    \item \textbf{ê²°ë¡ :} ì»´í“¨íŒ… íŒŒì›Œì™€ ë°ì´í„°ë§Œ ì¶©ë¶„í•˜ë‹¤ë©´, Biasì™€ Varianceë¥¼ ë™ì‹œì— ì¡ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Auto-Diagnosis Tool}

í•™ìŠµ ê¸°ë¡(History)ì„ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ ë³‘ëª…ì„ ì§„ë‹¨í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Model Diagnosis Class, breaklines=true]
import matplotlib.pyplot as plt

class ModelDiagnostician:
    def __init__(self, train_acc, dev_acc, human_acc=0.99):
        # ì •í™•ë„(Accuracy)ë¥¼ ì˜¤ì°¨(Error)ë¡œ ë³€í™˜
        self.train_err = 1.0 - train_acc
        self.dev_err = 1.0 - dev_acc
        self.human_err = 1.0 - human_acc
        
    def diagnose(self):
        print(f"Human Error: {self.human_err:.2%}")
        print(f"Train Error: {self.train_err:.2%}")
        print(f"Dev Error  : {self.dev_err:.2%}")
        print("-" * 30)
        
        # 1. Bias ì§„ë‹¨ (Trainê³¼ Humanì˜ ì°¨ì´)
        avoidable_bias = self.train_err - self.human_err
        
        # 2. Variance ì§„ë‹¨ (Devì™€ Trainì˜ ì°¨ì´)
        variance = self.dev_err - self.train_err
        
        threshold = 0.02 # 2% ì´ìƒ ì°¨ì´ë‚˜ë©´ ë¬¸ì œë¡œ ê°„ì£¼
        
        if avoidable_bias > threshold:
            print("[Diagnosis] High Bias (Underfitting)")
            print(">> Solution: Bigger Network, Train Longer (Epochs)")
            
        elif variance > threshold:
            print("[Diagnosis] High Variance (Overfitting)")
            print(">> Solution: More Data, Regularization (Dropout, L2)")
            
        else:
            print("[Diagnosis] Good Fit! Great Job.")

# --- ì‹¤í–‰ ì˜ˆì œ ---
if __name__ == "__main__":
    # ìƒí™©: í›ˆë ¨ì€ ì˜ ë˜ëŠ”ë°(99%), ê²€ì¦ì€ ì•ˆ ë¨(89%) -> High Variance
    train_accuracy = 0.99
    dev_accuracy = 0.89
    
    doctor = ModelDiagnostician(train_accuracy, dev_accuracy)
    doctor.diagnose()
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\begin{warningbox}{Train Errorê°€ ë†’ë‹¤ê³  ë¬´ì¡°ê±´ High Biasì¸ê°€ìš”?}
\textbf{ì•„ë‹™ë‹ˆë‹¤!} ë¹„êµ ëŒ€ìƒ(Bayes Error)ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ìƒí™©:} íë¦¿í•œ ì˜›ë‚  ë¬¸ì„œ ì¸ì‹. ì‚¬ëŒë„ 15\% í‹€ë¦¼(Human Error = 15\%).
    \item \textbf{ê²°ê³¼:} ëª¨ë¸ì˜ Train Errorê°€ 15\%ì„.
    \item \textbf{ì§„ë‹¨:} ì´ê²ƒì€ High Biasê°€ ì•„ë‹™ë‹ˆë‹¤. ì´ë¯¸ ì‚¬ëŒë§Œí¼ ì˜í•œ ê²ƒì…ë‹ˆë‹¤(Optimal). ì´ ê²½ìš°ì—” Biasë¥¼ ì¤„ì´ë ¤ ë…¸ë ¥í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
\end{itemize}
\end{warningbox}

\textbf{Q. High Biasì™€ High Varianceê°€ ë™ì‹œì— ë†’ìœ¼ë©´ìš”?} \\
\textbf{A.} ìµœì•…ì˜ ìƒí™©ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ì •ë‹µë„ ëª» ë§ì¶”ë©´ì„œ ì˜ˆì¸¡ê°’ì€ ë„ë›°ê¸°ë¥¼ í•©ë‹ˆë‹¤. ë³´í†µ ëª¨ë¸ êµ¬ì¡° ìì²´ê°€ ë°ì´í„°ì— ë§ì§€ ì•Šê±°ë‚˜, ë°ì´í„°ì— ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ìˆì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì§„ë‹¨ ê²°ê³¼, ë§Œì•½ ì—¬ëŸ¬ë¶„ì˜ ëª¨ë¸ì´ \textbf{High Variance(ê³¼ëŒ€ì í•©)} íŒì •ì„ ë°›ì•˜ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
"ë°ì´í„°ë¥¼ ë” ëª¨ìœ¼ì„¸ìš”"ë¼ëŠ” ì¡°ì–¸ì€ ì‰½ì§€ë§Œ, í˜„ì‹¤ì—ì„œëŠ” ëˆê³¼ ì‹œê°„ì´ ë“­ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ ê³¼ëŒ€ì í•©ì„ ì¹˜ë£Œí•˜ëŠ” ë§ˆë²•ì˜ ì•Œì•½ì´ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” \textbf{[Regularization]} ìœ ë‹›ì—ì„œ \textbf{L2 ì •ê·œí™”}ì™€ \textbf{Dropout}ì´ë¼ëŠ” ê°•ë ¥í•œ ì¹˜ë£Œë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{High Bias:} Train Errorê°€ ë†’ë‹¤. $\rightarrow$ ëª¨ë¸ì„ í‚¤ì›Œë¼(Bigger Network).
    \item \textbf{High Variance:} Dev Errorê°€ Train Errorë³´ë‹¤ í›¨ì”¬ ë†’ë‹¤. $\rightarrow$ ë°ì´í„°ë¥¼ ëª¨ìœ¼ê±°ë‚˜ ì •ê·œí™”(Regularization)í•˜ë¼.
    \item \textbf{Reference:} ì ˆëŒ€ì ì¸ ìˆ˜ì¹˜ê°€ ì•„ë‹ˆë¼ \textbf{Bayes Error(Human-level)}ì™€ì˜ ì°¨ì´(Gap)ë¥¼ ë´ì•¼ í•œë‹¤.
    \item \textbf{Priority:} ë³´í†µ Biasë¥¼ ë¨¼ì € ì¡ê³ , ê·¸ ë‹¤ìŒ Varianceë¥¼ ì¡ëŠ” ìˆœì„œë¡œ ì§„í–‰í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}