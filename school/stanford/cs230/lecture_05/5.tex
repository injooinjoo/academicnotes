\documentclass[a4paper, 11pt]{article}

% --- 패키지 설정 ---
\usepackage{kotex} % 한글 지원
\usepackage{geometry} % 여백 설정
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % 수식 패키지
\usepackage{graphicx}
\usepackage{adjustbox}  % 표/박스 크기 조절 % 이미지 삽입
\usepackage{hyperref} % 하이퍼링크
\usepackage{xcolor} % 색상 지원
\usepackage{listings} % 코드 블록
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % 박스 디자인
\usepackage{enumitem} % 리스트 스타일
\usepackage{booktabs} % 표 디자인

% --- 색상 정의 ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- 코드 스타일 설정 ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- 박스 스타일 정의 ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=📌 #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=💡 #1 (직관적 비유)
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=⚠️ #1 (오해 방지 가이드)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=🧮 #1 (실전 시나리오 \& 계산)
}

% --- 문서 정보 ---
\title{\textbf{[CS230] Foundations of Neural Networks: \\ Broadcasting \& Removing Loops}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. 전체 목차 (TOC) ---
\section*{📚 Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network
    \begin{itemize}
        \item 2.1 Architecture \& Forward Propagation \textit{- Completed}
        \item 2.2 Cost Function \& Gradient Descent \textit{- Completed}
        \item 2.3 Python \& Vectorization \textit{- Completed}
        \item \textbf{2.4 Broadcasting \& Removing Loops (Current Unit)}
        \begin{itemize}
            \item Definition \& Rules
            \item Visual Analogy
            \item Under the Hood (Strides)
            \item Implementation (Normalization)
        \end{itemize}
    \end{itemize}
    \item[Chapter 3.] Shallow Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. 이전 단원 연결 ---
\section*{🔗 지난 시간 복습 및 연결}
지난 강의에서 우리는 딥러닝 속도의 핵심인 \textbf{벡터화(Vectorization)}를 배웠습니다. 벡터화가 고속도로(Engine)라면, 오늘 배울 \textbf{브로드캐스팅(Broadcasting)}은 차선을 자유자재로 변경하는 \textbf{유연함(Flexibility)}입니다. 많은 학생들이 `np.dot`은 잘 쓰면서도, 모양이 다른 행렬끼리 연산할 때 발생하는 오류에는 속수무책입니다. 이 원리를 알아야 진정한 디버깅 마스터가 될 수 있습니다.

% --- 4. 개요 ---
\section{Unit Overview}
\begin{summarybox}{핵심 목표}
이 단원은 서로 다른 모양(Shape)의 데이터를 오류 없이 연산하는 방법을 다룹니다.
\begin{itemize}
    \item \textbf{개념:} NumPy가 작은 배열을 자동으로 확장(Stretch)하여 연산하는 규칙을 배웁니다.
    \item \textbf{구현:} `for-loop` 없이 데이터 정규화(Normalization)를 수행하는 코드를 작성합니다.
    \item \textbf{원리:} 메모리 복사 없이 \textbf{스트라이드(Strides)} 조작을 통해 효율적으로 동작하는 내부 원리를 이해합니다.
\end{itemize}
\end{summarybox}

% --- 5. 용어 정리 ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{용어} & \textbf{설명} & \textbf{한 줄 핵심 요약} \\ \hline
\textbf{Broadcasting} & 브로드캐스팅 & 모양이 다른 배열 간 연산 시, 작은 쪽을 자동으로 늘려주는 기능 \\ \hline
\textbf{Shape} & 형상 & 배열의 차원 크기 (예: $(4, 3)$은 4행 3열) \\ \hline
\textbf{Normalization} & 정규화 & 데이터의 평균을 0, 분산을 1로 맞추는 전처리 과정 \\ \hline
\textbf{Keepdims} & 차원 유지 & 연산 후에도 차원(Rank)을 삭제하지 않고 유지하는 옵션 \\ \hline
\textbf{Strides} & 스트라이드 & 메모리 상에서 다음 요소로 넘어가기 위한 보폭 (Byte 단위) \\ \hline
\end{tabular}
\end{center}

% --- 6. 핵심 개념 상세 설명 ---
\section{Core Concepts: 브로드캐스팅의 마법}

\subsection{1. Broadcasting의 정의와 비유}
\textbf{한 줄 요약:} 작은 행렬을 큰 행렬 크기에 맞춰 자동으로 '늘려서(Copy)' 연산합니다.

\begin{analogybox}{식빵과 버터 비유}
식빵 100개(데이터 $m=100$)에 버터($b$)를 발라야 합니다.
\begin{itemize}
    \item \textbf{For-loop:} 식빵을 하나 꺼내고, 버터를 바르고, 내려놓습니다. (100번 반복)
    \item \textbf{Broadcasting:} 마법을 부려 버터를 식빵 100개 길이만큼 \textbf{순식간에 늘린(Stretch)} 뒤, 한 번에 쾅 찍어버립니다.
\end{itemize}
\end{analogybox}



\subsection{2. General Broadcasting Rules (엄격한 규칙)}
아무거나 다 늘려주지는 않습니다. NumPy는 \textbf{뒤(오른쪽) 차원부터 비교}하여 다음 조건 중 하나를 만족해야만 연산을 허용합니다.

\begin{enumerate}
    \item \textbf{Equal:} 두 차원의 크기가 같다.
    \item \textbf{One:} 둘 중 하나의 크기가 1이다. (이 경우 1인 쪽이 늘어남)
\end{enumerate}

\begin{tcolorbox}[colback=white, colframe=black, title=Rule Check Example]
\textbf{Case 1: 가능 (Success)} \\
$A: (4, \mathbf{3})$ \\
$B: (4, \mathbf{1}) \rightarrow$ 1이 3으로 확장됨. \\
결과: $(4, 3)$

\textbf{Case 2: 불가능 (Fail - ValueError)} \\
$A: (4, \mathbf{3})$ \\
$B: (4, \mathbf{2}) \rightarrow$ 3과 2는 다르고, 둘 다 1이 아님.
\end{tcolorbox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{3. Under the Hood: 가상 복사 (Virtual Copying)}
"교수님, 데이터를 늘리면 메모리를 낭비하는 것 아닌가요?" \\
\textbf{아닙니다.} 이것이 브로드캐스팅 기술의 핵심입니다.

\begin{itemize}
    \item \textbf{Physical (실제):} $b = [1, 2, 3]$ (메모리엔 딱 3개만 존재)
    \item \textbf{Logical (가상):} CPU에게는 마치 $[1, 1, 1, \dots], [2, 2, 2, \dots]$ 인 것처럼 주소를 속여서 알려줍니다.
    \item \textbf{Strides Manipulation:} 메모리를 실제로 복사하지 않고, 데이터 접근 보폭(Stride)을 0으로 설정하여 같은 값을 반복해서 읽게 만듭니다. 마치 \textbf{홀로그램}과 같습니다.
\end{itemize}

% --- 7. 실전 계산 예시 ---
\section{Numerical Example: 손으로 푸는 브로드캐스팅}

\begin{examplebox}{칼로리 계산 시나리오}
\textbf{상황:} 4가지 음식(행)의 영양소(열: 탄, 단, 지) 데이터가 있습니다. 각 음식의 총 칼로리가 100g당 얼마인지 더하고 싶습니다.

\textbf{데이터 행렬 A (2개 음식 x 3개 영양소):}
$$
\begin{bmatrix} 
10 & 20 & 30 \\ 
40 & 50 & 60 
\end{bmatrix} 
$$
\textbf{조미료 B (각 영양소에 추가될 값, 1 x 3):}
$$
\begin{bmatrix} 1 & 2 & 3 \end{bmatrix}
$$

\textbf{연산 과정 ($A + B$):}
행렬 B의 행(Row) 차원이 1이므로, 행렬 A의 크기인 2로 확장됩니다.
$$
\begin{bmatrix} 
10 & 20 & 30 \\ 
40 & 50 & 60 
\end{bmatrix} 
+
\begin{bmatrix} 
1 & 2 & 3 \\ 
\mathbf{1} & \mathbf{2} & \mathbf{3} \leftarrow \text{(복사됨)} 
\end{bmatrix} 
=
\begin{bmatrix} 
11 & 22 & 33 \\ 
41 & 52 & 63 
\end{bmatrix}
$$
\end{examplebox}

% --- 8. 구현 코드 ---
\section{Implementation: Data Normalization}

브로드캐스팅이 가장 빛을 발하는 순간은 데이터를 전처리(Preprocessing) 할 때입니다. 
입력 데이터의 평균을 0, 분산을 1로 만드는 \textbf{정규화}를 구현해봅시다.

\begin{lstlisting}[language=Python, caption=Broadcasting Implementation, breaklines=true]
import numpy as np
import time

def normalization_demo():
    # 데이터: 4개의 특성(Feature), 100만 개의 샘플
    # Shape: (4, 1000000)
    X = np.random.rand(4, 1000000) * 100
    
    # 1. 평균과 표준편차 계산
    # axis=1: 열(column) 방향으로 계산 (각 행의 평균)
    # keepdims=True: (4,)가 아니라 (4, 1)로 유지 -> 브로드캐스팅 필수 조건!
    mu = np.mean(X, axis=1, keepdims=True)
    sigma = np.std(X, axis=1, keepdims=True)
    
    print(f"mu shape: {mu.shape}") # (4, 1) 확인
    
    # 2. Broadcasting 적용 (핵심)
    # (4, 1000000) - (4, 1) -> (4, 1)이 100만 번 복사되어 연산됨
    # 나눗셈도 마찬가지 원리
    tic = time.time()
    X_norm = (X - mu) / sigma
    toc = time.time()
    
    print(f"Broadcasting time: {1000 * (toc - tic):.2f} ms") # 매우 빠름

if __name__ == "__main__":
    normalization_demo()
\end{lstlisting}

% --- 9. FAQ ---
\section{FAQ \& Pitfalls}

\begin{warningbox}{주의: (m, 1) + (1, m) = (m, m)}
브로드캐스팅의 강력함이 독이 될 때가 있습니다.
\begin{itemize}
    \item 벡터 A: $(5, 1)$
    \item 벡터 B: $(1, 5)$
    \item $A + B$: $(5, 5)$ 행렬이 되어버립니다.
\end{itemize}
두 벡터를 더해서 같은 크기의 벡터를 만들고 싶었다면, 반드시 두 벡터의 Shape이 일치하는지 `assert` 문으로 확인해야 합니다.
\end{warningbox}

\textbf{Q. \texttt{keepdims=True}를 안 쓰면 어떻게 되나요?} \\
A. `mu`의 shape이 `(4,)`가 됩니다. 이를 `Rank-1 Array`라고 합니다. 대부분의 경우 브로드캐스팅이 잘 되지만, 특정 상황에서 예상치 못한 차원 확장이 일어나 디버깅이 매우 어려워집니다. 명시적으로 `(4, 1)`을 유지하는 것이 안전합니다.

% --- 10. 다음 단원 연결 ---
\section*{🔗 다음 단계 (Next Step)}
이제 여러분은 데이터의 모양(Shape)을 자유자재로 다루는 기술까지 익혔습니다. 벡터화와 브로드캐스팅이라는 두 개의 무기를 손에 쥐었습니다.

다음 장 \textbf{[Chapter 3. Shallow Neural Networks]}에서는 드디어 로지스틱 회귀(뉴런 1개)를 넘어서, 은닉층(Hidden Layer)이 있는 \textbf{진짜 신경망}을 구축합니다. 여기서부터 딥러닝의 마법이 시작됩니다.

\vspace{0.5cm}

\begin{summarybox}{단원 요약 (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Broadcasting:} 작은 배열을 큰 배열에 맞춰 '가상으로 확장'하여 연산한다.
    \item \textbf{Rule:} 차원을 오른쪽 끝부터 비교하여, 같거나 1이어야 한다.
    \item \textbf{Memory:} 데이터를 실제로 복사하지 않으므로(Strides 조작) 메모리 효율적이다.
    \item \textbf{Tip:} `np.sum`이나 `np.mean` 사용 시 `keepdims=True`를 습관화하라.
\end{enumerate}
\end{summarybox}

\end{document}