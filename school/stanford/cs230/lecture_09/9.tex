\documentclass[a4paper, 11pt]{article}

% --- 패키지 설정 ---
\usepackage{kotex} % 한글 지원
\usepackage{geometry} % 여백 설정
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % 수식 패키지
\usepackage{graphicx}
\usepackage{adjustbox}  % 표/박스 크기 조절 % 이미지 삽입
\usepackage{hyperref} % 하이퍼링크
\usepackage{xcolor} % 색상 지원
\usepackage{listings} % 코드 블록
\usepackage[most]{tcolorbox}
\tcbuselibrary{breakable} % 박스 디자인
\usepackage{enumitem} % 리스트 스타일
\usepackage{booktabs} % 표 디자인
\usepackage{array} % 표 정렬

% --- 색상 정의 ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- 코드 스타일 설정 ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- 박스 스타일 정의 ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=📌 #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=💡 #1 (직관적 비유)
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=⚠️ #1 (오해 방지 가이드)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=🧮 #1 (실전 시나리오 \& 계산)
}

% --- 문서 정보 ---
\title{\textbf{[CS230] Deep Neural Networks: \\ MLP Architecture}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. 전체 목차 (TOC) ---
\section*{📚 Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-3.] Foundations \& Shallow Networks \textit{- Completed}
    \item[\textbf{Chapter 4.}] \textbf{Deep Neural Networks (Current Unit)}
    \begin{itemize}
        \item \textbf{4.1 Deep L-Layer Neural Network Architecture}
        \begin{itemize}
            \item General Notation ($L$, $n^{[l]}$)
            \item Hierarchical Representation (Why Deep?)
            \item Matrix Dimensions Analysis
            \item Building Blocks Implementation
        \end{itemize}
        \item 4.2 Forward Propagation in Deep Network
        \item 4.3 Deep Network Backpropagation (Overview)
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. 이전 단원 연결 ---
\section*{🔗 지난 시간 복습 및 연결}
우리는 지금까지 은닉층이 하나뿐인 '얕은 신경망'을 다뤘습니다. 하지만 현실 세계의 복잡한 문제(자율주행, 자연어 처리 등)를 풀기엔 뇌 용량이 부족합니다.
이제 우리는 은닉층을 2개, 3개, 아니 수백 개까지 쌓아 올릴 것입니다. 이것이 바로 여러분이 매일 듣는 \textbf{'딥러닝(Deep Learning)'}의 실체입니다. 단순히 층만 늘리는 게 아니라, 코드를 \textbf{일반화(Generalization)}하여 어떤 깊이의 모델도 만들 수 있는 건축가가 되어 봅시다.

% --- 4. 개요 ---
\section{Unit Overview}
\begin{summarybox}{핵심 목표}
이 단원은 $L$개의 층을 가진 일반화된 심층 신경망(Deep MLP)을 설계하고 구현합니다.
\begin{itemize}
    \item \textbf{표기법:} 층의 개수가 $L$개일 때의 파라미터($W^{[l]}, b^{[l]}$)와 활성화값($A^{[l]}$)을 정의합니다.
    \item \textbf{원리:} 딥러닝이 데이터를 \textbf{계층적(Hierarchical)}으로 이해하는 방식(점 $\to$ 선 $\to$ 면)을 배웁니다.
    \item \textbf{차원:} 각 층의 뉴런 개수($n^{[l]}$)만 보고도 가중치 행렬의 크기를 즉시 계산해냅니다.
    \item \textbf{구현:} 하드코딩(W1, W2...)을 버리고, `for-loop`와 `Dictionary`를 이용해 유연한 코드를 작성합니다.
\end{itemize}
\end{summarybox}

% --- 5. 용어 정리 ---
\section{Essential Terminology: The Deep Notation}
얕은 신경망에서 쓰던 표기법을 확장합니다. $l$은 현재 층 번호를 의미합니다.

\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{기호} & \textbf{의미} & \textbf{설명} \\ \hline
$L$ & 전체 층 수 & 입력층(0번)을 제외한 층의 개수. \\ \hline
$n^{[l]}$ & $l$번째 층의 뉴런 수 & $n^{[0]}=n_x$ (입력), $n^{[L]}$ (출력). \\ \hline
$g^{[l]}$ & $l$번째 층의 활성화 함수 & 보통 은닉층은 ReLU, 출력층은 Sigmoid. \\ \hline
$A^{[l]}$ & $l$번째 층의 출력 & $A^{[l]} = g^{[l]}(Z^{[l]})$. 다음 층의 입력이 됨. \\ \hline
\end{tabular}
\end{center}

% --- 6. 핵심 개념 상세 설명 ---
\section{Core Concepts: 왜 깊게 쌓는가?}

\subsection{1. Hierarchical Representation (계층적 표현)}
"교수님, 그냥 은닉층 1개에 뉴런 100만 개를 넣는 게(Wide), 10만 개씩 10층 쌓는 것(Deep)보다 낫지 않나요?"
\textbf{아닙니다.} 딥러닝의 힘은 \textbf{'쪼개서 이해하기'}에서 나옵니다.



\begin{analogybox}{사람의 얼굴 인식 과정}
우리의 뇌나 딥러닝 모델은 복잡한 이미지를 한 번에 이해하지 않습니다.
\begin{enumerate}
    \item \textbf{Layer 1 (Low-level):} 픽셀을 보고 가로선, 세로선 같은 \textbf{경계(Edges)}를 찾습니다.
    \item \textbf{Layer 2 (Mid-level):} 선들을 조합해서 눈, 코, 귀 같은 \textbf{부분(Parts)}을 만듭니다.
    \item \textbf{Layer 3 (High-level):} 부분들을 조합해서 \textbf{사람 얼굴(Face)} 전체를 인식합니다.
\end{enumerate}
층을 깊게 쌓으면, 적은 파라미터로도 매우 복잡한 함수(사람 얼굴 등)를 효율적으로 표현할 수 있습니다.
\end{analogybox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\subsection{2. Matrix Dimensions (차원 분석)}
이 부분은 구현과 디버깅의 핵심입니다. 무조건 암기해야 합니다.

$l$번째 층의 가중치 $W^{[l]}$와 편향 $b^{[l]}$의 크기는 다음과 같습니다.
\begin{itemize}
    \item \textbf{$W^{[l]}$ Shape:} $(n^{[l]}, n^{[l-1]})$ $\rightarrow$ (현재 층 뉴런 수, 이전 층 뉴런 수)
    \item \textbf{$b^{[l]}$ Shape:} $(n^{[l]}, 1)$
    \item \textbf{$Z^{[l]}, A^{[l]}$ Shape:} $(n^{[l]}, m)$ $\rightarrow$ (현재 층 뉴런 수, 데이터 개수)
\end{itemize}

\begin{examplebox}{차원 계산 퀴즈}
\textbf{상황:}
입력 특성 $n_x = 12288$ (이미지).
Layer 1 뉴런: 20개.
Layer 2 뉴런: 7개.

\textbf{질문:} $W^{[1]}$과 $W^{[2]}$의 크기는?
\begin{itemize}
    \item $W^{[1]}$: $(n^{[1]}, n^{[0]}) = (20, 12288)$
    \item $W^{[2]}$: $(n^{[2]}, n^{[1]}) = (7, 20)$
\end{itemize}
\end{examplebox}

\vspace{0.5cm}\hrule\vspace{0.5cm}

\section{Implementation: Building Deep Network}

이제 $L$개의 층을 가진 신경망을 만듭니다. `W1`, `W2` 변수를 따로 만들지 않고 `parameters['W' + str(l)]` 형태로 관리하는 것이 핵심입니다.

\begin{lstlisting}[language=Python, caption=L-Layer Deep Neural Network Initialization \& Forward, breaklines=true]
import numpy as np

class DeepNN:
    def __init__(self, layer_dims):
        """
        layer_dims: 각 층의 뉴런 수를 담은 리스트 
                    예: [12288, 20, 7, 5, 1] (4-Layer Network)
        """
        self.params = {}
        self.L = len(layer_dims) - 1 # 입력층 제외한 층 수
        
        for l in range(1, self.L + 1):
            # He Initialization (ReLU 사용 시 필수!)
            # 0.01 대신 np.sqrt(2 / 이전 층 뉴런 수)를 곱함
            self.params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])
            self.params['b' + str(l)] = np.zeros((layer_dims[l], 1))
            
            # 차원 확인 (습관화!)
            assert(self.params['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))

    def forward(self, X):
        """
        [Linear -> ReLU] * (L-1) -> [Linear -> Sigmoid] * 1
        """
        caches = []
        A = X
        L = self.L
        
        # 1. 은닉층 (1 ~ L-1): ReLU
        for l in range(1, L):
            A_prev = A 
            W = self.params['W' + str(l)]
            b = self.params['b' + str(l)]
            
            # Linear
            Z = np.dot(W, A_prev) + b
            # Activation (ReLU)
            A = np.maximum(0, Z)
            
            # 역전파를 위해 저장 (W, b, A_prev, Z)
            caches.append((A_prev, W, b, Z))
            
        # 2. 출력층 (L): Sigmoid (이진 분류)
        W = self.params['W' + str(L)]
        b = self.params['b' + str(L)]
        
        Z = np.dot(W, A) + b
        AL = 1 / (1 + np.exp(-Z)) # Sigmoid
        caches.append((A, W, b, Z))
        
        return AL, caches

# --- 실행 예제 ---
if __name__ == "__main__":
    # [입력(3) -> 은닉(5) -> 은닉(3) -> 출력(1)] 구조
    layers = [3, 5, 3, 1] 
    model = DeepNN(layers)
    
    # 가상의 데이터 (3 features, 4 samples)
    X = np.random.randn(3, 4)
    
    AL, _ = model.forward(X)
    print("Output Shape:", AL.shape) # (1, 4) 예상
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ \& Pitfalls}

\begin{warningbox}{파라미터 초기화: 0.01 vs He Initialization}
얕은 신경망에서는 `* 0.01`로 초기화해도 괜찮았습니다.
하지만 층이 깊어지면($L > 5$), 값이 계속 곱해지면서 신호가 사라지거나 폭발합니다(Vanishing/Exploding Gradient).
따라서 ReLU를 쓸 때는 \textbf{He Initialization} (`np.sqrt(2/n)`)을 쓰는 것이 \textbf{딥러닝의 표준(Standard)}입니다.
\end{warningbox}

\textbf{Q. Cache 리스트는 왜 만드나요?} \\
\textbf{A.} 순전파(Forward)가 끝나면 바로 역전파(Backward)를 해야 합니다. 역전파 수식을 보면 $Z$, $A_{prev}$, $W$ 값이 필요합니다. 이미 계산한 값을 버리지 않고 `caches`에 저장해두면, 다시 계산할 필요 없이 효율적으로 역전파를 수행할 수 있습니다.

% --- 9. 다음 단원 연결 ---
\section*{🔗 다음 단계 (Next Step)}
이제 여러분은 어떤 깊이, 어떤 구조의 신경망도 만들 수 있는 설계 능력을 갖췄습니다. `layers` 리스트에 숫자만 바꿔 넣으면 됩니다.

하지만 깊은 신경망을 학습시키는 것은 생각보다 까다롭습니다. 
다음 시간에는 이 모델을 가지고 \textbf{'고양이 vs 개'} 이미지를 분류하는 실제 프로젝트를 수행하며, 학습 과정에서 발생하는 다양한 문제들을 해결해 보겠습니다.

\vspace{0.5cm}

\begin{summarybox}{단원 요약 (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Deep Learning:} 은닉층을 여러 개 쌓아 계층적 특징(Hierarchical Features)을 학습한다.
    \item \textbf{Dimensions:} $W^{[l]}$의 크기는 $(n^{[l]}, n^{[l-1]})$이다. (현재 층, 이전 층)
    \item \textbf{Implementation:} `for-loop`를 사용하여 $L$번 반복하는 일반화된 코드를 작성한다.
    \item \textbf{Initialization:} 깊은 망에서는 \textbf{He Initialization}을 사용하여 학습 불안정을 막는다.
\end{enumerate}
\end{summarybox}

\end{document}