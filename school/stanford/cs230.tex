\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{sblue}{RGB}{70, 130, 180}
\definecolor{wgray}{RGB}{245, 245, 245}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=#1
}

\newtcolorbox{analogybox}[1]{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ê¸ˆì§€)
}

\newtcolorbox{examplebox}[1]{
    colback=orange!5!white,
    colframe=orange!75!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ê³„ì‚°)
}

% --- ë¬¸ì„œ ì‹œì‘ ---
\begin{document}

% --- ì „ì²´ ëª©ì°¨ (TOC) ---
\tableofcontents
\newpage

% --- 1. ì´ì „ ë‹¨ì› (Dummy) ---
\section*{Chapter 1. Deep Learning Introduction (Completed)}
\addcontentsline{toc}{section}{Chapter 1. Deep Learning Introduction}
\textit{(ì´ì „ ë‹¨ì›ì—ì„œ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ì´ ë¬´ì—‡ì¸ì§€, ê·¸ë¦¬ê³  ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒˆë¡œìš´ ì„ìœ ê°€ ë˜ì—ˆëŠ”ì§€ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ ê·¸ ê±°ëŒ€í•œ ë”¥ëŸ¬ë‹ì´ë¼ëŠ” ê¸°ê³„ë¥¼ êµ¬ì„±í•˜ëŠ” ê°€ì¥ ì‘ì€ ë¶€í’ˆì„ ëœ¯ì–´ë³¼ ì°¨ë¡€ì…ë‹ˆë‹¤.)}

\vspace{1cm}
\hrule
\vspace{1cm}

% --- 2. í˜„ì¬ ë‹¨ì› ì‹œì‘ ---
\section{Logistic Regression as a Neural Network}

% 2.1 ì—°ê²° ë¬¸ì¥
\subsection*{ğŸ”— ì—°ê²° ê³ ë¦¬}
ê±°ëŒ€í•œ ë¹Œë”©ë„ ë²½ëŒ í•œ ì¥ì—ì„œ ì‹œì‘í•˜ë“¯, ì•„ë¬´ë¦¬ ë³µì¡í•œ AI(LLM, AlphaGo ë“±)ë„ ê²°êµ­ **'ë‰´ëŸ°(Neuron)'**ì´ë¼ëŠ” ì‘ì€ ë‹¨ìœ„ì˜ ì§‘í•©ì…ë‹ˆë‹¤. ì´ë²ˆ ë‹¨ì›ì—ì„œëŠ” ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ ê¸°ì´ˆ ë‹¨ìœ„ì¸ **ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)**ë¥¼ í•˜ë‚˜ì˜ ì‹ ê²½ë§ìœ¼ë¡œ í•´ì„í•˜ê³ , ê·¸ ë‚´ë¶€ ë™ì‘ ì›ë¦¬ë¥¼ ì™„ì „íˆ í•´ë¶€í•©ë‹ˆë‹¤.

% 2.2 ê°œìš”
\begin{summarybox}{ğŸ“Œ ë‹¨ì› ê°œìš” (Overview)}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ í•™ìŠµì˜ \textbf{'ê¸°ì´ˆ ì²´ë ¥'}ì„ ë‹¤ì§€ëŠ” êµ¬ê°„ì…ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ëª©í‘œ}: ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ì‹ ê²½ë§ êµ¬ì¡°(Input $\to$ Linear $\to$ Activation)ë¡œ ëª¨ë¸ë§í•©ë‹ˆë‹¤.
    \item \textbf{í•µì‹¬}: ê³„ì‚° ê·¸ë˜í”„ë¥¼ í†µí•´ ìˆœì „íŒŒ(Forward)ì™€ ì—­ì „íŒŒ(Backpropagation)ë¥¼ ìœ ë„í•©ë‹ˆë‹¤.
    \item \textbf{ì´ìœ }: MSE ëŒ€ì‹  \textbf{Binary Cross-Entropy}ë¥¼ ë¹„ìš© í•¨ìˆ˜ë¡œ ì“°ëŠ” ì´ìœ ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„}: Python(NumPy)ì„ ì‚¬ìš©í•˜ì—¬ for-loop ì—†ëŠ” \textbf{ë²¡í„°í™”(Vectorization)} ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% 2.3 ìš©ì–´ ì •ë¦¬
\subsection{í•µì‹¬ ìš©ì–´ ì •ë¦¬ (Terminology)}
\begin{center}
\begin{tabular}{|c|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì„¤ëª… (í•œ ì¤„ ì •ì˜)} \\
\hline
\textbf{íŠ¹ì§• ë²¡í„° (Feature Vector, $x$)} & ì˜ˆì¸¡ì„ ìœ„í•´ ì…ë ¥ë˜ëŠ” ë°ì´í„°ì˜ ì •ë³´ë“¤ (ì˜ˆ: ì´ë¯¸ì§€ì˜ í”½ì…€ê°’) \\
\hline
\textbf{ê°€ì¤‘ì¹˜ (Weight, $w$)} & ì…ë ¥ ì •ë³´ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì¤‘ìš”ë„ (í´ìˆ˜ë¡ ì¤‘ìš”í•œ ì •ë³´) \\
\hline
\textbf{í¸í–¥ (Bias, $b$)} & ì…ë ¥ê³¼ ìƒê´€ì—†ì´ ê¸°ë³¸ì ìœ¼ë¡œ ê°€ì§€ëŠ” ì„±í–¥ í˜¹ì€ ì„ê³„ê°’ \\
\hline
\textbf{ì‹œê·¸ëª¨ì´ë“œ (Sigmoid, $\sigma$)} & ê³„ì‚°ëœ ì ìˆ˜ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë³€í™˜í•˜ëŠ” í™œì„±í™” í•¨ìˆ˜ \\
\hline
\textbf{êµì°¨ ì—”íŠ¸ë¡œí”¼ (Cross-Entropy)} & í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë¹„ìš© í•¨ìˆ˜ (í‹€ë¦´ìˆ˜ë¡ ê°’ì´ ì»¤ì§) \\
\hline
\end{tabular}
\end{center}

% 2.4 í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª…
\subsection{Core Concepts: ì‹ ê²½ë§ì˜ í•´ë¶€}

\subsubsection{1. ë¬¸ì œ ì •ì˜: ì´ì§„ ë¶„ë¥˜ (Binary Classification)}
\textbf{í•œ ì¤„ ìš”ì•½}: ì§ˆë¬¸ì— ëŒ€í•´ YES(1) ë˜ëŠ” NO(0)ë¡œ ë‹µí•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.

\begin{analogybox}{ê³ ì–‘ì´ íƒì§€ê¸°}
ë‹¹ì‹ ì´ ì‚¬ì§„ì„ ë³´ê³  "ì´ê²ƒì€ ê³ ì–‘ì´ì…ë‹ˆê¹Œ?"ë¼ëŠ” ì§ˆë¬¸ì— ë‹µí•´ì•¼ í•œë‹¤ê³  ìƒìƒí•´ë´…ì‹œë‹¤.
\begin{itemize}
    \item **ì…ë ¥($x$):** ì‚¬ì§„ ì†ì˜ í„¸, ê·€ ëª¨ì–‘, ëˆˆë™ì ìƒ‰ê¹” ë“±ì˜ ë‹¨ì„œë“¤.
    \item **ì¶œë ¥($\hat{y}$):** "ê³ ì–‘ì´ì¼ í™•ë¥ ì€ 80%ì…ë‹ˆë‹¤." (ì¦‰, 0.8)
\end{itemize}
\end{analogybox}

\textbf{ê¸°ìˆ ì  ì •ì˜}:
$n_x$ ì°¨ì›ì˜ ì…ë ¥ ë²¡í„° $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì¶œë ¥ $y$ê°€ 1ì¼ í™•ë¥  $\hat{y} = P(y=1 | x)$ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.

---

\subsubsection{2. ë‰´ëŸ°ì˜ êµ¬ì¡°: ì„ í˜• ê²°í•©ê³¼ í™œì„±í™”}
ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë‘ ë‹¨ê³„ì˜ 'ìƒê° ê³¼ì •'ì„ ê±°ì¹©ë‹ˆë‹¤.

\paragraph{Step 1: ì„ í˜• ê²°í•© (Linear Function) - ì ìˆ˜ ë§¤ê¸°ê¸°}
$$z = w^T x + b$$
\begin{itemize}
    \item $w$ (ê°€ì¤‘ì¹˜): ê° ë‹¨ì„œê°€ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€ ê²°ì •í•©ë‹ˆë‹¤. (ì˜ˆ: 'ë¾°ì¡±í•œ ê·€'ëŠ” ê³ ì–‘ì´ íŒë³„ì— ì¤‘ìš”í•¨ $\to$ ë†’ì€ $w$)
    \item $b$ (í¸í–¥): ì…ë ¥ì´ 0ì´ì–´ë„ ê¸°ë³¸ì ìœ¼ë¡œ ê°–ëŠ” ì ìˆ˜ì…ë‹ˆë‹¤. (ì˜ˆ: "ë‚˜ëŠ” ë™ë¬¼ì„ ì¢‹ì•„í•´ì„œ ì¼ë‹¨ ê³ ì–‘ì´ë¡œ ë³´ê³  ì‹¶ì–´" $\to$ ë†’ì€ $b$)
\end{itemize}

\paragraph{Step 2: í™œì„±í™” í•¨ìˆ˜ (Sigmoid) - í™•ë¥ ë¡œ ë³€í™˜}
$$a = \sigma(z) = \frac{1}{1 + e^{-z}}$$
ì„ í˜• ê²°í•©ì˜ ê²°ê³¼ $z$ëŠ” $-\infty$ì—ì„œ $+\infty$ê¹Œì§€ì˜ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ $0 \sim 1$ ì‚¬ì´ì˜ í™•ë¥ ë¡œ ì••ì¶•í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.

\begin{itemize}
    \item $z$ê°€ ë§¤ìš° í¬ë©´ $\to$ $a \approx 1$ (í™•ì‹ )
    \item $z$ê°€ 0ì´ë©´ $\to$ $a = 0.5$ (ë°˜ë°˜)
    \item $z$ê°€ ë§¤ìš° ì‘ìœ¼ë©´ $\to$ $a \approx 0$ (ì•„ë‹˜)
\end{itemize}

---

\subsubsection{3. ë¹„ìš© í•¨ìˆ˜ (Cost Function): ì™œ MSEê°€ ì•„ë‹ê¹Œ?}
ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’($a$)ê³¼ ì‹¤ì œ ì •ë‹µ($y$)ì´ ë‹¤ë¥´ë‹¤ë©´ ëª¨ë¸ì„ 'í˜¼ë‚´ì¤˜ì•¼' í•©ë‹ˆë‹¤. ê·¸ ë²Œì ì„ ë§¤ê¸°ëŠ” ê·œì¹™ì´ ë¹„ìš© í•¨ìˆ˜ì…ë‹ˆë‹¤.

\begin{warningbox}{MSE(í‰ê·  ì œê³± ì˜¤ì°¨)ë¥¼ ì“°ë©´ ì•ˆ ë˜ë‚˜ìš”?}
ì„ í˜• íšŒê·€ì—ì„œëŠ” MSE($\frac{1}{2}(\hat{y}-y)^2$)ë¥¼ ì“°ì§€ë§Œ, ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ ì´ë¥¼ ì“°ë©´ ë¹„ìš© í•¨ìˆ˜ê°€ \textbf{ìš¸í‰ë¶ˆí‰í•œ(Non-Convex)} ëª¨ì–‘ì´ ë©ë‹ˆë‹¤.
ì¦‰, ê²½ì‚¬ í•˜ê°•ë²•ì„ í•  ë•Œ, ì§„ì§œ ìµœì†Œì (Global Minimum)ì´ ì•„ë‹Œ ì›…ë©ì´(Local Optima)ì— ë¹ ì ¸ í•™ìŠµì´ ë©ˆì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{warningbox}

ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ë§¤ë„ëŸ¬ìš´ ê·¸ë¦‡ ëª¨ì–‘(Convex)ì„ ë³´ì¥í•˜ëŠ” **ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼(Log Loss)**ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
$$L(a, y) = -(y \log(a) + (1-y) \log(1-a))$$
\begin{itemize}
    \item ì •ë‹µì´ 1($y=1$)ì¸ë° ì˜ˆì¸¡ì„ 0ì— ê°€ê¹ê²Œ í•˜ë©´($a \to 0$), $-\log(a)$ ë•Œë¬¸ì— ë¹„ìš©ì´ ë¬´í•œëŒ€ë¡œ ì¹˜ì†ŸìŠµë‹ˆë‹¤. (ì—„ì²­ë‚œ ë²Œì !)
\end{itemize}

% 2.5 ê³µì‹ ë° ê³„ì‚° ì˜ˆì‹œ
\subsection{Numerical Example: ì†ìœ¼ë¡œ í’€ì–´ë³´ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€}

ìˆ˜ì‹ì„ ì´í•´í•˜ëŠ” ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ ìˆ«ìë¥¼ ë„£ì–´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{examplebox}{ì‹œí—˜ í•©ê²© ì˜ˆì¸¡ ì‹œë‚˜ë¦¬ì˜¤}
\textbf{ìƒí™©}: í•™ìƒì˜ 'ê³µë¶€ ì‹œê°„($x_1$)'ê³¼ 'ìˆ˜ë©´ ì‹œê°„($x_2$)'ìœ¼ë¡œ 'í•©ê²©($y=1$)'ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë°ì´í„°}: ê³µë¶€ 2ì‹œê°„($x_1=2$), ìˆ˜ë©´ 5ì‹œê°„($x_2=5$). ì‹¤ì œ ê²°ê³¼: í•©ê²©($y=1$).
    \item \textbf{ì´ˆê¸° íŒŒë¼ë¯¸í„°}: $w_1=0.1, w_2=-0.1, b=0.0$ (ì´ˆê¸°í™” ìƒíƒœ)
\end{itemize}

\textbf{Step 1: ì„ í˜• ê³„ì‚° (Forward)}
$$z = (w_1 \cdot x_1) + (w_2 \cdot x_2) + b$$
$$z = (0.1 \cdot 2) + (-0.1 \cdot 5) + 0 = 0.2 - 0.5 = -0.3$$

\textbf{Step 2: í™œì„±í™” (Sigmoid)}
$$a = \frac{1}{1 + e^{-(-0.3)}} = \frac{1}{1 + 1.349} \approx 0.425$$
$\rightarrow$ ëª¨ë¸ì€ í•©ê²© í™•ë¥ ì„ **42.5\%**ë¡œ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤. (ì‹¤ì œëŠ” í•©ê²©ì¸ë°, ì˜ˆì¸¡ì´ í‹€ë ¸ë„¤ìš”!)

\textbf{Step 3: ë¹„ìš© ê³„ì‚° (Loss)}
$$L = -(1 \cdot \log(0.425) + 0 \cdot \log(\dots)) \approx -(-0.855) = 0.855$$
$\rightarrow$ ë²Œì ì€ **0.855**ì…ë‹ˆë‹¤.

\textbf{Step 4: ì—­ì „íŒŒ (Backward) - í•™ìŠµì˜ í•µì‹¬}
ìš°ë¦¬ëŠ” ì •ë‹µ($y=1$)ì— ê°€ê¹Œì›Œì§€ë„ë¡ $w$ë¥¼ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ **ë§ˆë²•ì˜ ê³µì‹** $dz = a - y$ê°€ ë“±ì¥í•©ë‹ˆë‹¤.
$$dz = a - y = 0.425 - 1 = -0.575$$

ì´ì œ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°($dw$)ë¥¼ êµ¬í•©ë‹ˆë‹¤.
$$dw_1 = x_1 \cdot dz = 2 \cdot (-0.575) = -1.15$$
$$dw_2 = x_2 \cdot dz = 5 \cdot (-0.575) = -2.875$$
$$db = dz = -0.575$$

\textbf{Step 5: íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (Gradient Descent)}
í•™ìŠµë¥  $\alpha = 0.1$ì´ë¼ê³  ê°€ì •í•©ì‹œë‹¤.
$$w_1 \leftarrow w_1 - \alpha \cdot dw_1 = 0.1 - 0.1(-1.15) = 0.1 + 0.115 = 0.215$$
$$w_2 \leftarrow w_2 - \alpha \cdot dw_2 = -0.1 - 0.1(-2.875) = -0.1 + 0.2875 = 0.1875$$

\textbf{ê²°ê³¼ í•´ì„}:
$w_1$ (ê³µë¶€ ì‹œê°„ ì¤‘ìš”ë„)ì´ 0.1ì—ì„œ 0.215ë¡œ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. ì¦‰, ëª¨ë¸ì€ "ê³µë¶€ë¥¼ ë§ì´ í• ìˆ˜ë¡ í•©ê²©í•œë‹¤"ëŠ” ê²ƒì„ ë°°ì› ìŠµë‹ˆë‹¤!
\end{examplebox}

% 2.6 êµ¬í˜„ ì½”ë“œ
\subsection{Python Implementation (Vectorization)}

ì´ë¡ ì„ ì‹¤ì œ ì½”ë“œë¡œ ì˜®ê²¨ë´…ì‹œë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œ ê²ƒì€ `for` ë£¨í”„ë¥¼ ì“°ì§€ ì•ŠëŠ” **ë²¡í„°í™”**ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Vectorized Logistic Regression]
import numpy as np

def propagate(w, b, X, Y):
    """
    Arguments:
    w -- ê°€ì¤‘ì¹˜ (n_x, 1)
    b -- í¸í–¥ (scalar)
    X -- ì…ë ¥ ë°ì´í„° (n_x, m) -> mì€ ë°ì´í„° ê°œìˆ˜
    Y -- ì‹¤ì œ ë ˆì´ë¸” (1, m)
    """
    m = X.shape[1]
    
    # --- Forward Propagation (ìˆœì „íŒŒ) ---
    # í–‰ë ¬ ì—°ì‚°ìœ¼ë¡œ mê°œì˜ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ê³„ì‚° (Vectorization)
    Z = np.dot(w.T, X) + b  
    A = 1 / (1 + np.exp(-Z)) # Sigmoid
    
    # Cost ê³„ì‚°
    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))
    
    # --- Backward Propagation (ì—­ì „íŒŒ) ---
    # ìˆ˜í•™ì ìœ¼ë¡œ ìœ ë„ëœ ê³µì‹: dZ = A - Y
    dZ = A - Y
    
    # Gradients ê³„ì‚°
    dw = 1/m * np.dot(X, dZ.T) # ì°¨ì› í™•ì¸: (n, m) * (m, 1) = (n, 1)
    db = 1/m * np.sum(dZ)
    
    return dw, db, cost
\end{lstlisting}

\begin{warningbox}{ì£¼ì˜: Rank-1 Array Pitfall}
NumPyì—ì„œ `a = np.random.randn(5)`ëŠ” `(5,)` í˜•íƒœë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ëŠ” í–‰ ë²¡í„°ë„ ì—´ ë²¡í„°ë„ ì•„ë‹ˆì–´ì„œ ì „ì¹˜(`T`)ë¥¼ í•´ë„ ëª¨ì–‘ì´ ë°”ë€Œì§€ ì•Šì•„ ë²„ê·¸ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ `a = np.random.randn(5, 1)` ì²˜ëŸ¼ **ì°¨ì›ì„ ëª…ì‹œ**í•˜ì‹­ì‹œì˜¤.
\end{warningbox}

% 2.7 FAQ
\subsection{ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)}

\textbf{Q1. í¸í–¥(Bias) $b$ëŠ” ì™œ í•„ìš”í•œê°€ìš”? ì—†ìœ¼ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
A. ì›ì ì„ ë°˜ë“œì‹œ ì§€ë‚˜ì•¼ í•œë‹¤ëŠ” ì œì•½ì´ ìƒê¹ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³µë¶€ë¥¼ í•˜ë‚˜ë„ ì•ˆ í–ˆì–´ë„($x=0$) í•©ê²©í•  í™•ë¥ ì´ 0ì´ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤. $b$ëŠ” ê·¸ë˜í”„ë¥¼ ì¢Œìš°ë¡œ ì´ë™ì‹œì¼œ ë°ì´í„°ì— ë” ì˜ ë§ë„ë¡ í•´ì£¼ëŠ” 'ìœ ì—°ì„±'ì„ ì œê³µí•©ë‹ˆë‹¤.

\textbf{Q2. ì´ˆê¸°í™”í•  ë•Œ $w$ë¥¼ 0ìœ¼ë¡œ ë‘¬ë„ ë˜ë‚˜ìš”?} \\
A. \textbf{ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œëŠ” ê°€ëŠ¥í•©ë‹ˆë‹¤.} ë¹„ìš© í•¨ìˆ˜ê°€ ë³¼ë¡(Convex)í•˜ê¸° ë•Œë¬¸ì— ì–´ë””ì„œ ì‹œì‘í•˜ë“  ë°”ë‹¥(ìµœì í•´)ìœ¼ë¡œ êµ´ëŸ¬ê°‘ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‚˜ì¤‘ì— ë°°ìš¸ ì‹¬ì¸µ ì‹ ê²½ë§(Deep Network)ì—ì„œëŠ” ì ˆëŒ€ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤(ëŒ€ì¹­ì„± ë¬¸ì œ).

\textbf{Q3. í•™ìŠµë¥ (Learning Rate)ì´ ë„ˆë¬´ í¬ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?} \\
A. ë³´í­ì´ ë„ˆë¬´ ì»¤ì„œ ìµœì ì ì„ ì§€ë‚˜ì³ ë²„ë¦¬ê±°ë‚˜(Overshooting), ì˜ì›íˆ ìˆ˜ë ´í•˜ì§€ ì•Šê³  ë°œì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµ ì†ë„ê°€ ë„ˆë¬´ ëŠë ¤ì§‘ë‹ˆë‹¤.

% 2.8 ë‹¤ìŒ ë‹¨ì› ì˜ˆê³  ë° ìš”ì•½
\vspace{1cm}
\hrule
\vspace{0.5cm}

\begin{summarybox}{ğŸ“ ë‹¨ì› ìš”ì•½ (Chapter Summary)}
\begin{enumerate}
    \item ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ ì‘ì€ ë‹¨ìœ„ì¸ **1-Layer Neural Network**ì´ë‹¤.
    \item êµ¬ì¡°: $z = w^Tx + b$ (ì„ í˜•) $\rightarrow$ $a = \sigma(z)$ (ë¹„ì„ í˜• í™œì„±í™”).
    \item í•™ìŠµ: **ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼**ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°•ë²•ì„ ìˆ˜í–‰í•œë‹¤.
    \item êµ¬í˜„: $m$ê°œì˜ ë°ì´í„°ë¥¼ `for`ë¬¸ ì—†ì´ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ **Vectorization(í–‰ë ¬ ì—°ì‚°)**ì„ ì‚¬ìš©í•œë‹¤.
\end{enumerate}
\end{summarybox}

\subsection*{ğŸ”œ ë‹¤ìŒ ë‹¨ì› ì˜ˆê³ }
ì¶•í•˜í•©ë‹ˆë‹¤! ì—¬ëŸ¬ë¶„ì€ ì´ì œ ì‹ ê²½ë§ì˜ 'ë‡Œì„¸í¬' í•˜ë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¥ **[Chapter 3. Shallow Neural Networks]**ì—ì„œëŠ” ì´ ì„¸í¬ë“¤ì„ ì˜†ìœ¼ë¡œ ë‚˜ë€íˆ ì—°ê²°í•˜ê³ , ë’¤ë¡œ ì¸µì¸µì´ ìŒ“ì•„ì„œ ë” ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” **ì€ë‹‰ì¸µ(Hidden Layer)**ì˜ ë§ˆë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{pointblue}{RGB}{0, 102, 204}
\definecolor{analogygreen}{RGB}{0, 153, 76}
\definecolor{warningred}{RGB}{204, 0, 0}
\definecolor{exampleorange}{RGB}{255, 128, 0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{pointblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=pointblue!5!white,
    colframe=pointblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=warningred!5!white,
    colframe=warningred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Foundations of Neural Networks: \\ Logistic Regression as a Neural Network}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture (Introduction) \textit{- Completed}
    \item[\textbf{Chapter 2.}] \textbf{Logistic Regression as a Neural Network (Current Unit)}
    \begin{itemize}
        \item 2.1 Overview \& Terminology
        \item 2.2 Neural Structure (The Architecture)
        \item 2.3 The "Admission Officer" Analogy
        \item 2.4 Cost Function \& Optimization
        \item 2.5 Implementation (Vectorization)
    \end{itemize}
    \item[Chapter 3.] Shallow Neural Networks (Next Unit)
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„, ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ì´ë¼ëŠ” ê±°ëŒ€í•œ ìˆ²(Big Picture)ì„ ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ì œ í˜„ë¯¸ê²½ì„ êº¼ë‚´ ë“¤ ì‹œê°„ì…ë‹ˆë‹¤. ìˆ²ì„ ì´ë£¨ëŠ” ê°€ì¥ ì‘ì€ ë‹¨ìœ„ì¸ **ë‚˜ë¬´(ë‰´ëŸ°)** í•˜ë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ í•´ë¶€í•´ ë´…ì‹œë‹¤. ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ë‹¨ìˆœí•œ í†µê³„ ê¸°ë²•ì´ ì•„ë‹Œ, **'ê°€ì¥ ì–•ì€ ì‹ ê²½ë§(Shallow Neural Network)'**ìœ¼ë¡œ ì´í•´í•˜ëŠ” ê²ƒì´ ë”¥ëŸ¬ë‹ ë§ˆìŠ¤í„°ì˜ ì²«ê±¸ìŒì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ìš”ì•½}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìµœì†Œ ë‹¨ìœ„ì¸ **'ë‹¨ì¼ ë‰´ëŸ°'**ì˜ ì‘ë™ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ëª©í‘œ:} ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¥¼ ì…ë ¥ $\to$ ì„ í˜• ê³„ì‚° $\to$ í™œì„±í™” $\to$ ì¶œë ¥ì˜ ì‹ ê²½ë§ êµ¬ì¡°ë¡œ ì¬í•´ì„í•©ë‹ˆë‹¤.
    \item \textbf{í•µì‹¬:} ê³„ì‚° ê·¸ë˜í”„ë¥¼ í†µí•´ ìˆœì „íŒŒ(Forward)ì™€ ì—­ì „íŒŒ(Backward)ì˜ ìˆ˜í•™ì  íë¦„ì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} `for-loop` ì—†ì´ í–‰ë ¬ ì—°ì‚°(Vectorization)ì„ ì‚¬ìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    \item \textbf{ì´ìœ :} MSE ëŒ€ì‹  **Binary Cross-Entropy**ë¥¼ ë¹„ìš© í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ë³¼ë¡(Convex) ìµœì í™” ê´€ì ì—ì„œ ë°°ì›ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
ë”¥ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë“¤ì´ ìˆ¨ ì‰¬ë“¯ ì‚¬ìš©í•˜ëŠ” ìš©ì–´ë“¤ì…ë‹ˆë‹¤.

\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ê¸°í˜¸} & \textbf{ìš©ì–´} & \textbf{í•œ ì¤„ ì •ì˜} \\ \hline
$x$ & ì…ë ¥ (Input) & íŒë‹¨ì˜ ê·¼ê±°ê°€ ë˜ëŠ” ë°ì´í„° (ì˜ˆ: ì´ë¯¸ì§€ í”½ì…€ê°’) \\ \hline
$w$ & ê°€ì¤‘ì¹˜ (Weight) & ê° ì…ë ¥ ì •ë³´ì˜ ì¤‘ìš”ë„ (í´ìˆ˜ë¡ ê²°ê³¼ì— í° ì˜í–¥) \\ \hline
$b$ & í¸í–¥ (Bias) & ì…ë ¥ê³¼ ë¬´ê´€í•œ ê¸°ë³¸ ì„±í–¥ í˜¹ì€ ì„ê³„ê°’ \\ \hline
$z$ & ì„ í˜• ê²°ê³¼ & ê°€ì¤‘ì¹˜ì™€ ì…ë ¥ì„ ê³±í•˜ê³  ë”í•œ 1ì°¨ ì ìˆ˜ ($w^Tx + b$) \\ \hline
$\sigma(z)$ & í™œì„±í™” í•¨ìˆ˜ & ì ìˆ˜($z$)ë¥¼ í™•ë¥ ($0 \sim 1$)ë¡œ ë³€í™˜í•˜ëŠ” í•„í„° (Sigmoid) \\ \hline
$\hat{y}$ & ì˜ˆì¸¡ê°’ (Output) & ëª¨ë¸ì´ ì¶”ì¸¡í•œ ì •ë‹µ í™•ë¥  ($a$ë¼ê³ ë„ ì”€) \\ \hline
$L$ & ì†ì‹¤ (Loss) & ì˜ˆì¸¡ì´ í‹€ë ¸ì„ ë•Œ ë¶€ê³¼í•˜ëŠ” ë²Œì  (í•˜ë‚˜ì˜ ë°ì´í„°) \\ \hline
$J$ & ë¹„ìš© (Cost) & ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ ì†ì‹¤ì˜ í‰ê·  (ì „ì²´ ì„±ì í‘œ) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë‰´ëŸ°ì˜ í•´ë¶€}

\subsection{1. ì‹ ê²½ë§ì  êµ¬ì¡° (The Architecture)}
ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë‘ ë‹¨ê³„ì˜ ìƒê° ê³¼ì •ì„ ê±°ì¹˜ëŠ” **ë‹¨ì¼ ë‰´ëŸ°**ì…ë‹ˆë‹¤.

\textbf{Step 1: ì„ í˜• ê²°í•© (Linear Combination)} \\
ì…ë ¥ëœ ì •ë³´ë“¤ì„ ì¤‘ìš”ë„($w$)ì— ë”°ë¼ í•©ì‚°í•©ë‹ˆë‹¤.
$$ z = w^T x + b $$

\textbf{Step 2: ë¹„ì„ í˜• í™œì„±í™” (Activation)} \\
ê³„ì‚°ëœ ì ìˆ˜($z$)ëŠ” $-\infty \sim \infty$ ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ë¥¼ í™•ë¥ ($0 \sim 1$)ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ **ì‹œê·¸ëª¨ì´ë“œ(Sigmoid)** í•¨ìˆ˜ë¥¼ í†µê³¼ì‹œí‚µë‹ˆë‹¤.
$$ \hat{y} = a = \sigma(z) = \frac{1}{1 + e^{-z}} $$

\begin{analogybox}{ëŒ€í•™ ì…í•™ ì‚¬ì •ê´€ ë¹„ìœ }
ì´ ë‰´ëŸ°ì„ **'ê¹ê¹í•œ ì…í•™ ì‚¬ì •ê´€'**ì´ë¼ê³  ìƒìƒí•´ ë´…ì‹œë‹¤.
\begin{enumerate}
    \item \textbf{ì…ë ¥ ($x$):} í•™ìƒì˜ ë‚´ì‹  ì„±ì ($x_1$), ìˆ˜ëŠ¥ ì ìˆ˜($x_2$), ë´‰ì‚¬ ì‹œê°„($x_3$).
    \item \textbf{ê°€ì¤‘ì¹˜ ($w$):} ì‚¬ì •ê´€ì˜ í‰ê°€ ê¸°ì¤€. (ìˆ˜ëŠ¥ì´ ì¤‘ìš”í•˜ë©´ $w_2$ê°€ í¼).
    \item \textbf{í¸í–¥ ($b$):} í•™êµì˜ ê´€ëŒ€í•¨. (ì ìˆ˜ê°€ ë‚®ì•„ë„ ì¼ë‹¨ ê¸ì •ì ìœ¼ë¡œ ë³´ë©´ $b > 0$).
    \item \textbf{ì„ í˜• ê²°í•© ($z$):} $z = (x_1 w_1 + x_2 w_2 + x_3 w_3) + b$. (í•™ìƒì˜ ì´ì  ê³„ì‚°).
    \item \textbf{í™œì„±í™” ($\sigma$):} ì´ì ì´ 1000ì ì´ë“  -500ì ì´ë“ , í•©ê²© í™•ë¥ ì€ 0\%ì—ì„œ 100\% ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤. ì‹œê·¸ëª¨ì´ë“œëŠ” ì´ ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.
\end{enumerate}
\end{analogybox}

\subsection{2. ë¹„ìš© í•¨ìˆ˜ (Cost Function): ì™œ MSEê°€ ì•„ë‹Œê°€?}
ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ì •ë‹µì„ ë§íˆë©´ ì¹­ì°¬í•˜ê³ , í‹€ë¦¬ë©´ ë²Œì (Cost)ì„ ì¤˜ì•¼ í•©ë‹ˆë‹¤. ì„ í˜• íšŒê·€ì—ì„œ ì“°ë˜ MSE(í‰ê·  ì œê³± ì˜¤ì°¨)ë¥¼ ì“°ë©´ ì•ˆ ë ê¹Œìš”?

\begin{warningbox}{MSE ì‚¬ìš© ê¸ˆì§€ ê²½ë³´}
ë¡œì§€ìŠ¤í‹± íšŒê·€(Sigmoid í¬í•¨)ì— MSEë¥¼ ì ìš©í•˜ë©´ ë¹„ìš© í•¨ìˆ˜ ê·¸ë˜í”„ê°€ **ìš¸í‰ë¶ˆí‰í•œ ê³„ë€íŒ ëª¨ì–‘(Non-Convex)**ì´ ë©ë‹ˆë‹¤. ê²½ì‚¬ í•˜ê°•ë²•ì„ ì“¸ ë•Œ, ê°€ì¥ ê¹Šì€ ê³¨ì§œê¸°(Global Minimum)ê°€ ì•„ë‹Œ ì—‰ëš±í•œ ì›…ë©ì´(Local Optima)ì— ë¹ ì ¸ í•™ìŠµì´ ë©ˆì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{warningbox}

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” **ë§¤ë„ëŸ¬ìš´ ê·¸ë¦‡ ëª¨ì–‘(Convex)**ì„ ë³´ì¥í•˜ëŠ” **ë¡œê·¸ ì†ì‹¤(Binary Cross-Entropy)**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
$$ J(w, b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)} \log(\hat{y}^{(i)}) + (1-y^{(i)}) \log(1-\hat{y}^{(i)})] $$
\begin{itemize}
    \item ì •ë‹µì´ 1ì¸ë° 0ì´ë¼ê³  ì˜ˆì¸¡í•˜ë©´? $-\log(0) = \infty$ (ë¬´í•œëŒ€ì˜ ë²Œì !)
    \item í‹€ë¦´ìˆ˜ë¡ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ í° í˜ë„í‹°ë¥¼ ë¶€ì—¬í•˜ì—¬ ë¹ ë¥´ê²Œ ìˆ˜ì •í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
\end{itemize}

% --- 7. ê³µì‹/ì ˆì°¨ + ì˜ˆì‹œ ê³„ì‚° ---
\section{Numerical Example: ì†ìœ¼ë¡œ í‘¸ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€}

ìˆ˜ì‹ì„ ëˆˆìœ¼ë¡œë§Œ ë³´ë©´ ì´í•´ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìˆ«ìë¥¼ ë„£ì–´ë´…ì‹œë‹¤.

\begin{examplebox}{ì•¼ê°„ ììœ¨í•™ìŠµ ë„ë§ì ì¡ê¸° ì‹œë‚˜ë¦¬ì˜¤}
\textbf{ìƒí™©:} ì„ ìƒë‹˜(ëª¨ë¸)ì´ í•™ìƒì˜ í–‰ë™ì„ ë³´ê³  'ë„ë§($y=1$)' ê°ˆì§€ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì…ë ¥ $x$:} ê°€ë°©ì„ ìŒˆ($x_1=1$), ëˆˆì¹˜ë¥¼ ë´„($x_2=5$, ë§¤ìš° ë§ì´ ë´„).
    \item \textbf{ê°€ì¤‘ì¹˜ $w$:} $w_1=2.0$ (ê°€ë°© ì‹¸ëŠ” ê±´ ì¤‘ìš”), $w_2=0.5$ (ëˆˆì¹˜ëŠ” ëœ ì¤‘ìš”).
    \item \textbf{í¸í–¥ $b$:} $-3.0$ (ì„ ìƒë‹˜ì€ ê¸°ë³¸ì ìœ¼ë¡œ í•™ìƒì„ ë¯¿ìŒ).
\end{itemize}

\textbf{1. ìˆœì „íŒŒ (Forward): ì˜ˆì¸¡í•˜ê¸°} \\
ì„ í˜• ì ìˆ˜ ê³„ì‚°:
$$ z = (1 \times 2.0) + (5 \times 0.5) + (-3.0) = 2.0 + 2.5 - 3.0 = 1.5 $$
í™œì„±í™”(í™•ë¥  ë³€í™˜):
$$ a = \frac{1}{1 + e^{-1.5}} \approx \frac{1}{1 + 0.223} \approx 0.817 $$
$\rightarrow$ ì„ ìƒë‹˜ì€ ì´ í•™ìƒì´ ë„ë§ê°ˆ í™•ë¥ ì„ **81.7\%**ë¡œ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.

\textbf{2. ì—­ì „íŒŒ (Backward): í•™ìŠµí•˜ê¸°} \\
ì‹¤ì œ ê²°ê³¼: í•™ìƒì´ ë„ë§ê°”ìŠµë‹ˆë‹¤ ($y=1$).
ì˜¤ì°¨ ê³„ì‚° (**Magic Step**):
$$ dz = a - y = 0.817 - 1 = -0.183 $$
ì´ ê°’ì€ "ë‚´ê°€ 0.183ë§Œí¼ ë¶€ì¡±í•˜ê²Œ ì˜ˆì¸¡í–ˆêµ¬ë‚˜"ë¼ëŠ” ì§ê´€ì ì¸ ì˜¤ì°¨ì…ë‹ˆë‹¤.
ì´ì œ $w$ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•´ ë¯¸ë¶„ê°’(Gradient)ì„ êµ¬í•©ë‹ˆë‹¤.
$$ dw_1 = x_1 \times dz = 1 \times (-0.183) = -0.183 $$
$$ dw_2 = x_2 \times dz = 5 \times (-0.183) = -0.915 $$

\textbf{3. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ê²½ì‚¬ í•˜ê°•ë²•)} \\
í•™ìŠµë¥  $\alpha = 0.1$ì´ë¼ë©´:
$$ w_1 \leftarrow 2.0 - 0.1(-0.183) = 2.0183 $$
ê²°ê³¼: $w_1$ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. ì¦‰, "ê°€ë°©ì„ ì‹¸ëŠ” í–‰ë™"ì´ ë„ë§ì— ë” ì¤‘ìš”í•œ ë‹¨ì„œë¼ê³  í•™ìŠµí–ˆìŠµë‹ˆë‹¤!
\end{examplebox}

% --- 8. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Vectorization (ë²¡í„°í™”)}

ì´ì œ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤. $m$ê°œì˜ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ `for` ë£¨í”„ë¥¼ ì“°ë©´ ëŠë¦½ë‹ˆë‹¤. `numpy`ì˜ í–‰ë ¬ ì—°ì‚°ì„ ì¨ì•¼ í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Vectorized Logistic Regression Unit]
import numpy as np

class LogisticUnit:
    def __init__(self, input_dim):
        # wëŠ” (input_dim, 1) í¬ê¸°ì˜ ì—´ ë²¡í„°ë¡œ ì´ˆê¸°í™”
        # ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤ (Deep NNì€ ì•ˆë¨!)
        self.w = np.zeros((input_dim, 1))
        self.b = 0.0

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def propagate(self, X, Y):
        """
        X: (n_x, m) í–‰ë ¬ - mê°œì˜ ë°ì´í„°ê°€ ì—´ë¡œ ë‚˜ì—´ë¨
        Y: (1, m) ë²¡í„° - ì •ë‹µ ë ˆì´ë¸”
        """
        m = X.shape[1] # ë°ì´í„° ê°œìˆ˜
        
        # 1. Forward Propagation (í•œ ë²ˆì— mê°œ ê³„ì‚°)
        # (n_x, 1).T @ (n_x, m) + scalar -> (1, m)
        Z = np.dot(self.w.T, X) + self.b 
        A = self.sigmoid(Z)             
        
        # ë¹„ìš© ê³„ì‚° (Binary Cross-Entropy)
        cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))
        
        # 2. Backward Propagation (í•µì‹¬: Chain Rule)
        # dZ = A - Y (ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´) -> ì´ ì‹ í•˜ë‚˜ë¡œ ëë‚©ë‹ˆë‹¤!
        dZ = A - Y
        
        # Gradient ê³„ì‚° (Vectorized)
        dw = 1/m * np.dot(X, dZ.T)
        db = 1/m * np.sum(dZ)
        
        return {"dw": dw, "db": db}, cost
\end{lstlisting}

% --- 9. ìì£¼ í•˜ëŠ” ì§ˆë¬¸ (FAQ) ---
\section{FAQ: ì´ˆì‹¬ìê°€ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸}

\begin{itemize}
    \item \textbf{Q1. í¸í–¥($b$)ì´ ì™œ í•„ìš”í•œê°€ìš”? ì—†ìœ¼ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
    \textbf{A.} í¸í–¥ì´ ì—†ìœ¼ë©´ ê²°ì • ê²½ê³„ê°€ ë¬´ì¡°ê±´ ì›ì (0,0)ì„ ì§€ë‚˜ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì•„ë¬´ëŸ° í–‰ë™ì„ ì•ˆ í•´ë„($x=0$) í•©ê²©ë¥ ì´ 50\%ê°€ ë„˜ì„ ìˆ˜ ìˆëŠ”ë°, $b$ê°€ ì—†ìœ¼ë©´ ì´ë¥¼ í‘œí˜„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. $b$ëŠ” ê·¸ë˜í”„ë¥¼ ì¢Œìš°ë¡œ ì›€ì§ì´ëŠ” 'ìœ ì—°ì„±'ì„ ì¤ë‹ˆë‹¤.

    \item \textbf{Q2. ê°€ì¤‘ì¹˜ $w$ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ë„ í•™ìŠµì´ ë˜ë‚˜ìš”?} \\
    \textbf{A.} \textbf{ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œëŠ” YES.} ë¹„ìš© í•¨ìˆ˜ê°€ ë°¥ê·¸ë¦‡ ëª¨ì–‘(Convex)ì´ë¼ì„œ ì–´ë””ì„œ ì‹œì‘í•˜ë“  ë°”ë‹¥ìœ¼ë¡œ êµ´ëŸ¬ê°‘ë‹ˆë‹¤. í•˜ì§€ë§Œ ë‚˜ì¤‘ì— ë°°ìš¸ ë‹¤ì¸µ ì‹ ê²½ë§ì—ì„œëŠ” ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤(ëŒ€ì¹­ì„± ë¬¸ì œ).

    \item \textbf{Q3. í•™ìŠµë¥ (Learning Rate)ì„ ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
    \textbf{A.} ë„ˆë¬´ í¬ë©´ ì •ë‹µì„ ì§€ë‚˜ì³ ë°œì‚°(Overshooting)í•˜ê³ , ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì´ ì˜ì›íˆ ê±¸ë¦½ë‹ˆë‹¤. ë³´í†µ 0.01, 0.001 ë“±ìœ¼ë¡œ ì‹œì‘í•´ ë¹„ìš©(Cost) ê·¸ë˜í”„ê°€ ì˜ ë‚´ë ¤ê°€ëŠ”ì§€ ë³´ë©° ì¡°ì •í•©ë‹ˆë‹¤.
\end{itemize}

% --- 10. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì¶•í•˜í•©ë‹ˆë‹¤! ì—¬ëŸ¬ë¶„ì€ ë”¥ëŸ¬ë‹ì˜ ê°€ì¥ ê¸°ë³¸ ë¶€í’ˆì¸ 'ë‰´ëŸ°' í•˜ë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê³  êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì¥ **[Chapter 3. Shallow Neural Networks]**ì—ì„œëŠ” ì´ ë‰´ëŸ°ë“¤ì„ ì˜†ìœ¼ë¡œ ë‚˜ë€íˆ ë°°ì¹˜í•˜ê³ , ë’¤ë¡œ ì—°ê²°í•˜ì—¬ **ì€ë‹‰ì¸µ(Hidden Layer)**ì„ ë§Œë“œëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤. ë‰´ëŸ° í•˜ë‚˜ë¡œëŠ” ë‹¨ìˆœí•œ ì„ í˜• ë¶„ë¥˜ë§Œ ê°€ëŠ¥í•˜ì§€ë§Œ, ë‰´ëŸ°ì´ ëª¨ì´ë©´ ë³µì¡í•œ ë¹„ì„ í˜• ë¬¸ì œë„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{êµ¬ì¡°:} ë¡œì§€ìŠ¤í‹± íšŒê·€ = $z = w^Tx + b$ (ì„ í˜•) $\to$ $\sigma(z)$ (ë¹„ì„ í˜• í™œì„±í™”).
    \item \textbf{í•™ìŠµ:} **Cross-Entropy** ë¹„ìš© í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ $w, b$ë¥¼ ì—…ë°ì´íŠ¸.
    \item \textbf{ìˆ˜í•™:} ì—­ì „íŒŒì˜ í•µì‹¬ ë¯¸ë¶„ ê°’ì€ $dZ = A - Y$ (ì˜ˆì¸¡ - ì •ë‹µ).
    \item \textbf{êµ¬í˜„:} `for-loop` ëŒ€ì‹  `np.dot`ì„ í™œìš©í•œ **Vectorization** í•„ìˆ˜.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{orange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (í•µì‹¬ ì£¼ì˜ì‚¬í•­)
}

\newtcolorbox{examplebox}[1]{
    colback=orange!5!white,
    colframe=orange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Foundations of Neural Networks: \\ Cost Function \& Gradient Descent}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network
    \begin{itemize}
        \item 2.1 Architecture \& Forward Propagation \textit{- Completed}
        \item \textbf{2.2 Cost Function \& Gradient Descent (Current Unit)}
        \begin{itemize}
            \item Overview: Loss vs. Cost
            \item The Engine: Gradient Descent
            \item Why Log Loss? (Convexity)
            \item Implementation
        \end{itemize}
    \end{itemize}
    \item[Chapter 3.] Shallow Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ë‚œ ì‹œê°„ì— ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ 'ë‡Œ êµ¬ì¡°(Architecture)'ë¥¼ ë§Œë“¤ê³ , ì…ë ¥ ì‹ í˜¸ë¥¼ í˜ë ¤ë³´ë‚´ëŠ” 'ìˆœì „íŒŒ(Forward Propagation)'ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆ ì´ ì‹ ê²½ë§ì€ ê°“ íƒœì–´ë‚œ ì•„ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ì„¸ìƒì— ëŒ€í•´ ì•„ë¬´ê²ƒë„ ëª¨ë¥´ì£ (íŒŒë¼ë¯¸í„°ê°€ ì´ˆê¸°í™”ëœ ìƒíƒœ). ì´ì œ ì´ ì•„ì´ë¥¼ ê°€ë¥´ì¹  ì‹œê°„ì…ë‹ˆë‹¤. í•™ìŠµì´ë€ **"ë‚´ê°€ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ í™•ì¸í•˜ê³ (Cost), ê³ ì³ ë‚˜ê°€ëŠ”(Gradient Descent) ê³¼ì •"**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ìœ ë‹›ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ **'ì—”ì§„(Engine)'**ì„ ë‹¤ë£¹ë‹ˆë‹¤. ì°¨ì²´(ëª¨ë¸ êµ¬ì¡°)ê°€ ì¢‹ì•„ë„ ì—”ì§„(í•™ìŠµ ì•Œê³ ë¦¬ì¦˜)ì´ ì—†ìœ¼ë©´ ì›€ì§ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ë¶„:} ë°ì´í„° í•˜ë‚˜ì— ëŒ€í•œ ì˜¤ì°¨(Loss)ì™€ ì „ì²´ ì„±ì í‘œ(Cost)ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    \item \textbf{ì´ìœ :} ì™œ MSE ëŒ€ì‹  **Log Loss(Binary Cross-Entropy)**ë¥¼ ì¨ì•¼ í•˜ëŠ”ì§€ 'ì§€í˜•(Topology)' ê´€ì ì—ì„œ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ì‚°ì—ì„œ ë‚´ë ¤ì˜¤ëŠ” ë°©ë²•ì¸ **ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)**ì˜ ì›ë¦¬ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì¡°ì ˆ:} í•™ìŠµë¥ (Learning Rate)ì´ í•™ìŠµ ì†ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{í•œ ì¤„ í•µì‹¬ ìš”ì•½} \\ \hline
\textbf{ì†ì‹¤ í•¨ìˆ˜ (Loss)} & $L(\hat{y}, y)$ & ë°ì´í„° \textbf{ìƒ˜í”Œ 1ê°œ}ì— ëŒ€í•œ ì˜¤ì°¨ (ì‘ì„ìˆ˜ë¡ ì¢‹ìŒ) \\ \hline
\textbf{ë¹„ìš© í•¨ìˆ˜ (Cost)} & $J(w, b)$ & ì „ì²´ í•™ìŠµ ë°ì´í„°($m$ê°œ)ì— ëŒ€í•œ \textbf{Lossì˜ í‰ê· } \\ \hline
\textbf{ë³¼ë¡ì„± (Convexity)} & - & ë°¥ê·¸ë¦‡ì²˜ëŸ¼ ë§¤ë„ëŸ¬ìš´ ëª¨ì–‘ (ìµœì†Œì ì´ í•˜ë‚˜ë¿ì¸ ì•ˆì „í•œ ì§€í˜•) \\ \hline
\textbf{ê¸°ìš¸ê¸° (Gradient)} & $dw, db$ & í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€íŒŒë¥¸ ê²½ì‚¬ì˜ ë°©í–¥ \\ \hline
\textbf{í•™ìŠµë¥  (Learning Rate)} & $\alpha$ & í•œ ë²ˆ ì—…ë°ì´íŠ¸í•  ë•Œ ì´ë™í•˜ëŠ” ë³´í­ì˜ í¬ê¸° \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í•™ìŠµì˜ ë§¤ì»¤ë‹ˆì¦˜}

\subsection{1. Loss vs. Cost (ì˜¤ì°¨ì˜ ì •ì˜)}
\textbf{í•œ ì¤„ ìš”ì•½:} LossëŠ” 'ìª½ì§€ì‹œí—˜ ì ìˆ˜', CostëŠ” 'í•™ê¸°ë§ í‰ê·  ì„±ì 'ì…ë‹ˆë‹¤.

\begin{analogybox}{ì‹œí—˜ ì ìˆ˜ ë¹„ìœ }
\begin{itemize}
    \item \textbf{Loss Function ($L$):} 1ë²ˆ í•™ìƒì´ ë¬¸ì œë¥¼ í‹€ë ¸ìŠµë‹ˆë‹¤. ì´ í•™ìƒ í•˜ë‚˜ì˜ ì˜¤ì°¨ì…ë‹ˆë‹¤.
    \item \textbf{Cost Function ($J$):} ìš°ë¦¬ ë°˜ 30ëª… ì „ì²´ì˜ í‰ê·  ì˜¤ì°¨ì…ë‹ˆë‹¤. ì„ ìƒë‹˜(ëª¨ë¸)ì˜ ëª©í‘œëŠ” íŠ¹ì • í•™ìƒë§Œ ì˜ ê°€ë¥´ì¹˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ë°˜ ì „ì²´ì˜ í‰ê·  ì„±ì ($J$)ì„ ì¢‹ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

$$ J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) $$

---

\subsection{2. Why Log Loss? (MSEì˜ í•¨ì •)}
\textbf{í•œ ì¤„ ìš”ì•½:} ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„œ MSEë¥¼ ì“°ë©´ í•¨ì •ì´ ë§ì€ ì‚°ì´ ë˜ì§€ë§Œ, Log Lossë¥¼ ì“°ë©´ ë§¤ë„ëŸ¬ìš´ ë°¥ê·¸ë¦‡ì´ ë©ë‹ˆë‹¤.

\textbf{ê¸°ìˆ ì  ì •ì˜:}
ì„ í˜• íšŒê·€ì™€ ë‹¬ë¦¬ Sigmoid í•¨ìˆ˜ê°€ í¬í•¨ëœ ë¡œì§€ìŠ¤í‹± íšŒê·€ì— MSE($\frac{1}{2}(\hat{y}-y)^2$)ë¥¼ ì ìš©í•˜ë©´ ë¹„ìš© í•¨ìˆ˜ê°€ **ë¹„ë³¼ë¡(Non-Convex)** í˜•íƒœê°€ ë©ë‹ˆë‹¤. ì´ëŠ” ìˆ˜ë§ì€ **êµ­ì†Œ ìµœì í•´(Local Optima)**ë¥¼ ë§Œë“­ë‹ˆë‹¤.



\begin{warningbox}{MSEë¥¼ ì“°ë©´ ì•ˆ ë˜ëŠ” ì´ìœ }
ìœ„ ê·¸ë¦¼ì˜ ì˜¤ë¥¸ìª½(Non-Convex)ì„ ë³´ì„¸ìš”. ìš¸í‰ë¶ˆí‰í•œ ì§€í˜•ì—ì„œëŠ” êµ¬ìŠ¬ì„ êµ´ë ¸ì„ ë•Œ ê°€ì¥ ê¹Šì€ ë°”ë‹¥(Global Minimum)ì´ ì•„ë‹ˆë¼, ì¤‘ê°„ì— ìˆëŠ” ì‘ì€ ì›…ë©ì´(Local Minimum)ì— ê°‡í˜€ë²„ë¦½ë‹ˆë‹¤. í•™ìŠµì´ ë§í–ˆë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
ë°˜ë©´, **ë¡œê·¸ ì†ì‹¤(Log Loss)**ì„ ì‚¬ìš©í•˜ë©´ ì™¼ìª½(Convex)ì²˜ëŸ¼ ë§¤ë„ëŸ¬ìš´ ê·¸ë¦‡ ëª¨ì–‘ì´ ë˜ì–´, ì–´ë””ì„œ ì‹œì‘í•˜ë“  ë°”ë‹¥ìœ¼ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{ìš°ë¦¬ê°€ ì‚¬ìš©í•  ê³µì‹ (Binary Cross-Entropy):}
$$ L(\hat{y}, y) = -(y \log(\hat{y}) + (1-y) \log(1-\hat{y})) $$
\begin{itemize}
    \item ì •ë‹µ($y$)ì´ 1ì¼ ë•Œ: ì˜ˆì¸¡($\hat{y}$)ì´ 1ì´ë©´ ë¹„ìš© 0, 0ì´ë©´ ë¹„ìš© $\infty$.
    \item í‹€ë ¸ì„ ë•Œ ë¬´í•œëŒ€ì˜ ë²Œì ì„ ì£¼ì–´ ë¹ ë¥´ê²Œ ê³ ì¹˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
\end{itemize}

---

\subsection{3. Gradient Descent (ê²½ì‚¬ í•˜ê°•ë²•)}
\textbf{í•œ ì¤„ ìš”ì•½:} ëˆˆì„ ê°€ë¦° ì±„ ì‚°ì—ì„œ ê°€ì¥ ë‚®ì€ ê³¨ì§œê¸°ë¡œ ë‚´ë ¤ê°€ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.



\begin{analogybox}{ì•ˆê°œ ë‚€ ì‚° í•˜ì‚°í•˜ê¸°}
ë‹¹ì‹ ì€ ì§™ì€ ì•ˆê°œê°€ ë‚€ ì‚° ì •ìƒì— ì„œ ìˆìŠµë‹ˆë‹¤. ì•ì´ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤.
ê°€ì¥ ë‚®ì€ ê³³(ë¹„ìš© ìµœì†Œí™” ì§€ì )ìœ¼ë¡œ ê°€ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
\begin{enumerate}
    \item ë°œë¡œ ë•…ì„ ë”ë“¬ì–´ ê²½ì‚¬ê°€ ê°€ì¥ ê¸‰í•˜ê²Œ ë‚´ë ¤ê°€ëŠ” ë°©í–¥ì„ ì°¾ìŠµë‹ˆë‹¤. (\textbf{Gradient ê³„ì‚°})
    \item ê·¸ ë°©í–¥ìœ¼ë¡œ í•œ ë°œìêµ­ ë‚´ë”›ìŠµë‹ˆë‹¤. (\textbf{Update})
    \item ë°”ë‹¥ì— ë„ì°©í•  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
\end{enumerate}
\end{analogybox}

\textbf{ì—…ë°ì´íŠ¸ ê³µì‹:}
$$ w := w - \alpha \frac{\partial J}{\partial w} $$
$$ b := b - \alpha \frac{\partial J}{\partial b} $$
\begin{itemize}
    \item \textbf{ë¹¼ê¸°($-$)ì˜ ì˜ë¯¸:} ê¸°ìš¸ê¸°ê°€ ì–‘ìˆ˜(ì˜¤ë¥´ë§‰)ë¼ë©´ $w$ë¥¼ ì¤„ì—¬ì•¼(ì™¼ìª½ìœ¼ë¡œ ê°€ì•¼) ë‚´ë ¤ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°€ì•¼ í•˜ë¯€ë¡œ ëºë‹ˆë‹¤.
    \item \textbf{$\alpha$ (Learning Rate):} í•œ ë°œìêµ­ì˜ í¬ê¸°ì…ë‹ˆë‹¤.
\end{itemize}

% --- 7. ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤ ---
\section{Practical Scenario: í•™ìŠµë¥  $\alpha$ì˜ ì¤‘ìš”ì„±}

í•™ìŠµë¥ (Learning Rate) $\alpha$ëŠ” ëª¨ë¸ì˜ ìš´ëª…ì„ ê²°ì •í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ìˆ«ì(Hyperparameter)ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Case A: $\alpha$ê°€ ë„ˆë¬´ ì‘ì„ ë•Œ (0.00001)} \\
    ê°œë¯¸ì²˜ëŸ¼ ê¸°ì–´ê°‘ë‹ˆë‹¤. í•´ê°€ ì§ˆ ë•Œê¹Œì§€(í•™ìŠµ ì¢…ë£Œê¹Œì§€) ì‚° ì¤‘í„±ì—ë„ ëª» ê°‘ë‹ˆë‹¤. (ìˆ˜ë ´ ì†ë„ ë§¤ìš° ëŠë¦¼)
    
    \item \textbf{Case B: $\alpha$ê°€ ë„ˆë¬´ í´ ë•Œ (10.0)} \\
    ê±°ì¸ì˜ ì í”„ì…ë‹ˆë‹¤. ê³¨ì§œê¸°ë¥¼ í–¥í•´ ë›°ì—ˆëŠ”ë° ë„ˆë¬´ ë©€ë¦¬ ë›°ì–´ì„œ ë°˜ëŒ€í¸ ì‚°ë“±ì„±ì´ì— ì²˜ë°•í™ë‹ˆë‹¤. ì˜¤íˆë ¤ ë” ë†’ì€ ê³³ìœ¼ë¡œ ì˜¬ë¼ê°ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. (\textbf{Overshooting / Divergence})
\end{itemize}

% --- 8. ê³µì‹/ì ˆì°¨ + ì˜ˆì‹œ ê³„ì‚° ---
\section{Numerical Example: ë¹„ìš© ê³„ì‚° í•´ë³´ê¸°}

\begin{examplebox}{ë¹„ìš© í•¨ìˆ˜ ê³„ì‚° ì‹¤ìŠµ}
\textbf{ìƒí™©:} ê³ ì–‘ì´ ì‚¬ì§„($y=1$)ì„ ë³´ì—¬ì¤¬ëŠ”ë°, ëª¨ë¸ì´ 0.8(80\%)ë¡œ ì˜ˆì¸¡í–ˆìŠµë‹ˆë‹¤.
$$ y = 1, \quad \hat{y} = 0.8 $$

\textbf{1. ì†ì‹¤(Loss) ê³„ì‚°:}
ê³µì‹: $L = -(1 \cdot \log(0.8) + 0 \cdot \log(0.2))$
$$ L = -\log(0.8) \approx -(-0.223) = 0.223 $$

\textbf{ìƒí™© ë³€ê²½:} ë§Œì•½ ëª¨ë¸ì´ 0.1(10\%)ë¡œ ì˜ëª» ì˜ˆì¸¡í–ˆë‹¤ë©´?
$$ L = -\log(0.1) \approx -(-2.30) = 2.30 $$
$\rightarrow$ ì˜ˆì¸¡ì´ í‹€ë¦´ìˆ˜ë¡ ë²Œì (Loss)ì´ 0.223ì—ì„œ 2.30ìœ¼ë¡œ 10ë°° ë„˜ê²Œ ì»¤ì¡ŒìŠµë‹ˆë‹¤! ì´ê²ƒì´ Log Lossì˜ ìœ„ë ¥ì…ë‹ˆë‹¤.
\end{examplebox}

% --- 9. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation (Python)}

ì´ë¡ ì„ `numpy` ì½”ë“œë¡œ ì˜®ê²¨ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=Cost Function and Optimization]
import numpy as np

def compute_cost(A, Y):
    """
    A: ì˜ˆì¸¡ê°’ (1, m), Y: ì‹¤ì œê°’ (1, m)
    """
    m = Y.shape[1]
    
    # log(0) ë°©ì§€ë¥¼ ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’(epsilon)ì„ ë”í•´ì£¼ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.
    epsilon = 1e-5 
    
    # Binary Cross-Entropy ìˆ˜ì‹ (Element-wise multiplication)
    cost = -1/m * np.sum(Y * np.log(A + epsilon) + (1-Y) * np.log(1-A + epsilon))
    
    return float(np.squeeze(cost)) # ë°°ì—´ì„ ìŠ¤ì¹¼ë¼ë¡œ ë³€í™˜

def update_parameters(w, b, dw, db, learning_rate):
    """
    ê²½ì‚¬ í•˜ê°•ë²• ì—…ë°ì´íŠ¸ ë‹¨ê³„
    """
    # í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê¸°ìš¸ê¸°(dw, db)ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ alphaë§Œí¼ ì´ë™
    w = w - learning_rate * dw
    b = b - learning_rate * db
    
    return w, b
\end{lstlisting}

% --- 10. FAQ ---
\section{FAQ: ì´ˆì‹¬ìê°€ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸}
\begin{itemize}
    \item \textbf{Q1. ê²½ì‚¬ í•˜ê°•ë²• ê³µì‹ì—ì„œ ì™œ ë”í•˜ì§€ ì•Šê³  ë¹¼ë‚˜ìš”?} \\
    \textbf{A.} ê¸°ìš¸ê¸°(Gradient)ëŠ” í•¨ìˆ˜ê°€ 'ì¦ê°€í•˜ëŠ”' ë°©í–¥ì„ ê°€ë¦¬í‚µë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¹„ìš©ì„ 'ì¤„ì—¬ì•¼' í•˜ë¯€ë¡œ ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ìœ¼ë¡œ ê°€ì•¼ í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ëºë‹ˆë‹¤.
    
    \item \textbf{Q2. ë¹„ìš© í•¨ìˆ˜ê°€ 0ì´ ë˜ë©´ ì¢‹ì€ ê±´ê°€ìš”?} \\
    \textbf{A.} ì´ë¡ ì ìœ¼ë¡œëŠ” ì™„ë²½í•˜ì§€ë§Œ, í˜„ì‹¤ì—ì„œëŠ” **ê³¼ì í•©(Overfitting)**ì„ ì˜ì‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ë¬¸ì œì§‘ ë‹µì„ ë‹¬ë‹¬ ì™¸ìš´ ìƒíƒœì¼ ìˆ˜ ìˆì–´ì„œ, ìƒˆë¡œìš´ ë¬¸ì œ(Test Set)ëŠ” ëª» í’€ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
\end{itemize}

% --- 11. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” **êµ¬ì¡°(Architecture)**ë¥¼ ë§Œë“¤ì—ˆê³ , **í•™ìŠµ ë°©ë²•(Optimizer)**ê¹Œì§€ ì¥ì°©í–ˆìŠµë‹ˆë‹¤. 
ë‹¤ìŒ ì¥ì—ì„œëŠ” ì´ ëª¨ë“  ë¶€í’ˆì„ ì¡°ë¦½í•˜ì—¬ ì‹¤ì œ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” **ì „ì²´ ëª¨ë¸(Full Model)**ì„ ì™„ì„±í•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Cost Function:} ì „ì²´ ë°ì´í„°ì˜ ì˜¤ì°¨ í‰ê· ($J$)ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ ëª©í‘œë‹¤.
    \item \textbf{Log Loss:} ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ëŠ” MSE ëŒ€ì‹  Log Lossë¥¼ ì¨ì•¼ Convex(ë³¼ë¡)í•´ì§„ë‹¤.
    \item \textbf{Gradient Descent:} $w_{new} = w_{old} - \alpha \cdot dw$. ê²½ì‚¬ë¥¼ íƒ€ê³  ë‚´ë ¤ê°€ëŠ” ì•Œê³ ë¦¬ì¦˜.
    \item \textbf{Learning Rate:} ë„ˆë¬´ í¬ë©´ ë°œì‚°, ë„ˆë¬´ ì‘ìœ¼ë©´ ëŠë¦¬ë‹¤. ì ì ˆí•œ íŠœë‹ì´ í•„ìš”í•˜ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ë²¤ì¹˜ë§ˆí¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Foundations of Neural Networks: \\ Python \& Vectorization}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network
    \begin{itemize}
        \item 2.1 Architecture \& Forward Propagation \textit{- Completed}
        \item 2.2 Cost Function \& Gradient Descent \textit{- Completed}
        \item \textbf{2.3 Python \& Vectorization (Current Unit)}
        \begin{itemize}
            \item Overview: Why Vectorization?
            \item SIMD: The Hardware Magic
            \item Broadcasting Rules
            \item Implementation \& Benchmark
        \end{itemize}
    \end{itemize}
    \item[Chapter 3.] Shallow Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ë‚œ ì‹œê°„ê¹Œì§€ ë”¥ëŸ¬ë‹ì˜ **'ì´ë¡ ì  í† ëŒ€(ë¹„ìš© í•¨ìˆ˜, ê²½ì‚¬ í•˜ê°•ë²•)'**ë¥¼ ì™„ì„±í–ˆìŠµë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œëŠ” ì™„ë²½í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ìˆ˜ì‹ì„ ì»´í“¨í„°ì—ê²Œ ê·¸ëŒ€ë¡œ ì£¼ë©´ í•™ìŠµí•˜ëŠ” ë° ìˆ˜ì‹­ ë…„ì´ ê±¸ë¦´ì§€ë„ ëª¨ë¦…ë‹ˆë‹¤. ì´ì œ ì´ ì´ë¡ ì— **'ì œíŠ¸ ì—”ì§„'**ì„ ë‹¬ì•„ì¤„ ì‹œê°„ì…ë‹ˆë‹¤. ì´ˆì‹¬ìì™€ ì „ë¬¸ê°€ë¥¼ ê°€ë¥´ëŠ” ê°€ì¥ ê²°ì •ì ì¸ ê¸°ìˆ , **ë²¡í„°í™”(Vectorization)**ë¥¼ ë°°ì›Œë´…ì‹œë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ ì½”ë“œë¥¼ ìˆ˜ë°± ë°° ë¹ ë¥´ê²Œ ë§Œë“œëŠ” **'ìµœì í™” ê¸°ìˆ '**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} `for-loop`ë¥¼ ì£„ì•…ì‹œí•˜ê³ , í–‰ë ¬ ë‹¨ìœ„ ì—°ì‚°(Vectorization)ì„ í•´ì•¼ í•˜ëŠ” ì´ìœ ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} CPU/GPUì˜ SIMD(ë³‘ë ¬ ì²˜ë¦¬) ì•„í‚¤í…ì²˜ê°€ ì–´ë–»ê²Œ ì—°ì‚°ì„ ê°€ì†í•˜ëŠ”ì§€ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ê¸°ìˆ :} NumPyì˜ í•µì‹¬ ê¸°ëŠ¥ì¸ **ë¸Œë¡œë“œìºìŠ¤íŒ…(Broadcasting)**ì˜ ê·œì¹™ê³¼ ìœ„í—˜ì„±ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{ê²€ì¦:} ì‹¤ì œ ì½”ë“œë¡œ 100ë§Œ ê°œì˜ ë°ì´í„°ë¥¼ ì—°ì‚°í•´ë³´ë©° ì†ë„ ì°¨ì´ë¥¼ ëˆˆìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì„¤ëª…} & \textbf{í•œ ì¤„ í•µì‹¬ ìš”ì•½} \\ \hline
\textbf{Vectorization} & ë²¡í„°í™” & ë°˜ë³µë¬¸ ì—†ì´ ë°ì´í„°ë¥¼ í†µì§¸ë¡œ(í–‰ë ¬ë¡œ) ì—°ì‚°í•˜ëŠ” ê¸°ë²• \\ \hline
\textbf{SIMD} & Single Instruction, Multiple Data & ëª…ë ¹ì–´ í•˜ë‚˜ë¡œ ì—¬ëŸ¬ ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ” CPU ê¸°ìˆ  \\ \hline
\textbf{Broadcasting} & ë¸Œë¡œë“œìºìŠ¤íŒ… & ëª¨ì–‘ì´ ë‹¤ë¥¸ ë°°ì—´ë¼ë¦¬ ì—°ì‚°í•  ë•Œ ìë™ìœ¼ë¡œ í¬ê¸°ë¥¼ ë§ì¶°ì£¼ëŠ” ê¸°ëŠ¥ \\ \hline
\textbf{NumPy} & ë„˜íŒŒì´ & íŒŒì´ì¬ì˜ ëŠë¦° ì†ë„ë¥¼ Cì–¸ì–´ ë ˆë²¨ ìµœì í™”ë¡œ ê·¹ë³µí•œ ìˆ˜ì¹˜ ì—°ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ \\ \hline
\textbf{Rank-1 Array} & ë­í¬-1 ë°°ì—´ & `(5,)` ì²˜ëŸ¼ í–‰ë„ ì—´ë„ ì•„ë‹Œ ì• ë§¤í•œ ë°°ì—´ (ë²„ê·¸ì˜ ì£¼ë²”) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì†ë„ì˜ ë¹„ë°€}

\subsection{1. Vectorization (ë²¡í„°í™”ë€ ë¬´ì—‡ì¸ê°€?)}
\textbf{í•œ ì¤„ ìš”ì•½:} í•˜ë‚˜ì”© ì²˜ë¦¬í•˜ì§€ ë§ê³ , íŠ¸ëŸ­ì— ì‹¤ì–´ì„œ í•œ ë²ˆì— ì˜®ê¸°ì‹­ì‹œì˜¤.

\begin{analogybox}{ì´ì‚¬ì§ ì˜®ê¸°ê¸° ë¹„ìœ }
100ë§Œ ê°œì˜ ë²½ëŒ(ë°ì´í„°)ì„ ì˜®ê²¨ì•¼ í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{For-loop (Non-vectorized):} ì¸ë¶€ê°€ ë²½ëŒì„ **ì†ì— í•˜ë‚˜ì”© ë“¤ê³ ** 100ë§Œ ë²ˆ ì™•ë³µí•©ë‹ˆë‹¤. (íŒŒì´ì¬ì´ ë°ì´í„°ë¥¼ í•˜ë‚˜ì”© êº¼ë‚´ì„œ ì²˜ë¦¬í•¨)
    \item \textbf{Vectorization:} 100ë§Œ ê°œì˜ ë²½ëŒì„ **ê±°ëŒ€í•œ ë¤í”„íŠ¸ëŸ­(í–‰ë ¬)**ì— ì‹£ê³  ë‹¨ í•œ ë²ˆì— ì´ë™í•©ë‹ˆë‹¤. (NumPyê°€ ë°ì´í„°ë¥¼ í†µì§¸ë¡œ ë©”ëª¨ë¦¬ì— ì˜¬ë ¤ ì²˜ë¦¬í•¨)
\end{itemize}
\end{analogybox}

\textbf{ê¸°ìˆ ì  ì •ì˜:}
$z = w^T x + b$ë¥¼ ê³„ì‚°í•  ë•Œ, $w_1x_1, w_2x_2 \dots$ë¥¼ ìˆœíšŒí•˜ì§€ ì•Šê³ , $w$ì™€ $x$ ì „ì²´ ë²¡í„°ë¥¼ í•œ ë²ˆì— ë‚´ì (Dot Product)í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

\subsection{2. Under the Hood: SIMD (í•˜ë“œì›¨ì–´ì˜ ë§ˆë²•)}
ì™œ NumPy(`np.dot`)ê°€ `for`ë¬¸ë³´ë‹¤ ë¹ ë¥¼ê¹Œìš”? ë‹¨ìˆœíˆ Cì–¸ì–´ë¡œ ì§œì—¬ì„œê°€ ì•„ë‹™ë‹ˆë‹¤. ì»´í“¨í„° êµ¬ì¡°ì ì¸ ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤.



\begin{itemize}
    \item \textbf{SISD (Single Instruction, Single Data):} ì¼ë°˜ì ì¸ `for`ë¬¸ì…ë‹ˆë‹¤. CPUê°€ "ê°€ì ¸ì™€", "ê³±í•´", "ì €ì¥í•´"ë¥¼ ë°ì´í„° í•˜ë‚˜ë§ˆë‹¤ ë°˜ë³µí•©ë‹ˆë‹¤.
    \item \textbf{SIMD (Single Instruction, Multiple Data):} ìµœì‹  CPUëŠ” "ì´ 8ê°œì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ê³±í•´!"ë¼ëŠ” ëª…ë ¹ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²¡í„°í™”ëŠ” ì´ ë³‘ë ¬ ì²˜ë¦¬ ê¸°ëŠ¥ì„ í™œìš©í•©ë‹ˆë‹¤.
\end{itemize}

---

\subsection{3. Broadcasting (ë¸Œë¡œë“œìºìŠ¤íŒ…)}
\textbf{í•œ ì¤„ ìš”ì•½:} ì‘ì€ í–‰ë ¬ì„ í° í–‰ë ¬ í¬ê¸°ì— ë§ê²Œ ìë™ìœ¼ë¡œ 'ëŠ˜ë ¤ì„œ(Stretch)' ì—°ì‚°í•©ë‹ˆë‹¤.



\begin{itemize}
    \item \textbf{ìƒí™©:} $(4 \times 1)$ í–‰ë ¬ì— ìˆ«ì $100$(ìŠ¤ì¹¼ë¼)ì„ ë”í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.
    \item \textbf{ì›ì¹™:} ìˆ˜í•™ì ìœ¼ë¡œëŠ” ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, Pythonì€ $100$ì„ ìë™ìœ¼ë¡œ $(4 \times 1)$ í¬ê¸°ë¡œ ë³µì‚¬í•˜ì—¬ ë”í•´ì¤ë‹ˆë‹¤.
    \item \textbf{ì˜ˆì‹œ:} 
    $$ \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} + 100 \rightarrow \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix} + \begin{bmatrix} 100 \\ 100 \\ 100 \\ 100 \end{bmatrix} = \begin{bmatrix} 101 \\ 102 \\ 103 \\ 104 \end{bmatrix} $$
\end{itemize}

% --- 7. ì‹¤ì „ ë²¤ì¹˜ë§ˆí¬ ---
\section{Implementation \& Benchmark (ì„±ëŠ¥ ê²€ì¦)}

"ë°±ë¬¸ì´ ë¶ˆì—¬ì¼ê²¬"ì…ë‹ˆë‹¤. 100ë§Œ ê°œì˜ ë°ì´í„°ë¥¼ ê³±í•˜ëŠ” ì‹œê°„ì„ ì§ì ‘ ì¸¡ì •í•´ ë´…ì‹œë‹¤.

\begin{examplebox}{For-loop vs Vectorization ì†ë„ ëŒ€ê²°}
\begin{lstlisting}[language=Python]
import numpy as np
import time

# ë°ì´í„° ì¤€ë¹„: 100ë§Œ ê°œì˜ ë‚œìˆ˜ ìƒì„±
a = np.random.rand(1000000)
b = np.random.rand(1000000)

# --- 1. For-loop (ëŠë¦° ë°©ë²•) ---
c = 0
tic = time.time()
for i in range(1000000):
    c += a[i] * b[i]
toc = time.time()

print(f"For-loop: {c:.4f}")
print(f"Time: {1000 * (toc - tic):.2f} ms") # ì•½ 400~500ms ì†Œìš”

# --- 2. Vectorization (ë¹ ë¥¸ ë°©ë²•) ---
tic = time.time()
c_vec = np.dot(a, b) # SIMD ë³‘ë ¬ ì²˜ë¦¬
toc = time.time()

print(f"Vectorized: {c_vec:.4f}")
print(f"Time: {1000 * (toc - tic):.2f} ms") # ì•½ 1~2ms ì†Œìš”
\end{lstlisting}
\textbf{ê²°ê³¼ ë¶„ì„:} ë²¡í„°í™” ì½”ë“œê°€ ì•½ \textbf{300~500ë°°} ë” ë¹ ë¦…ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œê°„ì´ 1ë‹¬ ê±¸ë¦´ ê²ƒì„ 2ì‹œê°„ìœ¼ë¡œ ì¤„ì—¬ì£¼ëŠ” ë§ˆë²•ì…ë‹ˆë‹¤.
\end{examplebox}

% --- 8. ì£¼ì˜ì‚¬í•­ ë° FAQ ---
\section{Pitfalls \& FAQ}

\begin{warningbox}{Rank-1 Arrayì˜ í•¨ì •}
NumPyì—ì„œ `a = np.random.randn(5)`ë¥¼ í•˜ë©´ ëª¨ì–‘(Shape)ì´ `(5,)`ê°€ ë©ë‹ˆë‹¤.
ì´ê²ƒì€ í–‰ ë²¡í„°ë„, ì—´ ë²¡í„°ë„ ì•„ë‹Œ ì• ë§¤í•œ ìƒíƒœë¼ ì „ì¹˜(Transpose)ê°€ ì•ˆ ë©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë‚˜ìœ ì˜ˆ:} `a = np.random.randn(5)` $\rightarrow$ ë²„ê·¸ ë°œìƒ ìœ„í—˜ ë†’ìŒ.
    \item \textbf{ì¢‹ì€ ì˜ˆ:} `a = np.random.randn(5, 1)` (ì—´ ë²¡í„°) ë˜ëŠ” `(1, 5)` (í–‰ ë²¡í„°)ë¡œ ëª…ì‹œí•˜ì‹­ì‹œì˜¤.
    \item \textbf{ìŠµê´€:} ì½”ë“œ ì¤‘ê°„ì— `assert(a.shape == (5, 1))`ì„ ë„£ì–´ ì°¨ì›ì„ í™•ì¸í•˜ì‹­ì‹œì˜¤.
\end{itemize}
\end{warningbox}

\subsection*{FAQ: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸}
\textbf{Q1. GPUëŠ” ì–¸ì œ ì“°ë‚˜ìš”?} \\
A. NumPyëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë‚˜ì¤‘ì— ë°°ìš¸ TensorFlowë‚˜ PyTorchëŠ” ì´ ë²¡í„°í™” ì—°ì‚°ì„ GPU(ê·¸ë˜í”½ ì¹´ë“œ)ì—ì„œ ìˆ˜í–‰í•˜ì—¬, CPUë³´ë‹¤ í›¨ì”¬ ë” ë§ì€ ë³‘ë ¬ ì²˜ë¦¬(ìˆ˜ì²œ ê°œì˜ ì½”ì–´)ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì›ë¦¬ëŠ” ë˜‘ê°™ìŠµë‹ˆë‹¤.

\textbf{Q2. ëª¨ë“  ì½”ë“œë¥¼ ë²¡í„°í™”í•  ìˆ˜ ìˆë‚˜ìš”?} \\
A. ëŒ€ë¶€ë¶„ì˜ ìˆ˜í•™ ì—°ì‚°ì€ ê°€ëŠ¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë³µì¡í•œ ì¡°ê±´ë¬¸(`if-else`)ì´ ë°ì´í„°ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì ìš©ë˜ì–´ì•¼ í•œë‹¤ë©´ ë²¡í„°í™”ê°€ ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ 99\%ì˜ ë”¥ëŸ¬ë‹ ì—°ì‚°ì€ ë²¡í„°í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì¶•í•˜í•©ë‹ˆë‹¤! ì—¬ëŸ¬ë¶„ì€ ì´ì œ \textbf{'ê³ ì† ì—°ì‚° ì—”ì§„(Vectorization)'}ì„ ì¥ì°©í–ˆìŠµë‹ˆë‹¤. 

ì§€ê¸ˆê¹Œì§€ëŠ” ë‰´ëŸ°ì´ ë”± í•˜ë‚˜(ë¡œì§€ìŠ¤í‹± íšŒê·€)ë¿ì´ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì¥ **[Chapter 3. Shallow Neural Networks]**ì—ì„œëŠ” ì´ ê°•ë ¥í•œ ì—”ì§„ì„ í™œìš©í•´ ë‰´ëŸ°ì„ ìˆ˜ë°± ê°œë¡œ ëŠ˜ë ¤ë³´ê² ìŠµë‹ˆë‹¤. ì´ì œ ì§„ì§œ 'ì‹ ê²½ë§'ë‹¤ìš´ ì‹ ê²½ë§ì„ ë§Œë“¤ ì°¨ë¡€ì…ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Vectorization:} `for`ë¬¸ì€ ì£„ì•…ì´ë‹¤. `np.dot` ë“±ì„ ì¨ì„œ í–‰ë ¬ ë‹¨ìœ„ë¡œ ê³„ì‚°í•˜ë¼.
    \item \textbf{SIMD:} ë²¡í„°í™”ëŠ” CPUì˜ ë³‘ë ¬ ì²˜ë¦¬ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ë„ë¥¼ ìˆ˜ë°± ë°° ë†’ì¸ë‹¤.
    \item \textbf{Broadcasting:} ì°¨ì›ì´ ë‹¬ë¼ë„ NumPyê°€ ì•Œì•„ì„œ ë§ì¶°ì£¼ì§€ë§Œ, ë²„ê·¸ë¥¼ ì¡°ì‹¬í•´ì•¼ í•œë‹¤.
    \item \textbf{Shape Check:} `(n,)` ëŒ€ì‹  `(n, 1)`ì„ ì‚¬ìš©í•˜ì—¬ ì°¨ì›ì„ ëª…ì‹œí•˜ëŠ” ìŠµê´€ì„ ë“¤ì—¬ë¼.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Foundations of Neural Networks: \\ Broadcasting \& Removing Loops}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network
    \begin{itemize}
        \item 2.1 Architecture \& Forward Propagation \textit{- Completed}
        \item 2.2 Cost Function \& Gradient Descent \textit{- Completed}
        \item 2.3 Python \& Vectorization \textit{- Completed}
        \item \textbf{2.4 Broadcasting \& Removing Loops (Current Unit)}
        \begin{itemize}
            \item Definition \& Rules
            \item Visual Analogy
            \item Under the Hood (Strides)
            \item Implementation (Normalization)
        \end{itemize}
    \end{itemize}
    \item[Chapter 3.] Shallow Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ê°•ì˜ì—ì„œ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ ì†ë„ì˜ í•µì‹¬ì¸ **ë²¡í„°í™”(Vectorization)**ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤. ë²¡í„°í™”ê°€ ê³ ì†ë„ë¡œ(Engine)ë¼ë©´, ì˜¤ëŠ˜ ë°°ìš¸ **ë¸Œë¡œë“œìºìŠ¤íŒ…(Broadcasting)**ì€ ì°¨ì„ ì„ ììœ ìì¬ë¡œ ë³€ê²½í•˜ëŠ” **ìœ ì—°í•¨(Flexibility)**ì…ë‹ˆë‹¤. ë§ì€ í•™ìƒë“¤ì´ `np.dot`ì€ ì˜ ì“°ë©´ì„œë„, ëª¨ì–‘ì´ ë‹¤ë¥¸ í–‰ë ¬ë¼ë¦¬ ì—°ì‚°í•  ë•Œ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì—ëŠ” ì†ìˆ˜ë¬´ì±…ì…ë‹ˆë‹¤. ì´ ì›ë¦¬ë¥¼ ì•Œì•„ì•¼ ì§„ì •í•œ ë””ë²„ê¹… ë§ˆìŠ¤í„°ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì„œë¡œ ë‹¤ë¥¸ ëª¨ì–‘(Shape)ì˜ ë°ì´í„°ë¥¼ ì˜¤ë¥˜ ì—†ì´ ì—°ì‚°í•˜ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} NumPyê°€ ì‘ì€ ë°°ì—´ì„ ìë™ìœ¼ë¡œ í™•ì¥(Stretch)í•˜ì—¬ ì—°ì‚°í•˜ëŠ” ê·œì¹™ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} `for-loop` ì—†ì´ ë°ì´í„° ì •ê·œí™”(Normalization)ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ë©”ëª¨ë¦¬ ë³µì‚¬ ì—†ì´ **ìŠ¤íŠ¸ë¼ì´ë“œ(Strides)** ì¡°ì‘ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ë‚´ë¶€ ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì„¤ëª…} & \textbf{í•œ ì¤„ í•µì‹¬ ìš”ì•½} \\ \hline
\textbf{Broadcasting} & ë¸Œë¡œë“œìºìŠ¤íŒ… & ëª¨ì–‘ì´ ë‹¤ë¥¸ ë°°ì—´ ê°„ ì—°ì‚° ì‹œ, ì‘ì€ ìª½ì„ ìë™ìœ¼ë¡œ ëŠ˜ë ¤ì£¼ëŠ” ê¸°ëŠ¥ \\ \hline
\textbf{Shape} & í˜•ìƒ & ë°°ì—´ì˜ ì°¨ì› í¬ê¸° (ì˜ˆ: $(4, 3)$ì€ 4í–‰ 3ì—´) \\ \hline
\textbf{Normalization} & ì •ê·œí™” & ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ë§ì¶”ëŠ” ì „ì²˜ë¦¬ ê³¼ì • \\ \hline
\textbf{Keepdims} & ì°¨ì› ìœ ì§€ & ì—°ì‚° í›„ì—ë„ ì°¨ì›(Rank)ì„ ì‚­ì œí•˜ì§€ ì•Šê³  ìœ ì§€í•˜ëŠ” ì˜µì…˜ \\ \hline
\textbf{Strides} & ìŠ¤íŠ¸ë¼ì´ë“œ & ë©”ëª¨ë¦¬ ìƒì—ì„œ ë‹¤ìŒ ìš”ì†Œë¡œ ë„˜ì–´ê°€ê¸° ìœ„í•œ ë³´í­ (Byte ë‹¨ìœ„) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë¸Œë¡œë“œìºìŠ¤íŒ…ì˜ ë§ˆë²•}

\subsection{1. Broadcastingì˜ ì •ì˜ì™€ ë¹„ìœ }
\textbf{í•œ ì¤„ ìš”ì•½:} ì‘ì€ í–‰ë ¬ì„ í° í–‰ë ¬ í¬ê¸°ì— ë§ì¶° ìë™ìœ¼ë¡œ 'ëŠ˜ë ¤ì„œ(Copy)' ì—°ì‚°í•©ë‹ˆë‹¤.

\begin{analogybox}{ì‹ë¹µê³¼ ë²„í„° ë¹„ìœ }
ì‹ë¹µ 100ê°œ(ë°ì´í„° $m=100$)ì— ë²„í„°($b$)ë¥¼ ë°œë¼ì•¼ í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{For-loop:} ì‹ë¹µì„ í•˜ë‚˜ êº¼ë‚´ê³ , ë²„í„°ë¥¼ ë°”ë¥´ê³ , ë‚´ë ¤ë†“ìŠµë‹ˆë‹¤. (100ë²ˆ ë°˜ë³µ)
    \item \textbf{Broadcasting:} ë§ˆë²•ì„ ë¶€ë ¤ ë²„í„°ë¥¼ ì‹ë¹µ 100ê°œ ê¸¸ì´ë§Œí¼ **ìˆœì‹ê°„ì— ëŠ˜ë¦°(Stretch)** ë’¤, í•œ ë²ˆì— ì¾… ì°ì–´ë²„ë¦½ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}



\subsection{2. General Broadcasting Rules (ì—„ê²©í•œ ê·œì¹™)}
ì•„ë¬´ê±°ë‚˜ ë‹¤ ëŠ˜ë ¤ì£¼ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. NumPyëŠ” **ë’¤(ì˜¤ë¥¸ìª½) ì°¨ì›ë¶€í„° ë¹„êµ**í•˜ì—¬ ë‹¤ìŒ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¥¼ ë§Œì¡±í•´ì•¼ë§Œ ì—°ì‚°ì„ í—ˆìš©í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item \textbf{Equal:} ë‘ ì°¨ì›ì˜ í¬ê¸°ê°€ ê°™ë‹¤.
    \item \textbf{One:} ë‘˜ ì¤‘ í•˜ë‚˜ì˜ í¬ê¸°ê°€ 1ì´ë‹¤. (ì´ ê²½ìš° 1ì¸ ìª½ì´ ëŠ˜ì–´ë‚¨)
\end{enumerate}

\begin{tcolorbox}[colback=white, colframe=black, title=Rule Check Example]
\textbf{Case 1: ê°€ëŠ¥ (Success)} \\
$A: (4, \mathbf{3})$ \\
$B: (4, \mathbf{1}) \rightarrow$ 1ì´ 3ìœ¼ë¡œ í™•ì¥ë¨. \\
ê²°ê³¼: $(4, 3)$

\textbf{Case 2: ë¶ˆê°€ëŠ¥ (Fail - ValueError)} \\
$A: (4, \mathbf{3})$ \\
$B: (4, \mathbf{2}) \rightarrow$ 3ê³¼ 2ëŠ” ë‹¤ë¥´ê³ , ë‘˜ ë‹¤ 1ì´ ì•„ë‹˜.
\end{tcolorbox}

---

\subsection{3. Under the Hood: ê°€ìƒ ë³µì‚¬ (Virtual Copying)}
"êµìˆ˜ë‹˜, ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ë©´ ë©”ëª¨ë¦¬ë¥¼ ë‚­ë¹„í•˜ëŠ” ê²ƒ ì•„ë‹Œê°€ìš”?" \\
\textbf{ì•„ë‹™ë‹ˆë‹¤.} ì´ê²ƒì´ ë¸Œë¡œë“œìºìŠ¤íŒ… ê¸°ìˆ ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Physical (ì‹¤ì œ):} $b = [1, 2, 3]$ (ë©”ëª¨ë¦¬ì—” ë”± 3ê°œë§Œ ì¡´ì¬)
    \item \textbf{Logical (ê°€ìƒ):} CPUì—ê²ŒëŠ” ë§ˆì¹˜ $[1, 1, 1, \dots], [2, 2, 2, \dots]$ ì¸ ê²ƒì²˜ëŸ¼ ì£¼ì†Œë¥¼ ì†ì—¬ì„œ ì•Œë ¤ì¤ë‹ˆë‹¤.
    \item \textbf{Strides Manipulation:} ë©”ëª¨ë¦¬ë¥¼ ì‹¤ì œë¡œ ë³µì‚¬í•˜ì§€ ì•Šê³ , ë°ì´í„° ì ‘ê·¼ ë³´í­(Stride)ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê°™ì€ ê°’ì„ ë°˜ë³µí•´ì„œ ì½ê²Œ ë§Œë“­ë‹ˆë‹¤. ë§ˆì¹˜ **í™€ë¡œê·¸ë¨**ê³¼ ê°™ìŠµë‹ˆë‹¤.
\end{itemize}

% --- 7. ì‹¤ì „ ê³„ì‚° ì˜ˆì‹œ ---
\section{Numerical Example: ì†ìœ¼ë¡œ í‘¸ëŠ” ë¸Œë¡œë“œìºìŠ¤íŒ…}

\begin{examplebox}{ì¹¼ë¡œë¦¬ ê³„ì‚° ì‹œë‚˜ë¦¬ì˜¤}
\textbf{ìƒí™©:} 4ê°€ì§€ ìŒì‹(í–‰)ì˜ ì˜ì–‘ì†Œ(ì—´: íƒ„, ë‹¨, ì§€) ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ê° ìŒì‹ì˜ ì´ ì¹¼ë¡œë¦¬ê°€ 100gë‹¹ ì–¼ë§ˆì¸ì§€ ë”í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.

\textbf{ë°ì´í„° í–‰ë ¬ A (2ê°œ ìŒì‹ x 3ê°œ ì˜ì–‘ì†Œ):}
$$
\begin{bmatrix} 
10 & 20 & 30 \\ 
40 & 50 & 60 
\end{bmatrix} 
$$
\textbf{ì¡°ë¯¸ë£Œ B (ê° ì˜ì–‘ì†Œì— ì¶”ê°€ë  ê°’, 1 x 3):}
$$
\begin{bmatrix} 1 & 2 & 3 \end{bmatrix}
$$

\textbf{ì—°ì‚° ê³¼ì • ($A + B$):}
í–‰ë ¬ Bì˜ í–‰(Row) ì°¨ì›ì´ 1ì´ë¯€ë¡œ, í–‰ë ¬ Aì˜ í¬ê¸°ì¸ 2ë¡œ í™•ì¥ë©ë‹ˆë‹¤.
$$
\begin{bmatrix} 
10 & 20 & 30 \\ 
40 & 50 & 60 
\end{bmatrix} 
+
\begin{bmatrix} 
1 & 2 & 3 \\ 
\mathbf{1} & \mathbf{2} & \mathbf{3} \leftarrow \text{(ë³µì‚¬ë¨)} 
\end{bmatrix} 
=
\begin{bmatrix} 
11 & 22 & 33 \\ 
41 & 52 & 63 
\end{bmatrix}
$$
\end{examplebox}

% --- 8. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Data Normalization}

ë¸Œë¡œë“œìºìŠ¤íŒ…ì´ ê°€ì¥ ë¹›ì„ ë°œí•˜ëŠ” ìˆœê°„ì€ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬(Preprocessing) í•  ë•Œì…ë‹ˆë‹¤. 
ì…ë ¥ ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ë§Œë“œëŠ” **ì •ê·œí™”**ë¥¼ êµ¬í˜„í•´ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=Broadcasting Implementation]
import numpy as np
import time

def normalization_demo():
    # ë°ì´í„°: 4ê°œì˜ íŠ¹ì„±(Feature), 100ë§Œ ê°œì˜ ìƒ˜í”Œ
    # Shape: (4, 1000000)
    X = np.random.rand(4, 1000000) * 100
    
    # 1. í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
    # axis=1: ì—´(column) ë°©í–¥ìœ¼ë¡œ ê³„ì‚° (ê° í–‰ì˜ í‰ê· )
    # keepdims=True: (4,)ê°€ ì•„ë‹ˆë¼ (4, 1)ë¡œ ìœ ì§€ -> ë¸Œë¡œë“œìºìŠ¤íŒ… í•„ìˆ˜ ì¡°ê±´!
    mu = np.mean(X, axis=1, keepdims=True)
    sigma = np.std(X, axis=1, keepdims=True)
    
    print(f"mu shape: {mu.shape}") # (4, 1) í™•ì¸
    
    # 2. Broadcasting ì ìš© (í•µì‹¬)
    # (4, 1000000) - (4, 1) -> (4, 1)ì´ 100ë§Œ ë²ˆ ë³µì‚¬ë˜ì–´ ì—°ì‚°ë¨
    # ë‚˜ëˆ—ì…ˆë„ ë§ˆì°¬ê°€ì§€ ì›ë¦¬
    tic = time.time()
    X_norm = (X - mu) / sigma
    toc = time.time()
    
    print(f"Broadcasting time: {1000 * (toc - tic):.2f} ms") # ë§¤ìš° ë¹ ë¦„

if __name__ == "__main__":
    normalization_demo()
\end{lstlisting}

% --- 9. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ì£¼ì˜: (m, 1) + (1, m) = (m, m)}
ë¸Œë¡œë“œìºìŠ¤íŒ…ì˜ ê°•ë ¥í•¨ì´ ë…ì´ ë  ë•Œê°€ ìˆìŠµë‹ˆë‹¤.
\begin{itemize}
    \item ë²¡í„° A: $(5, 1)$
    \item ë²¡í„° B: $(1, 5)$
    \item $A + B$: $(5, 5)$ í–‰ë ¬ì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤.
\end{itemize}
ë‘ ë²¡í„°ë¥¼ ë”í•´ì„œ ê°™ì€ í¬ê¸°ì˜ ë²¡í„°ë¥¼ ë§Œë“¤ê³  ì‹¶ì—ˆë‹¤ë©´, ë°˜ë“œì‹œ ë‘ ë²¡í„°ì˜ Shapeì´ ì¼ì¹˜í•˜ëŠ”ì§€ `assert` ë¬¸ìœ¼ë¡œ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. \texttt{keepdims=True}ë¥¼ ì•ˆ ì“°ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?} \\
A. `mu`ì˜ shapeì´ `(4,)`ê°€ ë©ë‹ˆë‹¤. ì´ë¥¼ `Rank-1 Array`ë¼ê³  í•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë¸Œë¡œë“œìºìŠ¤íŒ…ì´ ì˜ ë˜ì§€ë§Œ, íŠ¹ì • ìƒí™©ì—ì„œ ì˜ˆìƒì¹˜ ëª»í•œ ì°¨ì› í™•ì¥ì´ ì¼ì–´ë‚˜ ë””ë²„ê¹…ì´ ë§¤ìš° ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. ëª…ì‹œì ìœ¼ë¡œ `(4, 1)`ì„ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì•ˆì „í•©ë‹ˆë‹¤.

% --- 10. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ì—¬ëŸ¬ë¶„ì€ ë°ì´í„°ì˜ ëª¨ì–‘(Shape)ì„ ììœ ìì¬ë¡œ ë‹¤ë£¨ëŠ” ê¸°ìˆ ê¹Œì§€ ìµí˜”ìŠµë‹ˆë‹¤. ë²¡í„°í™”ì™€ ë¸Œë¡œë“œìºìŠ¤íŒ…ì´ë¼ëŠ” ë‘ ê°œì˜ ë¬´ê¸°ë¥¼ ì†ì— ì¥ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì¥ **[Chapter 3. Shallow Neural Networks]**ì—ì„œëŠ” ë“œë””ì–´ ë¡œì§€ìŠ¤í‹± íšŒê·€(ë‰´ëŸ° 1ê°œ)ë¥¼ ë„˜ì–´ì„œ, ì€ë‹‰ì¸µ(Hidden Layer)ì´ ìˆëŠ” **ì§„ì§œ ì‹ ê²½ë§**ì„ êµ¬ì¶•í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œë¶€í„° ë”¥ëŸ¬ë‹ì˜ ë§ˆë²•ì´ ì‹œì‘ë©ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Broadcasting:} ì‘ì€ ë°°ì—´ì„ í° ë°°ì—´ì— ë§ì¶° 'ê°€ìƒìœ¼ë¡œ í™•ì¥'í•˜ì—¬ ì—°ì‚°í•œë‹¤.
    \item \textbf{Rule:} ì°¨ì›ì„ ì˜¤ë¥¸ìª½ ëë¶€í„° ë¹„êµí•˜ì—¬, ê°™ê±°ë‚˜ 1ì´ì–´ì•¼ í•œë‹¤.
    \item \textbf{Memory:} ë°ì´í„°ë¥¼ ì‹¤ì œë¡œ ë³µì‚¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ(Strides ì¡°ì‘) ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì´ë‹¤.
    \item \textbf{Tip:} `np.sum`ì´ë‚˜ `np.mean` ì‚¬ìš© ì‹œ `keepdims=True`ë¥¼ ìŠµê´€í™”í•˜ë¼.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Shallow Neural Networks: \\ Concept of Hidden Layer}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network \textit{- Completed}
    \item[\textbf{Chapter 3.}] \textbf{Shallow Neural Networks (Current Unit)}
    \begin{itemize}
        \item \textbf{3.1 Concept of Hidden Layer \& Architecture}
        \item 3.2 Backpropagation Intuition
        \item 3.3 Random Initialization
    \end{itemize}
    \item[Chapter 4.] Deep Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ë¼ëŠ” **'ë‹¨ì¼ ë‰´ëŸ°(Single Neuron)'**ì„ ì™„ë²½í•˜ê²Œ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‰´ëŸ° í•˜ë‚˜ë¡œëŠ” ë‹¨ìˆœí•œ ì„ í˜• ë¬¸ì œ(ì§ì„ ìœ¼ë¡œ ê°€ë¥´ëŠ” ë¬¸ì œ)ë°–ì— í•´ê²°í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
ì´ì œ ì´ ë‰´ëŸ°ë“¤ì„ ìˆ˜ì§, ìˆ˜í‰ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ **ì§„ì •í•œ ì˜ë¯¸ì˜ ì‹ ê²½ë§**ì„ êµ¬ì¶•í•  ì‹œê°„ì…ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì˜¤ëŠ˜ ë‹¤ë£° 'ì–•ì€ ì‹ ê²½ë§(Shallow Neural Network)'ì€ ë”¥ëŸ¬ë‹ì´ë¼ëŠ” ê±°ëŒ€í•œ ë§ˆì²œë£¨ë¥¼ ìŒ“ê¸° ìœ„í•œ 1ì¸µ ê¸°ì´ˆ ê³µì‚¬ì™€ ê°™ìŠµë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ í™•ì¥í•˜ì—¬ **2-Layer ì‹ ê²½ë§**ì„ ë§Œë“œëŠ” ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} ì…ë ¥ì¸µê³¼ ì¶œë ¥ì¸µ ì‚¬ì´ì— ìˆëŠ” **'ì€ë‹‰ì¸µ(Hidden Layer)'**ì˜ ì—­í• ê³¼ ì •ì˜ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜í•™:} ì¸µ(Layer) ë²ˆí˜¸ì™€ ë°ì´í„° ìƒ˜í”Œ ë²ˆí˜¸ë¥¼ êµ¬ë¶„í•˜ëŠ” **í‘œê¸°ë²•(Notation)**ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ì™œ ì‹ ê²½ë§ì— **ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜(Tanh, ReLU)**ê°€ ë°˜ë“œì‹œ í•„ìš”í•œì§€ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} í–‰ë ¬ ì—°ì‚°ì„ í†µí•´ ì…ë ¥ì—ì„œ ì¶œë ¥ê¹Œì§€ ê°€ëŠ” **ìˆœì „íŒŒ(Forward Propagation)**ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
ë”¥ëŸ¬ë‹ ìˆ˜í•™ì˜ 50\%ëŠ” í‘œê¸°ë²•ì„ ì œëŒ€ë¡œ ì•„ëŠ” ê²ƒì—ì„œ ì‹œì‘í•©ë‹ˆë‹¤.

\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{í‘œê¸°} & \textbf{ì˜ë¯¸} & \textbf{ì˜ˆì‹œ ë° ì„¤ëª…} \\ \hline
$x = a^{[0]}$ & ì…ë ¥ì¸µ (Input Layer) & ì›ë³¸ ë°ì´í„°. ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë¯€ë¡œ 0ë²ˆì§¸ ì¸µ ì·¨ê¸‰. \\ \hline
$a^{[1]}$ & ì€ë‹‰ì¸µ (Hidden Layer) & ì…ë ¥ê°’ì„ ë³€í™˜í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¤‘ê°„ ë‹¨ê³„. \\ \hline
$a^{[2]} = \hat{y}$ & ì¶œë ¥ì¸µ (Output Layer) & ìµœì¢… ì˜ˆì¸¡ê°’ (ì˜ˆ: ê³ ì–‘ì´ì¼ í™•ë¥ ). \\ \hline
$[l]$ (ëŒ€ê´„í˜¸) & \textbf{ì¸µ(Layer) ë²ˆí˜¸} & $W^{[1]}$ (1ë²ˆ ì¸µì˜ ê°€ì¤‘ì¹˜) \\ \hline
$(i)$ (ì†Œê´„í˜¸) & \textbf{ë°ì´í„° ìƒ˜í”Œ ë²ˆí˜¸} & $x^{(i)}$ ($i$ë²ˆì§¸ í›ˆë ¨ ë°ì´í„°) \\ \hline
$n^{[l]}$ & $l$ë²ˆì§¸ ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜ & $n^{[1]} = 4$ (ì€ë‹‰ì¸µ ë‰´ëŸ°ì´ 4ê°œ) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì€ë‹‰ì¸µì˜ í•´ë¶€}

\subsection{1. Architecture (êµ¬ì¡°: 1ì¸µì—ì„œ 2ì¸µìœ¼ë¡œ)}
ë¡œì§€ìŠ¤í‹± íšŒê·€ê°€ 'ì…ë ¥ $\to$ ì¶œë ¥'ì˜ ì§í–‰ë²„ìŠ¤ë¼ë©´, ì–•ì€ ì‹ ê²½ë§ì€ ì¤‘ê°„ì— **í™˜ìŠ¹ ì„¼í„°(ì€ë‹‰ì¸µ)**ê°€ í•˜ë‚˜ ìˆëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.



\begin{itemize}
    \item **ì…ë ¥ì¸µ ($Input$):** $x_1, x_2, x_3$ (ë°ì´í„° íŠ¹ì„±)
    \item **ì€ë‹‰ì¸µ ($Hidden$):** ì…ë ¥ ì •ë³´ë¥¼ ì„ê³  ë¹„í‹€ì–´ì„œ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë§Œë“­ë‹ˆë‹¤.
    \item **ì¶œë ¥ì¸µ ($Output$):** ì€ë‹‰ì¸µì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ê²°ì •ì„ ë‚´ë¦½ë‹ˆë‹¤.
\end{itemize}

\begin{analogybox}{ìë™ì°¨ ê³µì¥ ì¡°ë¦½ ë¼ì¸}
\begin{itemize}
    \item **Input ($x$):** ì² íŒ, ìœ ë¦¬, ê³ ë¬´ ë“± ì›ìì¬.
    \item **Hidden Layer ($a^{[1]}$):** ê³µì¥ ë‚´ë¶€ì˜ ì‘ì—…ìë“¤. ì² íŒì„ êµ¬ë¶€ë ¤ ë¬¸ì§ì„ ë§Œë“¤ê³ , ì—”ì§„ì„ ì¡°ë¦½í•©ë‹ˆë‹¤. ì™¸ë¶€(ì‚¬ìš©ì)ì—ì„œëŠ” ì´ ê³¼ì •ì´ ë³´ì´ì§€ ì•Šìœ¼ë¯€ë¡œ **'Hidden(ì€ë‹‰)'**ì´ë¼ê³  í•©ë‹ˆë‹¤.
    \item **Output Layer ($a^{[2]}$):** ì™„ì„±ëœ ì°¨ë¥¼ ê²€ìˆ˜í•˜ê³  "ì¶œê³  ê°€ëŠ¥(1)" í˜¹ì€ "ë¶ˆëŸ‰(0)" íŒì •ì„ ë‚´ë¦½ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

\subsection{2. í–‰ë ¬ ì§€ì˜¥ íƒˆì¶œ (Matrix Dimensions)}
ì‹ ê²½ë§ êµ¬í˜„ì—ì„œ ê°€ì¥ ë§ì´ í‹€ë¦¬ëŠ” ë¶€ë¶„ì´ í–‰ë ¬ì˜ í¬ê¸°(Dimension)ì…ë‹ˆë‹¤. ì•„ë˜ í‘œë¥¼ ë³´ë©° ë°˜ë“œì‹œ ì°¨ì›ì„ ë§ì¶”ëŠ” ì—°ìŠµì„ í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{itemize}
    \item $n^{[0]} = n_x$: ì…ë ¥ íŠ¹ì„± ê°œìˆ˜ (ì˜ˆ: 3)
    \item $n^{[1]}$: ì€ë‹‰ì¸µ ë‰´ëŸ° ê°œìˆ˜ (ì˜ˆ: 4)
    \item $m$: ë°ì´í„° ê°œìˆ˜ (ì˜ˆ: 100)
\end{itemize}

\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{ë³€ìˆ˜} & \textbf{Shape (í–‰, ì—´)} & \textbf{ì•”ê¸° ê³µì‹} \\ \hline
$W^{[1]}$ & $(n^{[1]}, n^{[0]})$ & (ì€ë‹‰ ë‰´ëŸ° ìˆ˜, ì…ë ¥ íŠ¹ì„± ìˆ˜) \\ \hline
$b^{[1]}$ & $(n^{[1]}, 1)$ & (ì€ë‹‰ ë‰´ëŸ° ìˆ˜, 1) \\ \hline
$Z^{[1]}, A^{[1]}$ & $(n^{[1]}, m)$ & (ì€ë‹‰ ë‰´ëŸ° ìˆ˜, ë°ì´í„° ê°œìˆ˜) \\ \hline
$W^{[2]}$ & $(1, n^{[1]})$ & (ì¶œë ¥ ë‰´ëŸ° ìˆ˜, ì€ë‹‰ ë‰´ëŸ° ìˆ˜) \\ \hline
\end{tabular}
\end{center}

\subsection{3. ë¹„ì„ í˜•ì„±(Non-linearity)ì˜ í•„ìš”ì„±}
\textbf{ì§ˆë¬¸:} "êµìˆ˜ë‹˜, ê·¸ëƒ¥ ê³„ì‚°í•˜ê¸° í¸í•˜ê²Œ ì„ í˜• í•¨ìˆ˜($y=ax+b$)ë§Œ ê³„ì† ìŒ“ìœ¼ë©´ ì•ˆ ë˜ë‚˜ìš”?" \\
\textbf{ë‹µë³€:} \textbf{ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.} ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜(Sigmoid, Tanh, ReLU)ê°€ ì—†ë‹¤ë©´ ì‹ ê²½ë§ì€ ê¹Šì–´ì§ˆ ì˜ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤.

\textbf{ì¦ëª…:}
$$ Output = W_2(W_1 x + b_1) + b_2 = (W_2 W_1)x + (W_2 b_1 + b_2) = W'x + b' $$
ì„ í˜• í•¨ìˆ˜ë¼ë¦¬ì˜ ê²°í•©ì€ ê²°êµ­ ë˜ ë‹¤ë¥¸ í•˜ë‚˜ì˜ ì„ í˜• í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤. 100ì¸µì„ ìŒ“ì•„ë„ ìˆ˜í•™ì ìœ¼ë¡œëŠ” 1ì¸µì§œë¦¬ ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ë˜‘ê°™ì•„ì§‘ë‹ˆë‹¤. ë³µì¡í•œ ê³¡ì„ ì„ ê·¸ë¦¬ë ¤ë©´ ë¹„ì„ í˜• í•¨ìˆ˜ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.



% --- 7. ê³µì‹ ë° ê³„ì‚° ì˜ˆì‹œ ---
\section{Mathematical Forward Propagation}

ì…ë ¥ $x$ê°€ ì‹ ê²½ë§ì„ í†µê³¼í•˜ëŠ” ê³¼ì •ì„ ìˆ˜ì‹ìœ¼ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.

\textbf{Step 1: ì…ë ¥ $\to$ ì€ë‹‰ì¸µ (íŠ¹ì§• ì¶”ì¶œ)}
$$ Z^{[1]} = W^{[1]}X + b^{[1]} $$
$$ A^{[1]} = \tanh(Z^{[1]}) $$
\textit{(ì°¸ê³ : ì€ë‹‰ì¸µì—ì„œëŠ” Sigmoidë³´ë‹¤ í‰ê· ì´ 0ì¸ Tanhê°€ í•™ìŠµ ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤.)}

\textbf{Step 2: ì€ë‹‰ì¸µ $\to$ ì¶œë ¥ì¸µ (ìµœì¢… ì˜ˆì¸¡)}
$$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $$
$$ A^{[2]} = \sigma(Z^{[2]}) \quad (\hat{y}) $$

% --- 8. ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ ---
\section{Practical Scenario: ì–¼êµ´ ì¸ì‹}

\begin{examplebox}{ì–¼êµ´ ì¸ì‹ AIì˜ ì‚¬ê³  ê³¼ì •}
\begin{itemize}
    \item **Input ($x$):** ì´ë¯¸ì§€ì˜ ê° í”½ì…€ ë°ê¸°ê°’ (ë‹¨ìˆœí•œ ìˆ«ì ë‚˜ì—´).
    \item **Hidden Layer ($a^{[1]}$):** í”½ì…€ë“¤ì„ ì¡°í•©í•˜ì—¬ 'ì„ (Line)', 'ëª¨ì„œë¦¬(Edge)', 'ëˆˆ ëª¨ì–‘', 'ì½” ëª¨ì–‘' ê°™ì€ **íŠ¹ì§•(Feature)**ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
    \item **Output Layer ($a^{[2]}$):** ì°¾ì•„ë‚¸ ëˆˆ, ì½”, ì…ì˜ íŠ¹ì§•ì„ ì¢…í•©í•˜ì—¬ "ì´ê²ƒì€ ì² ìˆ˜ì˜ ì–¼êµ´ì´ë‹¤(Probability)"ë¼ê³  íŒë‹¨í•©ë‹ˆë‹¤.
\end{itemize}
\end{examplebox}

% --- 9. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation (Python with NumPy)}

\begin{lstlisting}[language=Python, caption=Shallow Neural Network Forward Propagation]
import numpy as np

class ShallowNN:
    def __init__(self, n_x, n_h, n_y):
        np.random.seed(1)
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”: 0ì´ ì•„ë‹Œ ì‘ì€ ëœë¤ ê°’ì´ì–´ì•¼ í•¨! (Symmetry Breaking)
        self.W1 = np.random.randn(n_h, n_x) * 0.01
        self.b1 = np.zeros((n_h, 1))
        self.W2 = np.random.randn(n_y, n_h) * 0.01
        self.b2 = np.zeros((n_y, 1))

    def forward(self, X):
        """
        X shape: (n_x, m)
        """
        # --- Layer 1 (Hidden) ---
        # Z1: (n_h, m)
        Z1 = np.dot(self.W1, X) + self.b1 
        A1 = np.tanh(Z1) # ì€ë‹‰ì¸µ í™œì„±í™” í•¨ìˆ˜ (Tanh)
        
        # --- Layer 2 (Output) ---
        # Z2: (n_y, m)
        Z2 = np.dot(self.W2, A1) + self.b2
        A2 = 1 / (1 + np.exp(-Z2)) # ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ (Sigmoid)
        
        return A2

# --- ì‹¤í–‰ ì˜ˆì‹œ ---
if __name__ == "__main__":
    # 3ê°œì˜ íŠ¹ì„±, 4ê°œì˜ ë°ì´í„° ìƒ˜í”Œ
    X = np.array([[1, 2, 3, 4], 
                  [4, 5, 6, 7], 
                  [7, 8, 9, 10]]) # shape (3, 4)
                  
    # ì…ë ¥(3) -> ì€ë‹‰(4) -> ì¶œë ¥(1)
    model = ShallowNN(n_x=3, n_h=4, n_y=1)
    output = model.forward(X)
    
    print("Output Shape:", output.shape) # (1, 4) ì˜ˆìƒ
    print("Prediction:", output)
\end{lstlisting}

% --- 10. FAQ ---
\section{FAQ: ì´ˆì‹¬ìê°€ ìì£¼ ë¬»ëŠ” ì§ˆë¬¸}
\begin{itemize}
    \item \textbf{Q1. ê°€ì¤‘ì¹˜ $W$ë¥¼ ì™œ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
    \textbf{A.} $W$ê°€ ëª¨ë‘ 0ì´ë©´ ì€ë‹‰ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ì´ ë˜‘ê°™ì€ ê³„ì‚°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤(**ëŒ€ì¹­ì„± ë¬¸ì œ**). ë‰´ëŸ°ì´ 100ê°œì—¬ë„ ì‚¬ì‹¤ìƒ 1ê°œì¸ ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì§•ì„ ë°°ìš°ê²Œ í•˜ë ¤ë©´ ëœë¤í•˜ê²Œ ê¹¨ëœ¨ë ¤ì•¼(Break Symmetry) í•©ë‹ˆë‹¤.
    
    \item \textbf{Q2. ì€ë‹‰ì¸µ ê°œìˆ˜ëŠ” ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
    \textbf{A.} **í•˜ì´í¼íŒŒë¼ë¯¸í„°**ì…ë‹ˆë‹¤. ì •ë‹µì€ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì œì˜ ë³µì¡ë„ì— ë”°ë¼ ë‹¤ë¥´ë©°, ì‹¤í—˜ì„ í†µí•´ ìµœì ì˜ ê°œìˆ˜ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ë³´í†µ ì…ë ¥ í¬ê¸°ë³´ë‹¤ ì•½ê°„ í¬ê²Œ ì¡ëŠ” ê²ƒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤.
\end{itemize}

% --- 11. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ì‹ ê²½ë§ì˜ ë¼ˆëŒ€ë¥¼ ì„¸ìš°ê³  ì‹ í˜¸(ë°ì´í„°)ë¥¼ ì•ìœ¼ë¡œ ë³´ë‚´ëŠ” ë²•(Forward)ì„ ì•Œì•˜ìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì•„ì§ í•™ìŠµì€ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µ ì‚¬ì´ì˜ ì˜¤ì°¨ë¥¼ êµ¬í•´ì„œ, ë‹¤ì‹œ ë’¤ë¡œ ë³´ë‚´ë©° ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •í•˜ëŠ” **ì—­ì „íŒŒ(Backpropagation)**ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. ì´ê²ƒì´ ë”¥ëŸ¬ë‹ í•™ìŠµì˜ ì§„ì •í•œ í•µì‹¬ì…ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Hidden Layer:} ì…ë ¥ê³¼ ì¶œë ¥ ì‚¬ì´ì—ì„œ ë¹„ì„ í˜•ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¸µ.
    \item \textbf{Notation:} $[l]$ì€ ì¸µ ë²ˆí˜¸, $(i)$ëŠ” ë°ì´í„° ë²ˆí˜¸. í˜¼ë™ ê¸ˆì§€!
    \item \textbf{Non-linearity:} í™œì„±í™” í•¨ìˆ˜(Tanh, ReLU ë“±)ê°€ ì—†ìœ¼ë©´ ì‹ ê²½ë§ì€ ë‹¨ìˆœ ì„ í˜• íšŒê·€ì™€ ê°™ë‹¤.
    \item \textbf{Dimension:} $W^{[1]}$ì˜ í¬ê¸°ëŠ” $(n^{[1]}, n^{[0]})$ì´ë‹¤. (í–‰ë ¬ í¬ê¸° ì£¼ì˜)
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{multirow} % í‘œ ë³‘í•©

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª… & ì‹œë‚˜ë¦¬ì˜¤)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Shallow Neural Networks: \\ Activation Functions}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network \textit{- Completed}
    \item[\textbf{Chapter 3.}] \textbf{Shallow Neural Networks (Current Unit)}
    \begin{itemize}
        \item 3.1 Concept of Hidden Layer \& Architecture \textit{- Completed}
        \item \textbf{3.2 Activation Functions (Sigmoid, Tanh, ReLU)}
        \begin{itemize}
            \item The Big 4 Functions
            \item Why Sigmoid Failed? (Vanishing Gradient)
            \item Professor's Choice (Best Practice)
            \item Implementation
        \end{itemize}
        \item 3.3 Backpropagation Intuition
    \end{itemize}
    \item[Chapter 4.] Deep Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ì€ë‹‰ì¸µ(Hidden Layer)ì„ ì¶”ê°€í•˜ì—¬ ì‹ ê²½ë§ì˜ ê¹Šì´ë¥¼ ë”í–ˆìŠµë‹ˆë‹¤. ê·¸ë•Œ ì œê°€ "ì€ë‹‰ì¸µì—ëŠ” Sigmoidë³´ë‹¤ Tanhë‚˜ ReLUê°€ ì¢‹ë‹¤"ê³  ìŠ¤ì³ ì§€ë‚˜ê°€ë“¯ ë§í–ˆìŠµë‹ˆë‹¤.
"ì™œìš”? Sigmoidê°€ ê°€ì¥ ìœ ëª…í•˜ì§€ ì•Šë‚˜ìš”?"
ì´ ì§ˆë¬¸ì— ë‹µí•˜ì§€ ëª»í•˜ë©´ ì—¬ëŸ¬ë¶„ì€ ë§¤ë²ˆ ëª¨ë¸ì„ ì„¤ê³„í•  ë•Œë§ˆë‹¤ 'ì„ íƒ ì¥ì• 'ì— ì‹œë‹¬ë¦´ ê²ƒì…ë‹ˆë‹¤. í™œì„±í™” í•¨ìˆ˜ëŠ” ë‹¨ìˆœí•œ ìŠ¤ìœ„ì¹˜ê°€ ì•„ë‹™ë‹ˆë‹¤. í•™ìŠµ ì‹ í˜¸(Gradient)ë¥¼ ì‚´ë¦´ ìˆ˜ë„, ì£½ì¼ ìˆ˜ë„ ìˆëŠ” **ìƒëª… ìœ ì§€ ì¥ì¹˜**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²°ì •ì§“ëŠ” **'4ëŒ€ í™œì„±í™” í•¨ìˆ˜'**ë¥¼ ì™„ë²½í•˜ê²Œ í•´ë¶€í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë¹„êµ:} Sigmoid, Tanh, ReLU, Leaky ReLUì˜ ìˆ˜ì‹ê³¼ ê·¸ë˜í”„ íŠ¹ì§•ì„ ë¹„êµí•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ê¹Šì€ ì‹ ê²½ë§ì—ì„œ Sigmoidë¥¼ ì“°ë©´ í•™ìŠµì´ ë©ˆì¶”ëŠ” **ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)** ë¬¸ì œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{ì „ëµ:} ì¶œë ¥ì¸µê³¼ ì€ë‹‰ì¸µì— ê°ê° ì–´ë–¤ í•¨ìˆ˜ë¥¼ ì¨ì•¼ í•˜ëŠ”ì§€ **Best Practice**ë¥¼ í™•ë¦½í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} NumPyë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í•¨ìˆ˜ì™€ ê·¸ ë„í•¨ìˆ˜(Derivative)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì½”ë”©í•©ë‹ˆë‹¤.
  \end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{í•¨ìˆ˜ëª…} & \textbf{ë²”ìœ„} & \textbf{í•œ ì¤„ íŠ¹ì§•} \\ \hline
\textbf{Sigmoid} & $(0, 1)$ & í™•ë¥  í‘œí˜„ì— ìµœì . í•˜ì§€ë§Œ ê¹Šì–´ì§€ë©´ í•™ìŠµ ë¶ˆê°€. \\ \hline
\textbf{Tanh} & $(-1, 1)$ & Sigmoidì˜ í™•ì¥íŒ. 0 ì¤‘ì‹¬(Zero-centered)ì´ë¼ í•™ìŠµì´ ë” ë¹ ë¦„. \\ \hline
\textbf{ReLU} & $[0, \infty)$ & \textbf{ë”¥ëŸ¬ë‹ì˜ í‘œì¤€.} ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ, ìŒìˆ˜ëŠ” ì°¨ë‹¨. ì—°ì‚° ë¹ ë¦„. \\ \hline
\textbf{Leaky ReLU} & $(-\infty, \infty)$ & ReLUì˜ ë³€í˜•. ìŒìˆ˜ì¼ ë•Œë„ ì•„ì£¼ ì•½ê°„ì˜ ê¸°ìš¸ê¸°ë¥¼ ì¤Œ. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: The Big 4 Functions}

\subsection{1. Sigmoid Function ($\sigma$)}


[Image of sigmoid function graph with equation]

\begin{itemize}
    \item \textbf{ìˆ˜ì‹:} $a = \frac{1}{1 + e^{-z}}$
    \item \textbf{íŠ¹ì§•:} 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ì••ì¶•í•©ë‹ˆë‹¤. 'í™•ë¥ ' ê°œë…ê³¼ ì˜ ë§ìŠµë‹ˆë‹¤.
    \item \textbf{ì¹˜ëª…ì  ë‹¨ì :} ì…ë ¥ê°’($z$)ì´ ì•„ì£¼ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ê¸°ìš¸ê¸°(ë¯¸ë¶„ê°’)ê°€ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤. í•™ìŠµì´ ë©ˆì¶¥ë‹ˆë‹¤.
    \item \textbf{ìš©ë„:} \textbf{ì´ì§„ ë¶„ë¥˜ì˜ ì¶œë ¥ì¸µ(Output Layer)}ì—ë§Œ ì”ë‹ˆë‹¤. ì€ë‹‰ì¸µì—” ì ˆëŒ€ ì“°ì§€ ë§ˆì„¸ìš”.
\end{itemize}

\subsection{2. Tanh (Hyperbolic Tangent)}

\begin{itemize}
    \item \textbf{ìˆ˜ì‹:} $a = \frac{e^z - e^{-z}}{e^z + e^{-z}}$
    \item \textbf{íŠ¹ì§•:} Sigmoidë¥¼ ìœ„ì•„ë˜ë¡œ ëŠ˜ë ¤ -1ì—ì„œ 1 ì‚¬ì´ ê°’ì„ ê°–ê²Œ í–ˆìŠµë‹ˆë‹¤. **í‰ê· ì´ 0(Zero-centered)**ì´ë¯€ë¡œ ë°ì´í„°ì˜ ì¤‘ì‹¬ì„ ì˜ ì¡ì•„ì£¼ì–´ Sigmoidë³´ë‹¤ í•™ìŠµ ìˆ˜ë ´ì´ ë¹ ë¦…ë‹ˆë‹¤.
    \item \textbf{ìš©ë„:} ì€ë‹‰ì¸µì—ì„œ Sigmoidë³´ë‹¤ ë¬´ì¡°ê±´ ì¢‹ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ì „íˆ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œëŠ” ìˆìŠµë‹ˆë‹¤.
\end{itemize}

\subsection{3. ReLU (Rectified Linear Unit) - The King}

\begin{itemize}
    \item \textbf{ìˆ˜ì‹:} $a = \max(0, z)$
    \item \textbf{íŠ¹ì§•:} ë‹¨ìˆœ ë¬´ì‹í•´ ë³´ì´ì§€ë§Œ ê°€ì¥ ê°•ë ¥í•©ë‹ˆë‹¤.
    \begin{itemize}
        \item $z > 0$: ê¸°ìš¸ê¸°ê°€ í•­ìƒ **1**ì…ë‹ˆë‹¤. (ì‹ í˜¸ê°€ ì•½í•´ì§€ì§€ ì•ŠìŒ)
        \item $z \le 0$: ê°’ì„ 0ìœ¼ë¡œ ì°¨ë‹¨í•©ë‹ˆë‹¤. (ë¶ˆí•„ìš”í•œ ì‹ í˜¸ ì œê±°)
    \end{itemize}
    \item \textbf{ìš©ë„:} \textbf{ëª¨ë“  ì€ë‹‰ì¸µì˜ ê¸°ë³¸ê°’(Default)}ì…ë‹ˆë‹¤. ê³ ë¯¼ë  ë• ë¬´ì¡°ê±´ ReLUë¥¼ ì“°ì„¸ìš”.
\end{itemize}

\begin{analogybox}{ì „ë“± ìŠ¤ìœ„ì¹˜ ë¹„ìœ }
\begin{itemize}
    \item \textbf{Sigmoid:} ì¡°ê´‘ê¸°(Dimmer). ë°ê¸°ë¥¼ 0\%ì—ì„œ 100\%ê¹Œì§€ ë¯¸ì„¸í•˜ê²Œ ì¡°ì ˆí•˜ì§€ë§Œ, ë„ˆë¬´ ë³µì¡í•©ë‹ˆë‹¤.
    \item \textbf{ReLU:} ë˜‘ë”± ìŠ¤ìœ„ì¹˜. ì¼œì§€ë©´ í™•ì‹¤í•˜ê²Œ ì¼œì§€ê³ (ê·¸ëŒ€ë¡œ í†µê³¼), êº¼ì§€ë©´ í™•ì‹¤í•˜ê²Œ êº¼ì§‘ë‹ˆë‹¤(0). ë‹¨ìˆœí•¨ì´ ì†ë„ì˜ ë¹„ê²°ì…ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

---

\section{Deep Dive: ì™œ SigmoidëŠ” í‡´ì¶œë‹¹í–ˆë‚˜?}

ì´ ë¶€ë¶„ì€ ë”¥ëŸ¬ë‹ ì—­ì‚¬ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì „í™˜ì  ì¤‘ í•˜ë‚˜ì¸ **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ(Vanishing Gradient Problem)**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

\subsection{The Vanishing Gradient Problem}
ì—­ì „íŒŒ(Backpropagation)ëŠ” ì¶œë ¥ì¸µì˜ ì˜¤ì°¨ë¥¼ ì…ë ¥ì¸µê¹Œì§€ ì „ë‹¬í•˜ê¸° ìœ„í•´ **ë¯¸ë¶„ê°’(ê¸°ìš¸ê¸°)ì„ ê³„ì† ê³±í•˜ëŠ”(Chain Rule)** ê³¼ì •ì…ë‹ˆë‹¤.

\begin{examplebox}{ìˆ˜í•™ì  ì¦ëª…: $0.25$ vs $1.0$}
Sigmoid í•¨ìˆ˜ì˜ ë¯¸ë¶„ ìµœëŒ“ê°’ì€ $z=0$ì¼ ë•Œ **0.25**ì…ë‹ˆë‹¤.
ë§Œì•½ ì€ë‹‰ì¸µì´ 10ê°œë¼ê³  ê°€ì •í•´ë´…ì‹œë‹¤.

\textbf{Case 1: Sigmoid ì‚¬ìš©}
$$ Gradient \approx 0.25 \times 0.25 \times \dots \times 0.25 = (0.25)^{10} \approx 0.0000009 $$
$\rightarrow$ ì…ë ¥ì¸µì— ë„ë‹¬í•  ë•Œì¯¤ ê¸°ìš¸ê¸°ëŠ” 0ì´ ë˜ì–´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ì•ë‹¨ì€ í•™ìŠµì´ ì „í˜€ ì•ˆ ë©ë‹ˆë‹¤.

\textbf{Case 2: ReLU ì‚¬ìš©} (ì–‘ìˆ˜ êµ¬ê°„)
$$ Gradient = 1 \times 1 \times \dots \times 1 = 1^{10} = 1 $$
$\rightarrow$ ê¸°ìš¸ê¸°ê°€ ì¤„ì–´ë“¤ì§€ ì•Šê³  ìƒìƒí•˜ê²Œ ì…ë ¥ì¸µê¹Œì§€ ì „ë‹¬ë©ë‹ˆë‹¤. ì´ê²ƒì´ 100ì¸µì§œë¦¬ ë”¥ëŸ¬ë‹ì´ ê°€ëŠ¥í•œ ì´ìœ ì…ë‹ˆë‹¤.
\end{examplebox}

---

\section{Professor's Cheat Sheet (Best Practice)}
ì‹¤ì „ì—ì„œ ë¬´ì—‡ì„ ì“¸ì§€ ê³ ë¯¼í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì´ ê·œì¹™ì„ ë”°ë¥´ë©´ ìƒìœ„ 10\%ì…ë‹ˆë‹¤.

\begin{tcolorbox}[colback=white, colframe=black, title=í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ê°€ì´ë“œ]
\begin{itemize}
    \item \textbf{ì¶œë ¥ì¸µ (Output Layer):}
    \begin{itemize}
        \item ì´ì§„ ë¶„ë¥˜ (0 or 1): \textbf{Sigmoid}
        \item ë‹¤ì¤‘ ë¶„ë¥˜ (Cat, Dog, Bird...): \textbf{Softmax}
        \item íšŒê·€ (ì§‘ê°’ ì˜ˆì¸¡): \textbf{Linear} (í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ)
    \end{itemize}
    
    \item \textbf{ì€ë‹‰ì¸µ (Hidden Layer):}
    \begin{itemize}
        \item \textbf{ê¸°ë³¸ (Default):} \textbf{ReLU}
        \item ReLU ì„±ëŠ¥ì´ ì•„ì‰½ê±°ë‚˜ ë‰´ëŸ°ì´ ì£½ëŠ” ê²½ìš°: \textbf{Leaky ReLU}
        \item ë°ì´í„°ê°€ ë§¤ìš° ì ê³  ëª¨ë¸ì´ ì–•ì„ ë•Œ: \textbf{Tanh}
        \item \textbf{ê¸ˆì§€:} \textbf{Sigmoid} (ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€)
    \end{itemize}
\end{itemize}
\end{tcolorbox}

---

\section{Implementation (Python with NumPy)}

í•¨ìˆ˜ê°’ë¿ë§Œ ì•„ë‹ˆë¼ ì—­ì „íŒŒì— í•„ìš”í•œ **ë„í•¨ìˆ˜(Derivative)**ê¹Œì§€ êµ¬í˜„í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Activation Functions & Derivatives]
import numpy as np

class Activations:
    @staticmethod
    def sigmoid(z):
        """ì¶œë ¥ì¸µìš©: 0 ~ 1"""
        return 1 / (1 + np.exp(-z))
    
    @staticmethod
    def sigmoid_derivative(z):
        """Sigmoid ë¯¸ë¶„: a * (1-a)"""
        s = 1 / (1 + np.exp(-z))
        return s * (1 - s)

    @staticmethod
    def relu(z):
        """ì€ë‹‰ì¸µìš©: max(0, z)"""
        return np.maximum(0, z)

    @staticmethod
    def relu_derivative(z):
        """
        ReLU ë¯¸ë¶„:
        z > 0 ì´ë©´ 1, z <= 0 ì´ë©´ 0
        """
        dZ = np.array(z, copy=True) # ì›ë³¸ ë³´ì¡´
        dZ[z <= 0] = 0
        dZ[z > 0] = 1
        return dZ

    @staticmethod
    def tanh(z):
        """ì€ë‹‰ì¸µìš©: -1 ~ 1"""
        return np.tanh(z) # NumPy ìµœì í™” í•¨ìˆ˜ ì‚¬ìš©

    @staticmethod
    def tanh_derivative(z):
        """Tanh ë¯¸ë¶„: 1 - a^2"""
        return 1 - np.power(np.tanh(z), 2)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ReLUì˜ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥ ì  ($z=0$)}
\textbf{Q. ìˆ˜í•™ì ìœ¼ë¡œ $z=0$ì—ì„œ ReLUëŠ” ë¯¸ë¶„ì´ ë¶ˆê°€ëŠ¥í•œë°(ë¾°ì¡±ì ), ì½”ë”©ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?} \\
\textbf{A.} ë§ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì»´í“¨í„° ê³µí•™ì—ì„œëŠ” ì‹¤ìš©ì ìœ¼ë¡œ ì ‘ê·¼í•©ë‹ˆë‹¤. $z=0$ì¼ ë•Œ ê¸°ìš¸ê¸°ë¥¼ ê·¸ëƒ¥ **0**ì´ë‚˜ **1** ì¤‘ í•˜ë‚˜ë¡œ ì •í•´ë²„ë¦½ë‹ˆë‹¤. (ë³´í†µ 0ìœ¼ë¡œ ë‘ ). $z$ê°€ ì •í™•íˆ 0.0000...ì´ ë  í™•ë¥ ì€ ë§¤ìš° ë‚®ìœ¼ë¯€ë¡œ í•™ìŠµì— ì•„ë¬´ëŸ° ì§€ì¥ì´ ì—†ìŠµë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Leaky ReLUëŠ” ì–¸ì œ ì“°ë‚˜ìš”?} \\
\textbf{A.} ReLUë¥¼ ì¼ëŠ”ë° í•™ìŠµ ì¤‘ì— ë‰´ëŸ°ì˜ ì¶œë ¥ì´ ê³„ì† 0ë§Œ ë‚˜ì™€ì„œ ì£½ì–´ë²„ë¦¬ëŠ” í˜„ìƒ(\textbf{Dying ReLU})ì´ ë°œìƒí•  ë•Œ ì”ë‹ˆë‹¤. ìŒìˆ˜ì¼ ë•Œ 0.01 ê°™ì€ ì‘ì€ ê¸°ìš¸ê¸°ë¥¼ ì£¼ì–´ ë‰´ëŸ°ì„ ì†Œìƒì‹œí‚µë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë‰´ëŸ°ì˜ êµ¬ì¡°(Layer)ì™€ ì‹ í˜¸ë¥¼ ì¡°ì ˆí•˜ëŠ” ìŠ¤ìœ„ì¹˜(Activation)ê¹Œì§€ ëª¨ë‘ ê°–ì·„ìŠµë‹ˆë‹¤. ìë™ì°¨ë¡œ ì¹˜ë©´ ì—”ì§„ê³¼ ë³€ì†ê¸°ë¥¼ ì¡°ë¦½í•œ ìƒíƒœì…ë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì´ ìë™ì°¨ë¥¼ ì‹¤ì œë¡œ ë‹¬ë¦¬ê²Œ ë§Œë“œëŠ” ì—”ì§„ ì í™” ê³¼ì •, ì¦‰ ì˜¤ì°¨ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ëŠ” **'ì—­ì „íŒŒ(Backpropagation)'**ì˜ ìˆ˜ì‹ì  ìœ ë„ ê³¼ì •ì„ ì•„ì£¼ ê¹Šì´ ìˆê²Œ íŒŒí—¤ì³ ë³´ê² ìŠµë‹ˆë‹¤. ê¸´ì¥í•˜ì‹­ì‹œì˜¤. ì´ì œ ì§„ì§œ ë¯¸ë¶„ì˜ ìˆ²ìœ¼ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Sigmoid:} ì¶œë ¥ì¸µ(ì´ì§„ ë¶„ë¥˜)ì—ë§Œ ì‚¬ìš©. ì€ë‹‰ì¸µ ì‚¬ìš© ì‹œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë°œìƒ.
    \item \textbf{ReLU:} ì€ë‹‰ì¸µì˜ \textbf{Default}. ì–‘ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ(ê¸°ìš¸ê¸° 1), ìŒìˆ˜ëŠ” 0. ì—°ì‚° ë¹ ë¦„.
    \item \textbf{Tanh:} Sigmoidë³´ë‹¤ ì¢‹ìŒ(Zero-centered). ì–•ì€ ëª¨ë¸ì— ì í•©.
    \item \textbf{Vanishing Gradient:} Sigmoid ë¯¸ë¶„ê°’ì´ 1ë³´ë‹¤ ì‘ì•„($\le 0.25$), ì¸µì´ ê¹Šì–´ì§€ë©´ í•™ìŠµ ì‹ í˜¸ê°€ ì‚¬ë¼ì§€ëŠ” í˜„ìƒ.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Shallow Neural Networks: \\ Forward \& Backward Propagation}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1.] Deep Learning Big Picture \textit{- Completed}
    \item[Chapter 2.] Logistic Regression as a Neural Network \textit{- Completed}
    \item[\textbf{Chapter 3.}] \textbf{Shallow Neural Networks (Current Unit)}
    \begin{itemize}
        \item 3.1 Concept of Hidden Layer \& Architecture \textit{- Completed}
        \item 3.2 Activation Functions \textit{- Completed}
        \item \textbf{3.3 Forward \& Backward Propagation (Math Heavy!)}
        \begin{itemize}
            \item Forward: The Flow of Prediction
            \item Backward: The "Blame Game" (Gradient Calculation)
            \item The 6 Magic Equations
            \item Implementation with Dimensions Check
        \end{itemize}
        \item 3.4 Random Initialization
    \end{itemize}
    \item[Chapter 4.] Deep Neural Networks \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ì‹ ê²½ë§ì˜ 'êµ¬ì¡°(Layer)'ì™€ 'ìŠ¤ìœ„ì¹˜(Activation Function)'ë¥¼ ì¥ì°©í–ˆìŠµë‹ˆë‹¤. ì´ì œ ìë™ì°¨ëŠ” ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ìë™ì°¨ë¥¼ ì•ìœ¼ë¡œ ë‹¬ë¦¬ê²Œë§Œ í•´ì„œëŠ” ìš´ì „ì„ ë°°ìš¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ê³ ë¥¼ ëƒˆì„ ë•Œ(ì˜¤ì°¨ê°€ ë°œìƒí–ˆì„ ë•Œ), ë¬´ì—‡ì´ ì˜ëª»ë˜ì—ˆëŠ”ì§€ íŒŒì•…í•˜ê³  í•¸ë“¤ì„ ëŒë¦¬ëŠ” ë²•(ìˆ˜ì •í•˜ëŠ” ë²•)ì„ ë°°ì›Œì•¼ í•©ë‹ˆë‹¤.
ì˜¤ëŠ˜ ë°°ìš¸ **ì—­ì „íŒŒ(Backpropagation)**ê°€ ë°”ë¡œ ê·¸ ê³¼ì •ì…ë‹ˆë‹¤. ìˆ˜í•™ ê¸°í˜¸ê°€ ìŸì•„ì§€ê² ì§€ë§Œ, í¬ê¸°í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. ì´ê²ƒì´ ë”¥ëŸ¬ë‹ì˜ ì‹¬ì¥ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ í•™ìŠµ ë©”ì»¤ë‹ˆì¦˜ì¸ 'ìˆœì „íŒŒ'ì™€ 'ì—­ì „íŒŒ'ë¥¼ ìˆ˜ì‹ê³¼ ì½”ë“œë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ìˆœì „íŒŒ (Forward):} ì…ë ¥ $X$ê°€ ì€ë‹‰ì¸µì„ ê±°ì³ ì¶œë ¥ $\hat{y}$ê°€ ë˜ëŠ” ê³¼ì •ì„ í–‰ë ¬ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    \item \textbf{ì—­ì „íŒŒ (Backward):} ì˜ˆì¸¡ì´ í‹€ë ¸ì„ ë•Œ, ë¹„ìš© í•¨ìˆ˜(Cost)ì˜ ê¸°ìš¸ê¸°(Gradient)ë¥¼ ë’¤ìª½ì—ì„œ ì•ìª½ìœ¼ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.
    \item \textbf{ë„êµ¬:} ë¯¸ì ë¶„ì˜ **ì—°ì‡„ ë²•ì¹™(Chain Rule)**ê³¼ í–‰ë ¬ì˜ **ì „ì¹˜(Transpose)**ê°€ ì™œ í•„ìš”í•œì§€ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ì°¨ì›(Dimension) ì˜¤ë¥˜ ì—†ì´ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì„ ì½”ë”©í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ê¸°í˜¸} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ  (ì—­í• )} \\ \hline
$Z^{[l]}$ & ì„ í˜• ì¶œë ¥ ($WX+b$) & ë‰´ëŸ°ì´ ë°›ì•„ë“¤ì¸ ì›ì‹œ ì ìˆ˜ \\ \hline
$A^{[l]}$ & í™œì„±í™” ì¶œë ¥ ($g(Z)$) & ì ìˆ˜ë¥¼ í™•ë¥ /ì‹ í˜¸ë¡œ ë³€í™˜í•œ ìµœì¢… ë¦¬í¬íŠ¸ \\ \hline
$dZ^{[l]}$ & ì˜¤ì°¨í•­ ($\partial J / \partial Z$) & "ì–¼ë§ˆë‚˜ í‹€ë ¸ë‹ˆ?" (ì±…ì„ì˜ í¬ê¸°) \\ \hline
$dW^{[l]}$ & ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° & "ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§ˆë‚˜ ìˆ˜ì •í• ê¹Œ?" \\ \hline
$*$ & \textbf{ìš”ì†Œë³„ ê³± (Element-wise)} & í–‰ë ¬ ê³±ì´ ì•„ë‹ˆë¼, ê°™ì€ ìœ„ì¹˜ë¼ë¦¬ ê³±í•¨ \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: íë¦„ì˜ ì´í•´}

\subsection{1. Forward Propagation (ì˜ˆì¸¡ì˜ íë¦„)}


ë°ì´í„°ê°€ ê°•ë¬¼ì²˜ëŸ¼ ì…ë ¥ì¸µì—ì„œ ì¶œë ¥ì¸µìœ¼ë¡œ íë¦…ë‹ˆë‹¤.
$$ Input(X) \xrightarrow{W^{[1]}, b^{[1]}} Hidden(A^{[1]}) \xrightarrow{W^{[2]}, b^{[2]}} Output(A^{[2]}) $$
ì´ ê³¼ì •ì€ ì§ê´€ì ì…ë‹ˆë‹¤. "ì…ë ¥ë°›ì•„ì„œ, ê³„ì‚°í•˜ê³ , ë„˜ê²¨ì¤€ë‹¤." ëì…ë‹ˆë‹¤.

---

\subsection{2. Backward Propagation (í•™ìŠµì˜ íë¦„)}


ì˜ˆì¸¡ê°’($A^{[2]}$)ê³¼ ì‹¤ì œê°’($Y$)ì˜ ì°¨ì´, ì¦‰ **ë¹„ìš©(Cost)**ì„ ì¤„ì´ê¸° ìœ„í•´ ë¯¸ë¶„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
ë¬¸ì œëŠ” ìš°ë¦¬ê°€ ìˆ˜ì •í•˜ê³  ì‹¶ì€ íŒŒë¼ë¯¸í„°($W^{[1]}$)ê°€ ì¶œë ¥ì¸µì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{analogybox}{í”„ë¡œì íŠ¸ ì‹¤íŒ¨ì˜ ì±…ì„ ì†Œì¬ ë”°ì§€ê¸° (The Blame Game)}
ì—¬ëŸ¬ë¶„ì´ íŒ€ì¥(ì¶œë ¥ì¸µ)ì´ê³  í”„ë¡œì íŠ¸ê°€ ì‹¤íŒ¨(Error)í–ˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.
\begin{enumerate}
    \item **Step 1 (Output Layer):** ë¨¼ì € ìµœì¢… ê²°ê³¼ë¬¼($A^{[2]}$)ì„ ë³´ê³  "ì–¼ë§ˆë‚˜ ë¶€ì¡±í–ˆëŠ”ì§€($dZ^{[2]}$)" íŒŒì•…í•©ë‹ˆë‹¤.
    \item **Step 2 (Hidden Layer):** íŒ€ì¥ì€ ìì‹ ì˜ ì‹¤íŒ¨ ì›ì¸ì„ ë¶„ì„í•˜ì—¬, ì¤‘ê°„ ê´€ë¦¬ì(ì€ë‹‰ì¸µ, $A^{[1]}$)ì—ê²Œ ì±…ì„ì„ ë¬»ìŠµë‹ˆë‹¤. "ë„¤ê°€ ì¤€ ë³´ê³ ì„œê°€ ì˜ëª»ë¼ì„œ ê²°ê³¼ê°€ ì´ë ‡ê²Œ ëì–ì•„!" ($dZ^{[1]}$ ì „íŒŒ)
    \item **Step 3 (Parameters):** ì¤‘ê°„ ê´€ë¦¬ìëŠ” ë‹¤ì‹œ ìì‹ ì˜ ì—…ë¬´ ë„êµ¬($W^{[1]}$)ë¥¼ íƒ“í•˜ë©° ìˆ˜ì •í•©ë‹ˆë‹¤. "ì´ ê°€ì¤‘ì¹˜ê°€ ë¬¸ì œì˜€êµ°, ê³ ì¹˜ì." ($dW^{[1]}$ ê³„ì‚°)
\end{enumerate}
ì—­ì „íŒŒëŠ” ì´ì²˜ëŸ¼ \textbf{ì˜¤ì°¨(ì±…ì„)ë¥¼ ë’¤ì—ì„œ ì•ìœ¼ë¡œ ì „ë‹¬í•˜ë©°} íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
\end{analogybox}

---

\section{Deep Dive: The 6 Magic Equations}

ì´ 6ê°œì˜ ìˆ˜ì‹ì€ ë”¥ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ì˜ êµ¬êµ¬ë‹¨ì…ë‹ˆë‹¤. **ì—°ì‡„ ë²•ì¹™(Chain Rule)**ì— ì˜í•´ ìœ ë„ë©ë‹ˆë‹¤.

\subsection{Phase 1: ì¶œë ¥ì¸µ (Layer 2) - ì—­ì „íŒŒì˜ ì‹œì‘}

\textbf{1. ì˜¤ì°¨ ê³„ì‚° ($dZ^{[2]}$)}
ê°€ì¥ ì§ê´€ì ì¸ ìˆ˜ì‹ì…ë‹ˆë‹¤. ì˜ˆì¸¡ê°’ê³¼ ì •ë‹µì˜ ì°¨ì´ì…ë‹ˆë‹¤.
$$ dZ^{[2]} = A^{[2]} - Y $$
\textit{(ì°¸ê³ : Cross-Entropyì™€ Sigmoid ë¯¸ë¶„ì´ ë§Œë‚˜ë©´ ì´ë ‡ê²Œ ê¹”ë”í•˜ê²Œ ì •ë¦¬ë©ë‹ˆë‹¤.)}

\textbf{2. ê°€ì¤‘ì¹˜ ê¸°ìš¸ê¸° ($dW^{[2]}$)}
ì˜¤ì°¨($dZ^{[2]}$)ì— ì…ë ¥ê°’($A^{[1]}$)ì„ ê³±í•©ë‹ˆë‹¤.
$$ dW^{[2]} = \frac{1}{m} dZ^{[2]} A^{[1]T} $$
\begin{warningbox}{ì™œ ì „ì¹˜($T$)ë¥¼ í•˜ë‚˜ìš”?}
í–‰ë ¬ ê³±ì…ˆì˜ ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤.
$dZ^{[2]}$ëŠ” $(1, m)$, $A^{[1]}$ì€ $(n^{[1]}, m)$ì…ë‹ˆë‹¤. ê³±í•˜ë ¤ë©´ $A^{[1]}$ì„ ë’¤ì§‘ì–´ì•¼ $(1, m) \times (m, n^{[1]}) = (1, n^{[1]})$ì´ ë˜ì–´ $W^{[2]}$ì™€ í¬ê¸°ê°€ ê°™ì•„ì§‘ë‹ˆë‹¤.
\end{warningbox}

\textbf{3. í¸í–¥ ê¸°ìš¸ê¸° ($db^{[2]}$)}
ì˜¤ì°¨ë“¤ì˜ í‰ê· ì…ë‹ˆë‹¤.
$$ db^{[2]} = \frac{1}{m} \sum_{rows} dZ^{[2]} $$

---

\subsection{Phase 2: ì€ë‹‰ì¸µ (Layer 1) - í•µì‹¬ êµ¬ê°„}

\textbf{4. ì€ë‹‰ì¸µ ì˜¤ì°¨ ($dZ^{[1]}$)}
ì—¬ê¸°ê°€ ê°€ì¥ ì–´ë µìŠµë‹ˆë‹¤. ì¶œë ¥ì¸µì˜ ì˜¤ì°¨ë¥¼ ê°€ì¤‘ì¹˜ ë¹„ìœ¨ë§Œí¼ ê°€ì ¸ì˜¤ê³ (Linear), í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’(Non-linear)ì„ ê³±í•©ë‹ˆë‹¤.
$$ dZ^{[1]} = \underbrace{W^{[2]T} dZ^{[2]}}_{\text{ì˜¤ì°¨ ì „íŒŒ}} \quad \underbrace{*}_{\text{ìš”ì†Œë³„ ê³±}} \quad \underbrace{g'^{[1]}(Z^{[1]})}_{\text{í™œì„±í™” ë¯¸ë¶„}} $$

\begin{itemize}
    \item $W^{[2]T} dZ^{[2]}$: ì¶œë ¥ì¸µì˜ ì˜¤ì°¨ë¥¼ ì€ë‹‰ì¸µìœ¼ë¡œ ì—­ì†¡ì‹ í•©ë‹ˆë‹¤.
    \item $*$: í–‰ë ¬ ê³±ì´ ì•„ë‹™ë‹ˆë‹¤! **Element-wise product**ì…ë‹ˆë‹¤.
    \item $g'(Z^{[1]})$: ë§Œì•½ Tanhë¥¼ ì¼ë‹¤ë©´ $(1 - A^{[1]2})$ì…ë‹ˆë‹¤.
\end{itemize}

\textbf{5, 6. íŒŒë¼ë¯¸í„° ê¸°ìš¸ê¸° ($dW^{[1]}, db^{[1]}$)}
Layer 2ì™€ ë™ì¼í•œ íŒ¨í„´ì…ë‹ˆë‹¤.
$$ dW^{[1]} = \frac{1}{m} dZ^{[1]} X^T $$
$$ db^{[1]} = \frac{1}{m} \sum dZ^{[1]} $$

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation (Python with NumPy)}

ìˆ˜ì‹ì„ ì½”ë“œë¡œ ì˜®ê¸¸ ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **ì°¨ì›(Shape) í™•ì¸**ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Full Backpropagation Implementation]
import numpy as np

def backward_propagation(parameters, cache, X, Y):
    """
    parameters: W1, b1, W2, b2
    cache: Z1, A1, Z2, A2 (Forward ë‹¨ê³„ì—ì„œ ì €ì¥í•´ë‘” ê°’)
    X, Y: ì…ë ¥ ë°ì´í„° ë° ì •ë‹µ
    """
    m = X.shape[1] # ë°ì´í„° ê°œìˆ˜
    
    # 1. íŒŒë¼ë¯¸í„° ë° ìºì‹œ ë¡œë“œ
    W2 = parameters["W2"]
    A1 = cache["A1"]
    A2 = cache["A2"]
    
    # --- Layer 2 (Output) ---
    # ìˆ˜ì‹ 1: dZ2 = A2 - Y
    dZ2 = A2 - Y
    
    # ìˆ˜ì‹ 2: dW2 (í–‰ë ¬ ê³± ì£¼ì˜: dZ2 @ A1.T)
    dW2 = (1 / m) * np.dot(dZ2, A1.T)
    
    # ìˆ˜ì‹ 3: db2 (í–‰ ë°©í–¥ í•©ê³„, keepdims í•„ìˆ˜)
    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)
    
    # --- Layer 1 (Hidden) ---
    # ìˆ˜ì‹ 4: dZ1 ê³„ì‚° (ê°€ì¥ ì¤‘ìš”!)
    # g'(z) for Tanh = 1 - a^2
    # '*' ì—°ì‚°ìëŠ” ìš”ì†Œë³„ ê³±(Element-wise)ì„ì— ìœ ì˜
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    
    # ìˆ˜ì‹ 5: dW1
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    
    # ìˆ˜ì‹ 6: db1
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
    return grads
\end{lstlisting}

% --- 8. ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤ ---
\section{Numerical Example: ê³„ì‚° íë¦„ ì¶”ì }

\begin{examplebox}{ê°„ë‹¨í•œ ì˜¤ì°¨ ì—­ì „íŒŒ ì˜ˆì‹œ}
\textbf{ìƒí™©:} ì •ë‹µ $y=1$ì¸ë°, ëª¨ë¸ì´ ì˜ˆì¸¡ $a^{[2]}=0.8$ì„ ë‚´ë†“ì•˜ìŠµë‹ˆë‹¤.
\begin{enumerate}
    \item \textbf{ì˜¤ì°¨ ë°œìƒ ($dZ^{[2]}$):} $0.8 - 1.0 = -0.2$. (0.2ë§Œí¼ ë¶€ì¡±í•¨)
    \item \textbf{ì€ë‹‰ì¸µ ì „ë‹¬:} $W^{[2]}$ê°€ $0.5$ë¼ê³  ê°€ì •í•©ì‹œë‹¤. ì€ë‹‰ì¸µìœ¼ë¡œ ì˜¤ì°¨ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
    $$ \text{ì „ë‹¬ëœ ì˜¤ì°¨} \approx 0.5 \times (-0.2) = -0.1 $$
    \item \textbf{í™œì„±í™” ë¯¸ë¶„ ë°˜ì˜:} ë§Œì•½ ì€ë‹‰ì¸µ í™œì„±í™” ë¯¸ë¶„ê°’ì´ $0.5$ë¼ë©´?
    $$ dZ^{[1]} = -0.1 \times 0.5 = -0.05 $$
    \item \textbf{ê²°ë¡ :} ì€ë‹‰ì¸µì˜ ì˜¤ì°¨ëŠ” -0.05ì…ë‹ˆë‹¤. ì´ ê°’ì„ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ $W^{[1]}$ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
\end{enumerate}
\end{examplebox}

% --- 9. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q1. $dZ^{[1]}$ êµ¬í•  ë•Œ ì™œ í–‰ë ¬ ê³±(dot)ì´ ì•„ë‹ˆë¼ ìš”ì†Œë³„ ê³±(*) ì¸ê°€ìš”?} \\
\textbf{A.} ì—°ì‡„ ë²•ì¹™ $\frac{\partial A}{\partial Z}$ ë¶€ë¶„ ë•Œë¬¸ì…ë‹ˆë‹¤. í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„ì€ ê° ë‰´ëŸ°ë§ˆë‹¤ ê°œë³„ì ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤. í–‰ë ¬ ì „ì²´ë¥¼ ì„ëŠ”(Linear mixing) ê³¼ì •ì´ ì•„ë‹ˆë¯€ë¡œ ê°™ì€ ìœ„ì¹˜ì˜ ì›ì†Œë¼ë¦¬ë§Œ ê³±í•´ì•¼ í•©ë‹ˆë‹¤.

\textbf{Q2. ì „ì¹˜($T$)ëŠ” ì–¸ì œ í•˜ë‚˜ìš”? ì™¸ì›Œì•¼ í•˜ë‚˜ìš”?} \\
\textbf{A.} ì™¸ìš°ì§€ ë§ˆì„¸ìš”. **ì°¨ì›(Dimensions)ì„ ê·¸ë ¤ë³´ë©´ ë©ë‹ˆë‹¤.**
$dW$ëŠ” $W$ì™€ ëª¨ì–‘ì´ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤. $(n, m)$ê³¼ $(1, m)$ì„ ê³±í•´ì„œ $(n, 1)$ì„ ë§Œë“¤ë ¤ë©´ ë’¤ì˜ ê²ƒì„ ë’¤ì§‘ì–´ì•¼ í•œë‹¤ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì…ë‹ˆë‹¤.

% --- 10. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ê³ ìƒí•˜ì…¨ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ë°©ê¸ˆ ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ í—˜ë‚œí•œ ê³ ê°œì¸ 'ì—­ì „íŒŒ'ë¥¼ ë„˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ì¤€ë¹„ê°€ ê±°ì˜ ë‹¤ ë˜ì—ˆìŠµë‹ˆë‹¤.

ê·¸ëŸ°ë°, í•™ìŠµì„ ì‹œì‘í•  ë•Œ **ê°€ì¤‘ì¹˜($W$)ë¥¼ ì²˜ìŒì— ì–´ë–»ê²Œ ì„¤ì •í•˜ëŠëƒ**ê°€ í•™ìŠµì˜ ì„±íŒ¨ë¥¼ ì¢Œìš°í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•„ì‹­ë‹ˆê¹Œ?
ë‹¤ìŒ ì‹œê°„ì—ëŠ” [Practice] ì„¸ì…˜ìœ¼ë¡œ, **ëœë¤ ì´ˆê¸°í™”(Random Initialization)**ì˜ ì¤‘ìš”ì„±ì„ ë‹¤ë£¨ê³ , ì™œ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ ì´ ëª¨ë“  ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì´ ë¬´ìš©ì§€ë¬¼ì´ ë˜ëŠ”ì§€ ì¦ëª…í•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{ì—­ì „íŒŒ:} ì¶œë ¥ì¸µì˜ ì˜¤ì°¨($dZ$)ë¥¼ êµ¬í•´ ì…ë ¥ì¸µ ë°©í–¥ìœ¼ë¡œ ì „íŒŒí•˜ë©° ê¸°ìš¸ê¸°($dW$)ë¥¼ êµ¬í•œë‹¤.
    \item \textbf{Chain Rule:} ì¸µì„ ê±´ë„ˆê°ˆ ë•Œë§ˆë‹¤ ë¯¸ë¶„ê°’ì„ ê³±í•œë‹¤ (ë¯¸ë¶„ì˜ ì—°ì‡„).
    \item \textbf{Transpose:} í–‰ë ¬ ê³±ì…ˆ ì‹œ ì°¨ì›ì„ ë§ì¶”ê¸° ìœ„í•´ ì „ì¹˜ í–‰ë ¬($A^T$)ì„ ì‚¬ìš©í•œë‹¤.
    \item \textbf{Element-wise:} í™œì„±í™” í•¨ìˆ˜ì˜ ë¯¸ë¶„ê°’ì€ ë°˜ë“œì‹œ ìš”ì†Œë³„ ê³±($*$)ìœ¼ë¡œ ê³„ì‚°í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Deep Neural Networks: \\ MLP Architecture}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-3.] Foundations \& Shallow Networks \textit{- Completed}
    \item[\textbf{Chapter 4.}] \textbf{Deep Neural Networks (Current Unit)}
    \begin{itemize}
        \item \textbf{4.1 Deep L-Layer Neural Network Architecture}
        \begin{itemize}
            \item General Notation ($L$, $n^{[l]}$)
            \item Hierarchical Representation (Why Deep?)
            \item Matrix Dimensions Analysis
            \item Building Blocks Implementation
        \end{itemize}
        \item 4.2 Forward Propagation in Deep Network
        \item 4.3 Deep Network Backpropagation (Overview)
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ì€ë‹‰ì¸µì´ í•˜ë‚˜ë¿ì¸ 'ì–•ì€ ì‹ ê²½ë§'ì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ ì„¸ê³„ì˜ ë³µì¡í•œ ë¬¸ì œ(ììœ¨ì£¼í–‰, ìì—°ì–´ ì²˜ë¦¬ ë“±)ë¥¼ í’€ê¸°ì—” ë‡Œ ìš©ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.
ì´ì œ ìš°ë¦¬ëŠ” ì€ë‹‰ì¸µì„ 2ê°œ, 3ê°œ, ì•„ë‹ˆ ìˆ˜ë°± ê°œê¹Œì§€ ìŒ“ì•„ ì˜¬ë¦´ ê²ƒì…ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ ì—¬ëŸ¬ë¶„ì´ ë§¤ì¼ ë“£ëŠ” **'ë”¥ëŸ¬ë‹(Deep Learning)'**ì˜ ì‹¤ì²´ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì¸µë§Œ ëŠ˜ë¦¬ëŠ” ê²Œ ì•„ë‹ˆë¼, ì½”ë“œë¥¼ **ì¼ë°˜í™”(Generalization)**í•˜ì—¬ ì–´ë–¤ ê¹Šì´ì˜ ëª¨ë¸ë„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê±´ì¶•ê°€ê°€ ë˜ì–´ ë´…ì‹œë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ $L$ê°œì˜ ì¸µì„ ê°€ì§„ ì¼ë°˜í™”ëœ ì‹¬ì¸µ ì‹ ê²½ë§(Deep MLP)ì„ ì„¤ê³„í•˜ê³  êµ¬í˜„í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{í‘œê¸°ë²•:} ì¸µì˜ ê°œìˆ˜ê°€ $L$ê°œì¼ ë•Œì˜ íŒŒë¼ë¯¸í„°($W^{[l]}, b^{[l]}$)ì™€ í™œì„±í™”ê°’($A^{[l]}$)ì„ ì •ì˜í•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ë”¥ëŸ¬ë‹ì´ ë°ì´í„°ë¥¼ **ê³„ì¸µì (Hierarchical)**ìœ¼ë¡œ ì´í•´í•˜ëŠ” ë°©ì‹(ì  $\to$ ì„  $\to$ ë©´)ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì°¨ì›:} ê° ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜($n^{[l]}$)ë§Œ ë³´ê³ ë„ ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ì¦‰ì‹œ ê³„ì‚°í•´ëƒ…ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} í•˜ë“œì½”ë”©(W1, W2...)ì„ ë²„ë¦¬ê³ , `for-loop`ì™€ `Dictionary`ë¥¼ ì´ìš©í•´ ìœ ì—°í•œ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology: The Deep Notation}
ì–•ì€ ì‹ ê²½ë§ì—ì„œ ì“°ë˜ í‘œê¸°ë²•ì„ í™•ì¥í•©ë‹ˆë‹¤. $l$ì€ í˜„ì¬ ì¸µ ë²ˆí˜¸ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ê¸°í˜¸} & \textbf{ì˜ë¯¸} & \textbf{ì„¤ëª…} \\ \hline
$L$ & ì „ì²´ ì¸µ ìˆ˜ & ì…ë ¥ì¸µ(0ë²ˆ)ì„ ì œì™¸í•œ ì¸µì˜ ê°œìˆ˜. \\ \hline
$n^{[l]}$ & $l$ë²ˆì§¸ ì¸µì˜ ë‰´ëŸ° ìˆ˜ & $n^{[0]}=n_x$ (ì…ë ¥), $n^{[L]}$ (ì¶œë ¥). \\ \hline
$g^{[l]}$ & $l$ë²ˆì§¸ ì¸µì˜ í™œì„±í™” í•¨ìˆ˜ & ë³´í†µ ì€ë‹‰ì¸µì€ ReLU, ì¶œë ¥ì¸µì€ Sigmoid. \\ \hline
$A^{[l]}$ & $l$ë²ˆì§¸ ì¸µì˜ ì¶œë ¥ & $A^{[l]} = g^{[l]}(Z^{[l]})$. ë‹¤ìŒ ì¸µì˜ ì…ë ¥ì´ ë¨. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì™œ ê¹Šê²Œ ìŒ“ëŠ”ê°€?}

\subsection{1. Hierarchical Representation (ê³„ì¸µì  í‘œí˜„)}
"êµìˆ˜ë‹˜, ê·¸ëƒ¥ ì€ë‹‰ì¸µ 1ê°œì— ë‰´ëŸ° 100ë§Œ ê°œë¥¼ ë„£ëŠ” ê²Œ(Wide), 10ë§Œ ê°œì”© 10ì¸µ ìŒ“ëŠ” ê²ƒ(Deep)ë³´ë‹¤ ë‚«ì§€ ì•Šë‚˜ìš”?"
\textbf{ì•„ë‹™ë‹ˆë‹¤.} ë”¥ëŸ¬ë‹ì˜ í˜ì€ **'ìª¼ê°œì„œ ì´í•´í•˜ê¸°'**ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤.



\begin{analogybox}{ì‚¬ëŒì˜ ì–¼êµ´ ì¸ì‹ ê³¼ì •}
ìš°ë¦¬ì˜ ë‡Œë‚˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ë³µì¡í•œ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ì´í•´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
\begin{enumerate}
    \item \textbf{Layer 1 (Low-level):} í”½ì…€ì„ ë³´ê³  ê°€ë¡œì„ , ì„¸ë¡œì„  ê°™ì€ **ê²½ê³„(Edges)**ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    \item \textbf{Layer 2 (Mid-level):} ì„ ë“¤ì„ ì¡°í•©í•´ì„œ ëˆˆ, ì½”, ê·€ ê°™ì€ **ë¶€ë¶„(Parts)**ì„ ë§Œë“­ë‹ˆë‹¤.
    \item \textbf{Layer 3 (High-level):} ë¶€ë¶„ë“¤ì„ ì¡°í•©í•´ì„œ **ì‚¬ëŒ ì–¼êµ´(Face)** ì „ì²´ë¥¼ ì¸ì‹í•©ë‹ˆë‹¤.
\end{enumerate}
ì¸µì„ ê¹Šê²Œ ìŒ“ìœ¼ë©´, ì ì€ íŒŒë¼ë¯¸í„°ë¡œë„ ë§¤ìš° ë³µì¡í•œ í•¨ìˆ˜(ì‚¬ëŒ ì–¼êµ´ ë“±)ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{analogybox}

---

\subsection{2. Matrix Dimensions (ì°¨ì› ë¶„ì„)}
ì´ ë¶€ë¶„ì€ êµ¬í˜„ê³¼ ë””ë²„ê¹…ì˜ í•µì‹¬ì…ë‹ˆë‹¤. ë¬´ì¡°ê±´ ì•”ê¸°í•´ì•¼ í•©ë‹ˆë‹¤.

$l$ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ $W^{[l]}$ì™€ í¸í–¥ $b^{[l]}$ì˜ í¬ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{$W^{[l]}$ Shape:} $(n^{[l]}, n^{[l-1]})$ $\rightarrow$ (í˜„ì¬ ì¸µ ë‰´ëŸ° ìˆ˜, ì´ì „ ì¸µ ë‰´ëŸ° ìˆ˜)
    \item \textbf{$b^{[l]}$ Shape:} $(n^{[l]}, 1)$
    \item \textbf{$Z^{[l]}, A^{[l]}$ Shape:} $(n^{[l]}, m)$ $\rightarrow$ (í˜„ì¬ ì¸µ ë‰´ëŸ° ìˆ˜, ë°ì´í„° ê°œìˆ˜)
\end{itemize}

\begin{examplebox}{ì°¨ì› ê³„ì‚° í€´ì¦ˆ}
\textbf{ìƒí™©:}
ì…ë ¥ íŠ¹ì„± $n_x = 12288$ (ì´ë¯¸ì§€).
Layer 1 ë‰´ëŸ°: 20ê°œ.
Layer 2 ë‰´ëŸ°: 7ê°œ.

\textbf{ì§ˆë¬¸:} $W^{[1]}$ê³¼ $W^{[2]}$ì˜ í¬ê¸°ëŠ”?
\begin{itemize}
    \item $W^{[1]}$: $(n^{[1]}, n^{[0]}) = (20, 12288)$
    \item $W^{[2]}$: $(n^{[2]}, n^{[1]}) = (7, 20)$
\end{itemize}
\end{examplebox}

---

\section{Implementation: Building Deep Network}

ì´ì œ $L$ê°œì˜ ì¸µì„ ê°€ì§„ ì‹ ê²½ë§ì„ ë§Œë“­ë‹ˆë‹¤. `W1`, `W2` ë³€ìˆ˜ë¥¼ ë”°ë¡œ ë§Œë“¤ì§€ ì•Šê³  `parameters['W' + str(l)]` í˜•íƒœë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=L-Layer Deep Neural Network Initialization & Forward]
import numpy as np

class DeepNN:
    def __init__(self, layer_dims):
        """
        layer_dims: ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜ë¥¼ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ 
                    ì˜ˆ: [12288, 20, 7, 5, 1] (4-Layer Network)
        """
        self.params = {}
        self.L = len(layer_dims) - 1 # ì…ë ¥ì¸µ ì œì™¸í•œ ì¸µ ìˆ˜
        
        for l in range(1, self.L + 1):
            # He Initialization (ReLU ì‚¬ìš© ì‹œ í•„ìˆ˜!)
            # 0.01 ëŒ€ì‹  np.sqrt(2 / ì´ì „ ì¸µ ë‰´ëŸ° ìˆ˜)ë¥¼ ê³±í•¨
            self.params['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2/layer_dims[l-1])
            self.params['b' + str(l)] = np.zeros((layer_dims[l], 1))
            
            # ì°¨ì› í™•ì¸ (ìŠµê´€í™”!)
            assert(self.params['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))

    def forward(self, X):
        """
        [Linear -> ReLU] * (L-1) -> [Linear -> Sigmoid] * 1
        """
        caches = []
        A = X
        L = self.L
        
        # 1. ì€ë‹‰ì¸µ (1 ~ L-1): ReLU
        for l in range(1, L):
            A_prev = A 
            W = self.params['W' + str(l)]
            b = self.params['b' + str(l)]
            
            # Linear
            Z = np.dot(W, A_prev) + b
            # Activation (ReLU)
            A = np.maximum(0, Z)
            
            # ì—­ì „íŒŒë¥¼ ìœ„í•´ ì €ì¥ (W, b, A_prev, Z)
            caches.append((A_prev, W, b, Z))
            
        # 2. ì¶œë ¥ì¸µ (L): Sigmoid (ì´ì§„ ë¶„ë¥˜)
        W = self.params['W' + str(L)]
        b = self.params['b' + str(L)]
        
        Z = np.dot(W, A) + b
        AL = 1 / (1 + np.exp(-Z)) # Sigmoid
        caches.append((A, W, b, Z))
        
        return AL, caches

# --- ì‹¤í–‰ ì˜ˆì œ ---
if __name__ == "__main__":
    # [ì…ë ¥(3) -> ì€ë‹‰(5) -> ì€ë‹‰(3) -> ì¶œë ¥(1)] êµ¬ì¡°
    layers = [3, 5, 3, 1] 
    model = DeepNN(layers)
    
    # ê°€ìƒì˜ ë°ì´í„° (3 features, 4 samples)
    X = np.random.randn(3, 4)
    
    AL, _ = model.forward(X)
    print("Output Shape:", AL.shape) # (1, 4) ì˜ˆìƒ
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”: 0.01 vs He Initialization}
ì–•ì€ ì‹ ê²½ë§ì—ì„œëŠ” `* 0.01`ë¡œ ì´ˆê¸°í™”í•´ë„ ê´œì°®ì•˜ìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì¸µì´ ê¹Šì–´ì§€ë©´($L > 5$), ê°’ì´ ê³„ì† ê³±í•´ì§€ë©´ì„œ ì‹ í˜¸ê°€ ì‚¬ë¼ì§€ê±°ë‚˜ í­ë°œí•©ë‹ˆë‹¤(Vanishing/Exploding Gradient).
ë”°ë¼ì„œ ReLUë¥¼ ì“¸ ë•ŒëŠ” **He Initialization** (`np.sqrt(2/n)`)ì„ ì“°ëŠ” ê²ƒì´ **ë”¥ëŸ¬ë‹ì˜ í‘œì¤€(Standard)**ì…ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Cache ë¦¬ìŠ¤íŠ¸ëŠ” ì™œ ë§Œë“œë‚˜ìš”?} \\
\textbf{A.} ìˆœì „íŒŒ(Forward)ê°€ ëë‚˜ë©´ ë°”ë¡œ ì—­ì „íŒŒ(Backward)ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤. ì—­ì „íŒŒ ìˆ˜ì‹ì„ ë³´ë©´ $Z$, $A_{prev}$, $W$ ê°’ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ë¯¸ ê³„ì‚°í•œ ê°’ì„ ë²„ë¦¬ì§€ ì•Šê³  `caches`ì— ì €ì¥í•´ë‘ë©´, ë‹¤ì‹œ ê³„ì‚°í•  í•„ìš” ì—†ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ì—¬ëŸ¬ë¶„ì€ ì–´ë–¤ ê¹Šì´, ì–´ë–¤ êµ¬ì¡°ì˜ ì‹ ê²½ë§ë„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì„¤ê³„ ëŠ¥ë ¥ì„ ê°–ì·„ìŠµë‹ˆë‹¤. `layers` ë¦¬ìŠ¤íŠ¸ì— ìˆ«ìë§Œ ë°”ê¿” ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤.

í•˜ì§€ë§Œ ê¹Šì€ ì‹ ê²½ë§ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì€ ìƒê°ë³´ë‹¤ ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤. 
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì´ ëª¨ë¸ì„ ê°€ì§€ê³  **'ê³ ì–‘ì´ vs ê°œ'** ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‹¤ì œ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•˜ë©°, í•™ìŠµ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•´ ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Deep Learning:} ì€ë‹‰ì¸µì„ ì—¬ëŸ¬ ê°œ ìŒ“ì•„ ê³„ì¸µì  íŠ¹ì§•(Hierarchical Features)ì„ í•™ìŠµí•œë‹¤.
    \item \textbf{Dimensions:} $W^{[l]}$ì˜ í¬ê¸°ëŠ” $(n^{[l]}, n^{[l-1]})$ì´ë‹¤. (í˜„ì¬ ì¸µ, ì´ì „ ì¸µ)
    \item \textbf{Implementation:} `for-loop`ë¥¼ ì‚¬ìš©í•˜ì—¬ $L$ë²ˆ ë°˜ë³µí•˜ëŠ” ì¼ë°˜í™”ëœ ì½”ë“œë¥¼ ì‘ì„±í•œë‹¤.
    \item \textbf{Initialization:} ê¹Šì€ ë§ì—ì„œëŠ” **He Initialization**ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë¶ˆì•ˆì •ì„ ë§‰ëŠ”ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{examplebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤ & ê³„ì‚°)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Deep Neural Networks: \\ Dimensions \& Initialization}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-3.] Foundations \& Shallow Networks \textit{- Completed}
    \item[Chapter 4.] Deep Neural Networks
    \begin{itemize}
        \item 4.1 Deep L-Layer Neural Network Architecture \textit{- Completed}
        \item \textbf{4.2 Dimensions \& Initialization (Current Unit)}
        \begin{itemize}
            \item The Law of Matrix Dimensions
            \item Symmetry Breaking (Why not Zero?)
            \item He Initialization (The Standard for ReLU)
            \item Implementation \& Verification
        \end{itemize}
        \item 4.3 Building a Deep Neural Network Application \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì´ì œ ê±°ëŒ€í•œ ì‹¬ì¸µ ì‹ ê²½ë§ì„ ì„¤ê³„í•  ìˆ˜ ìˆëŠ” ê±´ì¶•ê°€ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„¤ê³„ë„ë§Œ ê·¸ë ¸ì„ ë¿, ì•„ì§ ì°©ê³µë„ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
ê±´ë¬¼ì„ ì˜¬ë¦¬ê¸° ì „ì— ê°€ì¥ ë¨¼ì € í•´ì•¼ í•  ì¼ì€ ë¬´ì—‡ì¼ê¹Œìš”? **ì„¤ê³„ë„ ê²€ì¦(ì°¨ì› í™•ì¸)**ê³¼ **ê¸°ì´ˆ ê³µì‚¬(ì´ˆê¸°í™”)**ì…ë‹ˆë‹¤. ì´ ë‘ ê°€ì§€ë¥¼ ì†Œí™€íˆ í•˜ë©´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ìë§ˆì ì—ëŸ¬ê°€ í„°ì§€ê±°ë‚˜(Dimension Mismatch), ì—ëŸ¬ ë©”ì‹œì§€ í•˜ë‚˜ ì—†ì´ í•™ìŠµì´ ì „í˜€ ì•ˆ ë˜ëŠ”(Bad Initialization) ì¹¨ë¬µì˜ ë²„ê·¸ë¥¼ ë§Œë‚˜ê²Œ ë©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë””ë²„ê¹… ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì—¬ì£¼ëŠ” **'ì°¨ì› ë¶„ì„'**ê³¼ í•™ìŠµ ì„±ê³µì˜ ì—´ì‡ ì¸ **'íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”'**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë¶„ì„:} $L$ì¸µ ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„°($W, b$)ì™€ ë°ì´í„°($Z, A$)ì˜ í˜•ìƒ(Shape)ì„ ì •í™•íˆ ë„ì¶œí•©ë‹ˆë‹¤.
    \item \textbf{ì´ìœ :} ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í–ˆì„ ë•Œ ë°œìƒí•˜ëŠ” **'ëŒ€ì¹­ì„± ë¬¸ì œ(Symmetry Problem)'**ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{í•´ê²°:} ReLUë¥¼ ìœ„í•œ í‘œì¤€ ì´ˆê¸°í™” ë°©ë²•ì¸ **He Initialization**ì˜ ì›ë¦¬ì™€ ì½”ë“œë¥¼ ìµí™ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì„¤ëª…} & \textbf{í•µì‹¬ í¬ì¸íŠ¸} \\ \hline
\textbf{Shape} & í–‰ë ¬ì˜ ì°¨ì› (í–‰, ì—´) & ë””ë²„ê¹…ì˜ 90\%ëŠ” Shape ë§ì¶”ê¸°ì…ë‹ˆë‹¤. \\ \hline
\textbf{Symmetry Breaking} & ëŒ€ì¹­ì„± íŒŒê´´ & ë‰´ëŸ°ë“¤ì´ ì„œë¡œ ë‹¤ë¥´ê²Œ í•™ìŠµë˜ë„ë¡ ì´ˆê¸°ê°’ì„ ë‹¤ë¥´ê²Œ ì£¼ëŠ” ê²ƒ. \\ \hline
\textbf{Zero Init} & 0ìœ¼ë¡œ ì´ˆê¸°í™” & ëª¨ë“  ë‰´ëŸ°ì´ ë˜‘ê°™ì´ ë™ì‘í•˜ê²Œ ë§Œë“œëŠ” \textbf{ìµœì•…ì˜ ë°©ë²•}. \\ \hline
\textbf{He Init} & He ì´ˆê¸°í™” & ReLU ì‚¬ìš© ì‹œ ë¶„ì‚°ì„ ìœ ì§€í•´ì£¼ëŠ” \textbf{ìµœê³ ì˜ ë°©ë²•}. \\ \hline
\textbf{Xavier Init} & Xavier ì´ˆê¸°í™” & Sigmoid/Tanh ì‚¬ìš© ì‹œ ì í•©í•œ ì´ˆê¸°í™” ë°©ë²•. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë””ë²„ê¹…ì„ ìœ„í•œ í—Œë²•}

\subsection{1. Matrix Dimensions Rules (ì°¨ì›ì˜ ë²•ì¹™)}
ì½”ë”©í•˜ë‹¤ í—·ê°ˆë¦´ ë•Œë§ˆë‹¤ ì´ í‘œë¥¼ ë³´ì‹­ì‹œì˜¤. $n^{[l]}$ì€ í˜„ì¬ ì¸µì˜ ë‰´ëŸ° ìˆ˜, $m$ì€ ë°ì´í„° ê°œìˆ˜ì…ë‹ˆë‹¤.

\begin{tcolorbox}[colback=white, colframe=black, title=Shape Cheat Sheet]
\begin{itemize}
    \item \textbf{íŒŒë¼ë¯¸í„° (í•™ìŠµ ëŒ€ìƒ):}
    \begin{itemize}
        \item $W^{[l]}$: $(n^{[l]}, n^{[l-1]})$ $\rightarrow$ (í˜„ì¬ ì¸µ, ì´ì „ ì¸µ)
        \item $b^{[l]}$: $(n^{[l]}, 1)$ $\rightarrow$ ì—´ ë²¡í„° (Column Vector)
    \end{itemize}
    \item \textbf{ë°ì´í„° íë¦„ (Activations):}
    \begin{itemize}
        \item $Z^{[l]}, A^{[l]}$: $(n^{[l]}, m)$
        \item $dZ^{[l]}, dA^{[l]}$: $(n^{[l]}, m)$ $\rightarrow$ ì›ë˜ ë°ì´í„°ì™€ Shape ë™ì¼
    \end{itemize}
\end{itemize}
\end{tcolorbox}

---

\subsection{2. Why Not Zero Initialization? (0 ì´ˆê¸°í™”ì˜ ì €ì£¼)}
"êµìˆ˜ë‹˜, ë¡œì§€ìŠ¤í‹± íšŒê·€ì—ì„  0ìœ¼ë¡œ í•´ë„ ì˜ ëì–ì•„ìš”?"
ë„¤, í•˜ì§€ë§Œ **ì‹ ê²½ë§(ì€ë‹‰ì¸µì´ ìˆëŠ” ê²½ìš°)ì—ì„œëŠ” ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.**

\begin{analogybox}{ë³µì œ ì¸ê°„ êµ°ëŒ€ ë¹„ìœ }
\begin{itemize}
    \item \textbf{ìƒí™©:} ëª¨ë“  ê°€ì¤‘ì¹˜ $W$ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤.
    \item \textbf{Forward:} ëª¨ë“  ì€ë‹‰ ë‰´ëŸ°ì´ ì…ë ¥ê°’ì— ìƒê´€ì—†ì´ ë˜‘ê°™ì€ ê°’(0)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    \item \textbf{Backward:} ëª¨ë“  ë‰´ëŸ°ì´ ë˜‘ê°™ì€ ì˜¤ì°¨(Gradient)ë¥¼ ë³´ê³ ë°›ìŠµë‹ˆë‹¤.
    \item \textbf{Update:} ëª¨ë“  ë‰´ëŸ°ì´ ë˜‘ê°™ì€ ê°’ìœ¼ë¡œ ìˆ˜ì •ë©ë‹ˆë‹¤.
    \item \textbf{ê²°ê³¼:} ë‰´ëŸ°ì´ 100ë§Œ ê°œì—¬ë„, ê²°êµ­ \textbf{ë‰´ëŸ° 1ê°œì§œë¦¬ ì„ í˜• ëª¨ë¸}ê³¼ ë˜‘ê°™ì´ í–‰ë™í•©ë‹ˆë‹¤. ì´ë¥¼ **ëŒ€ì¹­ì„±(Symmetry)** ë¬¸ì œë¼ê³  í•˜ë©°, í•™ìŠµì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

---

\subsection{3. Best Practice: He Initialization}
ê·¸ë ‡ë‹¤ë©´ ëœë¤í•˜ê²Œ(Random) ì´ˆê¸°í™”í•˜ë©´ ë ê¹Œìš”? 
ë„ˆë¬´ í¬ë©´(x10) ê¸°ìš¸ê¸° ì†Œì‹¤ì´ ì˜¤ê³ , ë„ˆë¬´ ì‘ìœ¼ë©´(x0.0001) ì‹ í˜¸ê°€ ì£½ì–´ë²„ë¦½ë‹ˆë‹¤.



Kaiming He ë°•ì‚¬ê°€ ì œì•ˆí•œ **He Initialization**ì€ ReLUë¥¼ ì‚¬ìš©í•  ë•Œ ë¶„ì‚°ì„ ì¼ì •í•˜ê²Œ ìœ ì§€í•´ì£¼ëŠ” ë§ˆë²•ì˜ ê³µì‹ì…ë‹ˆë‹¤.

$$ W^{[l]} \sim \text{Random} \times \sqrt{\frac{2}{n^{[l-1]}}} $$
\begin{itemize}
    \item ì´ì „ ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜($n^{[l-1]}$)ê°€ ë§ì„ìˆ˜ë¡, ê°€ì¤‘ì¹˜ë¥¼ ë” ì‘ê²Œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.
    \item $\sqrt{2}$ëŠ” ReLUê°€ ìŒìˆ˜ ì˜ì—­ì„ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ë¶„ì‚°ì„ ì ˆë°˜ìœ¼ë¡œ ê¹ì•„ë¨¹ëŠ” ê²ƒì„ ë³´ìƒí•´ì¤ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Initialization Strategies}

ë‚˜ìœ ì˜ˆ(Zero, Large Random)ì™€ ì¢‹ì€ ì˜ˆ(He)ë¥¼ ì½”ë“œë¡œ ë¹„êµí•´ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=Parameter Initialization Methods]
import numpy as np

class Initializer:
    def __init__(self, layer_dims):
        self.layer_dims = layer_dims # ì˜ˆ: [1000, 100, 10]
        self.L = len(layer_dims) - 1

    def init_zeros(self):
        """
        BAD: ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™” -> í•™ìŠµ ë¶ˆê°€
        """
        params = {}
        for l in range(1, self.L + 1):
            params['W' + str(l)] = np.zeros((self.layer_dims[l], self.layer_dims[l-1]))
            params['b' + str(l)] = np.zeros((self.layer_dims[l], 1))
        return params

    def init_he(self):
        """
        BEST: He Initialization (Standard for ReLU)
        """
        params = {}
        for l in range(1, self.L + 1):
            # 1. ì°¨ì› ì •ì˜
            n_curr = self.layer_dims[l]
            n_prev = self.layer_dims[l-1]
            
            # 2. He Initialization ê³µì‹ ì ìš©
            # np.random.randn: í‰ê·  0, ë¶„ì‚° 1ì¸ ì •ê·œë¶„í¬
            # scaling: ë¶„ì‚°ì„ 2/n_prev ë¡œ ë§ì¶°ì¤Œ
            scaling = np.sqrt(2 / n_prev)
            
            params['W' + str(l)] = np.random.randn(n_curr, n_prev) * scaling
            params['b' + str(l)] = np.zeros((n_curr, 1)) # í¸í–¥ì€ 0ì´ì–´ë„ ë¨!
            
        return params

# --- ê²€ì¦ ---
if __name__ == "__main__":
    dims = [1000, 100, 10]
    init = Initializer(dims)
    
    # He Init ê²°ê³¼ í™•ì¸
    params = init.init_he()
    W1 = params['W1']
    
    print("Shape Check:", W1.shape) # (100, 1000)
    print("Variance Check:", np.var(W1)) 
    print("Expected Variance:", 2/1000) # 0.002 ê·¼ì²˜ì—¬ì•¼ í•¨
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{í¸í–¥(Bias) $b$ëŠ” 0ìœ¼ë¡œ í•´ë„ ë˜ë‚˜ìš”?}
\textbf{ë„¤, ë©ë‹ˆë‹¤!}
ëŒ€ì¹­ì„± ë¬¸ì œëŠ” ê°€ì¤‘ì¹˜ $W$ì—ì„œ ë°œìƒí•©ë‹ˆë‹¤. $W$ê°€ ì´ë¯¸ ëœë¤í•˜ê²Œ ì„ì—¬ ìˆë‹¤ë©´(Symmetry Broken), í¸í–¥ $b$ê°€ ëª¨ë‘ 0ì´ì–´ë„ ë‰´ëŸ°ë“¤ì€ ì„œë¡œ ë‹¤ë¥¸ ê°’ì„ ì¶œë ¥í•˜ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ $b$ëŠ” í¸ì˜ìƒ `np.zeros`ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Xavier ì´ˆê¸°í™”ëŠ” ë­”ê°€ìš”?} \\
\textbf{A.} Sigmoidë‚˜ Tanh í•¨ìˆ˜ë¥¼ ì“¸ ë•Œ ì‚¬ìš©í•˜ëŠ” ì´ˆê¸°í™” ë°©ë²•ì…ë‹ˆë‹¤. ê³„ìˆ˜ê°€ $\sqrt{1/n}$ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ìš”ì¦˜ ë”¥ëŸ¬ë‹ì€ ëŒ€ë¶€ë¶„ ReLUë¥¼ ì“°ê¸° ë•Œë¬¸ì— He ì´ˆê¸°í™”($\sqrt{2/n}$)ê°€ ë” ë§ì´ ì“°ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” **ì„¤ê³„(Architecture)**, **ê¸°ì´ˆ ê³µì‚¬(Initialization)**, **ìì¬ ê²€ìˆ˜(Dimension Check)**ê¹Œì§€ ì™„ë²½í•˜ê²Œ ë§ˆì³¤ìŠµë‹ˆë‹¤.

ì´ì œ ë‚¨ì€ ê²ƒì€ ê±´ë¬¼ì„ ì§“ëŠ” ê²ƒë¿ì…ë‹ˆë‹¤. ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Project] Building a Deep Neural Network Application**ì„ í†µí•´, ìš°ë¦¬ê°€ ë§Œë“  ì½”ë“œë¡œ **'ê³ ì–‘ì´ vs ê°œ'** ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì¸ê³µì§€ëŠ¥ì„ ì™„ì„±í•˜ê² ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì˜ ì²« ë²ˆì§¸ Deep Learning í”„ë¡œì íŠ¸ê°€ ì‹œì‘ë©ë‹ˆë‹¤!

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Dimensions:} $W$ëŠ” $(n^{[l]}, n^{[l-1]})$ì´ë‹¤. ì°¨ì› í™•ì¸ì´ ë””ë²„ê¹…ì˜ ì‹œì‘ì´ë‹¤.
    \item \textbf{Zero Init:} $W$ë¥¼ 0ìœ¼ë¡œ í•˜ë©´ í•™ìŠµì´ ì•ˆ ëœë‹¤. (ëŒ€ì¹­ì„± ë¬¸ì œ)
    \item \textbf{He Init:} ReLUë¥¼ ì“¸ ë•ŒëŠ” `randn * sqrt(2/n)` ê³µì‹ì„ ì‚¬ìš©í•˜ë¼.
    \item \textbf{Bias:} í¸í–¥ $b$ëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•´ë„ ì•ˆì „í•˜ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬
\usepackage{colortbl} % í‘œ ìƒ‰ìƒ

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{diagnosisbox}[1]{
    colback=purple!5!white,
    colframe=purple!80!black,
    fonttitle=\bfseries,
    title=ğŸ©º #1 (ëª¨ë¸ ì§„ë‹¨)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Data Setup Strategy}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item \textbf{5.1 Train / Dev / Test Sets Strategy}
        \begin{itemize}
            \item The Purpose of Splitting
            \item Big Data Era Ratio (98/1/1)
            \item Distribution Mismatch & Data Leakage
            \item Bias-Variance Diagnosis
        \end{itemize}
        \item 5.2 Regularization (L2, Dropout)
        \item 5.3 Optimization Algorithms (Adam, RMSProp)
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ì‹ ê²½ë§ì´ë¼ëŠ” **'ìµœê³ ê¸‰ ì—”ì§„'**ì„ ì¡°ë¦½í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í˜ë¼ë¦¬ ì—”ì§„ì„ íŠ¸ë™í„°ì— ë‹¬ê±°ë‚˜, ë¶ˆìˆœë¬¼ì´ ì„ì¸ ì—°ë£Œë¥¼ ë„£ìœ¼ë©´ ì•„ë¬´ ì†Œìš©ì´ ì—†ìŠµë‹ˆë‹¤.
ì´ì œë¶€í„°ëŠ” ì—”ì§„ì„ **'ì–´ë–»ê²Œ ìš´ìš©í•´ì•¼(Strategy)'** ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆëŠ”ì§€ ë°°ì›ë‹ˆë‹¤. ê·¸ ì²«ê±¸ìŒì€ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë‚˜ëˆ„ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§ì€ ì´ˆì‹¬ìê°€ ë°ì´í„°ë¥¼ ëª½ë•… í„¸ì–´ ë„£ê³  í•™ìŠµë¶€í„° ì‹œí‚¤ì§€ë§Œ, ì´ëŠ” "ì±„ì  ê¸°ì¤€ë„ ëª¨ë¥¸ ì±„ ì‹œí—˜ ê³µë¶€ë¥¼ í•˜ëŠ” ê²ƒ"ê³¼ ê°™ìŠµë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì„±ê³µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì˜ ë‚˜ì¹¨ë°˜ì¸ **'ë°ì´í„° ë¶„í•  ì „ëµ'**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì •ì˜:} Train(í•™ìŠµ), Dev(íŠœë‹), Test(í‰ê°€) ì„¸íŠ¸ì˜ ëª…í™•í•œ ì—­í•  ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ë¹„ìœ¨:} ë¹…ë°ì´í„° ì‹œëŒ€(100ë§Œ ê°œ ì´ìƒ)ì— ì™œ **98:1:1** ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ëŠ”ì§€ í†µê³„ì ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.
    \item \textbf{ì›ì¹™:} Devì™€ Test ì„¸íŠ¸ê°€ ë°˜ë“œì‹œ **ë™ì¼í•œ ë¶„í¬(Same Distribution)**ì—¬ì•¼ í•˜ëŠ” ì´ìœ ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì§„ë‹¨:} ë°ì´í„° ë¶„í•  ê²°ê³¼ë¥¼ í†µí•´ ëª¨ë¸ì˜ ê³¼ì†Œì í•©/ê³¼ëŒ€ì í•©ì„ ì§„ë‹¨í•˜ëŠ” í‘œë¥¼ í•´ì„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ë°ì´í„°ì…‹} & \textbf{ì—­í• } & \textbf{ë¹„ìœ  (ìˆ˜í—˜ìƒ)} \\ \hline
\textbf{Train Set} & ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°($W, b$) í•™ìŠµ & \textbf{êµê³¼ì„œ} (í‰ì†Œ ê³µë¶€) \\ \hline
\textbf{Dev Set} & í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ \& ëª¨ë¸ ì„ íƒ & \textbf{ëª¨ì˜ê³ ì‚¬} (ì‹¤ë ¥ ì ê²€ ë° ê³µë¶€ë²• ìˆ˜ì •) \\ \hline
\textbf{Test Set} & ìµœì¢… ì„±ëŠ¥ í‰ê°€ (í•™ìŠµ/íŠœë‹ ê´€ì—¬ X) & \textbf{ìˆ˜ëŠ¥/ë³¸ê³ ì‚¬} (ê²°ê³¼ ë²ˆë³µ ë¶ˆê°€) \\ \hline
\end{tabular}
\end{center}
\textit{* Note: ê³¼ê±°ì—ëŠ” 'Validation Set'ì´ë¼ê³  ë¶ˆë €ìœ¼ë‚˜, Andrew Ng êµìˆ˜ëŠ” 'Dev Set'ì´ë¼ëŠ” ìš©ì–´ë¥¼ ì„ í˜¸í•©ë‹ˆë‹¤.}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì „ëµì  ë¶„í• }

\subsection{1. The Era Shift: 60/20/20 vs 98/1/1}
ë°ì´í„°ì˜ ì–‘(Size)ì— ë”°ë¼ í™©ê¸ˆ ë¹„ìœ¨ì€ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Traditional ML (Small Data):} ë°ì´í„°ê°€ 1ë§Œ ê°œ ë¯¸ë§Œì¼ ë•Œ.
    \begin{itemize}
        \item ë¹„ìœ¨: \textbf{60\% : 20\% : 20\%}
        \item ì´ìœ : í‰ê°€ìš© ë°ì´í„°ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í†µê³„ì ìœ¼ë¡œ ì‹ ë¢°í•  ìˆ˜ ì—†ì–´ì„œ 20\%ë‚˜ ë–¼ì–´ë†”ì•¼ í–ˆìŠµë‹ˆë‹¤.
    \end{itemize}
    
    \item \textbf{Deep Learning Era (Big Data):} ë°ì´í„°ê°€ 100ë§Œ ê°œ ì´ìƒì¼ ë•Œ.
    \begin{itemize}
        \item ë¹„ìœ¨: \textbf{98\% : 1\% : 1\%}
        \item ì´ìœ : 100ë§Œ ê°œì˜ 1\%ë©´ 1ë§Œ ê°œì…ë‹ˆë‹¤. ì´ ì •ë„ë©´ í‰ê°€í•˜ê¸°ì— ì¶©ë¶„í•©ë‹ˆë‹¤. ë‚˜ë¨¸ì§€ 98\%ë¥¼ í•™ìŠµ(Train)ì— ëª°ì•„ì£¼ì–´ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê²ƒì´ ìœ ë¦¬í•©ë‹ˆë‹¤.
    \end{itemize}
\end{itemize}



\subsection{2. The Golden Rule: Same Distribution}
\textbf{"Dev Setê³¼ Test Setì€ ë°˜ë“œì‹œ ê°™ì€ ê³¼ë…ì„ ê²¨ëƒ¥í•´ì•¼ í•œë‹¤."}

\begin{warningbox}{ë‚˜ìœ ì˜ˆì‹œ (Bad Example)}
\begin{itemize}
    \item \textbf{Train:} ì›¹ì—ì„œ í¬ë¡¤ë§í•œ ê³ í™”ì§ˆ ê³ ì–‘ì´ ì‚¬ì§„ (20ë§Œ ì¥)
    \item \textbf{Dev/Test:} ì‚¬ìš©ìê°€ í°ìœ¼ë¡œ ì°ì€ íë¦¿í•œ ê³ ì–‘ì´ ì‚¬ì§„ (1ë§Œ ì¥)
\end{itemize}
\textbf{ê²°ê³¼:} í›ˆë ¨ ë•ŒëŠ” 99ì (ê³ í™”ì§ˆ ë§ˆìŠ¤í„°)ì´ì§€ë§Œ, ì‹¤ì „ì—ì„œëŠ” 0ì ì…ë‹ˆë‹¤.
\textbf{í•´ê²°:} ëª¨ë“  ë°ì´í„°ë¥¼ ì„ì–´ì„œ(Shuffle) ë‚˜ëˆ„ê±°ë‚˜, Dev/Testë¥¼ ì‹¤ì œ ëª©í‘œ(ëª¨ë°”ì¼ ì‚¬ì§„)ë¡œë§Œ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

---

\section{Deep Dive: ëª¨ë¸ ì§„ë‹¨ (Bias vs Variance)}

ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ì§„ì§œ ì´ìœ ëŠ” ëª¨ë¸ì˜ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤.

\begin{diagnosisbox}{ì„±ëŠ¥ ì§„ë‹¨í‘œ (Human Error $\approx$ 0\% ê°€ì •)}
\begin{center}
\begin{tabular}{c|c|l|l}
\hline
\textbf{Train Error} & \textbf{Dev Error} & \textbf{ì§„ë‹¨ (Diagnosis)} & \textbf{ì²˜ë°© (Action)} \\ \hline
1\% & 11\% & \textbf{High Variance} (ê³¼ëŒ€ì í•©) & ë°ì´í„° ì¶”ê°€, ì •ê·œí™”(Dropout), ëª¨ë¸ ì¶•ì†Œ \\ \hline
15\% & 16\% & \textbf{High Bias} (ê³¼ì†Œì í•©) & ë” í° ëª¨ë¸(ì¸µ ì¶”ê°€), í•™ìŠµ ì‹œê°„ ì—°ì¥ \\ \hline
15\% & 30\% & \textbf{High Bias \& Variance} & ëª¨ë¸ êµ¬ì¡° ë³€ê²½, ë°ì´í„° ì •ì œ \\ \hline
0.5\% & 1\% & \textbf{Low Bias \& Low Variance} & \textbf{Ideal (ì„±ê³µ!)} \\ \hline
\end{tabular}
\end{center}
\end{diagnosisbox}

\begin{itemize}
    \item \textbf{Bias(í¸í–¥) ë¬¸ì œ:} Train Setì¡°ì°¨ ì œëŒ€ë¡œ ëª» ë§ì¶¤. (ê³µë¶€ë¥¼ ì•ˆ í•¨)
    \item \textbf{Variance(ë¶„ì‚°) ë¬¸ì œ:} Trainì€ ì˜ ë§ì¶”ëŠ”ë° DevëŠ” ëª» ë§ì¶¤. (êµê³¼ì„œë§Œ ë‹¬ë‹¬ ì™¸ì›€, ì‘ìš© ë¶ˆê°€)
\end{itemize}



% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Data Leakage ë°©ì§€}

ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ **Data Leakage(ë°ì´í„° ëˆ„ìˆ˜)**ë¥¼ ë§‰ëŠ” ê²ƒì…ë‹ˆë‹¤. ì •ê·œí™”(Normalization)ë¥¼ í•  ë•Œ, ì „ì²´ ë°ì´í„°ì˜ í‰ê· ì„ ì“°ë©´ ì•ˆ ë©ë‹ˆë‹¤. **ì˜¤ì§ Train Setì˜ í†µê³„ëŸ‰**ë§Œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Stratified Split & Safe Normalization]
import numpy as np
from sklearn.model_selection import train_test_split

def prepare_data(X, y):
    """
    X: (m, n_x) features
    y: (m,) labels
    """
    # 1. Stratified Split (í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë¶„í• )
    # Train(98%) vs Temp(2%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.02, random_state=42, stratify=y
    )
    
    # Tempë¥¼ ë‹¤ì‹œ ë°˜ë°˜ ë‚˜ëˆ„ì–´ Dev(1%) vs Test(1%)
    X_dev, X_test, y_dev, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
    )
    
    # 2. Data Leakage ë°©ì§€ ì „ì²˜ë¦¬ (í•µì‹¬!)
    # ì „ì²´ Xê°€ ì•„ë‹ˆë¼, ì˜¤ì§ X_train ë§Œìœ¼ë¡œ í‰ê· /í‘œì¤€í¸ì°¨ ê³„ì‚°
    mean = np.mean(X_train, axis=0)
    std = np.std(X_train, axis=0)
    
    # ê³„ì‚°ëœ í†µê³„ëŸ‰ìœ¼ë¡œ Train, Dev, Test ëª¨ë‘ ë³€í™˜
    X_train_norm = (X_train - mean) / (std + 1e-8)
    X_dev_norm = (X_dev - mean) / (std + 1e-8)
    X_test_norm = (X_test - mean) / (std + 1e-8)
    
    return X_train_norm, X_dev_norm, X_test_norm, y_train, y_dev, y_test
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q1. Test Set ì—†ì´ Train/Devë§Œ ì“°ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} ê°€ëŠ¥ì€ í•©ë‹ˆë‹¤ë§Œ, ìœ„í—˜í•©ë‹ˆë‹¤. Dev Setì„ ë³´ê³  ëª¨ë¸ì„ ê³„ì† ìˆ˜ì •í•˜ë‹¤ ë³´ë©´, ëª¨ë¸ì´ Dev Setì— ê³¼ì í•©(Overfitting)ë©ë‹ˆë‹¤. ë§ˆì¹˜ ëª¨ì˜ê³ ì‚¬ ë‹µì„ ì™¸ì›Œë²„ë¦° ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ê°ê´€ì ì¸ ìµœì¢… í‰ê°€ë¥¼ ìœ„í•´ Test Setì€ í•œ ë²ˆë„ ë³´ì§€ ì•Šì€ ìƒíƒœë¡œ ë‚¨ê²¨ë‘¬ì•¼ í•©ë‹ˆë‹¤.

\textbf{Q2. ì‹œê³„ì—´ ë°ì´í„°(ì£¼ì‹)ë„ ëœë¤ ì…”í”Œ(Shuffle)í•´ë„ ë˜ë‚˜ìš”?} \\
\textbf{A.} \textbf{ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.} ë¯¸ë˜ ì •ë³´ê°€ ê³¼ê±° í•™ìŠµ ë°ì´í„°ì— ì„ì—¬ ë“¤ì–´ê°€ê²Œ ë©ë‹ˆë‹¤(Look-ahead Bias). ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì˜ë¼ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 1~9ì›” Train, 10ì›” Dev, 11ì›” Test)

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ë°ì´í„° ì„¸íŒ…ì´ ëë‚¬ìŠµë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ì€ ëª¨ë¸ì´ **High Variance(ê³¼ëŒ€ì í•©)** ìƒíƒœì¸ì§€, **High Bias(ê³¼ì†Œì í•©)** ìƒíƒœì¸ì§€ ì§„ë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë§Œì•½ ì§„ë‹¨ ê²°ê³¼ ëª¨ë¸ì´ **High Variance(ê³¼ëŒ€ì í•©)**ë¼ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? ë°ì´í„°ë¥¼ ë” ëª¨ìœ¼ëŠ” ê²ƒì´ ì¢‹ê² ì§€ë§Œ, ëˆê³¼ ì‹œê°„ì´ ë“­ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ ê³¼ëŒ€ì í•©ì„ í•´ê²°í•˜ëŠ” ë§ˆë²• ê°™ì€ ê¸°ë²•, **[Regularization] (L2 Regularization & Dropout)**ì„ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Split:} ë¹…ë°ì´í„° ì‹œëŒ€ì—ëŠ” **98/1/1** ë¹„ìœ¨ì´ ëŒ€ì„¸ë‹¤. Trainì— ì§‘ì¤‘í•˜ë¼.
    \item \textbf{Distribution:} Devì™€ TestëŠ” ë°˜ë“œì‹œ **ê°™ì€ ë¶„í¬**ì—¬ì•¼ í•œë‹¤.
    \item \textbf{Leakage:} ì •ê·œí™” ì‹œ í‰ê· ($\mu$)ê³¼ ë¶„ì‚°($\sigma$)ì€ **ì˜¤ì§ Train Set**ì—ì„œë§Œ êµ¬í•œë‹¤.
    \item \textbf{Diagnosis:} Train Errorì™€ Dev Errorì˜ ì°¨ì´ê°€ í¬ë©´ **Variance(ê³¼ëŒ€ì í•©)** ë¬¸ì œë‹¤.
  \end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬
\usepackage{colortbl} % í‘œ ìƒ‰ìƒ

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{diagnosisbox}[1]{
    colback=purple!5!white,
    colframe=purple!80!black,
    fonttitle=\bfseries,
    title=ğŸ©º #1 (ë‹¥í„° ë”¥ëŸ¬ë‹ì˜ ì²˜ë°©ì „)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Bias vs Variance Trade-off Analysis}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1 Train / Dev / Test Sets Strategy \textit{- Completed}
        \item \textbf{5.2 Bias vs Variance Analysis (Diagnosis)}
        \begin{itemize}
            \item The Bullseye Analogy
            \item Diagnosis Recipe (The Gap Analysis)
            \item Modern Trade-off in Deep Learning
            \item Implementation: Auto-Diagnosis Class
        \end{itemize}
        \item 5.3 Regularization (L2, Dropout) \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ Train, Dev, Testë¡œ ë‚˜ëˆ„ëŠ” ì „ëµì„ ì„¸ì› ìŠµë‹ˆë‹¤. ì´ì œ ëª¨ë¸ì„ í•™ìŠµì‹œì¼°ê³  ì„±ì í‘œ(Error Rate)ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì„±ì ì´ ê¸°ëŒ€ ì´í•˜ì…ë‹ˆë‹¤.
ì´ë•Œ "ì™œ ì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¤ì§€?"ë¼ê³  ë§‰ì—°í•´í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ê°€ ë‚´ë¦´ ìˆ˜ ìˆëŠ” ì§„ë‹¨ì€ ë”± ë‘ ê°€ì§€ì…ë‹ˆë‹¤. **"ê³µë¶€ë¥¼ ëœ í–ˆê±°ë‚˜(High Bias)"** ì•„ë‹ˆë©´ **"ë¬¸ì œì§‘ë§Œ ë‹¬ë‹¬ ì™¸ì› ê±°ë‚˜(High Variance)"**. ì´ ë‘ ê°€ì§€ ë³‘ëª…ì„ ì •í™•íˆ ì§„ë‹¨í•´ì•¼ ì˜¬ë°”ë¥¸ ì•½(Solution)ì„ ì“¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ ì›ì¸ì„ ê·œëª…í•˜ëŠ” **'ì§„ë‹¨(Diagnosis)'** ê¸°ìˆ ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} í¸í–¥(Bias)ê³¼ ë¶„ì‚°(Variance)ì„ ê°ê° ê³¼ì†Œì í•©(Underfitting)ê³¼ ê³¼ëŒ€ì í•©(Overfitting)ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ê¸°ì¤€:} **Bayes Error(ìµœì  ì˜¤ì°¨)**ë¥¼ ê¸°ì¤€ìœ¼ë¡œ Train Errorì™€ Dev Errorì˜ ê²©ì°¨(Gap)ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    \item \textbf{ë³€í™”:} ë”¥ëŸ¬ë‹ ì‹œëŒ€ì— Biasì™€ Varianceë¥¼ ë™ì‹œì— ì¤„ì´ëŠ” ê²ƒì´ ê°€ëŠ¥í•´ì§„ ì´ìœ ë¥¼ ì•Œì•„ë´…ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} í•™ìŠµ ê³¡ì„ (Learning Curve)ì„ ê·¸ë¦¬ê³  ìë™ìœ¼ë¡œ ìƒíƒœë¥¼ ì§„ë‹¨í•˜ëŠ” Python ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ  (í•™ìƒ)} \\ \hline
\textbf{High Bias} & ê³¼ì†Œì í•© (Underfitting) & ê³µë¶€ë¥¼ ëŒ€ì¶© í•´ì„œ êµê³¼ì„œ(Train) ë‚´ìš©ë„ ëª¨ë¦„. \\ \hline
\textbf{High Variance} & ê³¼ëŒ€ì í•© (Overfitting) & êµê³¼ì„œ ë‹µë§Œ ë‹¬ë‹¬ ì™¸ì›Œì„œ ì‘ìš© ë¬¸ì œ(Dev)ëŠ” ë‹¤ í‹€ë¦¼. \\ \hline
\textbf{Bayes Error} & ì´ë¡ ì  ìµœì†Œ ì˜¤ì°¨ & ì¸ê°„ë„ í‹€ë¦´ ìˆ˜ë°–ì— ì—†ëŠ” ë¬¸ì œì˜ ë‚œì´ë„ (í•œê³„ì¹˜). \\ \hline
\textbf{Avoidable Bias} & Train Error - Bayes Error & ìš°ë¦¬ê°€ ë…¸ë ¥ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” í¸í–¥. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ê³¼ë… ë§ì¶”ê¸° (Bullseye Analogy)}



\subsection{1. High Bias (Underfitting)}
\begin{itemize}
    \item **í˜„ìƒ:** í™”ì‚´ë“¤ì´ ì •ì¤‘ì•™ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆê³ , ìê¸°ë“¤ë¼ë¦¬ëŠ” ë­‰ì³ ìˆìŠµë‹ˆë‹¤.
    \item **ì›ì¸:** ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ(ì˜ˆ: ì§ì„ ) ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ì „í˜€ íŒŒì•…í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
\end{itemize}

\subsection{2. High Variance (Overfitting)}
\begin{itemize}
    \item **í˜„ìƒ:** í™”ì‚´ë“¤ì˜ í‰ê·  ìœ„ì¹˜ëŠ” ì •ì¤‘ì•™ì´ì§€ë§Œ, ì‚¬ë°©ìœ¼ë¡œ í©ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.
    \item **ì›ì¸:** í›ˆë ¨ ë°ì´í„°ì˜ ì‚¬ì†Œí•œ ë…¸ì´ì¦ˆê¹Œì§€ ê³¼ë„í•˜ê²Œ í•™ìŠµí•´ì„œ, ì¡°ê¸ˆë§Œ ë‹¤ë¥¸ ë°ì´í„°ê°€ ì˜¤ë©´ ì˜ˆì¸¡ì´ ë„ëœë‹ˆë‹¤.
\end{itemize}

\subsection{3. The Diagnostic Recipe (ì§„ë‹¨ ë ˆì‹œí”¼)}
ìˆ«ìë¥¼ ë³´ê³  ì§„ë‹¨í•˜ëŠ” ë²•ì…ë‹ˆë‹¤. Bayes Error(ì¸ê°„ ìˆ˜ì¤€ ì˜¤ì°¨)ê°€ 0\%ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.

\begin{diagnosisbox}{ì¦ìƒë³„ ì²˜ë°©ì „}
\begin{center}
\begin{tabular}{c|c|l|l}
\hline
\textbf{Train Error} & \textbf{Dev Error} & \textbf{ì§„ë‹¨ (Diagnosis)} & \textbf{ì²˜ë°© (Prescription)} \\ \hline
1\% & 11\% & \textbf{High Variance} & ë°ì´í„° ì¶”ê°€, ì •ê·œí™”(L2/Dropout) \\ \hline
15\% & 16\% & \textbf{High Bias} & ë” í° ëª¨ë¸(ì€ë‹‰ì¸µ ì¶”ê°€), ì˜¤ë˜ í•™ìŠµ \\ \hline
15\% & 30\% & \textbf{High Bias \& Variance} & ëª¨ë¸ êµ¬ì¡° ë³€ê²½, ë°ì´í„° ì •ì œ \\ \hline
0.5\% & 1\% & \textbf{Good Fit} & í˜„ì¬ ìƒíƒœ ìœ ì§€ \\ \hline
\end{tabular}
\end{center}
\end{diagnosisbox}

---

\section{Deep Dive: ë”¥ëŸ¬ë‹ ì‹œëŒ€ì˜ íŠ¸ë ˆì´ë“œì˜¤í”„}

\begin{itemize}
    \item \textbf{ê³¼ê±° (Traditional ML):} Biasë¥¼ ì¤„ì´ë©´ Varianceê°€ ëŠ˜ì–´ë‚˜ëŠ” 'ì‹œì†Œ' ê´€ê³„ì˜€ìŠµë‹ˆë‹¤.
    \item \textbf{í˜„ì¬ (Deep Learning):}
    \begin{itemize}
        \item \textbf{Bias ì¤„ì´ê¸°:} ë„¤íŠ¸ì›Œí¬ë¥¼ ë” í¬ê²Œ ë§Œë“­ë‹ˆë‹¤. (ë°ì´í„°ê°€ ë§ë‹¤ë©´ Varianceë¥¼ ê±°ì˜ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
        \item \textbf{Variance ì¤„ì´ê¸°:} ë°ì´í„°ë¥¼ ë” ë§ì´ ëª¨ìë‹ˆë‹¤. (Biasë¥¼ ê±°ì˜ ê±´ë“œë¦¬ì§€ ì•ŠìŒ)
    \end{itemize}
    \item \textbf{ê²°ë¡ :} ì»´í“¨íŒ… íŒŒì›Œì™€ ë°ì´í„°ë§Œ ì¶©ë¶„í•˜ë‹¤ë©´, Biasì™€ Varianceë¥¼ ë™ì‹œì— ì¡ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Auto-Diagnosis Tool}

í•™ìŠµ ê¸°ë¡(History)ì„ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ ë³‘ëª…ì„ ì§„ë‹¨í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“­ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Model Diagnosis Class]
import matplotlib.pyplot as plt

class ModelDiagnostician:
    def __init__(self, train_acc, dev_acc, human_acc=0.99):
        # ì •í™•ë„(Accuracy)ë¥¼ ì˜¤ì°¨(Error)ë¡œ ë³€í™˜
        self.train_err = 1.0 - train_acc
        self.dev_err = 1.0 - dev_acc
        self.human_err = 1.0 - human_acc
        
    def diagnose(self):
        print(f"Human Error: {self.human_err:.2%}")
        print(f"Train Error: {self.train_err:.2%}")
        print(f"Dev Error  : {self.dev_err:.2%}")
        print("-" * 30)
        
        # 1. Bias ì§„ë‹¨ (Trainê³¼ Humanì˜ ì°¨ì´)
        avoidable_bias = self.train_err - self.human_err
        
        # 2. Variance ì§„ë‹¨ (Devì™€ Trainì˜ ì°¨ì´)
        variance = self.dev_err - self.train_err
        
        threshold = 0.02 # 2% ì´ìƒ ì°¨ì´ë‚˜ë©´ ë¬¸ì œë¡œ ê°„ì£¼
        
        if avoidable_bias > threshold:
            print("[Diagnosis] High Bias (Underfitting)")
            print(">> Solution: Bigger Network, Train Longer (Epochs)")
            
        elif variance > threshold:
            print("[Diagnosis] High Variance (Overfitting)")
            print(">> Solution: More Data, Regularization (Dropout, L2)")
            
        else:
            print("[Diagnosis] Good Fit! Great Job.")

# --- ì‹¤í–‰ ì˜ˆì œ ---
if __name__ == "__main__":
    # ìƒí™©: í›ˆë ¨ì€ ì˜ ë˜ëŠ”ë°(99%), ê²€ì¦ì€ ì•ˆ ë¨(89%) -> High Variance
    train_accuracy = 0.99
    dev_accuracy = 0.89
    
    doctor = ModelDiagnostician(train_accuracy, dev_accuracy)
    doctor.diagnose()
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Train Errorê°€ ë†’ë‹¤ê³  ë¬´ì¡°ê±´ High Biasì¸ê°€ìš”?}
\textbf{ì•„ë‹™ë‹ˆë‹¤!} ë¹„êµ ëŒ€ìƒ(Bayes Error)ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ìƒí™©:} íë¦¿í•œ ì˜›ë‚  ë¬¸ì„œ ì¸ì‹. ì‚¬ëŒë„ 15\% í‹€ë¦¼(Human Error = 15\%).
    \item \textbf{ê²°ê³¼:} ëª¨ë¸ì˜ Train Errorê°€ 15\%ì„.
    \item \textbf{ì§„ë‹¨:} ì´ê²ƒì€ High Biasê°€ ì•„ë‹™ë‹ˆë‹¤. ì´ë¯¸ ì‚¬ëŒë§Œí¼ ì˜í•œ ê²ƒì…ë‹ˆë‹¤(Optimal). ì´ ê²½ìš°ì—” Biasë¥¼ ì¤„ì´ë ¤ ë…¸ë ¥í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
\end{itemize}
\end{warningbox}

\textbf{Q. High Biasì™€ High Varianceê°€ ë™ì‹œì— ë†’ìœ¼ë©´ìš”?} \\
\textbf{A.} ìµœì•…ì˜ ìƒí™©ì…ë‹ˆë‹¤. ëª¨ë¸ì´ ì •ë‹µë„ ëª» ë§ì¶”ë©´ì„œ ì˜ˆì¸¡ê°’ì€ ë„ë›°ê¸°ë¥¼ í•©ë‹ˆë‹¤. ë³´í†µ ëª¨ë¸ êµ¬ì¡° ìì²´ê°€ ë°ì´í„°ì— ë§ì§€ ì•Šê±°ë‚˜, ë°ì´í„°ì— ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ìˆì„ ë•Œ ë°œìƒí•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì§„ë‹¨ ê²°ê³¼, ë§Œì•½ ì—¬ëŸ¬ë¶„ì˜ ëª¨ë¸ì´ **High Variance(ê³¼ëŒ€ì í•©)** íŒì •ì„ ë°›ì•˜ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
"ë°ì´í„°ë¥¼ ë” ëª¨ìœ¼ì„¸ìš”"ë¼ëŠ” ì¡°ì–¸ì€ ì‰½ì§€ë§Œ, í˜„ì‹¤ì—ì„œëŠ” ëˆê³¼ ì‹œê°„ì´ ë“­ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ì§€ ì•Šê³ ë„ ê³¼ëŒ€ì í•©ì„ ì¹˜ë£Œí•˜ëŠ” ë§ˆë²•ì˜ ì•Œì•½ì´ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Regularization]** ìœ ë‹›ì—ì„œ **L2 ì •ê·œí™”**ì™€ **Dropout**ì´ë¼ëŠ” ê°•ë ¥í•œ ì¹˜ë£Œë²•ì„ ë°°ì›Œë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{High Bias:} Train Errorê°€ ë†’ë‹¤. $\rightarrow$ ëª¨ë¸ì„ í‚¤ì›Œë¼(Bigger Network).
    \item \textbf{High Variance:} Dev Errorê°€ Train Errorë³´ë‹¤ í›¨ì”¬ ë†’ë‹¤. $\rightarrow$ ë°ì´í„°ë¥¼ ëª¨ìœ¼ê±°ë‚˜ ì •ê·œí™”(Regularization)í•˜ë¼.
    \item \textbf{Reference:} ì ˆëŒ€ì ì¸ ìˆ˜ì¹˜ê°€ ì•„ë‹ˆë¼ **Bayes Error(Human-level)**ì™€ì˜ ì°¨ì´(Gap)ë¥¼ ë´ì•¼ í•œë‹¤.
    \item \textbf{Priority:} ë³´í†µ Biasë¥¼ ë¨¼ì € ì¡ê³ , ê·¸ ë‹¤ìŒ Varianceë¥¼ ì¡ëŠ” ìˆœì„œë¡œ ì§„í–‰í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Regularization (L2 / Weight Decay)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1 Train / Dev / Test Sets Strategy \textit{- Completed}
        \item 5.2 Bias vs Variance Analysis \textit{- Completed}
        \item \textbf{5.3 Regularization (L1/L2)}
        \begin{itemize}
            \item Why Regularize? (Penalizing Complexity)
            \item L2 Regularization (Ridge) Formula
            \item Math: Why is it called "Weight Decay"?
            \item Implementation
        \end{itemize}
        \item 5.4 Dropout Regularization \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ëª¨ë¸ì´ **High Variance(ê³¼ëŒ€ì í•©)**ë¼ëŠ” ë³‘ì— ê±¸ë ¸ìŒì„ ì§„ë‹¨í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ë„ˆë¬´ ì§‘ì°©í•´ì„œ(ì•”ê¸°í•´ì„œ), ì‹¤ì „ ë¬¸ì œ(Dev Set)ë¥¼ ëª» í‘¸ëŠ” ìƒí™©ì…ë‹ˆë‹¤.
ì´ì œ ì²˜ë°©ì „ì„ ì“¸ ì°¨ë¡€ì…ë‹ˆë‹¤. ê³¼ëŒ€ì í•©ì„ ì¹˜ë£Œí•˜ëŠ” ê°€ì¥ ì „í†µì ì´ê³  ê°•ë ¥í•œ í•­ìƒì œëŠ” ë°”ë¡œ **'ì •ê·œí™”(Regularization)'**ì…ë‹ˆë‹¤. ì •ê·œí™”ëŠ” ëª¨ë¸ì—ê²Œ \textbf{"ì •ë‹µì„ ë§ì¶”ë˜, ë„ˆë¬´ ê¼¼ìˆ˜(í° ê°€ì¤‘ì¹˜)ëŠ” ì“°ì§€ ë§ˆë¼"}ë¼ê³  ì œì•½(Penalty)ì„ ê±°ëŠ” ê²ƒì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì–µì œí•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” **L2 ì •ê·œí™”**ë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} ë¹„ìš© í•¨ìˆ˜ $J$ì— ê°€ì¤‘ì¹˜ í¬ê¸°($||W||^2$)ì— ë¹„ë¡€í•˜ëŠ” ë²Œì ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜í•™:} ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ê°€ ìŠ¤ìŠ¤ë¡œ ì¤„ì–´ë“œëŠ” **Weight Decay(ê°€ì¤‘ì¹˜ ê°ì‡ )** í˜„ìƒì„ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ì •ê·œí™” í•­ì´ í¬í•¨ëœ Forward ë° Backward ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
    \item \textbf{ë¹„êµ:} L1 ì •ê·œí™”(Lasso)ì™€ì˜ ì°¨ì´ì (í¬ì†Œì„±)ì„ ì´í•´í•©ë‹ˆë‹¤.
  \end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{í•µì‹¬ ì˜ë¯¸} \\ \hline
\textbf{L2 Regularization} & Ridge & ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©ì„ ë²Œì ìœ¼ë¡œ ì‚¬ìš©. $W \to 0$ (ì‘ì•„ì§). \\ \hline
\textbf{L1 Regularization} & Lasso & ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©ì„ ë²Œì ìœ¼ë¡œ ì‚¬ìš©. $W = 0$ (ì‚¬ë¼ì§/í¬ì†Œì„±). \\ \hline
\textbf{Lambda} & $\lambda$ & ì •ê·œí™” ê°•ë„. í´ìˆ˜ë¡ ëª¨ë¸ì´ ë‹¨ìˆœí•´ì§(Underfitting ìœ„í—˜). \\ \hline
\textbf{Weight Decay} & - & ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ê°€ì¤‘ì¹˜ê°€ ì¼ì • ë¹„ìœ¨ì”© ê°ì†Œí•˜ëŠ” í˜„ìƒ. \\ \hline
\textbf{Frobenius Norm} & $||W||_F^2$ & í–‰ë ¬ì˜ ëª¨ë“  ì›ì†Œë¥¼ ì œê³±í•´ì„œ ë”í•œ ê°’. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë²Œì  ì‹œìŠ¤í…œ}

\subsection{1. The Idea of Penalty}
ìš°ë¦¬ì˜ ëª©í‘œëŠ” ë¹„ìš© $J$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì—¬ê¸°ì— **"ê°€ì¤‘ì¹˜ $W$ê°€ ì»¤ì§€ë©´ ë²Œì ì„ ì£¼ê² ë‹¤"**ëŠ” ìƒˆë¡œìš´ ê·œì¹™ì„ ì¶”ê°€í•©ë‹ˆë‹¤.

$$ J_{regularized}(W, b) = \underbrace{J_{original}(W, b)}_{\text{ì˜¤ì°¨ (Cross Entropy)}} + \underbrace{\frac{\lambda}{2m} \sum_{l} ||W^{[l]}||_F^2}_{\text{ë²Œì  (L2 Penalty)}} $$

\begin{itemize}
    \item $\lambda$ (Lambda): ë²Œì ì˜ ê°•ë„ì…ë‹ˆë‹¤. í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤.
    \item $m$: ë°ì´í„° ê°œìˆ˜.
    \item $2m$: ë¯¸ë¶„í•  ë•Œ ì œê³±($^2$)ì´ ë‚´ë ¤ì™€ì„œ 2ì™€ ì•½ë¶„ë˜ë¼ê³  ë¯¸ë¦¬ 2ë¡œ ë‚˜ëˆ ë‘¡ë‹ˆë‹¤. (ìˆ˜í•™ì  í¸ì˜)
\end{itemize}

\subsection{2. L1 vs L2 (Which one to use?)}
\begin{itemize}
    \item \textbf{L2 (Standard):} ê°€ì¤‘ì¹˜ë¥¼ 0ì— ê°€ê¹ê²Œ ë§Œë“­ë‹ˆë‹¤. ëª¨ë“  íŠ¹ì„±ì„ ê³¨ê³ ë£¨ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤. \textbf{ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ê°’(Default)}ì…ë‹ˆë‹¤.
    \item \textbf{L1 (Sparse):} ê°€ì¤‘ì¹˜ë¥¼ ì™„ì „íˆ 0ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ì œê±°(Feature Selection)í•˜ê³  ì‹¶ì„ ë•Œ ì“°ì§€ë§Œ, ë¯¸ë¶„ì´ ê¹Œë‹¤ë¡œì›Œ ì˜ ì•ˆ ì”ë‹ˆë‹¤.
\end{itemize}

---

\section{Deep Dive: Why "Weight Decay"?}

ì´ ì„¹ì…˜ì€ L2 ì •ê·œí™”ë¥¼ ì™œ **'ê°€ì¤‘ì¹˜ ê°ì‡ '**ë¼ê³  ë¶€ë¥´ëŠ”ì§€ ìˆ˜í•™ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.

\begin{mathbox}{ì—­ì „íŒŒ ìˆ˜ì‹ ìœ ë„}
ë¹„ìš© í•¨ìˆ˜ $J_{reg}$ë¥¼ $W$ì— ëŒ€í•´ ë¯¸ë¶„í•´ ë´…ì‹œë‹¤.

$$ \frac{\partial J_{reg}}{\partial W} = \frac{\partial J_{orig}}{\partial W} + \frac{\partial}{\partial W} \left( \frac{\lambda}{2m} W^2 \right) $$
$$ dW_{reg} = dW_{orig} + \frac{\lambda}{m} W $$
(ë¶„ëª¨ì˜ 2ê°€ ë¯¸ë¶„ë˜ë©´ì„œ ì‚¬ë¼ì¡ŒìŠµë‹ˆë‹¤!)

ì´ì œ ê²½ì‚¬ í•˜ê°•ë²• ì—…ë°ì´íŠ¸ ì‹ì— ëŒ€ì…í•©ë‹ˆë‹¤.
$$ W_{new} = W - \alpha \cdot dW_{reg} $$
$$ W_{new} = W - \alpha \left( dW_{orig} + \frac{\lambda}{m} W \right) $$

ì´ ì‹ì„ $W$ë¡œ ë¬¶ìœ¼ë©´ ë†€ë¼ìš´ ê²°ê³¼ê°€ ë‚˜ì˜µë‹ˆë‹¤:
$$ W_{new} = \underbrace{\left( 1 - \frac{\alpha \lambda}{m} \right)}_{\text{Decay Factor } (< 1)} W - \alpha \cdot dW_{orig} $$
\end{mathbox}

\textbf{ê²°ë¡ :} ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤ ê°€ì¤‘ì¹˜ $W$ëŠ” ì›ë˜ í•™ìŠµ ë°©í–¥($-\alpha dW$)ìœ¼ë¡œ ê°€ê¸° ì „ì—, ìê¸° ìì‹ ì˜ í¬ê¸°ë¥¼ $(1 - \frac{\alpha \lambda}{m})$ ë¹„ìœ¨ë§Œí¼ ì¤„ì…ë‹ˆë‹¤. ì¦‰, ê°€ë§Œíˆ ìˆì–´ë„ **ìŠ¤ìŠ¤ë¡œ ê°ì†Œ(Decay)**í•©ë‹ˆë‹¤.

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: L2 Regularization}

ì •ê·œí™”ëŠ” Forward(ë¹„ìš© ê³„ì‚°)ì™€ Backward(ê¸°ìš¸ê¸° ê³„ì‚°) ì–‘ìª½ì— ëª¨ë‘ ì½”ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=L2 Regularization Implementation]
import numpy as np

class L2Regularizer:
    def __init__(self, lambd):
        self.lambd = lambd

    def compute_cost(self, cost_cross_entropy, parameters, m):
        """
        J_total = J_cross_entropy + (lambda / 2m) * sum(W^2)
        """
        L = len(parameters) // 2
        L2_cost = 0
        
        for l in range(1, L + 1):
            W = parameters['W' + str(l)]
            # Frobenius Norm ì œê³± ê³„ì‚°
            L2_cost += np.sum(np.square(W))
            
        L2_cost *= (self.lambd / (2 * m)) # 2mìœ¼ë¡œ ë‚˜ëˆ” ì£¼ì˜!
        
        return cost_cross_entropy + L2_cost

    def backward(self, dW_orig, W, m):
        """
        dW_reg = dW_orig + (lambda / m) * W
        """
        # ì •ê·œí™” í•­ì˜ ê¸°ìš¸ê¸° ì¶”ê°€ (ì—¬ê¸°ì„œëŠ” mìœ¼ë¡œ ë‚˜ëˆ”!)
        dW_reg = dW_orig + ((self.lambd / m) * W)
        
        return dW_reg

# --- ì‹¤í–‰ ì˜ˆì œ ---
if __name__ == "__main__":
    m = 1000
    lambd = 0.7
    reg = L2Regularizer(lambd)
    
    # ê°€ìƒì˜ W (Weight)
    W = np.array([[0.5, -0.2], [0.1, 0.8]])
    dW_orig = np.array([[0.01, 0.02], [-0.01, 0.05]]) # ì›ë˜ ê¸°ìš¸ê¸°
    
    # ì—­ì „íŒŒ ì ìš©
    dW_final = reg.backward(dW_orig, W, m)
    
    print("Original dW:\n", dW_orig)
    print("Regularized dW:\n", dW_final)
    # dW ê°’ì´ W ë¶€í˜¸ ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆ ë” ì»¤ì§ -> Wë¥¼ 0ìª½ìœ¼ë¡œ ë” ì„¸ê²Œ ë°ˆ
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{í¸í–¥(Bias) $b$ëŠ” ì •ê·œí™” ì•ˆ í•˜ë‚˜ìš”?}
\textbf{ë³´í†µ ì•ˆ í•©ë‹ˆë‹¤.}
$b$ëŠ” í•¨ìˆ˜ì˜ ëª¨ì–‘(ê³¡ë¥ )ì´ ì•„ë‹ˆë¼ ìœ„ì¹˜ë§Œ ì´ë™ì‹œí‚µë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì˜ ë³µì¡ë„ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. $W$ë§Œ ì •ê·œí™”í•´ë„ ì¶©ë¶„í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Lambda($\lambda$) ê°’ì€ ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ì—¬ëŸ¬ ê°’ì„ ì‹œë„í•´ë³´ê³  Dev Setì˜ ì˜¤ì°¨ê°€ ê°€ì¥ ë‚®ì€ ê°’ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ë³´í†µ 0.01, 0.001 ì²˜ëŸ¼ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” L2 ì •ê·œí™”ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ê°€ ë„ˆë¬´ ì»¤ì§€ëŠ” ê²ƒì„ ë§‰ì•„ ê³¼ëŒ€ì í•©ì„ ì–µì œí–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ë•Œë¡œëŠ” ë” ê³¼ê²©í•œ ë°©ë²•ì´ í•„ìš”í•  ë•Œê°€ ìˆìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ë¥¼ ì¤„ì´ëŠ” ê²Œ ì•„ë‹ˆë¼, ì•„ì˜ˆ **ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ êº¼ë²„ë¦¬ëŠ”(Shutdown)** ë°©ë²•ì…ë‹ˆë‹¤. "ì–´ë–»ê²Œ ë‡Œì„¸í¬ë¥¼ ì£½ì´ëŠ”ë° í•™ìŠµì´ ë” ì˜ ë˜ë‚˜ìš”?"
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ë…íŠ¹í•˜ê³  ê°•ë ¥í•œ ì •ê·œí™” ê¸°ë²•ì¸ **[Regularization] Dropout (ë“œë¡­ì•„ì›ƒ)**ì„ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{L2 Regularization:} ê°€ì¤‘ì¹˜ ì œê³±í•©($W^2$)ì„ ë¹„ìš© í•¨ìˆ˜ì— ì¶”ê°€í•˜ì—¬ í° ê°€ì¤‘ì¹˜ì— ë²Œì ì„ ì¤€ë‹¤.
    \item \textbf{Weight Decay:} ì—­ì „íŒŒ ì‹œ $W$ê°€ ë§¤ë²ˆ ì¡°ê¸ˆì”© 0ì„ í–¥í•´ ì¤„ì–´ë“ ë‹¤.
    \item \textbf{Effect:} $W$ê°€ ì‘ì•„ì§€ë©´ ëª¨ë¸ì´ ì„ í˜•(Linear)ì— ê°€ê¹Œì›Œì ¸ ë³µì¡ë„ê°€ ì¤„ì–´ë“ ë‹¤. (ê³¼ëŒ€ì í•© í•´ê²°)
    \item \textbf{Tip:} Cost ê³„ì‚° ì‹œì—” $2m$, Gradient ê³„ì‚° ì‹œì—” $m$ìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Dropout Regularization}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1 Train / Dev / Test Sets Strategy \textit{- Completed}
        \item 5.2 Bias vs Variance Analysis \textit{- Completed}
        \item 5.3 Regularization (L1/L2) \textit{- Completed}
        \item \textbf{5.4 Dropout Regularization}
        \begin{itemize}
            \item The Concept: Killing Neurons
            \item Why does it work? (Ensemble Effect)
            \item Inverted Dropout (Scaling)
            \item Implementation Details
        \end{itemize}
        \item 5.5 Input Normalization \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ê°€ì¤‘ì¹˜($W$)ì˜ í¬ê¸°ë¥¼ ê°•ì œë¡œ ì¤„ì—¬ë²„ë¦¬ëŠ” **L2 ì •ê·œí™”(Weight Decay)**ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤.
ì˜¤ëŠ˜ ë°°ìš¸ ê¸°ë²•ì€ ì¡°ê¸ˆ ë” 'ê³¼ê²©'í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ê³¼ëŒ€ì í•©(Overfitting)ì„ ë§‰ê¸° ìœ„í•´, í•™ìŠµ ê³¼ì •ì—ì„œ ë©€ì©¡í•œ ë‰´ëŸ°ë“¤ì„ ë¬´ì‘ìœ„ë¡œ **'ì œê±°(Kill)'**í•´ë²„ë¦½ë‹ˆë‹¤. ë°”ë¡œ **ë“œë¡­ì•„ì›ƒ(Dropout)**ì…ë‹ˆë‹¤.
"ë‡Œì„¸í¬ë¥¼ ì£½ì´ëŠ”ë° ë‡Œê°€ ë” ë˜‘ë˜‘í•´ì§„ë‹¤ë‹ˆ?"ë¼ëŠ” ì˜ë¬¸ì´ ë“¤ê² ì§€ë§Œ, ì´ê²ƒì´ í˜„ëŒ€ ë”¥ëŸ¬ë‹ì—ì„œ ê°€ì¥ ê°•ë ¥í•œ ì •ê·œí™” ê¸°ë²•ì…ë‹ˆë‹¤. ê·¸ ì—­ì„¤ì ì¸ ì›ë¦¬ë¥¼ íŒŒí—¤ì³ ë³´ê² ìŠµë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì‹ ê²½ë§ì˜ ê°•ê±´í•¨(Robustness)ì„ ë†’ì´ëŠ” **ë“œë¡­ì•„ì›ƒ**ì˜ ì›ë¦¬ì™€ êµ¬í˜„ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì›ë¦¬:} ë“œë¡­ì•„ì›ƒì´ ì–´ë–»ê²Œ íŠ¹ì • ë‰´ëŸ°ì— ëŒ€í•œ ì˜ì¡´ë„(Co-adaptation)ë¥¼ ë‚®ì¶”ëŠ”ì§€ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜í•™:} í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ ì‹œì˜ ì¶œë ¥ê°’ ì°¨ì´ë¥¼ ë³´ì •í•˜ëŠ” **Inverted Dropout** ê¸°ìˆ ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{ê·œì¹™:} ë“œë¡­ì•„ì›ƒì€ ì˜¤ì§ **í•™ìŠµ(Training)** ë•Œë§Œ ì¼œê³ , í…ŒìŠ¤íŠ¸(Test) ë•ŒëŠ” ëˆë‹¤ëŠ” ì›ì¹™ì„ ëª…ì‹¬í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} NumPyë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í¬ í–‰ë ¬(Mask Matrix)ì„ ë§Œë“¤ê³  ì ìš©í•´ë´…ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ë³€ìˆ˜} & \textbf{ì„¤ëª…} \\ \hline
\textbf{Dropout} & - & í•™ìŠµ ì‹œ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ì‚­ì œ(0ìœ¼ë¡œ ì„¤ì •)í•˜ëŠ” ê¸°ë²•. \\ \hline
\textbf{Keep Probability} & \texttt{keep\_prob} & ë‰´ëŸ°ì„ \textbf{'ì‚´ë ¤ë‘˜'} í™•ë¥ . (ì˜ˆ: 0.8 = 20\% ì‚­ì œ). \\ \hline
\textbf{Inverted Dropout} & - & í•™ìŠµ ì‹œ ê°’ì„ \texttt{keep\_prob}ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤ì¼€ì¼ì„ ë³´ì •í•˜ëŠ” í‘œì¤€ ë°©ì‹. \\ \hline
\textbf{Ensemble} & - & ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ í‰ê·  ë‚´ëŠ” ê²ƒ. ë“œë¡­ì•„ì›ƒì€ ì´ íš¨ê³¼ë¥¼ ëƒ„. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë¬´ì‘ìœ„ ì‚­ì œì˜ ë¯¸í•™}

\subsection{1. ì§ê´€ì  í•´ì„ (Why does it work?)}


\begin{analogybox}{ì²œì¬ì—ê²Œ ì˜ì¡´í•˜ëŠ” íŒ€ í”„ë¡œì íŠ¸}
\begin{itemize}
    \item \textbf{ìƒí™© (No Dropout):} íŒ€ì— ì²œì¬ í•œ ëª…(íŠ¹ì • ë‰´ëŸ°)ì´ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒ€ì›ë“¤ì€ ê·¸ ì²œì¬ë§Œ ë¯¿ê³  ì¼ì„ ì•ˆ í•©ë‹ˆë‹¤. ë§Œì•½ ì²œì¬ê°€ ê²°ê·¼í•˜ë©´(ìƒˆë¡œìš´ ë°ì´í„°), í”„ë¡œì íŠ¸ëŠ” ë§í•©ë‹ˆë‹¤. (ê³¼ëŒ€ì í•©)
    \item \textbf{ìƒí™© (Dropout):} ë§¤ì¼ ë¬´ì‘ìœ„ë¡œ íŒ€ì›ì„ ì¶œê·¼ì‹œí‚¤ì§€ ì•ŠìŠµë‹ˆë‹¤. ì²œì¬ê°€ ê²°ê·¼í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    \item \textbf{ê²°ê³¼:} íŒ€ì›ë“¤ì€ ëˆ„êµ¬ì—ê²Œë„ ì˜ì¡´í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, **ëª¨ë‘ê°€ ì—…ë¬´ ì „ë°˜ì„ ìµíˆê²Œ ë©ë‹ˆë‹¤.** ê²°êµ­ íŒ€ ì „ì²´ê°€ ê°•ë ¥í•˜ê³  ìœ ì—°í•´ì§‘ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}
ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ë©´ ë‰´ëŸ°ë“¤ì´ íŠ¹ì • ì…ë ¥(ì¹œêµ¬)ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , **ê°€ì¤‘ì¹˜ë¥¼ ê³¨ê³ ë£¨ ë¶„ì‚°(Spread out)**ì‹œí‚¤ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” L2 ì •ê·œí™”ì™€ ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.

---

\section{Deep Dive: Inverted Dropout (ì—­ ë“œë¡­ì•„ì›ƒ)}

ì´ ì„¹ì…˜ì€ ë©´ì ‘ ë‹¨ê³¨ ì§ˆë¬¸ì¸ "ì™œ í•™ìŠµ ë•Œ ê°’ì„ ë‚˜ëˆ„ë‚˜ìš”?"ì— ëŒ€í•œ ë‹µì…ë‹ˆë‹¤.

\begin{mathbox}{The Scale Problem}
\texttt{keep\_prob = 0.5} (50\% ì‚­ì œ)ë¼ê³  ê°€ì •í•©ì‹œë‹¤.

\textbf{1. í•™ìŠµ ë‹¨ê³„ (Train):}
ë‰´ëŸ°ì˜ ì ˆë°˜ì´ 0ì´ ë˜ë¯€ë¡œ, ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬ë˜ëŠ” í•©ê³„($Z = \sum w_i a_i$)ë„ ëŒ€ëµ \textbf{ì ˆë°˜}ìœ¼ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤.

\textbf{2. í…ŒìŠ¤íŠ¸ ë‹¨ê³„ (Test):}
í…ŒìŠ¤íŠ¸ ë•ŒëŠ” ë“œë¡­ì•„ì›ƒì„ ë•ë‹ˆë‹¤(ëª¨ë“  ë‰´ëŸ° ì‚¬ìš©). $Z$ ê°’ì´ í•™ìŠµ ë•Œë³´ë‹¤ \textbf{2ë°° ë»¥íŠ€ê¸°} ë©ë‹ˆë‹¤. ì˜ˆì¸¡ê°’ì´ ì™„ì „íˆ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

\textbf{3. í•´ê²°ì±… (Inverted Dropout):}
í•™ìŠµ ë‹¨ê³„ì—ì„œ ì‚´ì•„ë‚¨ì€ ë‰´ëŸ°ì˜ ê°’ì„ ë¯¸ë¦¬ \textbf{2ë°°ë¡œ í‚¤ì›Œì¤ë‹ˆë‹¤ ($A /= 0.5$)}.
ì´ë ‡ê²Œ í•˜ë©´ í•™ìŠµ ë•Œì˜ ê¸°ëŒ“ê°’($E[A]$)ì´ í…ŒìŠ¤íŠ¸ ë•Œì™€ ë¹„ìŠ·í•˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë•ŒëŠ” ì•„ë¬´ëŸ° ì—°ì‚°ë„ í•  í•„ìš”ê°€ ì—†ì–´ì§‘ë‹ˆë‹¤.
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Dropout Layer}

ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ `is_training` í”Œë˜ê·¸ì…ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ë•ŒëŠ” ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Inverted Dropout Implementation]
import numpy as np

class Dropout:
    def __init__(self, keep_prob=0.8):
        self.keep_prob = keep_prob
        self.mask = None # ì—­ì „íŒŒìš© ë§ˆìŠ¤í¬ ì €ì¥

    def forward(self, A, is_training=True):
        """
        A: Activation values
        """
        if is_training:
            # 1. ë§ˆìŠ¤í¬ ìƒì„± (0 ~ 1 ë‚œìˆ˜ < keep_prob)
            # keep_probë³´ë‹¤ ì‘ìœ¼ë©´ True(1), í¬ë©´ False(0)
            D = np.random.rand(A.shape[0], A.shape[1])
            D = (D < self.keep_prob).astype(int)
            self.mask = D
            
            # 2. ë‰´ëŸ° ë„ê¸° (Shut down)
            A = A * D
            
            # 3. ìŠ¤ì¼€ì¼ ë³´ì • (Inverted Dropout í•µì‹¬!)
            A = A / self.keep_prob
            
        return A

    def backward(self, dA):
        """
        ì—­ì „íŒŒ: ì£½ì€ ë‰´ëŸ°ì€ ë¯¸ë¶„ê°’ë„ 0ì´ì–´ì•¼ í•¨
        """
        # 1. ë§ˆìŠ¤í¬ ì ìš©
        dA = dA * self.mask
        
        # 2. ìŠ¤ì¼€ì¼ ë³´ì • (ìˆœì „íŒŒ ë•Œ ë‚˜ëˆ´ìœ¼ë‹ˆ ì—¬ê¸°ì„œë„ ë‚˜ëˆ ì•¼ í•¨)
        dA = dA / self.keep_prob
        
        return dA

# --- ì‹¤í–‰ ì˜ˆì œ ---
if __name__ == "__main__":
    np.random.seed(1)
    A = np.ones((5, 3)) * 10 # ëª¨ë“  ê°’ì´ 10ì¸ í–‰ë ¬
    
    dropout = Dropout(keep_prob=0.8)
    
    # Train Mode
    A_train = dropout.forward(A, is_training=True)
    print("Train Output:\n", A_train)
    # ì¼ë¶€ëŠ” 0, ë‚˜ë¨¸ì§€ëŠ” 12.5 (10 / 0.8)ê°€ ë¨
    
    # Test Mode
    A_test = dropout.forward(A, is_training=False)
    print("\nTest Output:\n", A_test)
    # ì›ë³¸ ê·¸ëŒ€ë¡œ 10 ìœ ì§€
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ë¹„ìš© í•¨ìˆ˜(Cost Function) ì§„ë™ ë¬¸ì œ}
ë“œë¡­ì•„ì›ƒì„ ì“°ë©´ ë§¤ë²ˆ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ê°€ ë¬´ì‘ìœ„ë¡œ ë°”ë€ë‹ˆë‹¤. ë”°ë¼ì„œ ë¹„ìš© í•¨ìˆ˜ $J$ê°€ ë§¤ë„ëŸ½ê²Œ ë‚´ë ¤ê°€ì§€ ì•Šê³  **í†±ë‹ˆë°”í€´ì²˜ëŸ¼ ì§„ë™**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë””ë²„ê¹…í•  ë•ŒëŠ” ì ì‹œ `keep_prob = 1.0`(ë“œë¡­ì•„ì›ƒ ë„ê¸°)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ $J$ê°€ ì˜ ë‚´ë ¤ê°€ëŠ”ì§€ í™•ì¸í•œ í›„, ë‹¤ì‹œ ì¼œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì…ë ¥ì¸µ(Input Layer)ì—ë„ ë“œë¡­ì•„ì›ƒì„ ì“°ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µì€ ì•ˆ ì”ë‹ˆë‹¤. ì›ë³¸ ë°ì´í„°($X$)ë¥¼ ì§€ì›Œë²„ë¦¬ë©´ ì •ë³´ ì†ì‹¤ì´ ë„ˆë¬´ í¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì£¼ë¡œ íŒŒë¼ë¯¸í„°ê°€ ë§ì€ ì€ë‹‰ì¸µ(FC Layer)ì— ì‚¬ìš©í•©ë‹ˆë‹¤.

\textbf{Q. \texttt{keep\_prob}ëŠ” ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} ê³¼ëŒ€ì í•©ì´ ì‹¬í•  ê²ƒ ê°™ì€ ì¸µ(ë‰´ëŸ°ì´ ë§ì€ ì¸µ)ì€ ë‚®ê²Œ(0.5), ê·¸ë ‡ì§€ ì•Šì€ ì¸µì€ ë†’ê²Œ(0.8~1.0) ì„¤ì •í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ë‘ ê°€ì§€ ê°•ë ¥í•œ ë°©íŒ¨(L2, Dropout)ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. 
í•˜ì§€ë§Œ ëª¨ë¸ í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¬ë‹¤ë©´ ì–´ë–¨ê¹Œìš”? ì•„ë¬´ë¦¬ ì¢‹ì€ ëª¨ë¸ë„ í•™ìŠµì— 1ë…„ì´ ê±¸ë¦°ë‹¤ë©´ ë¬´ìš©ì§€ë¬¼ì…ë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” í•™ìŠµ ì†ë„ë¥¼ ë¹„ì•½ì ìœ¼ë¡œ ë†’ì—¬ì£¼ëŠ” **[Optimization]** ê¸°ìˆ ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. ê·¸ ì²« ë²ˆì§¸ ì—´ì‡ ì¸ **'ì…ë ¥ ì •ê·œí™”(Input Normalization)'**ê°€ ì™œ ê²½ì‚¬ í•˜ê°•ë²•ì˜ ì†ë„ë¥¼ ë†’ì´ëŠ”ì§€ ê¸°í•˜í•™ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Dropout:} í•™ìŠµ ì‹œ ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ°ì„ ëˆë‹¤. (Ensemble íš¨ê³¼, ê³¼ëŒ€ì í•© ë°©ì§€)
    \item \textbf{Inverted Dropout:} í•™ìŠµ ì‹œ ì¶œë ¥ê°’ì„ \texttt{keep\_prob}ë¡œ ë‚˜ëˆ ì£¼ì–´ ê¸°ëŒ“ê°’ì„ ìœ ì§€í•œë‹¤.
    \item \textbf{Test Time:} í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì ˆëŒ€ ë“œë¡­ì•„ì›ƒì„ ì“°ì§€ ì•ŠëŠ”ë‹¤.
    \item \textbf{Caution:} Cost ê·¸ë˜í”„ê°€ ì§„ë™í•  ìˆ˜ ìˆìœ¼ë‹ˆ ë””ë²„ê¹… ì‹œì—” ë„ê³  í™•ì¸í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{tipbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì‹¤ì „ íŒ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Data Augmentation \& Early Stopping}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.3 Regularization (L2, Dropout) \textit{- Completed}
        \item \textbf{5.4 Data Augmentation \& Early Stopping}
        \begin{itemize}
            \item Concept: Free Data \& Time Machine
            \item Augmentation Techniques (Flip, Crop, Rotate)
            \item Early Stopping Mechanism (Patience)
            \item Implementation: On-the-fly Generation
        \end{itemize}
        \item 5.5 Input Normalization \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” L2 ì •ê·œí™”ì™€ ë“œë¡­ì•„ì›ƒì´ë¼ëŠ” ê°•ë ¥í•œ ìˆ˜í•™ì  ê¸°ë²•ìœ¼ë¡œ ê³¼ëŒ€ì í•©(Overfitting)ì„ ì–µì œí–ˆìŠµë‹ˆë‹¤.
ì˜¤ëŠ˜ì€ ì¡°ê¸ˆ ë” **'ì‹¤ìš©ì ì´ê³ (Practical)' 'ê²½ì œì ì¸(Economical)'** ì ‘ê·¼ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.
ë°ì´í„°ë¥¼ ë” ëª¨ìœ¼ëŠ” ê²ƒì€ ë¹„ìŒ‰ë‹ˆë‹¤. í•˜ì§€ë§Œ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„°ë¥¼ ë³€í˜•í•´ì„œ **'ê³µì§œ ë°ì´í„°'**ë¥¼ ë§Œë“œëŠ” ê¸°ìˆ (Augmentation)ê³¼, í•™ìŠµì„ ê°€ì¥ ì¢‹ì€ íƒ€ì´ë°ì— ë©ˆì¶”ëŠ” **'íƒ€ì„ë¨¸ì‹ '** ê¸°ìˆ (Early Stopping)ì€ ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ê°€ ì—„ì²­ë‚©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ê°€ì¥ ì§ê´€ì ì´ê³  ê°€ì„±ë¹„ ì¢‹ì€ ë‘ ê°€ì§€ ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Data Augmentation:} ì´ë¯¸ì§€ë¥¼ ë³€í˜•í•˜ì—¬ ë°ì´í„°ì…‹ì„ ë»¥íŠ€ê¸°í•˜ê³ , ëª¨ë¸ì—ê²Œ **ë¶ˆë³€ì„±(Invariance)**ì„ ê°€ë¥´ì¹©ë‹ˆë‹¤.
    \item \textbf{Early Stopping:} ê³¼ëŒ€ì í•©ì´ ì‹œì‘ë˜ê¸° ì§ì „ì— í•™ìŠµì„ ë©ˆì¶”ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
    \item \textbf{On-the-fly:} ë””ìŠ¤í¬ ìš©ëŸ‰ì„ ì•„ë¼ê¸° ìœ„í•´ í•™ìŠµ ë„ì¤‘ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì´í•´í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì„¤ëª…} & \textbf{ë¹„ìœ } \\ \hline
\textbf{Data Augmentation} & ì›ë³¸ ë°ì´í„°ë¥¼ ë³€í˜•í•´ ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„± & ë³µì‚¬ê¸°ë¡œ ë¬¸ì œì§‘ ë³µì‚¬í•˜ê¸° (ê·¼ë° ì•½ê°„ ë¹„ëš¤ê²Œ) \\ \hline
\textbf{Early Stopping} & ì„±ëŠ¥ ì•…í™” ì‹œì ì— í•™ìŠµ ì¤‘ë‹¨ & ë°•ìˆ˜ ì¹  ë•Œ ë– ë‚˜ë¼ \\ \hline
\textbf{Patience} & ì„±ëŠ¥ì´ ì•ˆ ì¢‹ì•„ì ¸ë„ ê¸°ë‹¤ë ¤ì£¼ëŠ” íšŸìˆ˜ & "í•œ ë²ˆë§Œ ë” ê¸°íšŒë¥¼ ì¤„ê²Œ" \\ \hline
\textbf{On-the-fly} & ë¯¸ë¦¬ ì €ì¥í•˜ì§€ ì•Šê³  í•„ìš”í•  ë•Œ ì¦‰ì„ ìƒì„± & ì£¼ë¬¸ ë“¤ì–´ì˜¤ë©´ ìš”ë¦¬í•˜ê¸° (ë¯¸ë¦¬ í•´ë‘ë©´ ìƒí•¨) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ê³µì§œ ì ì‹¬ì€ ìˆë‹¤}

\subsection{1. Data Augmentation (ë°ì´í„° ì¦ê°•)}
ëª¨ë¸ì—ê²Œ **"ê³ ì–‘ì´ëŠ” ë’¤ì§‘ì–´ë„, ì–´ë‘ì›Œë„, ì˜ë ¤ë„ ê³ ì–‘ì´ë‹¤"**ë¼ëŠ” ì‚¬ì‹¤ì„ ê°€ë¥´ì¹©ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Mirroring (Flipping):} ê±°ìš¸ì²˜ëŸ¼ ì¢Œìš° ë°˜ì „. (ìˆ«ì ì¸ì‹ ë“± ë°©í–¥ì´ ì¤‘ìš”í•œ ë°ì´í„°ì—” ê¸ˆì§€!)
    \item \textbf{Random Cropping:} ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¶„ì„ ë¬´ì‘ìœ„ë¡œ ì˜ë¼ëƒ„.
    \item \textbf{Rotation / Shearing:} íšŒì „ ë° ë¹„í‹€ê¸°.
    \item \textbf{Color Jittering:} ë°ê¸°, ì±„ë„ ë“±ì— ë…¸ì´ì¦ˆ ì¶”ê°€.
\end{itemize}



\begin{tipbox}{On-the-fly Generation (ì‹¤ì‹œê°„ ìƒì„±)}
"êµìˆ˜ë‹˜, ë³€í˜•ëœ ì´ë¯¸ì§€ë¥¼ í•˜ë“œë””ìŠ¤í¬ì— ì €ì¥í•´ë‘ê³  ì¨ì•¼ í•©ë‹ˆê¹Œ?"
\textbf{ì ˆëŒ€ ì•„ë‹™ë‹ˆë‹¤.} 1TBì§œë¦¬ ë°ì´í„°ì…‹ì„ 10ë°° ì¦ê°•í•˜ë©´ 10TBê°€ ë©ë‹ˆë‹¤. ê°ë‹¹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
\textbf{CPUê°€ í•™ìŠµ ë„ì¤‘ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€í˜•}í•´ì„œ GPUì—ê²Œ ë„˜ê²¨ì£¼ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë””ìŠ¤í¬ ìš©ëŸ‰ì€ ê·¸ëŒ€ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.
\end{tipbox}

---

\subsection{2. Early Stopping (ì¡°ê¸° ì¢…ë£Œ)}
í•™ìŠµ ê³¡ì„ (Learning Curve)ì„ ë³´ë‹¤ê°€, Train ErrorëŠ” ì¤„ì§€ë§Œ Dev Errorê°€ ë‹¤ì‹œ ì˜¬ë¼ê°€ë ¤ëŠ” ìˆœê°„(ê³¼ëŒ€ì í•© ì‹œì‘ì )ì— ë©ˆì¶¥ë‹ˆë‹¤.



\begin{itemize}
    \item \textbf{ì›ë¦¬:} í•™ìŠµì„ ì˜¤ë˜ í•˜ë©´ ê°€ì¤‘ì¹˜ $W$ê°€ ì ì  ì»¤ì ¸ì„œ ë³µì¡í•œ íŒ¨í„´ì„ ìµíˆê²Œ ë©ë‹ˆë‹¤. Early Stoppingì€ $W$ê°€ ë„ˆë¬´ ì»¤ì§€ê¸° ì „ì— ë©ˆì¶”ë¯€ë¡œ, ìˆ˜í•™ì ìœ¼ë¡œ **L2 ì •ê·œí™”ì™€ ìœ ì‚¬í•œ íš¨ê³¼**ë¥¼ ëƒ…ë‹ˆë‹¤.
    \item \textbf{ë‹¨ì  (Orthogonalization):} Andrew Ng êµìˆ˜ëŠ” ì´ ë°©ì‹ì´ 'Bias ì¤„ì´ê¸°'ì™€ 'Variance ì¤„ì´ê¸°'ë¥¼ ë™ì‹œì— ê±´ë“œë¦¬ê¸° ë•Œë¬¸ì—(ì§êµí™” ìœ„ë°°), íŠœë‹ì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆë‹¤ê³  ì§€ì í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í¸í•´ì„œ ë§ì´ ì”ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: On-the-fly Pipeline}

ë°ì´í„° ì¦ê°•ì€ NumPyë¡œ ê°„ë‹¨íˆ, ì¡°ê¸° ì¢…ë£ŒëŠ” í´ë˜ìŠ¤ë¡œ êµ¬í˜„í•˜ì—¬ ì›ë¦¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Data Augmentation & Early Stopping]
import numpy as np
import copy

class DataAugmentor:
    @staticmethod
    def random_flip(image, p=0.5):
        """ì¢Œìš° ë°˜ì „"""
        if np.random.rand() < p:
            return np.fliplr(image)
        return image

    @staticmethod
    def random_crop(image, crop_size=(200, 200)):
        """ë¬´ì‘ìœ„ ìœ„ì¹˜ ìë¥´ê¸°"""
        h, w, _ = image.shape
        top = np.random.randint(0, h - crop_size[0])
        left = np.random.randint(0, w - crop_size[1])
        return image[top:top+crop_size[0], left:left+crop_size[1], :]

class EarlyStopping:
    def __init__(self, patience=5, min_delta=0.0):
        self.patience = patience # ì°¸ì„ì„± (íšŸìˆ˜)
        self.min_delta = min_delta # ìµœì†Œ ê°œì„ í­
        self.counter = 0
        self.best_loss = np.inf
        self.best_model = None
        self.stop = False

    def check(self, val_loss, model_params):
        if val_loss < (self.best_loss - self.min_delta):
            # ì„±ëŠ¥ ê°œì„ ! -> ì €ì¥ ë° ì¹´ìš´í„° ì´ˆê¸°í™”
            self.best_loss = val_loss
            self.counter = 0
            # ì¤‘ìš”: deepcopyë¡œ ê°’ ìì²´ë¥¼ ë³µì‚¬í•´ë‘¬ì•¼ í•¨ (ì°¸ì¡° ë³µì‚¬ ê¸ˆì§€)
            self.best_model = copy.deepcopy(model_params)
        else:
            # ì„±ëŠ¥ ì •ì²´/ì•…í™” -> ì¹´ìš´í„° ì¦ê°€
            self.counter += 1
            print(f"EarlyStopping counter: {self.counter}/{self.patience}")
            if self.counter >= self.patience:
                self.stop = True
    
    def restore(self):
        return self.best_model
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Deep Copyë¥¼ ì•ˆ ì“°ë©´ ìƒê¸°ëŠ” ì¼}
íŒŒì´ì¬ì—ì„œ `best_model = model`ì´ë¼ê³  ì“°ë©´, `best_model`ì€ `model`ì„ ê°€ë¦¬í‚¤ëŠ” ë³„ëª…ì´ ë  ë¿ì…ë‹ˆë‹¤. í•™ìŠµì´ ê³„ì† ì§„í–‰ë˜ì–´ `model`ì´ ë§ê°€ì§€ë©´ `best_model`ë„ ê°™ì´ ë§ê°€ì§‘ë‹ˆë‹¤.
ë°˜ë“œì‹œ `copy.deepcopy(model)`ì„ ì‚¬ìš©í•˜ì—¬ **ê·¸ ìˆœê°„ì˜ ìŠ¤ëƒ…ìƒ·**ì„ ë©”ëª¨ë¦¬ì— ë”°ë¡œ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Data Augmentationì„ Test Setì—ë„ ì ìš©í•˜ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µì€ ì•ˆ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ 'Test Time Augmentation (TTA)'ì´ë¼ëŠ” ê¸°ë²•ì„ ì“°ê¸°ë„ í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ 5ê°€ì§€ë¡œ ë³€í˜•í•´ì„œ ì˜ˆì¸¡í•œ ë’¤ í‰ê· ì„ ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤. (ëŒ€íšŒìš© í…Œí¬ë‹‰)

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ëª¨ë“  ë¬´ê¸°(L2, Dropout, Augmentation, Early Stopping)ë¥¼ ê°–ì·„ìŠµë‹ˆë‹¤. ë°©ì–´ ì¤€ë¹„ëŠ” ëë‚¬ìŠµë‹ˆë‹¤.

ì´ì œ ê³µê²©(í•™ìŠµ ì†ë„)ì„ ê°•í™”í•  ì°¨ë¡€ì…ë‹ˆë‹¤. ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì€ ë„ˆë¬´ ì •ì§í•´ì„œ ëŠë¦½ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Optimization Algorithms]**ìœ¼ë¡œ ë„˜ì–´ê°€ì„œ, ê²½ì‚¬ í•˜ê°•ë²•ì— ê°€ì†ë„ë¥¼ ë¶™ì´ëŠ” **Momentum**, ë³´í­ì„ ì¡°ì ˆí•˜ëŠ” **Adam** ë“± ìµœì‹  ìµœì í™” ê¸°ë²•ì„ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Augmentation:} ë°ì´í„°ë¥¼ ë³€í˜•í•´ ì–‘ì„ ëŠ˜ë¦¬ê³  ë¶ˆë³€ì„±(Invariance)ì„ í•™ìŠµì‹œí‚¨ë‹¤.
    \item \textbf{On-the-fly:} ë””ìŠ¤í¬ ì ˆì•½ì„ ìœ„í•´ í•™ìŠµ ë„ì¤‘ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³€í˜•í•œë‹¤.
    \item \textbf{Early Stopping:} Dev Errorê°€ ì˜¤ë¥´ê¸° ì‹œì‘í•˜ë©´ ë©ˆì¶˜ë‹¤. (ê³¼ëŒ€ì í•© ë°©ì§€)
    \item \textbf{Patience:} ì¼ì‹œì ì¸ ì„±ëŠ¥ ì €í•˜ë¥¼ ê²¬ë””ê¸° ìœ„í•´ ì¸ë‚´ì‹¬(Patience) ê°’ì„ ì„¤ì •í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{tipbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì‹¤ì „ íŒ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Improving Deep Neural Networks: \\ Mini-batch Gradient Descent}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.4 Regularization \& Data Setup \textit{- Completed}
        \item 5.5 Input Normalization \textit{- Completed}
        \item \textbf{5.6 Optimization Algorithms 1: Mini-batch Gradient Descent}
        \begin{itemize}
            \item Batch vs Stochastic vs Mini-batch
            \item Epoch vs Iteration Definition
            \item Why Powers of 2? ($2^n$)
            \item Implementation: Shuffle and Partition
        \end{itemize}
        \item 5.7 Optimization Algorithms 2: Momentum \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ê°•ì˜ê¹Œì§€ ìš°ë¦¬ëŠ” ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” 'ë°©ì–´ ê¸°ìˆ (Regularization)'ì„ ìµí˜”ìŠµë‹ˆë‹¤. ì´ì œë¶€í„°ëŠ” ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ë¥¼ ê·¹í•œìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” **'ê°€ì† ê¸°ìˆ (Optimization)'**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
ë”¥ëŸ¬ë‹ì˜ ì—°ë£ŒëŠ” ë°ì´í„°ì…ë‹ˆë‹¤. ë°ì´í„°ê°€ 1000ë§Œ ê°œë¼ë©´, ê¸°ì¡´ ë°©ì‹(Batch Gradient Descent)ìœ¼ë¡œëŠ” í•œ ê±¸ìŒ ë–¼ëŠ” ë° ë©°ì¹ ì´ ê±¸ë¦½ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì‘ì€ ë©ì–´ë¦¬ë¡œ ìª¼ê°œì„œ í•™ìŠµí•˜ëŠ” **ë¯¸ë‹ˆ ë°°ì¹˜(Mini-batch)** ê¸°ë²•ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ **ì‚¬ì‹¤ìƒ í‘œì¤€(Standard)**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ëŠ” **ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë¹„êµ:} Batch(ì „ì²´), Stochastic(1ê°œ), Mini-batch(ë©ì–´ë¦¬)ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•©ë‹ˆë‹¤.
    \item \textbf{ìš©ì–´:} í—·ê°ˆë¦¬ê¸° ì‰¬ìš´ **Epoch(ì—í­)**ê³¼ **Iteration(ë°˜ë³µ)**ì˜ ê°œë…ì„ ëª…í™•íˆ í•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ì™œ ë°°ì¹˜ í¬ê¸°ë¥¼ **$2^n$ (64, 128 ë“±)**ìœ¼ë¡œ ì„¤ì •í•´ì•¼ í•˜ë“œì›¨ì–´(CPU/GPU)ê°€ ì¢‹ì•„í•˜ëŠ”ì§€ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ë°ì´í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ê³ (Shuffle) ë‚˜ëˆ„ëŠ”(Partition) ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì„¤ëª…} & \textbf{ì˜ˆì‹œ ($m=1000$, Batch=100)} \\ \hline
\textbf{Epoch} & ì „ì²´ ë°ì´í„°($m$)ë¥¼ í•œ ë²ˆ ë‹¤ í›‘ëŠ” ê²ƒ. & ì±… 1ê¶Œì„ 1íšŒë… í•¨. \\ \hline
\textbf{Iteration} & íŒŒë¼ë¯¸í„°ë¥¼ í•œ ë²ˆ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒ. & ë¬¸ì œ 100ê°œë¥¼ í’€ê³  ì±„ì í•¨. \\ \hline
\textbf{Mini-batch} & í•œ ë²ˆì˜ Iterationì— ì“°ì´ëŠ” ë°ì´í„° ë¬¶ìŒ. & 1 Epoch = 10 Iterations. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í•™ìŠµ ë°©ì‹ì˜ ìŠ¤í™íŠ¸ëŸ¼}

\subsection{1. The Three Types of Gradient Descent}
ë°ì´í„°ì…‹ í¬ê¸°ê°€ $m$ì¼ ë•Œ, í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì— ëª‡ ê°œì˜ ë°ì´í„°ë¥¼ ì“°ëŠ”ê°€?

\begin{itemize}
    \item \textbf{Batch Gradient Descent (BGD):} 
    \begin{itemize}
        \item ë°ì´í„°: ì „ì²´ $m$ê°œ ì‚¬ìš©.
        \item íŠ¹ì§•: ì•ˆì •ì ì´ì§€ë§Œ ë„ˆë¬´ ëŠë¦¼. ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜.
    \end{itemize}
    
    \item \textbf{Stochastic Gradient Descent (SGD):} 
    \begin{itemize}
        \item ë°ì´í„°: ë”± 1ê°œ ì‚¬ìš©.
        \item íŠ¹ì§•: ì—„ì²­ ë¹ ë¥´ì§€ë§Œ ë²¡í„°í™”(ë³‘ë ¬ ì²˜ë¦¬) ì´ì ì´ ì—†ìŒ. ì§„ë™ì´ ì‹¬í•¨.
    \end{itemize}
    
    \item \textbf{Mini-batch Gradient Descent:} 
    \begin{itemize}
        \item ë°ì´í„°: $T$ê°œ ì‚¬ìš© (ì˜ˆ: 64, 128).
        \item íŠ¹ì§•: \textbf{Sweet Spot.} BGDì˜ ì•ˆì •ì„± + SGDì˜ ì†ë„ + ë²¡í„°í™” íš¨ìœ¨ì„±ì„ ëª¨ë‘ ì¡ìŒ.
    \end{itemize}
\end{itemize}



\begin{analogybox}{ì‚° ë‚´ë ¤ì˜¤ê¸° ë¹„ìœ }
\begin{itemize}
    \item \textbf{Batch:} ì§€ë„ë¥¼ í¼ì³ ì‚° ì „ì²´ë¥¼ íŒŒì•…í•œ ë’¤, ì •í™•í•˜ê²Œ í•œ ë°œìêµ­ ë‚´ë”›ìŠµë‹ˆë‹¤. (ë„ˆë¬´ ì‹ ì¤‘í•¨)
    \item \textbf{Stochastic:} ëˆˆì„ ê°ê³  ë°œë ê°ê°ë§Œìœ¼ë¡œ ë¯¸ì¹œ ë“¯ì´ ë›°ì–´ë‚´ë ¤ ê°‘ë‹ˆë‹¤. (ë¹ ë¥´ì§€ë§Œ ë¹„í‹€ê±°ë¦¼)
    \item \textbf{Mini-batch:} 100ê±¸ìŒ ì•ë§Œ ë³´ê³  ë°©í–¥ì„ ì¡ì•„ ë‚´ë ¤ê°‘ë‹ˆë‹¤. (ì ë‹¹íˆ ë¹ ë¥´ê³  ì ë‹¹íˆ ì •í™•í•¨)
\end{itemize}
\end{analogybox}

---

\subsection{2. Why Powers of 2? ($2^n$)}
"êµìˆ˜ë‹˜, ë°°ì¹˜ í¬ê¸°ë¥¼ 100ê°œë‚˜ 50ê°œë¡œ í•˜ë©´ ì•ˆ ë˜ë‚˜ìš”?"
\begin{itemize}
    \item ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ **ë¹„íš¨ìœ¨ì **ì…ë‹ˆë‹¤.
    \item ì»´í“¨í„° ë©”ëª¨ë¦¬(CPU/GPU)ì˜ ì£¼ì†Œ ì²´ê³„ì™€ ìºì‹œëŠ” **2ì§„ìˆ˜ ê¸°ë°˜**ì…ë‹ˆë‹¤.
    \item $32, 64, 128, 256, 512$ ë“±ìœ¼ë¡œ ì„¤ì •í•˜ë©´ ë©”ëª¨ë¦¬ ì •ë ¬(Alignment)ì´ ë”± ë§ì•„ë–¨ì–´ì ¸ ì—°ì‚° ì†ë„ê°€ ìµœì í™”ë©ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Shuffle and Partition}

ë°ì´í„°ë¥¼ ì„ê³ (Shuffle) ìë¥´ëŠ”(Partition) ê³¼ì •ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
ê°€ì¥ ì¤‘ìš”í•œ ì ì€ **$X$ì™€ $Y$ë¥¼ ë˜‘ê°™ì€ ìˆœì„œë¡œ ì„ì–´ì•¼ í•œë‹¤**ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Random Mini-batches Generator]
import numpy as np
import math

def random_mini_batches(X, Y, mini_batch_size=64, seed=0):
    """
    X: (n_x, m), Y: (1, m)
    """
    np.random.seed(seed)            
    m = X.shape[1]  # ë°ì´í„° ìƒ˜í”Œ ìˆ˜
    mini_batches = []
        
    # Step 1: Shuffle (X, Y)
    # 0 ~ m-1 ê¹Œì§€ì˜ ë¬´ì‘ìœ„ ì¸ë±ìŠ¤ ìƒì„±
    permutation = list(np.random.permutation(m))
    
    # ì¤‘ìš”: Xì™€ Yë¥¼ ê°™ì€ ì¸ë±ìŠ¤ë¡œ ì„ìŒ (ì—´ ë‹¨ìœ„)
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1, m))

    # Step 2: Partition (ë‚˜ëˆ„ê¸°)
    # ê½‰ ì°¬ ë°°ì¹˜ì˜ ê°œìˆ˜
    num_complete_minibatches = math.floor(m / mini_batch_size) 
    
    for k in range(0, num_complete_minibatches):
        begin = k * mini_batch_size
        end = (k + 1) * mini_batch_size
        
        mini_batch_X = shuffled_X[:, begin : end]
        mini_batch_Y = shuffled_Y[:, begin : end]
        mini_batches.append((mini_batch_X, mini_batch_Y))
    
    # Step 3: Handling the Remainder (ìíˆ¬ë¦¬ ì²˜ë¦¬)
    # ë°ì´í„°ê°€ ë”± ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ì§€ ì•Šì„ ë•Œ ë§ˆì§€ë§‰ ë°°ì¹˜ë¥¼ ì²˜ë¦¬
    if m % mini_batch_size != 0:
        begin = num_complete_minibatches * mini_batch_size
        # ëê¹Œì§€ ë‹¤ ë‹´ê¸°
        mini_batch_X = shuffled_X[:, begin : ]
        mini_batch_Y = shuffled_Y[:, begin : ]
        mini_batches.append((mini_batch_X, mini_batch_Y))
    
    return mini_batches
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Shuffle ì‹œ ì£¼ì˜ì‚¬í•­}
ì‹œê³„ì—´ ë°ì´í„°(ì£¼ì‹, ë‚ ì”¨, ìŒì„± ë“±)ì²˜ëŸ¼ **ìˆœì„œ(Time)**ê°€ ì¤‘ìš”í•œ ë°ì´í„°ëŠ” ì ˆëŒ€ ì„ìœ¼ë©´ ì•ˆ ë©ë‹ˆë‹¤! ê³¼ê±° ë°ì´í„°ë¡œ ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ”ë°, ì„ì–´ë²„ë¦¬ë©´ ë¯¸ë˜ë¥¼ ë³´ê³  ê³¼ê±°ë¥¼ ë§ì¶”ëŠ” ê¼´(Data Leakage)ì´ ë©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ë©´(8192 ì´ìƒ) ì–´ë–»ê²Œ ë˜ë‚˜ìš”?} \\
\textbf{A.} GPU ë©”ëª¨ë¦¬ ë¶€ì¡±(OOM Error)ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ëª¨ë¸ì´ ë„ˆë¬´ ì¼ë°˜í™”ëœ íŒ¨í„´ë§Œ ë°°ì›Œì„œ ì„±ëŠ¥(Generalization)ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒ(Sharp Minima)ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³´í†µ 32~512 ì‚¬ì´ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ë¯¸ë‹ˆ ë°°ì¹˜ë¥¼ ì“°ë‹ˆ í•™ìŠµì´ ë¹¨ë¼ì¡ŒìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë¹„ìš© í•¨ìˆ˜ ê·¸ë˜í”„ë¥¼ í™•ëŒ€í•´ ë³´ë©´ ì—¬ì „íˆ ì§€ê·¸ì¬ê·¸ë¡œ ì§„ë™í•˜ë©° ë‚´ë ¤ê°‘ë‹ˆë‹¤.

ì´ ì§„ë™ì„ ì¤„ì´ê³ , ë‚´ë¦¬ë§‰ê¸¸ì—ì„œ ê³µì´ êµ´ëŸ¬ê°€ë“¯ **ê´€ì„±(Inertia)**ì„ ë¶™ì—¬ ë” ë¹ ë¥´ê²Œ ë‚´ë ¤ê°€ê²Œ í•  ìˆ˜ëŠ” ì—†ì„ê¹Œìš”?
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë‹¨ìˆœí•œ ê²½ì‚¬ í•˜ê°•ë²•ì„ ë„˜ì–´ì„  **[Optimization] Momentum (ëª¨ë©˜í…€)** ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Mini-batch GD:} ë°ì´í„°ë¥¼ ì‘ì€ ë¬¶ìŒìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì—…ë°ì´íŠ¸í•œë‹¤. (ì†ë„ + ì•ˆì •ì„±)
    \item \textbf{Power of 2:} ë°°ì¹˜ í¬ê¸°ëŠ” $2^n$ (32, 64, 128...)ì´ í•˜ë“œì›¨ì–´ íš¨ìœ¨ì ì´ë‹¤.
    \item \textbf{Shuffle:} ë§¤ ì—í­ë§ˆë‹¤ ë°ì´í„°ë¥¼ ì„ì–´ì£¼ì–´ì•¼ í•™ìŠµì´ ê³¨ê³ ë£¨ ëœë‹¤.
    \item \textbf{Last Batch:} ë°ì´í„°ê°€ ë‚˜ëˆ„ì–´ë–¨ì–´ì§€ì§€ ì•Šì„ ë•Œ, ë§ˆì§€ë§‰ ìíˆ¬ë¦¬ ë°°ì¹˜ë¥¼ ë²„ë¦¬ì§€ ë§ê³  ì²˜ë¦¬í•´ì•¼ í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Optimization Algorithms: \\ Momentum \& RMSprop}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.5 Regularization \& Data Setup \textit{- Completed}
        \item 5.6 Mini-batch Gradient Descent \textit{- Completed}
        \item \textbf{5.7 Optimization Algorithms (Momentum \& RMSprop)}
        \begin{itemize}
            \item Exponential Weighted Moving Average (The Trend)
            \item Momentum (Physics: Velocity)
            \item RMSprop (Adaptive Learning Rate)
            \item Implementation
        \end{itemize}
        \item 5.8 Adam Optimizer \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ë°ì´í„°ë¥¼ ì‘ì€ ë©ì–´ë¦¬ë¡œ ìª¼ê°œ í•™ìŠµí•˜ëŠ” **ë¯¸ë‹ˆ ë°°ì¹˜ ê²½ì‚¬ í•˜ê°•ë²•**ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì†ë„ëŠ” ë¹¨ë¼ì¡Œì§€ë§Œ, ê·¸ë˜í”„ë¥¼ ë³´ë©´ ì—¬ì „íˆ ìµœì ì ì„ í–¥í•´ ê³§ë°”ë¡œ ê°€ì§€ ëª»í•˜ê³  **ì§€ê·¸ì¬ê·¸(Zigzag)ë¡œ ì§„ë™(Oscillation)**í•˜ë©° ë‚´ë ¤ê°‘ë‹ˆë‹¤.
ì´ ì§„ë™ì„ ì¤„ì´ê³ , ìµœì í•´ë¥¼ í–¥í•´ **'ê°€ì†ë„(Acceleration)'**ë¥¼ ë¶™ì¼ ìˆ˜ëŠ” ì—†ì„ê¹Œìš”? ë¬¼ë¦¬í•™ì˜ ê´€ì„±ì„ ì´ìš©í•œ **Momentum**ê³¼, ë³´í­ì„ ìë™ìœ¼ë¡œ ì¡°ì ˆí•˜ëŠ” **RMSprop**ì´ ê·¸ í•´ë‹µì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë‹¨ìˆœí•œ ê²½ì‚¬ í•˜ê°•ë²•ì„ ë„˜ì–´ì„  **ê³ ê¸‰ ìµœì í™” ì•Œê³ ë¦¬ì¦˜** ë‘ ê°€ì§€ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê¸°ì´ˆ:} ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¸ë Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” **ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· (EWMA)**ì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{Momentum:} ê³¼ê±°ì˜ ê¸°ìš¸ê¸°ë¥¼ ëˆ„ì í•˜ì—¬ ê´€ì„±ì„ ë§Œë“œëŠ” ì›ë¦¬ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{RMSprop:} ê¸°ìš¸ê¸°ì˜ í¬ê¸°(ì œê³±)ì— ë”°ë¼ í•™ìŠµ ë³´í­ì„ ì¡°ì ˆí•˜ëŠ” ì ì‘í˜• ì•Œê³ ë¦¬ì¦˜ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ë‘ ì•Œê³ ë¦¬ì¦˜ì˜ ìˆ˜ì‹ì„ Python ì½”ë“œë¡œ ì˜®ê¸°ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°($\beta$)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{ì„¤ëª…} \\ \hline
\textbf{EWMA} & $v_t$ & ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê· . ìµœê·¼ ë°ì´í„°ì˜ ê²½í–¥ì„±ì„ ë‚˜íƒ€ëƒ„. \\ \hline
\textbf{Momentum} & $v$ & ê´€ì„±(ì†ë„). ê³¼ê±°ì˜ ì§„í–‰ ë°©í–¥ì„ ìœ ì§€í•˜ë ¤ëŠ” ì„±ì§ˆ. \\ \hline
\textbf{RMSprop} & $S$ & Root Mean Square Prop. ê¸°ìš¸ê¸° ì œê³±ì„ ì´ìš©í•´ ë³´í­ ì¡°ì ˆ. \\ \hline
\textbf{Beta} & $\beta$ & ê³¼ê±° ë°ì´í„°ë¥¼ ì–¼ë§ˆë‚˜ ê¸°ì–µí• ì§€ ê²°ì •í•˜ëŠ” ê³„ìˆ˜ (0.9 ë“±). \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ê°€ì†ì˜ ì›ë¦¬}

\subsection{1. ì§€ìˆ˜ ê°€ì¤‘ ì´ë™ í‰ê·  (Exponentially Weighted Moving Average)}
ì´ ì•Œê³ ë¦¬ì¦˜ë“¤ì˜ ê¸°ì´ˆê°€ ë˜ëŠ” ìˆ˜í•™ì…ë‹ˆë‹¤.
$$ v_t = \beta v_{t-1} + (1 - \beta) \theta_t $$
\begin{itemize}
    \item $\beta = 0.9$: ìµœê·¼ 10ì¼ê°„ì˜ í‰ê·  ($\frac{1}{1-0.9} = 10$)
    \item $\beta = 0.98$: ìµœê·¼ 50ì¼ê°„ì˜ í‰ê·  ($\frac{1}{1-0.98} = 50$)
    \item $\beta$ê°€ í´ìˆ˜ë¡ ê·¸ë˜í”„ê°€ ë¶€ë“œëŸ¬ì›Œì§€ì§€ë§Œ(Smoothing), ë³€í™”ì— ë‘”ê°í•´ì§‘ë‹ˆë‹¤(Latency).
\end{itemize}



---

\subsection{2. Momentum (ê´€ì„±)}
\textbf{"ê³µì´ ì–¸ë•ì„ êµ´ëŸ¬ ë‚´ë ¤ê°ˆ ë•Œ ì†ë„ê°€ ë¶™ëŠ” ë¬¼ë¦¬ ë²•ì¹™"}

\begin{analogybox}{ì–¼ìŒíŒ ìœ„ì˜ ì‡ êµ¬ìŠ¬}
\begin{itemize}
    \item \textbf{SGD (ì¼ë°˜ ê²½ì‚¬ í•˜ê°•ë²•):} ë§ˆì°°ë ¥ì´ ë¬´í•œëŒ€ì¸ ë°”ë‹¥. í˜(ê¸°ìš¸ê¸°)ì„ ì£¼ë©´ ì›€ì§ì´ê³ , ì•ˆ ì£¼ë©´ ë”± ë©ˆì¶¥ë‹ˆë‹¤. ë°©í–¥ì´ ë°”ë€Œë©´ ì¦‰ì‹œ êº¾ì…ë‹ˆë‹¤ (ì§€ê·¸ì¬ê·¸).
    \item \textbf{Momentum:} ë§ˆì°°ë ¥ì´ ì—†ëŠ” ì–¼ìŒíŒ. í˜ì„ ì£¼ì§€ ì•Šì•„ë„ ê¸°ì¡´ì— ë‚´ë ¤ì˜¤ë˜ **ì†ë„(Velocity, $v$)** ë•Œë¬¸ì— ê³„ì† ë¯¸ë„ëŸ¬ì ¸ ë‚´ë ¤ê°‘ë‹ˆë‹¤. ì´ ê´€ì„±ì´ ì§„ë™ì„ ìƒì‡„í•˜ê³  ì›…ë©ì´(Local Minima)ë¥¼ ë„˜ê²Œ í•´ì¤ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

\begin{mathbox}{Update Rule}
1. ì†ë„ ê³„ì‚°: $v = \beta v + (1-\beta) dW$ \\
2. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: $W = W - \alpha v$ \\
($\alpha$: í•™ìŠµë¥ , $\beta$: ë³´í†µ 0.9 ì‚¬ìš©)
\end{mathbox}

---

\subsection{3. RMSprop (Root Mean Square Propagation)}
\textbf{"ê°€íŒŒë¥¸ ê³³ì€ ì²œì²œíˆ, ì™„ë§Œí•œ ê³³ì€ ë¹ ë¥´ê²Œ"}

ì œí”„ë¦¬ íŒíŠ¼ êµìˆ˜ê°€ ì œì•ˆí•œ ë°©ë²•ì…ë‹ˆë‹¤. ê¸°ìš¸ê¸°($dW$)ì˜ í¬ê¸°ë¥¼ ë³´ê³  ë³´í­ì„ ì¡°ì ˆí•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì›ë¦¬:} í•™ìŠµë¥  $\alpha$ë¥¼ $\sqrt{S}$ë¡œ ë‚˜ëˆ ì¤ë‹ˆë‹¤.
    \item \textbf{íš¨ê³¼:} 
    \begin{itemize}
        \item ê¸°ìš¸ê¸°ê°€ í¼($S$ í¼) $\rightarrow$ ë¶„ëª¨ê°€ ì»¤ì§ $\rightarrow$ ì—…ë°ì´íŠ¸ í­ ê°ì†Œ (ì§„ë™ ì–µì œ)
        \item ê¸°ìš¸ê¸°ê°€ ì‘ìŒ($S$ ì‘ìŒ) $\rightarrow$ ë¶„ëª¨ê°€ ì‘ì•„ì§ $\rightarrow$ ì—…ë°ì´íŠ¸ í­ ì¦ê°€ (ê°€ì†)
    \end{itemize}
\end{itemize}

\begin{mathbox}{Update Rule}
1. ì œê³± í‰ê· : $S = \beta_2 S + (1-\beta_2) dW^2$ (ìš”ì†Œë³„ ì œê³±) \\
2. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: $W = W - \alpha \frac{dW}{\sqrt{S} + \epsilon}$ \\
($\epsilon$: 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ìš©, $10^{-8}$)
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Momentum \& RMSprop}

\begin{lstlisting}[language=Python, caption=Optimization Algorithms Implementation]
import numpy as np

def update_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    v: ì†ë„(Velocity) ë”•ì…”ë„ˆë¦¬ (ì´ˆê¸°ê°’ì€ 0)
    beta: Momentum ê³„ìˆ˜ (ë³´í†µ 0.9)
    """
    L = len(parameters) // 2
    
    for l in range(1, L + 1):
        # 1. ì†ë„(v) ì—…ë°ì´íŠ¸ (ê´€ì„± ëˆ„ì )
        v["dW" + str(l)] = beta * v["dW" + str(l)] + (1 - beta) * grads["dW" + str(l)]
        v["db" + str(l)] = beta * v["db" + str(l)] + (1 - beta) * grads["db" + str(l)]
        
        # 2. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (vë¥¼ ë¹¼ì¤Œ)
        parameters["W" + str(l)] -= learning_rate * v["dW" + str(l)]
        parameters["b" + str(l)] -= learning_rate * v["db" + str(l)]
        
    return parameters, v

def update_with_rmsprop(parameters, grads, s, beta2, learning_rate, epsilon=1e-8):
    """
    s: ì œê³± í‰ê· (Squared Gradient) ë”•ì…”ë„ˆë¦¬
    beta2: RMSprop ê³„ìˆ˜ (ë³´í†µ 0.999)
    """
    L = len(parameters) // 2
    
    for l in range(1, L + 1):
        # 1. ì œê³± í‰ê· (s) ì—…ë°ì´íŠ¸ (ê¸°ìš¸ê¸° ì œê³± ì£¼ì˜!)
        s["dW" + str(l)] = beta2 * s["dW" + str(l)] + (1 - beta2) * np.square(grads["dW" + str(l)])
        s["db" + str(l)] = beta2 * s["db" + str(l)] + (1 - beta2) * np.square(grads["db" + str(l)])
        
        # 2. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ì ì‘í˜• í•™ìŠµë¥ )
        # ë¶„ëª¨ì— sqrt(s) + epsilon
        parameters["W" + str(l)] -= learning_rate * (grads["dW" + str(l)] / (np.sqrt(s["dW" + str(l)]) + epsilon))
        parameters["b" + str(l)] -= learning_rate * (grads["db" + str(l)] / (np.sqrt(s["db" + str(l)]) + epsilon))
        
    return parameters, s
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ë³€ìˆ˜ ì´ˆê¸°í™” ì‹¤ìˆ˜}
$v$ì™€ $s$ëŠ” í•™ìŠµ ë£¨í”„(Iteration)ê°€ ëŒ ë•Œë§ˆë‹¤ ì´ˆê¸°í™”í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤! 
ê·¸ëŸ¬ë©´ ê´€ì„±ì´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ë°˜ë“œì‹œ **í•™ìŠµ ì‹œì‘ ì „(Epoch 0 ì´ì „)ì— í•œ ë²ˆë§Œ 0ìœ¼ë¡œ ì´ˆê¸°í™”**í•˜ê³ , ê³„ì† ê°’ì„ ëˆ„ì í•´ê°€ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. $\beta$(Momentum)ì™€ $\beta_2$(RMSprop) ê°’ì€ íŠœë‹í•´ì•¼ í•˜ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µì€ **ê¸°ë³¸ê°’($\beta=0.9, \beta_2=0.999$)**ì„ ê·¸ëŒ€ë¡œ ì”ë‹ˆë‹¤. ì´ ê°’ë“¤ì´ ê²½í—˜ì ìœ¼ë¡œ ëŒ€ë¶€ë¶„ì˜ ë¬¸ì œì—ì„œ ì˜ ì‘ë™í•©ë‹ˆë‹¤. í•™ìŠµë¥ ($\alpha$) íŠœë‹ì´ í›¨ì”¬ ì¤‘ìš”í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” ìµœê³ ì˜ ê°€ì† ì—”ì§„ ë‘ ê°œë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.
\begin{itemize}
    \item **Momentum:** ê´€ì„±ì„ ì´ìš©í•˜ì—¬ ì†ë„ë¥¼ ë†’ì„.
    \item **RMSprop:** ë³´í­ì„ ì¡°ì ˆí•˜ì—¬ ì§„ë™ì„ ì¤„ì„.
\end{itemize}
"ë‘˜ ë‹¤ ì“°ë©´ ì•ˆ ë˜ë‚˜ìš”?" 
ë‹¹ì—°íˆ ë©ë‹ˆë‹¤. ì´ ë‘˜ì„ ê²°í•©í•œ ê²ƒì´ ë°”ë¡œ **Adam (Adaptive Moment Estimation)**ì…ë‹ˆë‹¤. í˜„ì¬ ë”¥ëŸ¬ë‹ ì„¸ê³„ë¥¼ ì§€ë°°í•˜ê³  ìˆëŠ” Adam ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ìŒ ì‹œê°„ì— ì™„ì„±í•˜ê³ , ìµœì í™” ë‹¨ì›ì„ ë§ˆë¬´ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Momentum:} $v \leftarrow dW$. ê³¼ê±°ì˜ ì†ë„ë¥¼ ìœ ì§€í•˜ì—¬ Local Minima íƒˆì¶œ ë° ê°€ì†.
    \item \textbf{RMSprop:} $S \leftarrow dW^2$. ê¸°ìš¸ê¸°ê°€ í¬ë©´ í•™ìŠµë¥ ì„ ë‚®ì¶° ì§„ë™ì„ ë°©ì§€.
    \item \textbf{Math:} $dW^2$ì€ ìš”ì†Œë³„ ì œê³±ì´ë‹¤. ë‚˜ëˆ—ì…ˆ ì‹œ $\epsilon$ì„ ë”í•´ ì—ëŸ¬ë¥¼ ë°©ì§€í•œë‹¤.
    \item \textbf{Hyperparam:} $\beta=0.9$, $\beta_2=0.999$ê°€ êµ­ë£°(Standard)ì´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Optimization Algorithms: \\ Adam Optimizer}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.5 Regularization \& Data Setup \textit{- Completed}
        \item 5.6 Mini-batch Gradient Descent \textit{- Completed}
        \item 5.7 Momentum \& RMSprop \textit{- Completed}
        \item \textbf{5.8 Adam Optimizer}
        \begin{itemize}
            \item The Ultimate Fusion: Momentum + RMSprop
            \item Bias Correction Mechanism
            \item Hyperparameter Standards ($\alpha, \beta_1, \beta_2, \epsilon$)
            \item Implementation from Scratch
        \end{itemize}
        \item 5.9 Hyperparameter Tuning Strategy \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ë‚œ ë‘ ê°•ì˜ë¥¼ í†µí•´ 'ê´€ì„±'ì„ ì´ìš©í•´ ì†ë„ë¥¼ ë†’ì´ëŠ” **Momentum**ê³¼, 'ë³´í­'ì„ ì¡°ì ˆí•´ ì§„ë™ì„ ì¤„ì´ëŠ” **RMSprop**ì„ ë°°ì› ìŠµë‹ˆë‹¤.
ê·¸ë ‡ë‹¤ë©´ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ì´ ìƒê¹ë‹ˆë‹¤. **"ì´ ë‘ ê°€ì§€ ì¥ì ì„ ëª¨ë‘ í•©ì¹  ìˆ˜ëŠ” ì—†ì„ê¹Œ?"**
ê·¸ í•´ë‹µì´ ë°”ë¡œ **Adam (Adaptive Moment Estimation)**ì…ë‹ˆë‹¤. Adamì€ í˜„ì¬ ë”¥ëŸ¬ë‹ í•™ê³„ì™€ í˜„ì—…ì—ì„œ **'Default Optimizer(ê¸°ë³¸ ì„¤ì •)'**ë¡œ í†µí•©ë‹ˆë‹¤. ì–´ë–¤ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì“¸ì§€ ê³ ë¯¼ë  ë•Œ, ì¼ë‹¨ Adamì„ ì“°ë©´ 80ì  ì´ìƒì€ ê°‘ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ í‘œì¤€ì¸ **Adam Optimizer**ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ í•´ë¶€í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{í†µí•©:} Adamì´ Momentumì˜ í‰ê· (1ì°¨)ê³¼ RMSpropì˜ ë¶„ì‚°(2ì°¨)ì„ ì–´ë–»ê²Œ ê²°í•©í•˜ëŠ”ì§€ ìˆ˜ì‹ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ë³´ì •:} í•™ìŠµ ì´ˆê¸°ì— 0ìœ¼ë¡œ ì ë¦¬ëŠ” í˜„ìƒì„ ë§‰ê¸° ìœ„í•œ **í¸í–¥ ë³´ì •(Bias Correction)**ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{í‘œì¤€:} $\beta_1, \beta_2, \epsilon$ ë“± í•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ êµ­ë£°(Standard Value)ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} Pythonìœ¼ë¡œ í¸í–¥ ë³´ì •ì´ í¬í•¨ëœ ì „ì²´ ì•Œê³ ë¦¬ì¦˜ì„ ë°‘ë°”ë‹¥ë¶€í„° êµ¬í˜„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{ê¸°í˜¸} & \textbf{í‘œì¤€ê°’} & \textbf{ì—­í• } \\ \hline
$\alpha$ & íŠœë‹ í•„ìš” & \textbf{í•™ìŠµë¥  (Learning Rate)}. ê°€ì¥ ì¤‘ìš”í•¨. \\ \hline
$\beta_1$ & 0.9 & \textbf{Momentum ê³„ìˆ˜}. (ê¸°ìš¸ê¸°ì˜ ì§€ìˆ˜ í‰ê· ) \\ \hline
$\beta_2$ & 0.999 & \textbf{RMSprop ê³„ìˆ˜}. (ê¸°ìš¸ê¸° ì œê³±ì˜ ì§€ìˆ˜ í‰ê· ) \\ \hline
$\epsilon$ & $10^{-8}$ & \textbf{ì•ˆì •ì„± ìƒìˆ˜}. 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìµœê°•ì˜ ìœµí•©}

\subsection{1. The Fusion Algorithm}
Adamì€ ë§¤ ìŠ¤í…($t$)ë§ˆë‹¤ ë‹¤ìŒ 4ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item \textbf{Momentum ($v$):} ì†ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (1ì°¨ ëª¨ë©˜íŠ¸)
    $$ v_t = \beta_1 v_{t-1} + (1 - \beta_1) dW $$
    
    \item \textbf{RMSprop ($s$):} ê°€ì†ë„ ì œì–´(ë§ˆì°°ë ¥)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. (2ì°¨ ëª¨ë©˜íŠ¸)
    $$ s_t = \beta_2 s_{t-1} + (1 - \beta_2) dW^2 $$
    
    \item \textbf{Bias Correction (í•µì‹¬):} ì´ˆê¸° 0ìœ¼ë¡œ ì ë¦° ê°’ì„ ë³´ì •í•©ë‹ˆë‹¤.
    $$ v^{corr}_t = \frac{v_t}{1 - \beta_1^t}, \quad s^{corr}_t = \frac{s_t}{1 - \beta_2^t} $$
    
    \item \textbf{Update:} íŒŒë¼ë¯¸í„°ë¥¼ ê°±ì‹ í•©ë‹ˆë‹¤.
    $$ W = W - \alpha \frac{v^{corr}_t}{\sqrt{s^{corr}_t} + \epsilon} $$
\end{enumerate}

---

\section{Deep Dive: Bias Correction (í¸í–¥ ë³´ì •)}

"êµìˆ˜ë‹˜, ì™œ êµ³ì´ $(1 - \beta^t)$ë¡œ ë‚˜ëˆ ì£¼ë‚˜ìš”?"
ì´ê²ƒì€ Adamì˜ ì •êµí•¨ì„ ë³´ì—¬ì£¼ëŠ” ëŒ€ëª©ì…ë‹ˆë‹¤.

\begin{mathbox}{ì´ˆê¸°ê°’ 0ì˜ ì €ì£¼}
ìš°ë¦¬ëŠ” $v_0 = 0$ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ ìŠ¤í…($t=1$)ì„ ë´…ì‹œë‹¤.
($\beta_1 = 0.9$ ê°€ì •)

$$ v_1 = 0.9 \times 0 + 0.1 \times dW = 0.1 dW $$

\textbf{ë¬¸ì œì :} ì‹¤ì œ ê¸°ìš¸ê¸°($dW$)ì˜ \textbf{10ë¶„ì˜ 1(0.1)}ë°–ì— ë°˜ì˜ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í•™ìŠµ ì´ˆë°˜ì— ê±°ë¶ì´ì²˜ëŸ¼ ëŠë ¤ì§‘ë‹ˆë‹¤.

\textbf{í•´ê²°ì±… (ë³´ì •):}
$$ 1 - \beta_1^1 = 1 - 0.9 = 0.1 $$
$$ v^{corr}_1 = \frac{v_1}{0.1} = \frac{0.1 dW}{0.1} = dW $$

\textbf{ê²°ê³¼:} ë³´ì • ë•ë¶„ì— ì´ˆê¸°ì—ë„ ê¸°ìš¸ê¸°ë¥¼ 100\% ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
$t$ê°€ ì»¤ì§€ë©´ $\beta^t \to 0$ì´ ë˜ì–´, ë¶„ëª¨ê°€ 1ì´ ë˜ë¯€ë¡œ ë³´ì • íš¨ê³¼ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Adam from Scratch}

Adam êµ¬í˜„ ì‹œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ í˜„ì¬ ë°˜ë³µ íšŸìˆ˜ **$t$**ë¥¼ ì¶”ì í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Adam Optimizer Implementation]
import numpy as np

def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,
                                beta1=0.9, beta2=0.999, epsilon=1e-8):
    """
    t: í˜„ì¬ iteration count (1ë¶€í„° ì‹œì‘í•´ì•¼ í•¨!)
    v, s: ì´ì „ ìŠ¤í…ê¹Œì§€ ëˆ„ì ëœ Momentum, RMSprop ë³€ìˆ˜
    """
    L = len(parameters) // 2
    v_corrected = {} 
    s_corrected = {} 
    
    for l in range(1, L + 1):
        # --- 1. Momentum (v) ---
        v["dW" + str(l)] = beta1 * v["dW" + str(l)] + (1 - beta1) * grads["dW" + str(l)]
        v["db" + str(l)] = beta1 * v["db" + str(l)] + (1 - beta1) * grads["db" + str(l)]
        
        # --- 2. Bias Correction (v) ---
        # 1 - beta^t ë¡œ ë‚˜ëˆ”
        v_corrected["dW" + str(l)] = v["dW" + str(l)] / (1 - np.power(beta1, t))
        v_corrected["db" + str(l)] = v["db" + str(l)] / (1 - np.power(beta1, t))
        
        # --- 3. RMSprop (s) ---
        # ê¸°ìš¸ê¸° ì œê³±(square) ì£¼ì˜!
        s["dW" + str(l)] = beta2 * s["dW" + str(l)] + (1 - beta2) * np.square(grads["dW" + str(l)])
        s["db" + str(l)] = beta2 * s["db" + str(l)] + (1 - beta2) * np.square(grads["db" + str(l)])
        
        # --- 4. Bias Correction (s) ---
        s_corrected["dW" + str(l)] = s["dW" + str(l)] / (1 - np.power(beta2, t))
        s_corrected["db" + str(l)] = s["db" + str(l)] / (1 - np.power(beta2, t))
        
        # --- 5. Update Parameters ---
        # ë¶„ëª¨: sqrt(s_corr) + epsilon
        parameters["W" + str(l)] -= learning_rate * (v_corrected["dW" + str(l)] / (np.sqrt(s_corrected["dW" + str(l)]) + epsilon))
        parameters["b" + str(l)] -= learning_rate * (v_corrected["db" + str(l)] / (np.sqrt(s_corrected["db" + str(l)]) + epsilon))
        
    return parameters, v, s
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Iteration Count $t$ ì£¼ì˜}
í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•Œ $t$ëŠ” ë°˜ë“œì‹œ 1ë¶€í„° ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤. ë§Œì•½ $t=0$ì´ë©´ $1 - \beta^0 = 1 - 1 = 0$ì´ ë˜ì–´ **ZeroDivisionError**ê°€ ë°œìƒí•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Adamì´ í•­ìƒ ìµœê³ ì¸ê°€ìš”?} \\
\textbf{A.} ëŒ€ë¶€ë¶„ì˜ ê²½ìš°(CV, NLP, GAN) ê·¸ë ‡ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•„ì£¼ ì •êµí•œ ìˆ˜ë ´ì´ í•„ìš”í•  ë•Œ(SOTA ë…¼ë¬¸ ë“±)ëŠ” ì¼ë°˜ SGD+Momentumì´ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ë•Œë„ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ë„ ì‹œì‘ì€ ë¬´ì¡°ê±´ Adamì„ ì¶”ì²œí•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ë¡œì¨ ìš°ë¦¬ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì •ì ì¸ Adamì„ ì •ë³µí–ˆìŠµë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ì€ ì–´ë–¤ ëª¨ë¸ì´ë“  ë¹ ë¥´ê³  ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆëŠ” ì—”ì§„ì„ ê°–ì·„ìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ì—”ì§„ ì„±ëŠ¥ì´ ì¢‹ì•„ë„, ê¸°ì–´ ë³€ì†(í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •)ì„ ì˜ëª»í•˜ë©´ ì°¨ê°€ ë‚˜ê°€ì§€ ì•ŠìŠµë‹ˆë‹¤. $\alpha$, $\beta$, ë°°ì¹˜ í¬ê¸°, ì€ë‹‰ì¸µ ê°œìˆ˜... ë„ëŒ€ì²´ ë¬´ì—‡ë¶€í„° ì¡°ì ˆí•´ì•¼ í• ê¹Œìš”?
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì´ ìˆ˜ë§ì€ ë‹¤ì´ì–¼ì„ ì–´ë–¤ ìˆœì„œë¡œ ëŒë ¤ì•¼ í•˜ëŠ”ì§€, **[Hyperparameter Tuning]**ì˜ ì²´ê³„ì ì¸ ì „ëµ(Random Search vs Grid Search)ì„ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Adam:} Momentum(ì†ë„) + RMSprop(ê°€ì†ë„ ì œì–´) + Bias Correction(ì´ˆê¸° ë³´ì •).
    \item \textbf{Standard Params:} $\alpha$(íŠœë‹), $\beta_1(0.9)$, $\beta_2(0.999)$, $\epsilon(10^{-8})$.
    \item \textbf{Bias Correction:} í•™ìŠµ ì´ˆë°˜ì— íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ê°€ ë„ˆë¬´ ì‘ì•„ì§€ëŠ” ê²ƒì„ ë§‰ì•„ì¤€ë‹¤.
    \item \textbf{Memory:} $v$ì™€ $s$ë¥¼ ë”°ë¡œ ì €ì¥í•´ì•¼ í•˜ë¯€ë¡œ ì¼ë°˜ SGDë³´ë‹¤ ë©”ëª¨ë¦¬ë¥¼ ë” ì“´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Optimization Algorithms: \\ Learning Rate Decay}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.5 Regularization \& Data Setup \textit{- Completed}
        \item 5.6 Mini-batch Gradient Descent \textit{- Completed}
        \item 5.7-5.8 Momentum, RMSprop, Adam \textit{- Completed}
        \item \textbf{5.9 Learning Rate Decay}
        \begin{itemize}
            \item The Parking Problem (Oscillation)
            \item Decay Schedules (Inverse Time, Exponential, Step)
            \item Implementation \& Visualization
        \end{itemize}
        \item 5.10 Hyperparameter Tuning Strategy \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” Adam Optimizerë¥¼ í†µí•´ ìµœì í™”ì˜ ì •ì ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì•„ì£¼ ë¯¸ì„¸í•œ ë¬¸ì œê°€ í•˜ë‚˜ ë‚¨ì•˜ìŠµë‹ˆë‹¤. í•™ìŠµ í›„ë°˜ë¶€ê°€ ë˜ë©´ ì†ì‹¤ í•¨ìˆ˜(Cost)ì˜ ìµœì €ì  ê·¼ì²˜ì—ì„œ ëª¨ë¸ì´ ì•ˆì°©í•˜ì§€ ëª»í•˜ê³  ê³„ì† ë§´ë„ëŠ” **ì§„ë™(Oscillation)** í˜„ìƒì´ ë°œìƒí•©ë‹ˆë‹¤.
ì£¼ì°¨ì¥ì—ì„œ ì‹œì† 100kmë¡œ ë‹¬ë¦¬ë©´ ì ˆëŒ€ ì£¼ì°¨ ì¹¸ì— ì°¨ë¥¼ ë„£ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª©ì ì§€ ê·¼ì²˜ì—ì„œëŠ” ì†ë„ë¥¼ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ **í•™ìŠµë¥  ê°ì‡ (Learning Rate Decay)**ê°€ í•„ìš”í•œ ì´ìœ ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í•™ìŠµ ë‹¨ê³„ë³„ë¡œ í•™ìŠµë¥ ($\alpha$)ì„ ì¡°ì ˆí•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ê¸°ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì´ìœ :} ê³ ì •ëœ í•™ìŠµë¥ ì´ ìµœì €ì  ê·¼ì²˜ì—ì„œ ìˆ˜ë ´í•˜ì§€ ëª»í•˜ëŠ” ì´ìœ ë¥¼ ê¸°í•˜í•™ì ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì „ëµ:} ì‹œê°„ ê¸°ë°˜(Inverse Time), ì§€ìˆ˜(Exponential), ê³„ë‹¨ì‹(Step) ê°ì‡ ì˜ ìˆ˜ì‹ì  ì°¨ì´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} Pythonìœ¼ë¡œ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ êµ¬í˜„í•˜ê³  ì—í­(Epoch)ì— ë”°ë¥¸ ë³€í™”ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{ì„¤ëª…} \\ \hline
\textbf{Learning Rate} & $\alpha$ & í•œ ë²ˆ ì—…ë°ì´íŠ¸í•  ë•Œ ì´ë™í•˜ëŠ” ë³´í­ì˜ í¬ê¸°. \\ \hline
\textbf{Decay Rate} & $k$ & í•™ìŠµë¥ ì„ ì–¼ë§ˆë‚˜ ë¹¨ë¦¬ ì¤„ì¼ì§€ ê²°ì •í•˜ëŠ” ê³„ìˆ˜. \\ \hline
\textbf{Epoch} & $t$ & ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ í•™ìŠµí•œ íšŸìˆ˜. (ì‹œê°„ ë‹¨ìœ„) \\ \hline
\textbf{Initial LR} & $\alpha_0$ & í•™ìŠµ ì‹œì‘ ì‹œì ì˜ ì´ˆê¸° í•™ìŠµë¥ . \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì†ë„ ì¡°ì ˆì˜ ë¯¸í•™}

\subsection{1. The Parking Problem (ì™œ ì¤„ì—¬ì•¼ í•˜ëŠ”ê°€?)}


\begin{analogybox}{ê³ ì†ë„ë¡œì™€ ì£¼ì°¨ì¥ ë¹„ìœ }
\begin{itemize}
    \item \textbf{Early Stage (ê³ ì†ë„ë¡œ):} ìµœì €ì ì´ ë©€ë¦¬ ìˆìŠµë‹ˆë‹¤. ì´ë•ŒëŠ” ë³´í­ì´ ì»¤ì•¼(High $\alpha$) ë¹¨ë¦¬ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    \item \textbf{Late Stage (ì£¼ì°¨ì¥):} ìµœì €ì  ê·¼ì²˜ì…ë‹ˆë‹¤. ì´ë•Œë„ ë³´í­ì´ í¬ë‹¤ë©´ êµ¬ë©ì„ ì§€ë‚˜ì³ ë²„ë¦¬ê³ (Overshooting), ë‹¤ì‹œ ëŒì•„ì˜¤ë ¤ë‹¤ ë˜ ì§€ë‚˜ì¹©ë‹ˆë‹¤.
    \item \textbf{Solution:} ëª©ì ì§€ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì†ë„ë¥¼ ì„œì„œíˆ ì¤„ì—¬ì„œ ì •ë°€í•˜ê²Œ ì£¼ì°¨(ìˆ˜ë ´)í•´ì•¼ í•©ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

---

\subsection{2. Decay Schedules (ê°ì‡  ì „ëµ)}
ì‹œê°„($t$, Epoch)ì´ ì§€ë‚ ìˆ˜ë¡ $\alpha$ë¥¼ ì¤„ì´ëŠ” ëŒ€í‘œì ì¸ ê³µì‹ë“¤ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{1. Inverse Time Decay (ì‹œê°„ ê¸°ë°˜):}
    $$ \alpha = \frac{1}{1 + k \cdot t} \alpha_0 $$
    ê°€ì¥ ì™„ë§Œí•˜ê²Œ ì¤„ì–´ë“­ë‹ˆë‹¤.
    
    \item \textbf{2. Exponential Decay (ì§€ìˆ˜ ê°ì‡ ):}
    $$ \alpha = k^t \cdot \alpha_0 \quad (k < 1, \text{ì˜ˆ: } 0.95) $$
    ë¹ ë¥´ê²Œ 0ìœ¼ë¡œ ìˆ˜ë ´í•©ë‹ˆë‹¤.
    
    \item \textbf{3. Step Decay (ê³„ë‹¨ì‹ ê°ì‡ ):}
    10 ì—í­ë§ˆë‹¤ ì ˆë°˜ìœ¼ë¡œ ëš ë–¨ì–´ëœ¨ë¦½ë‹ˆë‹¤. (ResNet ë“± ì‹¬ì¸µ ëª¨ë¸ì—ì„œ ì„ í˜¸)
    
\end{itemize}

---

\section{Deep Dive: Adaptive Methodsì™€ì˜ ê´€ê³„}

"êµìˆ˜ë‹˜, Adamì´ ì•Œì•„ì„œ í•™ìŠµë¥  ì¡°ì ˆí•´ì£¼ì§€ ì•Šë‚˜ìš”?"
\begin{itemize}
    \item \textbf{Adam/RMSprop:} íŒŒë¼ë¯¸í„°ë§ˆë‹¤ *ê°œë³„ì ìœ¼ë¡œ* í•™ìŠµë¥ ì„ ì¡°ì ˆ(Adaptive)í•˜ì§€ë§Œ, ì „ì²´ì ì¸ *ê¸€ë¡œë²Œ í•™ìŠµë¥ *($\alpha$) ìì²´ëŠ” ê³ ì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    \item \textbf{ê²°ë¡ :} Adamì„ ì“°ë”ë¼ë„ Learning Rate Decayë¥¼ í•¨ê»˜ ì ìš©í•˜ë©´, ìµœì €ì ì—ì„œì˜ ì§„ë™ì„ ì¤„ì—¬ ì„±ëŠ¥ì„ ë” ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (SOTA ëª¨ë¸ë“¤ì˜ í•„ìˆ˜ í…Œí¬ë‹‰)
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Decay Scheduler}

ì§ì ‘ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë§Œë“¤ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=Learning Rate Schedulers]
import numpy as np
import matplotlib.pyplot as plt

class LRScheduler:
    def __init__(self, init_lr=1.0):
        self.init_lr = init_lr
        
    def inverse_time_decay(self, epoch, k=0.1):
        """lr = lr0 / (1 + kt)"""
        return self.init_lr / (1 + k * epoch)
        
    def exponential_decay(self, epoch, k=0.95):
        """lr = lr0 * k^t"""
        return self.init_lr * np.power(k, epoch)
        
    def step_decay(self, epoch, drop=0.5, interval=10):
        """íŠ¹ì • ê°„ê²©(interval)ë§ˆë‹¤ drop ë¹„ìœ¨ë§Œí¼ ê°ì†Œ"""
        exponent = np.floor((1 + epoch) / interval)
        return self.init_lr * np.power(drop, exponent)

# --- ì‹œê°í™” ---
if __name__ == "__main__":
    epochs = np.arange(0, 100)
    scheduler = LRScheduler(init_lr=1.0)
    
    lr1 = [scheduler.inverse_time_decay(e) for e in epochs]
    lr2 = [scheduler.exponential_decay(e) for e in epochs]
    lr3 = [scheduler.step_decay(e) for e in epochs]
    
    plt.figure(figsize=(10, 6))
    plt.plot(epochs, lr1, label='Inverse Time')
    plt.plot(epochs, lr2, label='Exponential')
    plt.plot(epochs, lr3, label='Step Decay')
    plt.title('Learning Rate Decay Schedules')
    plt.xlabel('Epochs')
    plt.ylabel('Learning Rate')
    plt.legend()
    plt.grid(True)
    plt.show()
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{tipbox}{ReduceLROnPlateau (ì‹¤ì „ ê¿€íŒ)}
ê°€ì¥ ì‹¤ìš©ì ì¸ ë°©ë²•ì€ ìˆ˜ì‹ë³´ë‹¤ëŠ” **ì„±ëŠ¥**ì„ ë³´ê³  ì¤„ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.
\textbf{"ì§€ë‚œ 10 ì—í­ ë™ì•ˆ Dev Errorê°€ ì¤„ì–´ë“¤ì§€ ì•Šì•˜ë„¤? (Plateau)"} $\rightarrow$ \textbf{"ì´ì œ ì •ë°€ íƒ€ê²©í•  ë•Œë‹¤. í•™ìŠµë¥ ì„ 1/10ë¡œ ì¤„ì—¬ë¼."}
Kerasë‚˜ PyTorchì—ì„œ `ReduceLROnPlateau` ì½œë°±ì„ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.
\end{tipbox}

\textbf{Q. $k$(ê°ì‡ ìœ¨)ê°€ ë„ˆë¬´ í¬ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?} \\
\textbf{A.} í•™ìŠµë¥ ì´ ë„ˆë¬´ ë¹¨ë¦¬ 0ì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤. ìµœì €ì ì— ë„ë‹¬í•˜ê¸°ë„ ì „ì— ëª¨ë¸ì´ ë©ˆì¶°ë²„ë¦¬ëŠ” **ì¡°ê¸° ìˆ˜ë ´(Premature Convergence)** ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìµœì í™” ë„êµ¬ë“¤ì€ ëª¨ë‘ ê°–ì·„ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ë„ˆë¬´ ë§ì•„ì¡ŒìŠµë‹ˆë‹¤.
($\alpha, \beta_1, \beta_2, \epsilon, k, \lambda, \text{batch\_size} \dots$)

ì´ ë§ì€ ë‹¤ì´ì–¼ì„ ì–´ë–¤ ìˆœì„œë¡œ, ì–´ë–»ê²Œ ë§ì¶°ì•¼ í• ê¹Œìš”? ì‚¬ëŒì´ ì¼ì¼ì´ ëŒë ¤ë³´ê¸°ì—” ì‹œê°„ì´ ë„ˆë¬´ ë¶€ì¡±í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Hyperparameter Tuning]** ì „ëµì„ í†µí•´, ì´ ë³µì¡í•œ í¼ì¦ì„ ì²´ê³„ì ìœ¼ë¡œ í‘¸ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤. **Grid Search**ì™€ **Random Search**ì˜ ìŠ¹ë¶€ê°€ í¼ì³ì§‘ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Need for Decay:} ê³ ì • í•™ìŠµë¥ ì€ ìµœì €ì  ê·¼ì²˜ì—ì„œ ì§„ë™í•œë‹¤. ì •ë°€í•œ ìˆ˜ë ´ì„ ìœ„í•´ ì¤„ì—¬ì•¼ í•œë‹¤.
    \item \textbf{Schedules:} Inverse Time(ì™„ë§Œ), Exponential(ê¸‰ê²©), Step(ê³„ë‹¨ì‹) ë“±ì´ ìˆë‹¤.
    \item \textbf{Best Practice:} Dev Errorê°€ ì •ì²´ë  ë•Œ ì¤„ì´ëŠ” \texttt{ReduceLROnPlateau} ë°©ì‹ì´ ê°€ì¥ íš¨ê³¼ì ì´ë‹¤.
    \item \textbf{With Adam:} Adamì„ ì“°ë”ë¼ë„ Decayë¥¼ í•¨ê»˜ ì“°ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§„ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{tipbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì‹¤ì „ íŒ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Optimization Algorithms: \\ Hyperparameter Tuning Strategy}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.5 Regularization \& Data Setup \textit{- Completed}
        \item 5.6-5.9 Optimization (Mini-batch, Adam, Decay) \textit{- Completed}
        \item \textbf{5.10 Hyperparameter Tuning Strategy}
        \begin{itemize}
            \item Tuning Priority (Alpha is King)
            \item Grid Search vs Random Search
            \item Picking Appropriate Scale (Log Scale)
            \item Coarse to Fine Strategy
        \end{itemize}
        \item 5.11 Batch Normalization \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ì‹ ê²½ë§ì„ ë§Œë“¤ê³ (Architecture), í•™ìŠµì‹œí‚¤ê³ (Adam), ê·œì œ(Regularization)í•˜ëŠ” ëª¨ë“  ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ë§‰ìƒ ì—¬ëŸ¬ë¶„ì´ ëª¨ë¸ì„ ëŒë¦¬ë ¤ê³  í•˜ë©´ ê±°ëŒ€í•œ ë²½ì— ë¶€ë”ªí™ë‹ˆë‹¤.
"í•™ìŠµë¥ ì€ 0.01? 0.0001?", "ë°°ì¹˜ í¬ê¸°ëŠ” 32? 64?"
ìˆ˜ì‹­ ê°œì˜ ë‹¤ì´ì–¼ì„ ë¬´ì‘ìœ„ë¡œ ëŒë¦¬ëŠ” ê²ƒì€ ë„ë°•ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” **ì²´ê³„ì ì´ê³  ê³¼í•™ì ì¸ íƒìƒ‰ ì „ëµ**ì´ í•„ìš”í•©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ SOTA(State-of-the-art) ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì˜ **ìš°ì„ ìˆœìœ„**ì™€ **íƒìƒ‰ ê¸°ë²•**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ìš°ì„ ìˆœìœ„:} í•™ìŠµë¥ ($\alpha$)ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ëŠ” ê³„ì¸µ êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì „ëµ:} ê³ ì°¨ì› ê³µê°„ì—ì„œëŠ” **Random Search**ê°€ Grid Searchë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ìœ ë¦¬í•œ ì´ìœ ë¥¼ ê¸°í•˜í•™ì ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{ìŠ¤ì¼€ì¼:} í•™ìŠµë¥  ë“±ì„ íƒìƒ‰í•  ë•Œ ì„ í˜•(Linear)ì´ ì•„ë‹Œ **ë¡œê·¸ ìŠ¤ì¼€ì¼(Log Scale)**ì„ ì¨ì•¼ í•˜ëŠ” ìˆ˜í•™ì  ì´ìœ ë¥¼ ë°°ì›ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ê¸°ë²•} & \textbf{ì„¤ëª…} & \textbf{ë¹„ìœ } \\ \hline
\textbf{Grid Search} & ëª¨ë“  ì¡°í•©ì„ ê²©ìë¬´ëŠ¬ë¡œ ë‹¤ í•´ë³´ëŠ” ê²ƒ. & ìë¬¼ì‡  ë²ˆí˜¸ë¥¼ 0000ë¶€í„° 9999ê¹Œì§€ ë‹¤ ëŒë ¤ë´„. \\ \hline
\textbf{Random Search} & ë¬´ì‘ìœ„ë¡œ ê°’ì„ ì°ì–´ë³´ëŠ” ê²ƒ. & ê°ìœ¼ë¡œ ì°ì–´ì„œ ë§ì¶¤ (ê³ ì°¨ì›ì—ì„œ ìœ ë¦¬). \\ \hline
\textbf{Log Scale} & ìë¦¿ìˆ˜ ë‹¨ìœ„ë¡œ íƒìƒ‰ ($10^{-4}, 10^{-3} \dots$). & í˜„ë¯¸ê²½ ë°°ìœ¨ì„ 10ë°°, 100ë°°ë¡œ ì¡°ì ˆí•˜ë©° ê´€ì°°. \\ \hline
\textbf{Coarse to Fine} & ë„“ê²Œ í›‘ê³ (Coarse), ì¢‹ì€ ê³³ì„ ì§‘ì¤‘ ê³µëµ(Fine). & ìˆ² ì „ì²´ë¥¼ ìŠ¤ìº”í•˜ê³ , ì˜ì‹¬ ê°€ëŠ” êµ¬ì—­ë§Œ ìˆ˜ìƒ‰. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë¬´ì—‡ì´ ì¤‘ìš”í•œê°€?}

\subsection{1. Tuning Priority (ìš°ì„ ìˆœìœ„ ê³„ê¸‰ë„)}
ëª¨ë“  íŒŒë¼ë¯¸í„°ê°€ í‰ë“±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì•¤ë“œë¥˜ ì‘ êµìˆ˜ì˜ ê²½í—˜ì  ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Tier 1 (King - ê°€ì¥ ì¤‘ìš”):}
    \begin{itemize}
        \item \textbf{Learning Rate ($\alpha$)}: ì´ê²ƒì´ í‹€ë¦¬ë©´ ë‹¤ë¥¸ ê±¸ ì•„ë¬´ë¦¬ ì˜ ë§ì¶°ë„ ì†Œìš©ì—†ìŠµë‹ˆë‹¤.
    \end{itemize}
    \item \textbf{Tier 2 (Queen - ì¤‘ìš”):}
    \begin{itemize}
        \item Momentum ($\beta$), Mini-batch Size, Hidden Units ê°œìˆ˜.
    \end{itemize}
    \item \textbf{Tier 3 (Pawn - ëœ ì¤‘ìš”):}
    \begin{itemize}
        \item Layer ê°œìˆ˜, Learning Rate Decay.
    \end{itemize}
    \item \textbf{Do Not Touch (ê±´ë“œë¦¬ì§€ ë§ˆì„¸ìš”):}
    \begin{itemize}
        \item Adamì˜ $\beta_1(0.9), \beta_2(0.999), \epsilon(10^{-8})$.
    \end{itemize}
\end{itemize}

---

\subsection{2. Grid Search vs Random Search}


\begin{analogybox}{ë³´ë¬¼ ì°¾ê¸°}
ì§€ë„ìƒì— ë³´ë¬¼ì´ ì–´ë”” ìˆëŠ”ì§€ ëª¨ë¦…ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Grid Search:} ì§€ë„ë¥¼ ë°”ë‘‘íŒì²˜ëŸ¼ ë‚˜ëˆ„ê³  êµì°¨ì ë§Œ íŒë‹ˆë‹¤. ë§Œì•½ ë³´ë¬¼ì´ êµì°¨ì  ì‚¬ì´ì— ìˆë‹¤ë©´? ì˜ì›íˆ ëª» ì°¾ìŠµë‹ˆë‹¤.
    \item \textbf{Random Search:} ì§€ë„ë¥¼ ë¬´ì‘ìœ„ë¡œ ì½•ì½• ì°Œë¦…ë‹ˆë‹¤. ê°™ì€ íšŸìˆ˜ë¥¼ ì‹œë„í•˜ë”ë¼ë„, ì¤‘ìš”í•œ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ **í›¨ì”¬ ë” ë‹¤ì–‘í•œ ê°’**ì„ í…ŒìŠ¤íŠ¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

---

\subsection{3. Scale Matters: Log Scale Sampling}
í•™ìŠµë¥  $\alpha$ë¥¼ $0.0001$ì—ì„œ $1$ ì‚¬ì´ì—ì„œ ì°¾ëŠ”ë‹¤ê³  í•©ì‹œë‹¤.

\textbf{ë‚˜ìœ ë°©ë²• (Linear):} `np.random.rand()`
\begin{itemize}
    \item 90\%ì˜ ê°’ì´ $0.1 \sim 1$ êµ¬ê°„ì— ëª°ë¦½ë‹ˆë‹¤.
    \item ì •ì‘ ì¤‘ìš”í•œ $0.0001 \sim 0.1$ êµ¬ê°„ì€ ì „ì²´ì˜ 10\%ë°–ì— íƒìƒ‰í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
\end{itemize}

\textbf{ì¢‹ì€ ë°©ë²• (Log Scale):}
\begin{itemize}
    \item $10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}$ ê° êµ¬ê°„ì„ ê³µí‰í•˜ê²Œ íƒìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.
    \item ì§€ìˆ˜($r$)ë¥¼ $-4 \sim 0$ ì‚¬ì´ì—ì„œ ë½‘ê³ , $10^r$ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Scientific Search}

ì˜¬ë°”ë¥¸ ìŠ¤ì¼€ì¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ë½‘ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Hyperparameter Sampling Strategies]
import numpy as np

class HyperparameterSearch:
    def sample_linear(self, low, high, num_samples):
        """
        ì€ë‹‰ ìœ ë‹› ìˆ˜, ì¸µ ìˆ˜ ë“± (ë“±ê°„ê²©ì´ ì˜ë¯¸ ìˆëŠ” ê²½ìš°)
        """
        return np.random.uniform(low, high, num_samples)

    def sample_log_scale(self, low_exp, high_exp, num_samples):
        """
        í•™ìŠµë¥ (alpha), ì •ê·œí™”ìƒìˆ˜(lambda) ë“± (ìë¦¿ìˆ˜ê°€ ì¤‘ìš”í•œ ê²½ìš°)
        ë²”ìœ„: 10^low_exp ~ 10^high_exp
        """
        # 1. ì§€ìˆ˜(r)ë¥¼ ê· ë“±í•˜ê²Œ ë½‘ìŒ (-4 ~ 0)
        r = np.random.uniform(low_exp, high_exp, num_samples)
        # 2. 10ì˜ ê±°ë“­ì œê³±ìœ¼ë¡œ ë³€í™˜
        return 10 ** r

    def sample_beta(self, num_samples):
        """
        Momentum Beta (0.9 ~ 0.999)
        1-beta ê°’ì„ ë¡œê·¸ ìŠ¤ì¼€ì¼ë¡œ ë½‘ëŠ” ê²ƒì´ í•µì‹¬!
        """
        # 1-beta ë²”ìœ„: 0.1 ~ 0.001 (10^-1 ~ 10^-3)
        r = np.random.uniform(-3, -1, num_samples)
        return 1 - (10 ** r)

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    searcher = HyperparameterSearch()
    
    # í•™ìŠµë¥  íƒìƒ‰: 0.0001 ~ 1.0
    alphas = searcher.sample_log_scale(-4, 0, 5)
    print("Alphas:", np.round(alphas, 6))
    
    # ëª¨ë©˜í…€ íƒìƒ‰: 0.9 ~ 0.999
    betas = searcher.sample_beta(5)
    print("Betas:", np.round(betas, 6))
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{tipbox}{Coarse to Fine ì „ëµ}
ì²˜ìŒë¶€í„° 100 Epochì”© ëŒë¦¬ë©° ì™„ë²½í•œ ê°’ì„ ì°¾ìœ¼ë ¤ í•˜ì§€ ë§ˆì„¸ìš”.
1. **Coarse:** ë„“ì€ ë²”ìœ„ì—ì„œ 5~10 Epochë§Œ ì§§ê²Œ ëŒë ¤ ëŒ€ëµì ì¸ ì„±ëŠ¥ì„ ë´…ë‹ˆë‹¤.
2. **Zoom In:** ì„±ëŠ¥ì´ ì¢‹ì€ ì˜ì—­ì„ ë°œê²¬í•˜ë©´ ê·¸ êµ¬ê°„ì„ ì§‘ì¤‘ í™•ëŒ€í•©ë‹ˆë‹¤.
3. **Fine:** ì¢ì€ ì˜ì—­ì—ì„œ ì •ë°€í•˜ê²Œ ë‹¤ì‹œ Random Searchë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
\end{tipbox}

\textbf{Q. $\beta$(Momentum)ëŠ” ì™œ $1-\beta$ë¡œ ë¡œê·¸ ìƒ˜í”Œë§í•˜ë‚˜ìš”?} \\
\textbf{A.} $\beta$ëŠ” 1ì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ë¯¼ê°í•´ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
$0.9 \to 0.9005$ëŠ” ë³„ ì°¨ì´ ì—†ì§€ë§Œ, $0.999 \to 0.9995$ëŠ” í‰ê·  ê¸°ê°„ì´ 1000ì¼ì—ì„œ 2000ì¼ë¡œ 2ë°°ê°€ ë©ë‹ˆë‹¤. 1ì— ê°€ê¹Œìš´ ê°’ì„ ë” ì„¸ë°€í•˜ê²Œ íƒìƒ‰í•´ì•¼ í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìµœì ì˜ íŒŒë¼ë¯¸í„°ê¹Œì§€ ì°¾ì•˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ëª¨ë¸ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ **"í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§€ê³  ì´ˆê¸°ê°’ì— ë„ˆë¬´ ë¯¼ê°í•´ì§€ëŠ” ë¬¸ì œ"**ê°€ ë°œìƒí•©ë‹ˆë‹¤. ë°ì´í„° ë¶„í¬ê°€ ì¸µì„ ì§€ë‚  ë•Œë§ˆë‹¤ í‹€ì–´ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë”¥ëŸ¬ë‹ ì—­ì‚¬ìƒ ê°€ì¥ ìœ„ëŒ€í•œ ë°œëª… ì¤‘ í•˜ë‚˜ì¸ **[Batch Normalization]**ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‹œê°„ì— ì´ ë§ˆë²• ê°™ì€ ê¸°ë²•ì„ íŒŒí—¤ì³ ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Priority:} í•™ìŠµë¥ ($\alpha$)ì´ 1ìˆœìœ„ë‹¤.
    \item \textbf{Strategy:} Grid Searchë³´ë‹¤ëŠ” **Random Search**ê°€ íš¨ìœ¨ì ì´ë‹¤.
    \item \textbf{Log Scale:} $\alpha$ë‚˜ $\lambda$ëŠ” ìë¦¿ìˆ˜ ë‹¨ìœ„ë¡œ íƒìƒ‰í•´ì•¼ í•œë‹¤. ($10^{-4}, 10^{-3}\dots$)
    \item \textbf{Process:} ë„“ê²Œ í›‘ê³ (Coarse), ì¢ê²Œ íŒŒê³ ë“¤ì–´ë¼(Fine).
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Optimization Algorithms: \\ Batch Normalization}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-4.] Neural Networks Basics \textit{- Completed}
    \item[\textbf{Chapter 5.}] \textbf{Practical Aspects of Deep Learning (Current Unit)}
    \begin{itemize}
        \item 5.1-5.5 Regularization \& Data Setup \textit{- Completed}
        \item 5.6-5.10 Optimization (Adam, Tuning) \textit{- Completed}
        \item \textbf{5.11 Batch Normalization}
        \begin{itemize}
            \item Concept: Internal Covariate Shift
            \item The Algorithm: Norm, Scale, Shift
            \item Train Mode vs Test Mode (Running Average)
            \item Implementation
        \end{itemize}
        \item 5.12 Softmax Regression \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ì™€ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì¸µì´ ê¹Šì–´ì§ˆìˆ˜ë¡ ì—¬ì „íˆ í•™ìŠµì´ ë¶ˆì•ˆì •í•˜ê³ , í•™ìŠµë¥ ì„ ì¡°ê¸ˆë§Œ ë†’ì—¬ë„ ë°œì‚°í•´ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.
ì´ìœ ëŠ” **ì•ë‹¨ ì¸µì˜ íŒŒë¼ë¯¸í„°ê°€ ë°”ë€Œë©´, ë’·ë‹¨ ì¸µìœ¼ë¡œ ë„˜ì–´ì˜¤ëŠ” ë°ì´í„°ì˜ ë¶„í¬ê°€ ê³„ì† ë°”ë€Œê¸° ë•Œë¬¸**ì…ë‹ˆë‹¤. ë’·ë‹¨ ì¸µ ì…ì¥ì—ì„œëŠ” ê³„ì† í”ë“¤ë¦¬ëŠ” ë•… ìœ„ì—ì„œ ê· í˜•ì„ ì¡ìœ¼ë ¤ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.
ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 2015ë…„, **"ë°ì´í„° ë¶„í¬ë¥¼ ê°•ì œë¡œ ê³ ì •ì‹œí‚¤ì"**ëŠ” í˜ëª…ì ì¸ ì•„ì´ë””ì–´ê°€ ë“±ì¥í•©ë‹ˆë‹¤. ë°”ë¡œ **ë°°ì¹˜ ì •ê·œí™”(Batch Normalization)**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë”¥ëŸ¬ë‹ ì—­ì‚¬ìƒ ê°€ì¥ ìœ„ëŒ€í•œ ë°œëª… ì¤‘ í•˜ë‚˜ì¸ **ë°°ì¹˜ ì •ê·œí™”**ì˜ ì›ë¦¬ì™€ êµ¬í˜„ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì›ì¸:} í•™ìŠµì„ ë°©í•´í•˜ëŠ” **ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”(Internal Covariate Shift)** í˜„ìƒì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì•Œê³ ë¦¬ì¦˜:} ë¯¸ë‹ˆ ë°°ì¹˜ ë‹¨ìœ„ë¡œ í‰ê· /ë¶„ì‚°ì„ ì •ê·œí™”í•˜ê³ , **Scale($\gamma$) \& Shift($\beta$)** íŒŒë¼ë¯¸í„°ë¡œ ë³µì›í•˜ëŠ” ê³¼ì •ì„ ìœ ë„í•©ë‹ˆë‹¤.
    \item \textbf{ì°¨ì´:} í•™ìŠµ(Train) ë•ŒëŠ” ë°°ì¹˜ í†µê³„ëŸ‰ì„, ì¶”ë¡ (Test) ë•ŒëŠ” **ì´ë™ í‰ê· (Running Average)**ì„ ì¨ì•¼ í•¨ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ë‘ ê°€ì§€ ëª¨ë“œë¥¼ ì§€ì›í•˜ëŠ” BN í´ë˜ìŠ¤ë¥¼ Pythonìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´/ê¸°í˜¸} & \textbf{ì˜ë¯¸} & \textbf{ì—­í• } \\ \hline
\textbf{Batch Norm} & ë°°ì¹˜ ì •ê·œí™” & ì€ë‹‰ì¸µì˜ í™œì„±í™” ê°’ì„ ì •ê·œ ë¶„í¬ë¡œ ë§Œë“¦. \\ \hline
\textbf{Gamma ($\gamma$)} & ìŠ¤ì¼€ì¼ íŒŒë¼ë¯¸í„° & ì •ê·œí™”ëœ ê°’ì˜ **ë¶„ì‚°**ì„ ì¡°ì ˆ (í•™ìŠµ ê°€ëŠ¥). \\ \hline
\textbf{Beta ($\beta$)} & ì‹œí”„íŠ¸ íŒŒë¼ë¯¸í„° & ì •ê·œí™”ëœ ê°’ì˜ **í‰ê· **ì„ ì¡°ì ˆ (í•™ìŠµ ê°€ëŠ¥). \\ \hline
\textbf{Running Stats} & ì´ë™ í‰ê·  í†µê³„ëŸ‰ & í…ŒìŠ¤íŠ¸ ì‹œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•™ìŠµ ì¤‘ ëˆ„ì í•´ë‘” í‰ê· /ë¶„ì‚°. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í”ë“¤ë¦¬ëŠ” ë•… ê³ ì •í•˜ê¸°}

\subsection{1. Internal Covariate Shift (ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”)}


\begin{analogybox}{í”ë“¤ë¦¬ëŠ” ë‹¤ë¦¬ ìœ„ì—ì„œ ê±·ê¸°}
\begin{itemize}
    \item \textbf{Without BN:} ì•ì‚¬ëŒ(Layer 1)ì´ ë°œì„ êµ¬ë¥¼ ë•Œë§ˆë‹¤ ë‹¤ë¦¬ê°€ í”ë“¤ë¦½ë‹ˆë‹¤. ë’·ì‚¬ëŒ(Layer 2)ì€ ì¤‘ì‹¬ ì¡ëŠë¼ ì•ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤. (í•™ìŠµ ì†ë„ ì €í•˜)
    \item \textbf{With BN:} ê° ì¸µë§ˆë‹¤ **"ë°œíŒì„ ìˆ˜í‰ìœ¼ë¡œ ê³ ì •(Normalize)"**í•´ì¤ë‹ˆë‹¤. ë’·ì‚¬ëŒì€ ì•ì‚¬ëŒì˜ ì›€ì§ì„ì— ìƒê´€ì—†ì´ ì•ˆì •ì ìœ¼ë¡œ ë‹¬ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í•™ìŠµ ì†ë„ ë¹„ì•½ì  í–¥ìƒ)
\end{itemize}
\end{analogybox}

---

\subsection{2. The Algorithm: Norm, Scale, Shift}
ì€ë‹‰ì¸µì˜ ê°’ $Z$ì— ëŒ€í•´ ë¯¸ë‹ˆ ë°°ì¹˜ ë‹¨ìœ„ë¡œ 4ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item \textbf{Mean:} $\mu = \frac{1}{m} \sum z^{(i)}$
    \item \textbf{Variance:} $\sigma^2 = \frac{1}{m} \sum (z^{(i)} - \mu)^2$
    \item \textbf{Normalize:} $z_{norm} = \frac{z - \mu}{\sqrt{\sigma^2 + \epsilon}}$ \quad ($\epsilon$: 0 ë‚˜ëˆ„ê¸° ë°©ì§€)
    \item \textbf{Scale \& Shift (í•µì‹¬):} $\tilde{z} = \gamma z_{norm} + \beta$
\end{enumerate}

\begin{warningbox}{ì™œ ë‹¤ì‹œ $\gamma, \beta$ë¡œ ë§ê°€ëœ¨ë¦¬ë‚˜ìš”?}
ë¬´ì¡°ê±´ í‰ê·  0, ë¶„ì‚° 1ë¡œ ê³ ì •í•˜ë©´, ë°ì´í„°ê°€ Sigmoidì˜ ì„ í˜• êµ¬ê°„(ê°€ìš´ë°)ì—ë§Œ ëª°ë¦¬ê²Œ ë˜ì–´ **ë¹„ì„ í˜•ì„±(í‘œí˜„ë ¥)ì„ ìƒê²Œ ë©ë‹ˆë‹¤.**
$\gamma$ì™€ $\beta$ë¥¼ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë‘ì–´, "í•„ìš”í•˜ë‹¤ë©´ ì›ë˜ ë¶„í¬ë¡œ ë˜ëŒë¦´ ìˆ˜ ìˆëŠ” ììœ "ë¥¼ ëª¨ë¸ì—ê²Œ ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.
\end{warningbox}

---

\section{Deep Dive: Train vs Test Mode}

ë°°ì¹˜ ì •ê·œí™” êµ¬í˜„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{Training Mode:} í˜„ì¬ ë“¤ì–´ì˜¨ **ë¯¸ë‹ˆ ë°°ì¹˜ì˜ í‰ê· /ë¶„ì‚°**ì„ ê³„ì‚°í•´ì„œ ì”ë‹ˆë‹¤. ë™ì‹œì— ì´ ê°’ë“¤ì„ `running_mean`, `running_var`ì— ëˆ„ì (ì—…ë°ì´íŠ¸)í•´ë‘¡ë‹ˆë‹¤.
    \item \textbf{Test Mode:} í…ŒìŠ¤íŠ¸ ë•ŒëŠ” ë°ì´í„°ê°€ 1ê°œë§Œ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤(ë¶„ì‚° ê³„ì‚° ë¶ˆê°€). ë”°ë¼ì„œ í•™ìŠµ ë•Œ ë¯¸ë¦¬ ì €ì¥í•´ë‘” **`running_mean`, `running_var`**ë¥¼ ê°€ì ¸ì™€ì„œ ì •ê·œí™”í•©ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Batch Norm Class}

\begin{lstlisting}[language=Python, caption=Batch Normalization Implementation]
import numpy as np

class BatchNorm:
    def __init__(self, n_features, momentum=0.9):
        self.gamma = np.ones((n_features, 1)) # ì´ˆê¸°ê°’ 1 (ë³€í™” ì—†ìŒ)
        self.beta = np.zeros((n_features, 1)) # ì´ˆê¸°ê°’ 0 (ë³€í™” ì—†ìŒ)
        
        # í…ŒìŠ¤íŠ¸ ë‹¨ê³„ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ (Running Stats)
        self.running_mean = np.zeros((n_features, 1))
        self.running_var = np.ones((n_features, 1))
        self.momentum = momentum
        self.epsilon = 1e-8

    def forward(self, Z, mode='train'):
        if mode == 'train':
            # 1. ë¯¸ë‹ˆ ë°°ì¹˜ í†µê³„ëŸ‰ ê³„ì‚°
            mu = np.mean(Z, axis=1, keepdims=True)
            var = np.var(Z, axis=1, keepdims=True)
            
            # 2. ì •ê·œí™”
            Z_norm = (Z - mu) / np.sqrt(var + self.epsilon)
            
            # 3. Running Stats ì—…ë°ì´íŠ¸ (ì§€ìˆ˜ ê°€ì¤‘ í‰ê· )
            # ì—­ì „íŒŒì™€ ë¬´ê´€í•˜ê²Œ ë³„ë„ë¡œ ê¸°ë¡í•´ë‘ 
            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mu
            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var
            
        elif mode == 'test':
            # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì €ì¥í•´ë‘” í†µê³„ëŸ‰ ì‚¬ìš©
            Z_norm = (Z - self.running_mean) / np.sqrt(self.running_var + self.epsilon)
            
        # 4. Scale and Shift (ê³µí†µ)
        out = self.gamma * Z_norm + self.beta
        return out
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. BNì„ ì“°ë©´ ì™œ $b$(Bias)ë¥¼ ì—†ì• ë„ ë˜ë‚˜ìš”?} \\
\textbf{A.} $Z = WX + b$ì—ì„œ í‰ê·  $\mu$ë¥¼ ë¹¼ëŠ” ê³¼ì •($Z - \mu$) ë•Œë¬¸ì— ìƒìˆ˜ $b$ëŠ” ì–´ì°¨í”¼ ìƒì‡„ë˜ì–´ ì‚¬ë¼ì§‘ë‹ˆë‹¤. ëŒ€ì‹  BNì˜ $\beta$ê°€ í¸í–¥ ì—­í• ì„ ëŒ€ì‹ í•©ë‹ˆë‹¤.

\textbf{Q. BNì€ ì–´ë””ì— ë„£ë‚˜ìš”? Activation ì „? í›„?} \\
\textbf{A.} ì›ë˜ ë…¼ë¬¸(Andrew Ng ìŠ¤íƒ€ì¼)ì€ **Activation ì „**($Z \to BN \to A$)ì„ ê¶Œì¥í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìµœê·¼ì—ëŠ” í›„($Z \to A \to BN$)ì— ë„£ëŠ” ê²½ìš°ë„ ë§ìŠµë‹ˆë‹¤. ë‘˜ ë‹¤ ì˜ ë™ì‘í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê·¹í•œìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ëŠ” ëª¨ë“  ë„êµ¬(ì´ˆê¸°í™”, ì •ê·œí™”, ìµœì í™”, ë°°ì¹˜ ì •ê·œí™”)ë¥¼ ì†ì— ë„£ì—ˆìŠµë‹ˆë‹¤.

ì§€ê¸ˆê¹Œì§€ëŠ” 'ê³ ì–‘ì´ vs ê°œ'ì²˜ëŸ¼ ë‹µì´ ë‘ ê°œì¸ ì´ì§„ ë¶„ë¥˜ë§Œ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„¸ìƒì—ëŠ” ë‹µì´ ì—¬ëŸ¬ ê°œì¸ ë¬¸ì œê°€ ë” ë§ìŠµë‹ˆë‹¤. (ìˆ«ì 0~9, ì˜· ì¢…ë¥˜ ë“±)
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ í´ë˜ìŠ¤ë¥¼ ë™ì‹œì— ë¶„ë¥˜í•˜ëŠ” **[Softmax Regression]**ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Batch Norm:} ê° ì¸µì˜ ì…ë ¥ì„ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì„ ì•ˆì •í™”í•˜ê³  ê°€ì†í•œë‹¤.
    \item \textbf{Gamma/Beta:} ì •ê·œí™”ë¡œ ìƒì–´ë²„ë¦° í‘œí˜„ë ¥ì„ ë³µêµ¬í•˜ê¸° ìœ„í•œ í•™ìŠµ íŒŒë¼ë¯¸í„°.
    \item \textbf{Train/Test:} í•™ìŠµ ì‹œì—” ë°°ì¹˜ í†µê³„ëŸ‰, í…ŒìŠ¤íŠ¸ ì‹œì—” ì´ë™ í‰ê· (Running Avg)ì„ ì“´ë‹¤.
    \item \textbf{Effect:} ì´ˆê¸°í™”ì— ëœ ë¯¼ê°í•´ì§€ê³ , ë†’ì€ í•™ìŠµë¥ ì„ ì“¸ ìˆ˜ ìˆë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{strategybox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§­ #1 (ì „ëµ ê°€ì´ë“œ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ Orthogonalization Strategy}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-5.] Deep Learning Fundamentals \textit{- Completed}
    \item[\textbf{Chapter 6.}] \textbf{Structuring ML Projects (Current Unit)}
    \begin{itemize}
        \item \textbf{6.1 Orthogonalization Strategy}
        \begin{itemize}
            \item Concept: One Knob for One Function
            \item The 4-Step Chain of Assumptions
            \item Mapping Tools to Problems
            \item Why Early Stopping is problematic?
        \end{itemize}
        \item 6.2 Evaluation Metric (Single Number) \textit{- Upcoming}
        \item 6.3 Train/Dev/Test Distributions \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ê³ (Build), í•™ìŠµì‹œí‚¤ê³ (Train), ìµœì í™”(Optimize)í•˜ëŠ” ê¸°ìˆ ì ì¸ ë°©ë²•ë¡ ì„ ë°°ì› ìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì—¬ëŸ¬ë¶„ì´ í˜„ì—…ì—ì„œ ë¦¬ë”ê°€ ë˜ì–´ íŒ€ì„ ì´ëŒê²Œ ë˜ë©´ ì´ëŸ° ì§ˆë¬¸ì— ë´‰ì°©í•©ë‹ˆë‹¤.
**"êµìˆ˜ë‹˜, ì„±ëŠ¥ì´ ì•ˆ ë‚˜ì˜¤ëŠ”ë° ë°ì´í„°ë¥¼ ë” ëª¨ì„ê¹Œìš”, ì¸µì„ ë” ìŒ“ì„ê¹Œìš”, ì•„ë‹ˆë©´ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í• ê¹Œìš”?"**
ì´ë•Œ "ì¼ë‹¨ ë‹¤ í•´ë´"ë¼ê³  ë§í•˜ë©´ í”„ë¡œì íŠ¸ëŠ” ë§í•©ë‹ˆë‹¤. ìì›ì€ ìœ í•œí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ ë°°ìš¸ **ì§êµí™”(Orthogonalization)**ëŠ” ë³µì¡í•œ ë¬¸ì œ ìƒí™©ì—ì„œ ë¬´ì—‡ë¶€í„° í•´ê²°í•´ì•¼ í• ì§€ ì•Œë ¤ì£¼ëŠ” ë‚˜ì¹¨ë°˜ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì´ë„ëŠ” **ì „ëµì  ì‚¬ê³ ë°©ì‹**ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} ì‹œìŠ¤í…œì˜ ë³€ìˆ˜ë“¤ì„ ì„œë¡œ ë…ë¦½ì ìœ¼ë¡œ(Orthogonal) ì œì–´í•˜ì—¬ ì›í•˜ëŠ” íš¨ê³¼ë¥¼ ì •ë°€ íƒ€ê²©í•˜ëŠ” ì›ë¦¬ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì§„ë‹¨:} í”„ë¡œì íŠ¸ ì„±ê³µì„ ìœ„í•œ **4ë‹¨ê³„ ê°€ì •(Chain of Assumptions)**ì„ í™•ë¦½í•©ë‹ˆë‹¤.
    \item \textbf{ë„êµ¬:} ê° ë¬¸ì œ(Bias, Variance ë“±)ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ **'ì§êµí™”ëœ ë„êµ¬'**ë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.
    \item \textbf{ì£¼ì˜:} ì¡°ê¸° ì¢…ë£Œ(Early Stopping)ê°€ ì™œ ì§êµí™” ì›ì¹™ì— ìœ„ë°°ë˜ëŠ”ì§€ ë¶„ì„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ } \\ \hline
\textbf{Orthogonalization} & ë³€ìˆ˜ ê°„ ë…ë¦½ì„± í™•ë³´ & í•¸ë“¤ì€ ë°©í–¥ë§Œ, í˜ë‹¬ì€ ì†ë„ë§Œ ì¡°ì ˆí•¨. \\ \hline
\textbf{Chain of Assumptions} & ìˆœì°¨ì  í•´ê²° ë‹¨ê³„ & 1ë‹¨ê³„(Train) $\to$ 2ë‹¨ê³„(Dev) $\to$ 3ë‹¨ê³„(Test). \\ \hline
\textbf{Orthogonal Tool} & í•œ ê°€ì§€ ë¬¸ì œë§Œ í•´ê²°í•˜ëŠ” ë„êµ¬ & Bigger Network (Biasë§Œ ì¡ìŒ). \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í•œ ë²ˆì— í•˜ë‚˜ì”©}

\subsection{1. What is Orthogonalization? (ì§êµí™”ë€?)}


ìˆ˜í•™ì ìœ¼ë¡œ ë‘ ë²¡í„°ê°€ ì§êµ(90ë„)í•˜ë©´, í•œ ë²¡í„°ë¥¼ ì¡°ì ˆí•´ë„ ë‹¤ë¥¸ ë²¡í„°ì—ëŠ” ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ì—”ì§€ë‹ˆì–´ë§ì—ì„œë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. **"í•˜ë‚˜ì˜ ë‹¤ì´ì–¼ì€ í•˜ë‚˜ì˜ ê¸°ëŠ¥ë§Œ ì¡°ì ˆí•´ì•¼ í•œë‹¤"**ëŠ” ì›ì¹™ì…ë‹ˆë‹¤.

\begin{analogybox}{ìë™ì°¨ ìš´ì „ ë¹„ìœ }
\begin{itemize}
    \item \textbf{Orthogonal (ì¢‹ì€ ì„¤ê³„):}
    \begin{itemize}
        \item í•¸ë“¤ $\rightarrow$ ë°©í–¥ ì¡°ì ˆ (ì†ë„ ì˜í–¥ X)
        \item í˜ë‹¬ $\rightarrow$ ì†ë„ ì¡°ì ˆ (ë°©í–¥ ì˜í–¥ X)
        \item ìš´ì „í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. ì›í•˜ëŠ” ëŒ€ë¡œ ì œì–´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    \end{itemize}
    \item \textbf{Non-Orthogonal (ë‚˜ìœ ì„¤ê³„):}
    \begin{itemize}
        \item í•¸ë“¤ì„ êº¾ì„ ë•Œë§ˆë‹¤ ë¸Œë ˆì´í¬ê°€ ê±¸ë¦¬ê³  ë¼ë””ì˜¤ ë³¼ë¥¨ì´ ì»¤ì§„ë‹¤ë©´?
        \item ìš´ì „ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. íŠœë‹ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤.
    \end{itemize}
\end{itemize}
\end{analogybox}

---

\subsection{2. The Chain of Assumptions (4ë‹¨ê³„ ì§„ë‹¨)}
ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ 4ë‹¨ê³„ë¥¼ ìˆœì„œëŒ€ë¡œ í†µê³¼í•´ì•¼ í•©ë‹ˆë‹¤. ì• ë‹¨ê³„ê°€ í•´ê²°ë˜ì§€ ì•Šìœ¼ë©´ ë’· ë‹¨ê³„ëŠ” ì˜ë¯¸ê°€ ì—†ìŠµë‹ˆë‹¤.

\begin{enumerate}
    \item \textbf{Fit Training Set:} í›ˆë ¨ ë°ì´í„°ì—ì„œ ì¸ê°„ ìˆ˜ì¤€ ì„±ëŠ¥ì„ ë‚¸ë‹¤. (Bias ë¬¸ì œ)
    \item \textbf{Fit Dev Set:} í›ˆë ¨ëœ ëª¨ë¸ì´ ê²€ì¦ ë°ì´í„°ì—ì„œë„ ì˜í•œë‹¤. (Variance ë¬¸ì œ)
    \item \textbf{Fit Test Set:} ê²€ì¦ ë°ì´í„° ì„±ëŠ¥ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ë¹„ìŠ·í•˜ë‹¤. (Data Mismatch)
    \item \textbf{Perform in Real World:} ì‹¤ì œ ì‚¬ìš©ìê°€ ë§Œì¡±í•œë‹¤. (Cost Function ì˜¤ë¥˜)
\end{enumerate}

---

\section{Deep Dive: ë„êµ¬ ë§¤í•‘ (Tool Mapping)}

ê° ë‹¨ê³„ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë•Œ, ë‹¤ë¥¸ ë‹¨ê³„ì— ë¶€ì‘ìš©ì„ ì£¼ì§€ ì•Šê³  í•´ë‹¹ ë¬¸ì œë§Œ í•´ê²°í•˜ëŠ” **ì§êµí™”ëœ ë„êµ¬**ë¥¼ ì¨ì•¼ í•©ë‹ˆë‹¤.

\begin{strategybox}{Dr. Ng's Prescription Table}
\begin{center}
\begin{tabular}{l|l|l}
\hline
\textbf{ë¬¸ì œ (Problem)} & \textbf{ì§êµí™”ëœ ë„êµ¬ (Good)} & \textbf{ë¹„ì¶”ì²œ ë„êµ¬ (Bad)} \\ \hline
\textbf{1. High Bias} & \textbf{Bigger Network} & Early Stopping \\
(Train ì„±ëŠ¥ ë‚®ìŒ) & Adam Optimizer & (Bias/Var ë‘˜ ë‹¤ ê±´ë“œë¦¼) \\ \hline
\textbf{2. High Variance} & \textbf{More Data} & Early Stopping \\
(Dev ì„±ëŠ¥ ë‚®ìŒ) & Regularization (L2, Dropout) & \\ \hline
\textbf{3. Test Mismatch} & Bigger Dev Set & - \\ \hline
\textbf{4. Real World Fail} & Change Cost Function & - \\ \hline
\end{tabular}
\end{center}
\end{strategybox}

\begin{warningbox}{Early Stoppingì˜ ë”œë ˆë§ˆ}
Early Stoppingì€ í•™ìŠµì„ ì¤‘ê°„ì— ë©ˆì¶¥ë‹ˆë‹¤.
\begin{itemize}
    \item ë¹„ìš©í•¨ìˆ˜ $J$ ìµœì†Œí™” ì¤‘ë‹¨ $\rightarrow$ **Bias ì•…í™”**
    \item ê°€ì¤‘ì¹˜ $W$ ì¦ê°€ ì–µì œ $\rightarrow$ **Variance ê°œì„ **
\end{itemize}
ë‘ ê°€ì§€ íš¨ê³¼ê°€ ì„ì—¬ ìˆì–´(Coupled), ë¬¸ì œê°€ ìƒê²¼ì„ ë•Œ ì›ì¸ì„ íŒŒì•…í•˜ê¸° ì–´ë µê²Œ ë§Œë“­ë‹ˆë‹¤. ì§êµí™” ê´€ì ì—ì„œëŠ” **"BiasëŠ” ë„¤íŠ¸ì›Œí¬ í¬ê¸°ë¡œ ì¡ê³ , VarianceëŠ” ì •ê·œí™”ë¡œ ì¡ëŠ” ê²ƒ"**ì´ ë” ëª…í™•í•©ë‹ˆë‹¤.
\end{warningbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Automated Strategist}

ì§êµí™”ëŠ” ì½”ë“œê°€ ì•„ë‹ˆë¼ **íŒë‹¨ ë¡œì§**ì…ë‹ˆë‹¤. ì´ë¥¼ ìë™í™”ëœ ì§„ë‹¨ í´ë˜ìŠ¤ë¡œ êµ¬í˜„í•´ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=ML Project Diagnosis Logic]
class MLStrategist:
    def __init__(self, human_err, train_err, dev_err, test_err):
        self.human = human_err
        self.train = train_err
        self.dev = dev_err
        self.test = test_err
        self.threshold = 0.02 # 2% ì´ìƒ ì°¨ì´ë©´ ë¬¸ì œ

    def diagnose(self):
        print("--- Diagnosis Report ---")
        
        # 1. Bias Check
        avoidable_bias = self.train - self.human
        if avoidable_bias > self.threshold:
            print("[Problem] High Bias (Underfitting)")
            print("[Action] Increase Network Size, Train Longer.")
            return # ì• ë‹¨ê³„ í•´ê²° ì „ì—” ë’¤ë¥¼ ë³´ì§€ ì•ŠìŒ (Orthogonal)

        # 2. Variance Check
        variance = self.dev - self.train
        if variance > self.threshold:
            print("[Problem] High Variance (Overfitting)")
            print("[Action] Get More Data, Add Regularization.")
            return

        # 3. Mismatch Check
        mismatch = self.test - self.dev
        if mismatch > self.threshold:
            print("[Problem] Overfitting to Dev Set")
            print("[Action] Collect More Dev/Test Data.")
            return

        print("[Result] Good Job! Ready for Deployment.")

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    # ì‹œë‚˜ë¦¬ì˜¤: í›ˆë ¨ë„ ì•ˆ ëœ ìƒíƒœ
    strategist = MLStrategist(0.01, 0.15, 0.16, 0.17)
    strategist.diagnose()
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. Cost Functionì€ ì–¸ì œ ë°”ê¾¸ë‚˜ìš”?} \\
\textbf{A.} ëª¨ë¸ì˜ ìˆ˜ì¹˜ì  ì„±ëŠ¥(Accuracy)ì€ ì¢‹ì€ë°, ì‹¤ì œ ì•± ì‚¬ìš©ìë“¤ì˜ ë¶ˆë§Œ(Real World)ì´ ë§ì„ ë•Œì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê³ ì–‘ì´ ë¶„ë¥˜ê¸°ê°€ ì•¼í•œ ì‚¬ì§„ì„ ê³ ì–‘ì´ë¡œ ë¶„ë¥˜í–ˆë‹¤ë©´ ì •í™•ë„ê°€ ë†’ì•„ë„ ì¹˜ëª…ì ì…ë‹ˆë‹¤. ì´ë•ŒëŠ” Cost Functionì— 'ì•¼í•œ ì‚¬ì§„ íŒ¨ë„í‹°'ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤. (ê³¼ë… ìì²´ë¥¼ ìˆ˜ì •)

\textbf{Q. Biasë¥¼ ì¡ìœ¼ë ¤ê³  Regularizationì„ ì“°ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} ë¹„ì¶”ì²œí•©ë‹ˆë‹¤. Regularizationì€ ëª¨ë¸ì„ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ì–´ Varianceë¥¼ ì¤„ì´ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ë¶€ì‘ìš©ìœ¼ë¡œ Biasê°€ ì•½ê°„ ë†’ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Biasë¥¼ ì¡ì„ ë•ŒëŠ” ê·¸ëƒ¥ **ë” í° ë„¤íŠ¸ì›Œí¬(Bigger Network)**ë¥¼ ì“°ëŠ” ê²ƒì´ ë¶€ì‘ìš© ì—†ëŠ”(Orthogonal) í•´ê²°ì±…ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë¬¸ì œ í•´ê²°ì˜ ìˆœì„œì™€ ì „ëµì„ ì•Œì•˜ìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° ëª¨ë¸ AëŠ” ì •í™•ë„ê°€ ë†’ì€ë° ì†ë„ê°€ ëŠë¦¬ê³ , ëª¨ë¸ BëŠ” ì •í™•ë„ëŠ” ì¡°ê¸ˆ ë‚®ì€ë° ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤. ë„ëŒ€ì²´ ë¬´ì—‡ì„ ì„ íƒí•´ì•¼ í• ê¹Œìš”?

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì• ë§¤ëª¨í˜¸í•œ ìƒí™©ì„ ìˆ«ì í•˜ë‚˜ë¡œ ì •ë¦¬í•˜ì—¬ ì˜ì‚¬ê²°ì • ì†ë„ë¥¼ ë†’ì´ëŠ” **[Single Number Evaluation Metric]**ì— ëŒ€í•´ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Orthogonalization:} ë³€ìˆ˜ë“¤ì„ ë…ë¦½ì ìœ¼ë¡œ ì œì–´í•˜ì—¬ íŠœë‹ì„ ëª…í™•í•˜ê²Œ ë§Œë“œëŠ” ì „ëµ.
    \item \textbf{Sequence:} Train Fit $\to$ Dev Fit $\to$ Test Fit ìˆœì„œë¡œ í•´ê²°í•œë‹¤.
    \item \textbf{Bias Tool:} Bigger Network, Adam Optimizer.
    \item \textbf{Variance Tool:} More Data, Regularization.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ Single Number Evaluation Metric (F1 Score)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-5.] Deep Learning Fundamentals \textit{- Completed}
    \item[\textbf{Chapter 6.}] \textbf{Structuring ML Projects (Current Unit)}
    \begin{itemize}
        \item 6.1 Orthogonalization Strategy \textit{- Completed}
        \item \textbf{6.2 Single Number Evaluation Metric}
        \begin{itemize}
            \item Confusion Matrix (TP, TN, FP, FN)
            \item Precision vs Recall Trade-off
            \item Why Harmonic Mean? (F1 Score)
            \item Implementation
        \end{itemize}
        \item 6.3 Satisficing and Optimizing Metrics \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” í”„ë¡œì íŠ¸ì˜ ë°©í–¥ì„ ì¡ëŠ” 'ì§êµí™”' ì „ëµì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë°©í–¥ì„ ì¡ì•˜ë‹¤ê³  ëì´ ì•„ë‹™ë‹ˆë‹¤.
íŒ€ì›ì´ ë‘ ê°œì˜ ëª¨ë¸ì„ ë“¤ê³  ì™”ìŠµë‹ˆë‹¤.
**"AëŠ” ì •ë°€ë„ê°€ ë†’ì€ë° ì¬í˜„ìœ¨ì´ ë‚®ê³ , BëŠ” ì •ë°€ë„ëŠ” ë‚®ì€ë° ì¬í˜„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤. ë­˜ ì“¸ê¹Œìš”?"**
ì—¬ê¸°ì„œ ë¨¸ë­‡ê±°ë¦¬ë©´ í”„ë¡œì íŠ¸ê°€ ë©ˆì¶¥ë‹ˆë‹¤. ì•¤ë“œë¥˜ ì‘ êµìˆ˜ëŠ” **"í‰ê°€ ì§€í‘œëŠ” í•˜ë‚˜ì—¬ì•¼ í•œë‹¤(Single Number Metric)"**ê³  ê°•ì¡°í•©ë‹ˆë‹¤. ê·¸ë˜ì•¼ ìˆ˜ë§ì€ ì‹¤í—˜ ê²°ê³¼ë¥¼ í•œ ì¤„ë¡œ ì„¸ìš°ê³ , 1ë“±ì„ ë°”ë¡œ ë½‘ì„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ ëª¨ë¸ ì„±ëŠ¥ì„ ì •í™•íˆ í‰ê°€í•˜ëŠ” **F1 Score**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì •ì˜:} ì •ë°€ë„(Precision)ì™€ ì¬í˜„ìœ¨(Recall)ì˜ ê°œë…ì„ ì˜¤ì°¨ í–‰ë ¬ì„ í†µí•´ ìµí™ë‹ˆë‹¤.
    \item \textbf{ì´ìœ :} ì™œ ë‹¨ìˆœ í‰ê· ì´ ì•„ë‹Œ **ì¡°í™” í‰ê· (Harmonic Mean)**ì„ ì¨ì•¼ í•˜ëŠ”ì§€ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{ê´€ê³„:} ì„ê³„ê°’ ë³€í™”ì— ë”°ë¼ ë‘ ì§€í‘œê°€ ë°˜ëŒ€ë¡œ ì›€ì§ì´ëŠ” **Trade-off** ê´€ê³„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} Pythonìœ¼ë¡œ ì§ì ‘ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology: Confusion Matrix}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{êµ¬ë¶„} & \textbf{ì˜ˆì¸¡: Positive (1)} & \textbf{ì˜ˆì¸¡: Negative (0)} \\ \hline
\textbf{ì‹¤ì œ: Positive (1)} & \textbf{TP} (ì •ë‹µ) & \textbf{FN} (ë†“ì¹¨/ë¯¸ê²€ì¶œ) \\ \hline
\textbf{ì‹¤ì œ: Negative (0)} & \textbf{FP} (ì˜¤í•´/ê±°ì§“ ì•ŒëŒ) & \textbf{TN} (ì •ë‹µ) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë‘ ë§ˆë¦¬ í† ë¼ ì¡ê¸°}

\subsection{1. Precision (ì •ë°€ë„)}

\textbf{ì§ˆë¬¸:} "ëª¨ë¸ì´ ì°¾ì€ ê²ƒ ì¤‘ì— ì§„ì§œëŠ” ì–¼ë§ˆë‚˜ ë˜ëŠ”ê°€?"
$$ P = \frac{TP}{TP + FP} $$
\begin{analogybox}{ê¹ê¹í•œ ë¯¸ì‹ê°€}
"ë‚˜ëŠ” ë§›ì—†ëŠ” ê±´ ì ˆëŒ€ ì•ˆ ë¨¹ì–´." (FP ì‹«ì–´í•¨)
ë§›ìˆëŠ” ê²ƒ(TP)ì„ ì¢€ ë†“ì¹˜ë”ë¼ë„, ë‚´ ì…ì— ë“¤ì–´ì˜¤ëŠ” ê±´ ë¬´ì¡°ê±´ ë§›ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
\textbf{í™œìš©:} ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜ (ì •ìƒ ë©”ì¼ì„ ìŠ¤íŒ¸í†µì— ë„£ìœ¼ë©´ ì¹˜ëª…ì ì„).
\end{analogybox}

\subsection{2. Recall (ì¬í˜„ìœ¨)}

\textbf{ì§ˆë¬¸:} "ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ê²ƒ ì¤‘ì— ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì°¾ì•˜ëŠ”ê°€?"
$$ R = \frac{TP}{TP + FN} $$
\begin{analogybox}{ê·¸ë¬¼ë§ ì–´ì„ }
"ì“°ë ˆê¸°ê°€ ì¢€ ì„ì—¬ë„ ì¢‹ìœ¼ë‹ˆ, ë¬¼ê³ ê¸°ëŠ” ë‹¤ ì¡ì•„ë¼." (FN ì‹«ì–´í•¨)
ì¡ë™ì‚¬ë‹ˆ(FP)ê°€ ê±¸ë ¤ë„ ê´œì°®ì§€ë§Œ, ë¬¼ê³ ê¸°(TP)ë¥¼ ë†“ì¹˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.
\textbf{í™œìš©:} ì•” ì§„ë‹¨ (ì•” í™˜ìë¥¼ ì •ìƒì´ë¼ í•˜ë©´ ìƒëª…ì´ ìœ„í—˜í•¨), ë„ë‘‘ íƒì§€.
\end{analogybox}

---

\section{Deep Dive: Why Harmonic Mean? (F1 Score)}

ì™œ ë‘ ì§€í‘œë¥¼ ê·¸ëƒ¥ ë”í•´ì„œ 2ë¡œ ë‚˜ëˆ„ë©´(ì‚°ìˆ  í‰ê· ) ì•ˆ ë ê¹Œìš”?

\begin{mathbox}{ë°”ë³´ ëª¨ë¸ì˜ í•¨ì •}
ì•” í™˜ìê°€ 1\%ì¸ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.
ì–´ë–¤ ëª¨ë¸ì´ **"ë¬´ì¡°ê±´ ì•”ì´ë‹¤(1)"**ë¼ê³  ì˜ˆì¸¡í•œë‹¤ê³  í•©ì‹œë‹¤. (Recall = 1.0, Precision $\approx$ 0.01)

\textbf{1. ì‚°ìˆ  í‰ê·  (Arithmetic Mean):}
$$ \frac{P + R}{2} = \frac{0.01 + 1.0}{2} \approx \textbf{0.5} $$
$\rightarrow$ ë§ë„ ì•ˆ ë˜ëŠ” ëª¨ë¸ì—ê²Œ 50ì ì´ë‚˜ ì¤ë‹ˆë‹¤. ê³¼ëŒ€í‰ê°€ì…ë‹ˆë‹¤.

\textbf{2. ì¡°í™” í‰ê·  (Harmonic Mean, F1 Score):}
$$ F1 = \frac{2}{\frac{1}{P} + \frac{1}{R}} = 2 \frac{P \times R}{P + R} $$
$$ 2 \frac{0.01 \times 1.0}{0.01 + 1.0} \approx \textbf{0.019} $$
$\rightarrow$ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ë‚®ìœ¼ë©´ ì ìˆ˜ë¥¼ í™• ê¹ì•„ë²„ë¦½ë‹ˆë‹¤. ì´ê²ƒì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” í‰ê°€ ë°©ì‹ì…ë‹ˆë‹¤.
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Metric Calculation}

`scikit-learn`ì„ ì“°ë©´ ì‰½ì§€ë§Œ, ì›ë¦¬ ì´í•´ë¥¼ ìœ„í•´ `numpy`ë¡œ ì§ì ‘ êµ¬í˜„í•´ ë´…ì‹œë‹¤.

\begin{lstlisting}[language=Python, caption=Precision, Recall, F1 Implementation]
import numpy as np

def calculate_metrics(y_true, y_pred):
    """
    y_true: ì‹¤ì œê°’ (0 or 1)
    y_pred: ì˜ˆì¸¡ê°’ (0 or 1)
    """
    # ë¶ˆë¦¬ì–¸ ì¸ë±ì‹±ìœ¼ë¡œ TP, FP, FN ê³„ì‚° (Vectorized)
    TP = np.sum((y_true == 1) & (y_pred == 1))
    FP = np.sum((y_true == 0) & (y_pred == 1))
    FN = np.sum((y_true == 1) & (y_pred == 0))
    
    # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€ìš© ì—¡ì‹¤ë¡ 
    epsilon = 1e-7
    
    # Precision & Recall
    precision = TP / (TP + FP + epsilon)
    recall = TP / (TP + FN + epsilon)
    
    # F1 Score (Harmonic Mean)
    f1 = 2 * (precision * recall) / (precision + recall + epsilon)
    
    return precision, recall, f1

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    # ì•”í™˜ì(1) 2ëª…, ì •ìƒ(0) 8ëª…
    y_true = np.array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0])
    
    # ëª¨ë¸ ì˜ˆì¸¡: 1ëª…ì€ ë§ì·„ì§€ë§Œ, ì •ìƒì¸ 1ëª…ì„ ì˜¤ì§„í•¨
    y_pred = np.array([0, 1, 0, 0, 0, 1, 0, 0, 0, 0])
    
    p, r, f1 = calculate_metrics(y_true, y_pred)
    
    print(f"Precision: {p:.2f}") # 0.50 (2ë²ˆ ì˜ˆì¸¡í•´ì„œ 1ë²ˆ ë§ìŒ)
    print(f"Recall:    {r:.2f}") # 0.50 (ì‹¤ì œ 2ëª… ì¤‘ 1ëª… ì°¾ìŒ)
    print(f"F1 Score:  {f1:.2f}") # 0.50
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. Accuracy(ì •í™•ë„)ëŠ” ì–¸ì œ ì“°ë‚˜ìš”?} \\
\textbf{A.} ë°ì´í„° í´ë˜ìŠ¤ ë¹„ìœ¨ì´ 50:50ìœ¼ë¡œ ê· í˜• ì¡í˜€ ìˆì„ ë•Œë§Œ ì”ë‹ˆë‹¤. ë¶ˆê· í˜• ë°ì´í„°(99:1)ì—ì„œëŠ” ë¬´ì¡°ê±´ 0ìœ¼ë¡œ ì°ì–´ë„ ì •í™•ë„ê°€ 99\%ê°€ ë‚˜ì˜¤ë¯€ë¡œ ë¬´ì˜ë¯¸í•©ë‹ˆë‹¤.

\textbf{Q. Recallì´ Precisionë³´ë‹¤ í›¨ì”¬ ì¤‘ìš”í•œ ê²½ìš°ëŠ”ìš”?} \\
\textbf{A.} F1 ScoreëŠ” ë‘ ì§€í‘œë¥¼ 1:1ë¡œ ë´…ë‹ˆë‹¤. Recallì„ ë” ì¤‘ìš”í•˜ê²Œ ë³´ê³  ì‹¶ë‹¤ë©´ **F2 Score**(Recallì— ê°€ì¤‘ì¹˜)ë¥¼ ì“°ë©´ ë©ë‹ˆë‹¤. ë°˜ëŒ€ëŠ” F0.5 Scoreì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë‹¨ì¼ ìˆ«ì(F1 Score)ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° í˜„ì‹¤ì—ì„œëŠ” ì„±ëŠ¥ë¿ë§Œ ì•„ë‹ˆë¼ ì œì•½ ì¡°ê±´ë„ ìˆìŠµë‹ˆë‹¤. **"ì •í™•ë„ëŠ” ë†’ì•„ì•¼ í•˜ì§€ë§Œ, ì‹¤í–‰ ì‹œê°„ì€ 10ms ì´ë‚´ì—¬ì•¼ í•œë‹¤."**

ì´ëŸ° ë³µì¡í•œ ìš”êµ¬ì‚¬í•­ì„ ì–´ë–»ê²Œ ë‹¨ì¼ ì§€í‘œë¡œ ì •ë¦¬í• ê¹Œìš”? ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Satisficing and Optimizing Metrics]**ë¥¼ í†µí•´, 'ìµœì í™”í•´ì•¼ í•  ê²ƒ'ê³¼ 'ë§Œì¡±ì‹œì¼œì•¼ í•  ê²ƒ'ì„ êµ¬ë¶„í•˜ëŠ” ì „ëµì„ ë°°ì›ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Need:} ë¶ˆê· í˜• ë°ì´í„°ì—ì„œëŠ” ì •í™•ë„ ëŒ€ì‹  Precision/Recallì„ ë´ì•¼ í•œë‹¤.
    \item \textbf{Trade-off:} Precisionê³¼ Recallì€ ë°˜ë¹„ë¡€ ê´€ê³„ë‹¤. ë‘˜ ë‹¤ ë†’ì€ ê²Œ ìµœê³ ë‹¤.
    \item \textbf{F1 Score:} ì¡°í™” í‰ê· ì´ë‹¤. ê·¹ë‹¨ì ì¸ ê°’(í•˜ë‚˜ë§Œ ë†’ì€ ê²½ìš°)ì— í˜ë„í‹°ë¥¼ ì£¼ì–´ ê· í˜•ì„ ì¡ëŠ”ë‹¤.
    \item \textbf{Single Number:} ì§€í‘œë¥¼ í•˜ë‚˜ë¡œ í•©ì³ì•¼ ë¹ ë¥¸ ì˜ì‚¬ê²°ì •ì´ ê°€ëŠ¥í•˜ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{strategybox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§­ #1 (ì „ëµ ê°€ì´ë“œ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ Satisficing and Optimizing Metrics}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-5.] Deep Learning Fundamentals \textit{- Completed}
    \item[\textbf{Chapter 6.}] \textbf{Structuring ML Projects (Current Unit)}
    \begin{itemize}
        \item 6.1 Orthogonalization Strategy \textit{- Completed}
        \item 6.2 Single Number Evaluation Metric \textit{- Completed}
        \item \textbf{6.3 Satisficing and Optimizing Metrics}
        \begin{itemize}
            \item The Dilemma: Accuracy vs Latency
            \item Definition: Optimization vs Constraint
            \item The $N$-Metric Rule
            \item Implementation: Filtering Logic
        \end{itemize}
        \item 6.4 Human-level Performance \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì„ F1 Scoreë¡œ í•©ì¹˜ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° í˜„ì‹¤ ë¬¸ì œëŠ” ë” ë³µì¡í•©ë‹ˆë‹¤.
\textbf{ëª¨ë¸ A: ì •í™•ë„ 99\%, ì‘ë‹µì‹œê°„ 1.5ì´ˆ}
\textbf{ëª¨ë¸ B: ì •í™•ë„ 98\%, ì‘ë‹µì‹œê°„ 0.05ì´ˆ}
AëŠ” ì„±ëŠ¥ì€ ì¢‹ì§€ë§Œ 1.5ì´ˆë‚˜ ê±¸ë ¤ì„œ ì•„ë¬´ë„ ì•ˆ ì“¸ ê²ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ê³  ì •í™•ë„ì™€ ì‹œê°„ì„ í‰ê·  ë‚¼ ìˆ˜ë„ ì—†ìŠµë‹ˆë‹¤(ë‹¨ìœ„ê°€ ë‹¤ë¦„).
ì´ëŸ° ë”œë ˆë§ˆë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì•¤ë“œë¥˜ ì‘ êµìˆ˜ëŠ” **"ë§Œì¡±(Satisficing) ì§€í‘œì™€ ìµœì í™”(Optimizing) ì§€í‘œì˜ ë¶„ë¦¬"**ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì—¬ëŸ¬ ê°œì˜ ëª©í‘œê°€ ì¶©ëŒí•  ë•Œ, ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ëŠ” **ì „ëµì  í”„ë ˆì„ì›Œí¬**ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ë¶„:} í‰ê°€ ì§€í‘œë¥¼ **'ìµœëŒ€í•œ ì¢‹ê²Œ í•  ê²ƒ(Optimizing)'**ê³¼ **'ê¸°ì¤€ë§Œ ë„˜ê¸°ë©´ ë˜ëŠ” ê²ƒ(Satisficing)'**ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    \item \textbf{ê·œì¹™:} $N$ê°œì˜ ì§€í‘œê°€ ìˆë‹¤ë©´, 1ê°œë§Œ ìµœì í™”í•˜ê³  ë‚˜ë¨¸ì§€ $N-1$ê°œëŠ” ì œì•½ ì¡°ê±´ìœ¼ë¡œ ë‘¡ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ì—¬ëŸ¬ ëª¨ë¸ í›„ë³´ ì¤‘ ìµœì ì˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ ì„ ë³„í•˜ëŠ” íŒŒì´ì¬ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{êµ¬ë¶„} & \textbf{ì •ì˜} & \textbf{ì˜ˆì‹œ} \\ \hline
\textbf{Optimizing Metric} & ë‹¤ë‹¤ìµì„ . ë¬´í•œíˆ ì¢‹ì•„ì§ˆìˆ˜ë¡ ì¢‹ì€ ë‹¨ í•˜ë‚˜ì˜ ì§€í‘œ. & **ì •í™•ë„(Accuracy)**, F1 Score \\ \hline
\textbf{Satisficing Metric} & ì„ê³„ê°’(Threshold)ë§Œ ë„˜ìœ¼ë©´ í†µê³¼(Pass). & **ì‹¤í–‰ ì‹œê°„(Latency)** $\le$ 100ms \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì˜¬ë¦¼í”½ ë‹¬ë¦¬ê¸°}

\subsection{1. The Rule of $N$ Metrics}
ê³ ë ¤í•´ì•¼ í•  ì§€í‘œê°€ 3ê°œ(ì •í™•ë„, ì†ë„, ë©”ëª¨ë¦¬)ë¼ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”?
\textbf{"3ë§ˆë¦¬ í† ë¼ë¥¼ ë‹¤ ì¡ìœ¼ë ¤ë‹¤ê°€ëŠ” ë‹¤ ë†“ì¹©ë‹ˆë‹¤."}

\begin{analogybox}{ì˜¬ë¦¼í”½ ë‹¬ë¦¬ê¸°ì™€ ë„í•‘ í…ŒìŠ¤íŠ¸}
\begin{itemize}
    \item \textbf{Optimizing (ë‹¬ë¦¬ê¸° ê¸°ë¡):} 0.01ì´ˆë¼ë„ ë¹ ë¥´ë©´ ë¬´ì¡°ê±´ ì¢‹ìŠµë‹ˆë‹¤. ê¸ˆë©”ë‹¬ì˜ ê¸°ì¤€ì…ë‹ˆë‹¤.
    \item \textbf{Satisficing (ë„í•‘ í…ŒìŠ¤íŠ¸):} ì•½ë¬¼ì´ "ì•„ì£¼ ì¡°ê¸ˆ ê²€ì¶œë¨"ì´ë‚˜ "ì „í˜€ ê²€ì¶œ ì•ˆ ë¨"ì´ë‚˜ ë˜‘ê°™ì´ **í†µê³¼(Pass)**ì…ë‹ˆë‹¤. ê¸°ì¤€ì¹˜ë§Œ ì•ˆ ë„˜ìœ¼ë©´ ë©ë‹ˆë‹¤. ë” ê¹¨ë—í•˜ë‹¤ê³  ê°€ì‚°ì ì„ ì£¼ì§„ ì•ŠìŠµë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

---

\subsection{2. Mathematical Formulation (ìˆ˜í•™ì  ì •ì˜)}
ì´ ì „ëµì€ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¥¼ **'ì œì•½ ì¡°ê±´ì´ ìˆëŠ” ìµœì í™” ë¬¸ì œ'**ë¡œ ë°”ê¿‰ë‹ˆë‹¤.

$$ \text{Maximize } \textbf{Accuracy} $$
$$ \text{subject to } \textbf{Latency} \le 100ms $$
$$ \text{and } \textbf{Memory} \le 500MB $$

\begin{itemize}
    \item **Accuracy:** ëª©ì  í•¨ìˆ˜ (Objective Function) $\rightarrow$ Optimizing
    \item **Latency, Memory:** ì œì•½ ì¡°ê±´ (Constraints) $\rightarrow$ Satisficing
\end{itemize}

\begin{warningbox}{ê°€ì¤‘ì¹˜ í•©ì˜ í•¨ì •}
"ê·¸ëƒ¥ $\text{Score} = \text{Accuracy} - 0.5 \times \text{Latency}$ ì²˜ëŸ¼ í•©ì¹˜ë©´ ì•ˆ ë˜ë‚˜ìš”?"
\textbf{ë¹„ì¶”ì²œí•©ë‹ˆë‹¤.}
1. ë‹¨ìœ„ê°€ ë‹¤ë¦…ë‹ˆë‹¤(AccuracyëŠ” \%, LatencyëŠ” ms).
2. ì‚¬ìš©ì ê²½í—˜ì€ ë¹„ì„ í˜•ì ì…ë‹ˆë‹¤. 100msê°€ ë„˜ìœ¼ë©´ 'ë ‰ ê±¸ë¦¼'ì„ ëŠê»´ ë°”ë¡œ ì•±ì„ ë•ë‹ˆë‹¤. ì´ë¥¼ ì„ í˜• ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. Satisficing(Cut-off)ì´ í›¨ì”¬ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤.
\end{warningbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Model Selector}

ì—¬ëŸ¬ ëª¨ë¸ì˜ ì„±ëŠ¥í‘œë¥¼ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ ìµœì  ëª¨ë¸ì„ ë½‘ëŠ” ë¡œì§ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Auto Model Selector Logic]
class ModelSelector:
    def __init__(self, optimize_key, constraints):
        """
        optimize_key: ìµœì í™”í•  ì§€í‘œ (ì˜ˆ: 'accuracy')
        constraints: {ì§€í‘œëª…: (ì„ê³„ê°’, ì—°ì‚°ì)} 
                     ì˜ˆ: {'latency': (100, 'lt')} -> less than 100
        """
        self.opt_key = optimize_key
        self.constraints = constraints

    def select(self, models):
        # 1. Satisficing ë‹¨ê³„ (Filtering)
        valid_models = []
        for m in models:
            is_valid = True
            for key, (thresh, op) in self.constraints.items():
                val = m[key]
                if op == 'lt' and val > thresh: is_valid = False
                if op == 'gt' and val < thresh: is_valid = False
            
            if is_valid:
                valid_models.append(m)
        
        if not valid_models:
            print("No models satisfied constraints!")
            return None

        # 2. Optimizing ë‹¨ê³„ (Sorting)
        # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        best_model = sorted(valid_models, key=lambda x: x[self.opt_key], reverse=True)[0]
        return best_model

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    candidates = [
        {'id': 'A', 'acc': 0.99, 'lat': 1500}, # ì„±ëŠ¥ êµ¿, ë„ˆë¬´ ëŠë¦¼
        {'id': 'B', 'acc': 0.98, 'lat': 90},   # ì„±ëŠ¥ ì ë‹¹, ë¹ ë¦„ (Pass)
        {'id': 'C', 'acc': 0.90, 'lat': 50},   # ë„ˆë¬´ ë¹ ë¦„, ì„±ëŠ¥ ë³„ë¡œ (Pass)
        {'id': 'D', 'acc': 0.985, 'lat': 110}, # ì•„ê¹ê²Œ ëŠë¦¼ (Fail)
    ]
    
    # ì „ëµ: Latency < 100ms ì¸ ê²ƒ ì¤‘ì—ì„œ Accuracy ìµœëŒ€í™”
    constraints = {'lat': (100, 'lt')}
    selector = ModelSelector('acc', constraints)
    
    best = selector.select(candidates)
    print(f"Best Model: {best['id']} (Acc: {best['acc']}, Lat: {best['lat']})")
    # A, D íƒˆë½. B(0.98) vs C(0.90) -> B ìŠ¹ë¦¬.
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ë§Œì¡± ì§€í‘œ(Satisficing)ê°€ ë„ˆë¬´ ë¹¡ë¹¡í•˜ë©´ ì–´ë–¡í•˜ë‚˜ìš”?} \\
\textbf{A.} ë§Œì•½ `Latency < 10ms`ë¡œ ê±¸ì—ˆëŠ”ë° í†µê³¼í•˜ëŠ” ëª¨ë¸ì´ í•˜ë‚˜ë„ ì—†ë‹¤ë©´, í•˜ë“œì›¨ì–´ë¥¼ ë°”ê¾¸ê±°ë‚˜ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­(ì„ê³„ê°’)ì„ ì™„í™”í•´ì•¼ í•©ë‹ˆë‹¤.

\textbf{Q. ìµœì í™” ì§€í‘œë¥¼ 2ê°œë¡œ í•˜ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} ì•ˆ ë©ë‹ˆë‹¤. "ì •í™•ë„ë„ ë†’ê³  ì†ë„ë„ ë¹ ë¥¸ ê²ƒ"ì„ ì°¾ìœ¼ë ¤ í•˜ë©´, A(ì •í™•ë„ì§±)ì™€ B(ì†ë„ì§±) ì‚¬ì´ì—ì„œ ê²°ì •ì„ ëª» ë‚´ë¦½ë‹ˆë‹¤. ê²°êµ­ ë‘˜ì„ í•©ì¹œ ë‹¨ì¼ ì§€í‘œë¥¼ ë§Œë“¤ê±°ë‚˜, í•˜ë‚˜ë¥¼ ì œì•½ ì¡°ê±´ìœ¼ë¡œ ëŒë ¤ì•¼ í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ í‰ê°€ ê¸°ì¤€(Metric)ê¹Œì§€ ì™„ë²½í•˜ê²Œ ì„¸íŒ…í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ì„ ì—´ì‹¬íˆ ê°œì„ í•˜ê³  ìˆëŠ”ë°, ë¬¸ë“ ì˜ë¬¸ì´ ë“­ë‹ˆë‹¤.
**"ë„ëŒ€ì²´ ì´ ëª¨ë¸ì€ ì–´ë””ê¹Œì§€ ì¢‹ì•„ì§ˆ ìˆ˜ ìˆëŠ” ê±¸ê¹Œ? 100\%ê°€ ê°€ëŠ¥í•œê°€?"**

ì´ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ì„œëŠ” ë¹„êµ ëŒ€ìƒ, ì¦‰ **ê¸°ì¤€ì (Baseline)**ì´ í•„ìš”í•©ë‹ˆë‹¤. 
ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Human-level Performance]**ë¥¼ í†µí•´, ë‚˜ì˜ ëª¨ë¸ì´ í˜„ì¬ ì–´ëŠ ìˆ˜ì¤€ì¸ì§€, ê·¸ë¦¬ê³  ì–¼ë§ˆë‚˜ ë” ë°œì „í•  ì—¬ì§€ê°€ ìˆëŠ”ì§€(Bayes Error) ê°€ëŠ í•˜ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Optimizing:} ìµœëŒ€í•œ ì¢‹ê²Œ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ë‹¨ í•˜ë‚˜ì˜ ì§€í‘œ (ì˜ˆ: Accuracy).
    \item \textbf{Satisficing:} ê¸°ì¤€ì„ (Threshold)ë§Œ ë„˜ìœ¼ë©´ ë˜ëŠ” ì§€í‘œë“¤ (ì˜ˆ: Latency, Cost).
    \item \textbf{Process:} Satisficing ì§€í‘œë¡œ í•„í„°ë§(Cut)í•˜ê³ , Optimizing ì§€í‘œë¡œ ì¤„ ì„¸ìš´ë‹¤(Rank).
    \item \textbf{Rule:} $N$ê°œì˜ ì§€í‘œê°€ ìˆë‹¤ë©´ 1ê°œëŠ” Optimizing, $N-1$ê°œëŠ” Satisficingì´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{strategybox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§­ #1 (ì „ëµ ê°€ì´ë“œ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ Error Analysis}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-6.] ML Strategy Basics \textit{- Completed}
    \item[\textbf{Chapter 7.}] \textbf{Error Analysis (Current Unit)}
    \begin{itemize}
        \item \textbf{7.1 Manual Error Analysis}
        \begin{itemize}
            \item Philosophy: Don't Guess, Look
            \item Ceiling Analysis (Prioritization)
            \item Incorrectly Labeled Data Strategy
            \item Implementation: Analysis Tool
        \end{itemize}
        \item 7.2 Training vs Dev/Test Distribution Mismatch \textit{- Upcoming}
        \item 7.3 Transfer Learning & Multi-task Learning \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ìµœì ì˜ ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„ íƒëœ ëª¨ë¸ë„ ëª©í‘œ ì„±ëŠ¥(ì˜ˆ: 99\%)ì—ëŠ” ë„ë‹¬í•˜ì§€ ëª»í–ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ë•Œ ë§ì€ ì—”ì§€ë‹ˆì–´ë“¤ì´ **"ì§ê°"**ì— ì˜ì¡´í•©ë‹ˆë‹¤. "ê°œê°€ ê³ ì–‘ì´ì²˜ëŸ¼ ë³´ì´ë„¤? ê°œ ë°ì´í„°ë¥¼ ë” ëª¨ìœ¼ì", "íë ¤ì„œ ê·¸ëŸ°ê°€? í¬í† ìƒµ ì „ì²˜ë¦¬ë¥¼ í•˜ì."
í•˜ì§€ë§Œ ì´ê²ƒì´ ìˆ˜ê°œì›”ì„ ë‚­ë¹„í•˜ëŠ” ìµœì•…ì˜ ê²°ì •ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì§ê°ì´ ì•„ë‹ˆë¼ **'ë°ì´í„°'**ê°€ ë§í•˜ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ ë°°ìš¸ **ì—ëŸ¬ ë¶„ì„**ì€ ê°€ì¥ íš¨ìœ¨ì ì¸ ì„±ëŠ¥ í–¥ìƒ ê²½ë¡œë¥¼ ì•Œë ¤ì£¼ëŠ” ë‚˜ì¹¨ë°˜ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í‹€ë¦° ë°ì´í„°ë¥¼ ì§ì ‘ ë¶„ì„í•˜ì—¬ í”„ë¡œì íŠ¸ì˜ ìš°ì„ ìˆœìœ„ë¥¼ ì •í•˜ëŠ” ì „ëµì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì² í•™:} "ì¶”ì¸¡í•˜ì§€ ë§ê³  í™•ì¸í•˜ë¼(Don't guess, look)." ì˜¤ë¶„ë¥˜ëœ ë°ì´í„°ë¥¼ ì§ì ‘ ëˆˆìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
    \item \textbf{ì²œì¥ ë¶„ì„:} íŠ¹ì • ë¬¸ì œë¥¼ í•´ê²°í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ì˜¤ë¥¼ì§€ ìƒí•œì„ (Ceiling)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    \item \textbf{ë¼ë²¨ ì˜¤ë¥˜:} ì •ë‹µ ë¼ë²¨ ìì²´ê°€ í‹€ë¦° ê²½ìš°(Incorrect Label), ì´ë¥¼ ìˆ˜ì •í•´ì•¼ í• ì§€ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ì—ëŸ¬ ë¦¬í¬íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ìƒì„±í•˜ëŠ” Python í´ë˜ìŠ¤ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{í™œìš©} \\ \hline
\textbf{Ceiling Analysis} & ì„±ëŠ¥ í–¥ìƒì˜ ìµœëŒ€ì¹˜ ë¶„ì„ & "ì´ê±° ê³ ì¹˜ë©´ ëª‡ ì  ì˜¤ë¥´ì§€?" ê³„ì‚° \\ \hline
\textbf{Misclassified} & ì˜¤ë¶„ë¥˜ëœ ë°ì´í„° & ëª¨ë¸ì´ í‹€ë¦° ê²ƒë§Œ ëª¨ì•„ë†“ì€ ì§‘í•© \\ \hline
\textbf{Incorrect Label} & ì˜ëª»ëœ ì •ë‹µì§€ & ì‚¬ëŒì´ ì‹¤ìˆ˜ë¡œ ë¼ë²¨ë§ì„ ì˜ëª»í•œ ê²½ìš° \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë°ì´í„°ê°€ ë§í•˜ê²Œ í•˜ë¼}

\subsection{1. The Philosophy: Don't Guess, Look}
ëª¨ë¸ ì •í™•ë„ê°€ 90\%ì…ë‹ˆë‹¤. (ì—ëŸ¬ìœ¨ 10\%).
íŒ€ì›ì´ ì œì•ˆí•©ë‹ˆë‹¤. **"íë¦¿í•œ(Blurry) ì‚¬ì§„ ë•Œë¬¸ì— í‹€ë¦¬ëŠ” ê²ƒ ê°™ì•„ìš”. íë¦¼ ì œê±° ëª¨ë¸ì„ ë§Œë“­ì‹œë‹¤! (ì˜ˆìƒ ì†Œìš”: 3ê°œì›”)"**
ì´ ì œì•ˆì„ ìˆ˜ë½í•´ì•¼ í• ê¹Œìš”? **ì²œì¥ ë¶„ì„(Ceiling Analysis)**ì„ í•´ë³´ê¸° ì „ì—” ëª¨ë¦…ë‹ˆë‹¤.

\subsection{2. Ceiling Analysis (ì²œì¥ ë¶„ì„)}
Dev Setì—ì„œ ëª¨ë¸ì´ í‹€ë¦° ìƒ˜í”Œ 100ê°œë¥¼ ë¬´ì‘ìœ„ë¡œ ë½‘ì•„ ì—‘ì…€ì— ì •ë¦¬í•©ë‹ˆë‹¤.



\begin{strategybox}{ë¶„ì„ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜}
\begin{center}
\begin{tabular}{l|c|c}
\hline
\textbf{ì—ëŸ¬ ì›ì¸ (Category)} & \textbf{ë¹„ìœ¨ (Count)} & \textbf{Ceiling (ì˜ˆìƒ í–¥ìƒ)} \\ \hline
íë¦¿í•¨ (Blurry) & 5\% & $10\% \times 0.05 = \mathbf{0.5\%}$ \\ \hline
ë°°ê²½ ë…¸ì´ì¦ˆ (Noise) & 60\% & $10\% \times 0.60 = \mathbf{6.0\%}$ \\ \hline
ê³ ì–‘ì´ ë‹®ì€ ê°œ & 35\% & $10\% \times 0.35 = \mathbf{3.5\%}$ \\ \hline
\end{tabular}
\end{center}
\textbf{ê²°ë¡ :} íë¦¼ ì œê±° ëª¨ë¸ì„ ì™„ë²½í•˜ê²Œ ë§Œë“¤ì–´ë„ ì„±ëŠ¥ì€ ê³ ì‘ **0.5\%** ì˜¤ë¦…ë‹ˆë‹¤. 3ê°œì›”ì„ ë‚­ë¹„í•  ë»”í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” **'ë°°ê²½ ë…¸ì´ì¦ˆ'** ë¬¸ì œ(6.0\% í–¥ìƒ ê°€ëŠ¥)ì— ì§‘ì¤‘í•´ì•¼ í•©ë‹ˆë‹¤.
\end{strategybox}

---

\subsection{3. Incorrectly Labeled Data (ë¼ë²¨ ì˜¤ë¥˜)}
ë°ì´í„°ë¥¼ ë³´ë‹¤ ë³´ë‹ˆ, ëª¨ë¸ì€ ë§ì•˜ëŠ”ë° ì‚¬ëŒì´ ì •ë‹µì„ ì˜ëª» ë‹¬ì•„ë†“ì€ ê²½ìš°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê³ ì³ì•¼ í• ê¹Œìš”?

\begin{itemize}
    \item \textbf{Training Set:} ë”¥ëŸ¬ë‹ì€ **ë¬´ì‘ìœ„ ì˜¤ë¥˜(Random Error)**ì— ê°•í•©ë‹ˆë‹¤. ë°ì´í„°ê°€ ë§ë‹¤ë©´ ë¬´ì‹œí•´ë„ ë©ë‹ˆë‹¤. (ë‹¨, ì²´ê³„ì  ì˜¤ë¥˜ëŠ” ìˆ˜ì • í•„ìˆ˜)
    \item \textbf{Dev/Test Set:} ì—ëŸ¬ ë¶„ì„ í‘œì— 'Incorrect Label' ì—´ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    \begin{itemize}
        \item ë§Œì•½ ì´ ë¹„ìœ¨ì´ ì „ì²´ ì—ëŸ¬ì˜ ìƒë‹¹ìˆ˜ë¼ë©´(ì˜ˆ: ì—ëŸ¬ì˜ 30\%), **ë°˜ë“œì‹œ ê³ ì³ì•¼ í•©ë‹ˆë‹¤.**
        \item \textbf{ì£¼ì˜:} Devì™€ TestëŠ” í•­ìƒ **ë™ì‹œì—** ê³ ì³ì•¼ ë¶„í¬ê°€ ìœ ì§€ë©ë‹ˆë‹¤.
    \end{itemize}
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Error Report Generator}

ì—ëŸ¬ ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” ìë™í™” ë„êµ¬ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Error Analysis Tool]
import pandas as pd
import numpy as np

class ErrorAnalyzer:
    def __init__(self, y_true, y_pred):
        self.y_true = np.array(y_true)
        self.y_pred = np.array(y_pred)
        # í‹€ë¦° ì¸ë±ìŠ¤ ì¶”ì¶œ
        self.error_indices = np.where(self.y_true != self.y_pred)[0]
        self.total_errors = len(self.error_indices)
        self.total_samples = len(y_true)

    def generate_report(self, manual_tags):
        """
        manual_tags: {index: ['Blurry', 'Noise'], ...}
        """
        counts = {}
        for tags in manual_tags.values():
            for tag in tags:
                counts[tag] = counts.get(tag, 0) + 1
        
        df = pd.DataFrame(list(counts.items()), columns=['Category', 'Count'])
        
        # Ceiling Analysis ê³„ì‚°
        # ì „ì²´ ë°ì´í„° ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ì¹˜
        df['Ceiling (%)'] = (df['Count'] / self.total_samples) * 100
        
        # ì •ë ¬
        df = df.sort_values(by='Count', ascending=False)
        return df

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    # ì „ì²´ ë°ì´í„° 1000ê°œ ì¤‘ ì—ëŸ¬ 100ê°œë¼ê³  ê°€ì •
    y_true = np.zeros(1000)
    y_pred = np.zeros(1000)
    y_pred[:100] = 1 # 100ê°œ í‹€ë¦¼
    
    analyzer = ErrorAnalyzer(y_true, y_pred)
    
    # ì‚¬ëŒì´ ì§ì ‘ ë³´ê³  íƒœê¹…í–ˆë‹¤ê³  ê°€ì • (ì—‘ì…€ ì—°ë™)
    tags = {
        0: ['Blurry'], 1: ['Noise'], 2: ['Blurry', 'Noise'],
        # ... (ìƒëµ) ...
        99: ['Noise']
    }
    # (ê°€ì •: Blurry 5ê°œ, Noise 60ê°œ)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    # df = analyzer.generate_report(tags)
    # print(df)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Dev Setë§Œ ê³ ì¹˜ë©´ ì•ˆ ë˜ë‚˜ìš”?}
\textbf{ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.}
Dev Setì˜ ë¼ë²¨ë§Œ ê³ ì¹˜ê³  Test Setì„ ê·¸ëŒ€ë¡œ ë‘ë©´, ë‘ ë°ì´í„°ì…‹ì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤(Distribution Mismatch). í‰ê°€ì˜ ì‹ ë¢°ë„ê°€ ê¹¨ì§‘ë‹ˆë‹¤. ê·€ì°®ë”ë¼ë„ Devì™€ TestëŠ” **í•œ ëª¸ì²˜ëŸ¼** ë‹¤ë¤„ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ëª‡ ê°œë‚˜ ë¶„ì„í•´ì•¼ í•˜ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µ **100ê°œ** ì •ë„ë©´ ì¶©ë¶„í•œ í†µê³„ì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜¼ìì„œ 100ê°œ ë³´ëŠ” ë° 1~2ì‹œê°„ì´ë©´ ë©ë‹ˆë‹¤. íŒ€ì›ë“¤ê³¼ 100ê°œì”© ë‚˜ëˆ ì„œ ë³´ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì—ëŸ¬ ë¶„ì„ì„ í†µí•´ 'ë¬´ì—‡'ì„ ê³ ì³ì•¼ í• ì§€ ì•Œê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° ë¶„ì„ ê²°ê³¼, **"í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë„ˆë¬´ ë‹¤ë¥´ë‹¤"**ëŠ” ê²°ë¡ ì´ ë‚˜ì˜¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œìš”? (ì˜ˆ: í›ˆë ¨ì€ ê³ í™”ì§ˆ, ê²€ì¦ì€ ì €í™”ì§ˆ)

ì´ê²ƒì€ ë‹¨ìˆœí•œ ê³¼ëŒ€ì í•©ê³¼ëŠ” ë‹¤ë¥¸ ì°¨ì›ì˜ ë¬¸ì œì…ë‹ˆë‹¤. ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[Training vs Dev/Test Distribution Mismatch]** ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ê³  í•´ê²°í•˜ëŠ” ê³ ê¸‰ ì „ëµì¸ **'Training-Dev Set'**ì˜ ê°œë…ì— ëŒ€í•´ ë°°ìš°ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Don't Guess:} ì§ê°ì´ ì•„ë‹Œ ë°ì´í„°ë¥¼ ë³´ê³  ê²°ì •í•˜ë¼.
    \item \textbf{Ceiling Analysis:} íŠ¹ì • ë¬¸ì œë¥¼ í•´ê²°í–ˆì„ ë•Œ ì–»ì„ ìˆ˜ ìˆëŠ” ìµœëŒ€ ì´ìµ(ROI)ì„ ê³„ì‚°í•˜ë¼.
    \item \textbf{Incorrect Label:} ì „ì²´ ì—ëŸ¬ ì¤‘ ë¹„ì¤‘ì´ ë†’ë‹¤ë©´ ìˆ˜ì •í•˜ë¼. ë‹¨, Dev/Testë¥¼ ë™ì‹œì— ìˆ˜ì •í•´ì•¼ í•œë‹¤.
    \item \textbf{Spreadsheet:} ì—‘ì…€ ë“±ì„ í™œìš©í•´ íŒ€ì›ê³¼ ì—ëŸ¬ ì›ì¸ì„ ê³µìœ í•˜ë¼.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{diagnosisbox}[1]{
    colback=purple!5!white,
    colframe=purple!80!black,
    fonttitle=\bfseries,
    title=ğŸ©º #1 (ë‹¥í„° ë”¥ëŸ¬ë‹ì˜ ì§„ë‹¨í‘œ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ Data Mismatch \& Train-Dev Set}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-6.] ML Strategy Basics \textit{- Completed}
    \item[\textbf{Chapter 7.}] \textbf{Error Analysis \& Data Mismatch (Current Unit)}
    \begin{itemize}
        \item 7.1 Manual Error Analysis \textit{- Completed}
        \item \textbf{7.2 Training vs Dev/Test Distribution Mismatch}
        \begin{itemize}
            \item The Web vs Mobile Image Problem
            \item New Dataset: "Train-Dev Set"
            \item Diagnostic Logic Table
            \item Artificial Data Synthesis
        \end{itemize}
        \item 7.3 Transfer Learning (Next Chapter)
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ì—ëŸ¬ ë¶„ì„ì„ í†µí•´ 'ë¬´ì—‡ì„ ê³ ì¹ ì§€' ìš°ì„ ìˆœìœ„ë¥¼ ì •í–ˆìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° ë§Œì•½ ì—¬ëŸ¬ë¶„ì´ ìˆ˜ì§‘í•œ **20ë§Œ ì¥ì˜ 'ê³ í™”ì§ˆ ì›¹ ì´ë¯¸ì§€'**ë¡œ í•™ìŠµì‹œì¼°ëŠ”ë°, ì •ì‘ ì„œë¹„ìŠ¤í•  **'ì €í™”ì§ˆ ëª¨ë°”ì¼ ì´ë¯¸ì§€'**ì—ì„œëŠ” ëª¨ë¸ì´ ì „í˜€ ë™ì‘í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ ì–´ë–¨ê¹Œìš”?
ì´ê²ƒì€ ë‹¨ìˆœí•œ ê³¼ëŒ€ì í•©(Variance) ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤. ê³µë¶€í•œ ì±…ê³¼ ì‹œí—˜ ê³¼ëª©ì´ ì•„ì˜ˆ ë‹¤ë¥¸ ìƒí™©, ì¦‰ **ë°ì´í„° ë¶ˆì¼ì¹˜(Data Mismatch)**ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì´ ê¹Œë‹¤ë¡œìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë¹„ë°€ ë¬´ê¸°ì¸ **'Train-Dev Set'**ì„ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í•™ìŠµ ë°ì´í„°ì™€ ì‹¤ì „ ë°ì´í„°ì˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì§„ë‹¨:} ê³¼ëŒ€ì í•©(Variance)ê³¼ ë°ì´í„° ë¶ˆì¼ì¹˜(Mismatch)ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ **Train-Dev Set**ì„ ë„ì…í•©ë‹ˆë‹¤.
    \item \textbf{ë…¼ë¦¬:} Train, Train-Dev, Dev ì—ëŸ¬ ê°„ì˜ ê²©ì°¨(Gap)ë¥¼ ë¶„ì„í•˜ì—¬ ëª¨ë¸ ìƒíƒœë¥¼ íŒë³„í•©ë‹ˆë‹¤.
    \item \textbf{í•´ê²°:} **ì¸ê³µ ë°ì´í„° í•©ì„±(Data Synthesis)**ì„ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ì‹¤ì „ ë¶„í¬ì— ë§ì¶”ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ë°ì´í„°ì…‹} & \textbf{êµ¬ì„± (Source)} & \textbf{ì—­í• } \\ \hline
\textbf{Train Set} & ì›¹ ì´ë¯¸ì§€ (20ë§Œ ì¥) & ëª¨ë¸ íŒŒë¼ë¯¸í„° í•™ìŠµ \\ \hline
\textbf{Train-Dev Set} & \textbf{ì›¹ ì´ë¯¸ì§€ (ì¼ë¶€ ë–¼ì–´ëƒ„)} & \textbf{Variance ì§„ë‹¨ìš© (í•™ìŠµ X)} \\ \hline
\textbf{Dev Set} & ëª¨ë°”ì¼ ì´ë¯¸ì§€ (5ì²œ ì¥) & íƒ€ê²Ÿ ì„±ëŠ¥ ê²€ì¦ ë° Mismatch ì§„ë‹¨ \\ \hline
\textbf{Test Set} & ëª¨ë°”ì¼ ì´ë¯¸ì§€ (5ì²œ ì¥) & ìµœì¢… ì„±ëŠ¥ í‰ê°€ \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìƒˆë¡œìš´ ì§„ë‹¨ ë„êµ¬}

\subsection{1. The Trap of Standard Split (í•¨ì •)}
ì›¹ ì´ë¯¸ì§€ 20ë§Œ ì¥ + ëª¨ë°”ì¼ ì´ë¯¸ì§€ 1ë§Œ ì¥ì´ ìˆìŠµë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë‚˜ìœ ë°©ë²•:} ì „ë¶€ ì„ì–´ì„œ(Shuffle) ë‚˜ëˆˆë‹¤. $\rightarrow$ Dev Setì˜ 95\%ê°€ ì›¹ ì´ë¯¸ì§€ê°€ ë¨. ì‹¤ì „(ëª¨ë°”ì¼) ì„±ëŠ¥ì„ ì¸¡ì •í•  ìˆ˜ ì—†ìŒ.
    \item \textbf{ì¢‹ì€ ë°©ë²•:} Dev/TestëŠ” **ì˜¤ì§ ëª¨ë°”ì¼ ì´ë¯¸ì§€**ë¡œë§Œ êµ¬ì„±í•œë‹¤. (Target ê³ ì •). Trainì—ëŠ” ì›¹ ì´ë¯¸ì§€ë¥¼ ëª°ì•„ì¤€ë‹¤.
\end{itemize}

\subsection{2. Introducing "Train-Dev Set"}
ìœ„ì˜ 'ì¢‹ì€ ë°©ë²•'ì„ ì“°ë©´ Trainê³¼ Devì˜ ë¶„í¬ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ë•Œ ì—ëŸ¬ê°€ ë†’ìœ¼ë©´ **"ê³¼ëŒ€ì í•© ë•Œë¬¸ì¸ê°€? ì•„ë‹ˆë©´ ë°ì´í„°ê°€ ë‹¬ë¼ì„œì¸ê°€?"**ë¥¼ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ì´ë¥¼ êµ¬ë¶„í•˜ê¸° ìœ„í•´ **Train-Dev Set**ì„ ë§Œë“­ë‹ˆë‹¤.
\begin{itemize}
    \item **ì •ì˜:** Train Setì—ì„œ ë¬´ì‘ìœ„ë¡œ ë–¼ì–´ë‚¸ ì¼ë¶€ ë°ì´í„°. í•™ìŠµì—ëŠ” ì“°ì§€ ì•ŠìŒ.
    \item **íŠ¹ì§•:** Train Setê³¼ **ë¶„í¬ê°€ ì™„ë²½íˆ ë™ì¼**í•¨.
\end{itemize}

---

\section{Deep Dive: The Logic of Diagnosis}

ì´ í‘œê°€ ì˜¤ëŠ˜ ê°•ì˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤. **ì—ëŸ¬ì˜ ì°¨ì´(Gap)**ê°€ ì–´ë””ì„œ ë²Œì–´ì§€ëŠ”ì§€ ë´ì•¼ í•©ë‹ˆë‹¤.

\begin{diagnosisbox}{Mismatch ì§„ë‹¨ ë¡œì§}
\begin{center}
\begin{tabular}{l|c|l}
\hline
\textbf{ë¹„êµ ëŒ€ìƒ} & \textbf{Gapì˜ ì˜ë¯¸} & \textbf{ì§„ë‹¨ëª… (Diagnosis)} \\ \hline
\textbf{Train} vs \textbf{Human} & Avoidable Bias & \textbf{High Bias} (ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•¨) \\ \hline
\textbf{Train-Dev} vs \textbf{Train} & Variance Gap & \textbf{High Variance} (ê³¼ëŒ€ì í•©) \\ \hline
\textbf{Dev} vs \textbf{Train-Dev} & \textbf{Mismatch Gap} & \textbf{Data Mismatch} (ë¶„í¬ ì°¨ì´) \\ \hline
\textbf{Test} vs \textbf{Dev} & Overfitting to Dev & Dev Set ê³¼ì í•© \\ \hline
\end{tabular}
\end{center}
\end{diagnosisbox}

\begin{analogybox}{ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„}
\textbf{ìƒí™©:} Train Error 1\%.
\begin{itemize}
    \item \textbf{Case A:} Train-Dev 9\%, Dev 10\%.
    \begin{itemize}
        \item Trainê³¼ Train-Dev(ê°™ì€ ë¶„í¬) ì‚¬ì´ì—ì„œ ì—ëŸ¬ê°€ í­ì¦í–ˆìŠµë‹ˆë‹¤.
        \item \textbf{ì§„ë‹¨:} **High Variance (ê³¼ëŒ€ì í•©).** ì •ê·œí™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    \end{itemize}
    \item \textbf{Case B:} Train-Dev 1.5\%, Dev 10\%.
    \begin{itemize}
        \item ê°™ì€ ë¶„í¬(Train-Dev)ì—ì„œëŠ” ì˜í•˜ëŠ”ë°, ë‹¤ë¥¸ ë¶„í¬(Dev)ì— ê°€ë‹ˆ ëª»í•©ë‹ˆë‹¤.
        \item \textbf{ì§„ë‹¨:} **Data Mismatch.** ë°ì´í„°ë¥¼ í•©ì„±í•˜ê±°ë‚˜ ë” ëª¨ì•„ì•¼ í•©ë‹ˆë‹¤.
    \end{itemize}
\end{itemize}
\end{analogybox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Automated Diagnosis}

ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„í• í•˜ê³  ë¬¸ì œë¥¼ ì§„ë‹¨í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Data Mismatch Diagnosis Tool]
import numpy as np
from sklearn.model_selection import train_test_split

class MismatchDiagnostician:
    def __init__(self, X_web, y_web, X_mobile, y_mobile):
        # 1. Target(Dev/Test)ì€ ëª¨ë°”ì¼ë¡œ ê³ ì •
        self.X_dev, self.X_test, self.y_dev, self.y_test = train_test_split(
            X_mobile, y_mobile, test_size=0.5, random_state=1
        )
        
        # 2. Train Setì€ ì›¹ ë°ì´í„° ì‚¬ìš©
        X_train_all, y_train_all = X_web, y_web
        
        # 3. [í•µì‹¬] Train Setì—ì„œ Train-Dev Setì„ ë–¼ì–´ëƒ„ (ê°™ì€ ë¶„í¬)
        self.X_train, self.X_train_dev, self.y_train, self.y_train_dev = train_test_split(
            X_train_all, y_train_all, test_size=0.02, random_state=1
        )

    def diagnose(self, train_err, train_dev_err, dev_err):
        print("--- Diagnosis Report ---")
        variance_gap = train_dev_err - train_err
        mismatch_gap = dev_err - train_dev_err
        
        print(f"Variance Gap (TrainDev - Train): {variance_gap:.2%}")
        print(f"Mismatch Gap (Dev - TrainDev):   {mismatch_gap:.2%}")
        
        if variance_gap > mismatch_gap:
            print(">> Diagnosis: High Variance (Overfitting)")
            print(">> Action: More Data, Regularization, Dropout")
        else:
            print(">> Diagnosis: Data Mismatch (Distribution Shift)")
            print(">> Action: Artificial Data Synthesis, Collect Target Data")

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    # ë”ë¯¸ ë°ì´í„° (ì›¹ 5ë§Œ, ëª¨ë°”ì¼ 2ì²œ)
    X_w, y_w = np.zeros((50000, 10)), np.zeros(50000)
    X_m, y_m = np.zeros((2000, 10)), np.zeros(2000)
    
    doctor = MismatchDiagnostician(X_w, y_w, X_m, y_m)
    
    # ì‹œë‚˜ë¦¬ì˜¤: Train 1%, Train-Dev 1.5%, Dev 10%
    doctor.diagnose(0.01, 0.015, 0.10)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. Train-Dev Setì„ Dev Setì—ì„œ ë–¼ì–´ë‚´ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} \textbf{ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.} ê·¸ëŸ¬ë©´ Train-Devì™€ Devì˜ ë¶„í¬ê°€ ê°™ì•„ì§‘ë‹ˆë‹¤. Train-DevëŠ” ë°˜ë“œì‹œ **Train Setì˜ ë¶€ë¶„ì§‘í•©**ì´ì–´ì•¼ "í•™ìŠµ ë°ì´í„° ë¶„í¬ì—ì„œì˜ ì¼ë°˜í™” ì„±ëŠ¥"ì„ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

\textbf{Q. Data Mismatch í•´ê²°ì±…ì¸ 'ë°ì´í„° í•©ì„±'ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?} \\
\textbf{A.} ê¹¨ë—í•œ ìŒì„±(Train)ì— ìë™ì°¨ ì†ŒìŒ(Noise)ì„ ì„ì–´ì„œ ì‹œë„ëŸ¬ìš´ ìŒì„±(Devì™€ ë¹„ìŠ·í•¨)ì„ ë§Œë“œëŠ” ì‹ì…ë‹ˆë‹¤. ë‹¨, ë„ˆë¬´ ì ì€ ì¢…ë¥˜ì˜ ì†ŒìŒë§Œ ë°˜ë³µí•´ì„œ ì“°ë©´ ëª¨ë¸ì´ ê·¸ ì†ŒìŒ íŒ¨í„´ì— ê³¼ì í•©ë  ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ê²ƒìœ¼ë¡œ ì•¤ë“œë¥˜ ì‘ êµìˆ˜ì˜ **'ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ êµ¬ì¡°í™” ì „ëµ'** íŒŒíŠ¸ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ì´ì œ ë‹¨ìˆœíˆ ì½”ë”©ë§Œ í•˜ëŠ” ì—”ì§€ë‹ˆì–´ê°€ ì•„ë‹ˆë¼, í”„ë¡œì íŠ¸ì˜ ë°©í–¥ì„ ì§€íœ˜í•˜ëŠ” **ì „ëµê°€(Strategist)**ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ë¶€í„°ëŠ” ë‹¤ì‹œ ëª¨ë¸ë§ì˜ ì„¸ê³„ë¡œ ëŒì•„ì˜µë‹ˆë‹¤. ì»´í“¨í„° ë¹„ì „(Computer Vision)ì˜ í˜ëª…ì„ ì¼ìœ¼í‚¨ **[Convolutional Neural Networks (CNN)]**ì˜ ê¸°ì´ˆë¶€í„° ì‹¬í™”ê¹Œì§€, ì´ë¯¸ì§€ ì²˜ë¦¬ì˜ ë§ˆë²•ì„ ìˆ˜í•™ì ìœ¼ë¡œ íŒŒí—¤ì³ ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Problem:} í•™ìŠµ ë°ì´í„°(Web)ì™€ ì‹¤ì „ ë°ì´í„°(Mobile)ì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤.
    \item \textbf{Tool:} \textbf{Train-Dev Set} (Trainê³¼ ë¶„í¬ëŠ” ê°™ìœ¼ë‚˜ í•™ìŠµì—” ì•ˆ ì”€).
    \item \textbf{Logic:} Train-Dev ì—ëŸ¬ê°€ ë‚®ê³  Dev ì—ëŸ¬ê°€ ë†’ë‹¤ë©´ \textbf{Data Mismatch}ë‹¤.
    \item \textbf{Action:} ì¸ê³µ ë°ì´í„° í•©ì„±(Synthesis) ë“±ì„ í†µí•´ Trainì„ DevìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ë¼.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{strategybox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§­ #1 (ì „ëµ ê°€ì´ë“œ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ Transfer Learning \& Multi-task Learning}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-6.] ML Strategy Basics \textit{- Completed}
    \item[\textbf{Chapter 7.}] \textbf{Advanced Learning Strategies (Current Unit)}
    \begin{itemize}
        \item 7.1 Error Analysis \textit{- Completed}
        \item 7.2 Data Mismatch \textit{- Completed}
        \item \textbf{7.3 Transfer Learning \& Multi-task Learning}
        \begin{itemize}
            \item Concept: Standing on the Shoulders of Giants
            \item Fine-tuning Strategies (Freeze vs Unfreeze)
            \item Multi-task Learning (Shared Representation)
            \item Implementation: Keras Code
        \end{itemize}
    \end{itemize}
    \item[Chapter 8.] End-to-End Deep Learning \textit{- Upcoming}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” ëª¨ë¸ì„ ë°‘ë°”ë‹¥ë¶€í„°(Scratch) í•™ìŠµì‹œí‚¤ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ ì„¸ê³„ì˜ ì—”ì§€ë‹ˆì–´ëŠ” **"ì•„ë¬´ê²ƒë„ ì—†ëŠ” ìƒíƒœì—ì„œ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."**
ì±…ì„ ì½ì„ ë•Œë§ˆë‹¤ 'ê°€ë‚˜ë‹¤ë¼'ë¶€í„° ë‹¤ì‹œ ë°°ìš°ì§€ ì•Šë“¯, AI ëª¨ë¸ë„ ë‚¨ì´ ì´ë¯¸ í•™ìŠµí•´ë‘” ì§€ì‹(Knowledge)ì„ ë¹Œë ¤ì™€ì„œ ìì‹ ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ **ì „ì´ í•™ìŠµ(Transfer Learning)**ì´ë©°, í˜„ëŒ€ ë”¥ëŸ¬ë‹ ì„±ê³µì˜ 90\%ëŠ” ì—¬ê¸°ì— ê¸°ì¸í•©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ 'ë°ì´í„° ë¶€ì¡±'ì„ í•´ê²°í•˜ê³  í•™ìŠµ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•˜ëŠ” ë‘ ê°€ì§€ íŒ¨ëŸ¬ë‹¤ì„ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Transfer Learning:} ëŒ€ê·œëª¨ ë°ì´í„°(ImageNet)ë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ê°€ì ¸ì™€ ë‚´ ë¬¸ì œ(X-ray)ì— ì ìš©í•˜ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{Fine-tuning:} ê°€ì¤‘ì¹˜ë¥¼ ê³ ì •(Freeze)í•˜ê±°ë‚˜ ë¯¸ì„¸ ì¡°ì •(Unfreeze)í•˜ëŠ” ë‹¨ê³„ë³„ ì „ëµì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{Multi-task Learning:} í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ë©° ì§€ëŠ¥ì„ ë†’ì´ëŠ” ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} Kerasë¥¼ í™œìš©í•´ Pre-trained Modelì„ ë¡œë“œí•˜ê³  ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ } \\ \hline
\textbf{Transfer Learning} & ì§€ì‹ ì „ì´ (A $\to$ B) & ì˜ì–´ ì˜í•˜ëŠ” ì‚¬ëŒì´ ë¶ˆì–´ë„ ë¹¨ë¦¬ ë°°ì›€. \\ \hline
\textbf{Pre-trained Model} & ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ (Source) & ì´ë¯¸ ë°•ì‚¬ í•™ìœ„ë¥¼ ë°›ì€ ì „ë¬¸ê°€. \\ \hline
\textbf{Fine-tuning} & ë¯¸ì„¸ ì¡°ì • & ì „ë¬¸ê°€ì—ê²Œ ìš°ë¦¬ íšŒì‚¬ì˜ ì—…ë¬´ ë§¤ë‰´ì–¼ë§Œ ê°€ë¥´ì¹¨. \\ \hline
\textbf{Multi-task Learning} & ë™ì‹œ í•™ìŠµ (A \& B) & ìˆ˜í•™ê³¼ ë¬¼ë¦¬ë¥¼ ë™ì‹œì— ë°°ìš°ë©´ ì‹œë„ˆì§€ê°€ ë‚¨. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì§€ì‹ì˜ ì¬í™œìš©}

\subsection{1. Transfer Learning (ì „ì´ í•™ìŠµ)}


ë°ì´í„°ê°€ í’ë¶€í•œ **Task A(Source)**ì—ì„œ ë°°ìš´ ì§€ì‹ì„ ë°ì´í„°ê°€ ì ì€ **Task B(Target)**ë¡œ ì˜®ê¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì•ë‹¨ (Early Layers):} ì—£ì§€, ê³¡ì„ , ì§ˆê° ë“± ë³´í¸ì ì¸ íŠ¹ì§•ì„ ë°°ì›ë‹ˆë‹¤. (ì¬í™œìš© ê°€ëŠ¥)
    \item \textbf{ë’·ë‹¨ (Later Layers):} êµ¬ì²´ì ì¸ ì‚¬ë¬¼(ê³ ì–‘ì´, ìë™ì°¨)ì„ ë°°ì›ë‹ˆë‹¤. (ìƒˆë¡œ í•™ìŠµ í•„ìš”)
\end{itemize}

\subsection{2. Multi-task Learning (ë‹¤ì¤‘ ì‘ì—… í•™ìŠµ)}


í•˜ë‚˜ì˜ ì‹ ê²½ë§ì´ ì—¬ëŸ¬ ì‘ì—…(Task A, B, C)ì„ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Shared Layers:} ëª¨ë“  ì‘ì—…ì— ê³µí†µì ìœ¼ë¡œ í•„ìš”í•œ ì €ìˆ˜ì¤€ íŠ¹ì§•(Low-level features)ì„ ê³µìœ í•©ë‹ˆë‹¤.
    \item \textbf{íš¨ê³¼:} ì„œë¡œ ë‹¤ë¥¸ ì‘ì—…ë“¤ì´ ì¼ì¢…ì˜ ë…¸ì´ì¦ˆ(Regularization) ì—­í• ì„ í•˜ì—¬ ê³¼ëŒ€ì í•©ì„ ë§‰ê³  ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì…ë‹ˆë‹¤.
    \item \textbf{ì˜ˆì‹œ:} ììœ¨ì£¼í–‰ (í‘œì§€íŒ ì¸ì‹ + ì‹ í˜¸ë“± ì¸ì‹ + ë³´í–‰ì ê°ì§€).
\end{itemize}

---

\section{Deep Dive: Fine-tuning Strategies}

"ë°ì´í„°ê°€ ì–¼ë§ˆë‚˜ ìˆëŠëƒ"ì— ë”°ë¼ ì „ëµì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

\begin{strategybox}{ë°ì´í„° ê·œëª¨ë³„ ì „ëµ}
\begin{enumerate}
    \item \textbf{Small Data (ë°ì´í„° ë§¤ìš° ì ìŒ):}
    \begin{itemize}
        \item **ì „ëµ:** Backbone ì „ì²´ ê³ ì • (**Freeze**).
        \item **í–‰ë™:** ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸°(Head)ë§Œ ë–¼ì–´ë‚´ê³  ìƒˆë¡œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    \end{itemize}
    
    \item \textbf{Medium Data (ì ë‹¹í•¨):}
    \begin{itemize}
        \item **ì „ëµ:** ì•ë‹¨ ì¼ë¶€ ê³ ì •, ë’·ë‹¨ ì¼ë¶€ í•´ì œ (**Fine-tuning**).
        \item **í–‰ë™:** ìƒìœ„ ì¸µ(Later Layers)ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë¯¸ì„¸í•˜ê²Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    \end{itemize}
    
    \item \textbf{Big Data (ë°ì´í„° ë§ìŒ):}
    \begin{itemize}
        \item **ì „ëµ:** ì „ì²´ ì¬í•™ìŠµ (**Retrain All**).
        \item **í–‰ë™:** ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°ê°’(Initialization)ìœ¼ë¡œë§Œ ì“°ê³  ì „ì²´ë¥¼ ë‹¤ í•™ìŠµí•©ë‹ˆë‹¤.
    \end{itemize}
\end{enumerate}
\end{strategybox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Transfer Learning with Keras}

MobileNetV2ë¥¼ ê°€ì ¸ì™€ì„œ ì»¤ìŠ¤í…€ ë¶„ë¥˜ê¸°ë¥¼ ë§Œë“œëŠ” ì½”ë“œì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Transfer Learning Pipeline]
import tensorflow as tf
from tensorflow.keras import layers, models

def build_transfer_model(input_shape, num_classes):
    # 1. Load Pre-trained Model (MobileNetV2)
    # include_top=False: 1000ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ê¸°(Head)ëŠ” ë²„ë¦¼
    # weights='imagenet': ImageNetìœ¼ë¡œ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš©
    base_model = tf.keras.applications.MobileNetV2(
        input_shape=input_shape,
        include_top=False, 
        weights='imagenet'
    )
    
    # 2. Freeze the Base Model (í•µì‹¬!)
    # ì—­ì „íŒŒ ì‹œ ì´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ë³€í•˜ì§€ ì•Šë„ë¡ ì ê¸ˆ
    base_model.trainable = False
    
    # 3. Add Custom Head
    model = models.Sequential([
        base_model,
        layers.GlobalAveragePooling2D(), # íŠ¹ì§•ë§µì„ ë²¡í„°ë¡œ ë³€í™˜
        layers.Dropout(0.2),             # ê³¼ëŒ€ì í•© ë°©ì§€
        layers.Dense(num_classes, activation='softmax') # ë‚´ ë¬¸ì œì— ë§ëŠ” ì¶œë ¥ì¸µ
    ])
    
    return model

# --- ì‹¤í–‰ ë° Fine-tuning ---
if __name__ == "__main__":
    model = build_transfer_model((160, 160, 3), 10)
    
    # 1ë‹¨ê³„: Headë§Œ í•™ìŠµ (BaseëŠ” ê³ ì •)
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
    # model.fit(...) 
    
    # 2ë‹¨ê³„: Fine-tuning (Baseì˜ ì¼ë¶€ë¥¼ í’ˆ)
    base_model = model.layers[0]
    base_model.trainable = True
    
    # ì•ìª½ 100ê°œ ì¸µì€ ê³„ì† ê³ ì •, ë‚˜ë¨¸ì§€ ë’¤ìª½ë§Œ í•™ìŠµ
    fine_tune_at = 100
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
        
    # ì¤‘ìš”: ë¯¸ì„¸ ì¡°ì • ì‹œì—ëŠ” í•™ìŠµë¥ ì„ ì•„ì£¼ ë‚®ê²Œ(1/10 ~ 1/100) ì¡ì•„ì•¼ í•¨
    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),
                  loss='categorical_crossentropy', metrics=['acc'])
    # model.fit(...)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ì²˜ìŒë¶€í„° Fine-tuningì„ í•˜ë©´ ì•ˆ ë˜ë‚˜ìš”?}
\textbf{ì ˆëŒ€ ì•ˆ ë©ë‹ˆë‹¤.}
ìƒˆë¡œ ë¶™ì¸ Head(ë¶„ë¥˜ê¸°)ëŠ” ëœë¤ ì´ˆê¸°í™” ìƒíƒœë¼ ì—‰ëš±í•œ ì˜¤ì°¨(Gradient)ë¥¼ ë¿œì–´ëƒ…ë‹ˆë‹¤. Base Modelì„ ê³ ì •í•˜ì§€ ì•Šìœ¼ë©´, ì´ í° ì˜¤ì°¨ ë•Œë¬¸ì— ì˜ í•™ìŠµë˜ì–´ ìˆë˜ Base Modelì˜ ê°€ì¤‘ì¹˜ê°€ ë‹¤ ë§ê°€ì ¸ë²„ë¦½ë‹ˆë‹¤ (**Catastrophic Forgetting**).
ë°˜ë“œì‹œ **Headë¥¼ ë¨¼ì € í•™ìŠµì‹œì¼œ ì•ˆì •í™”í•œ ë’¤**, Base Modelì„ í’€ì–´ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ê°€ ë‹¬ë¼ë„ ë˜ë‚˜ìš”?} \\
\textbf{A.} CNNì˜ íŠ¹ì„±ìƒ ê°€ëŠ¥ì€ í•˜ì§€ë§Œ, ì„±ëŠ¥ì„ ìœ„í•´ ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì´ ì‚¬ìš©í–ˆë˜ í¬ê¸°(ì˜ˆ: 224x224)ë¡œ ë¦¬ì‚¬ì´ì§•(Resize)í•´ì„œ ë„£ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ê²ƒìœ¼ë¡œ 'ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ êµ¬ì¡°í™” ì „ëµ' íŒŒíŠ¸ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ì´ì œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ì „ëµë¶€í„°, ë‚¨ì˜ ì§€ì‹ì„ ë¹Œë ¤ì˜¤ëŠ” ì „ëµê¹Œì§€ ëª¨ë‘ ê°–ì¶˜ **ì „ëµê°€**ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì±•í„°ë¶€í„°ëŠ” ë”¥ëŸ¬ë‹ì„ ë”ìš± ê¹Šì´ ìˆê²Œ ë§Œë“œëŠ” **[End-to-End Deep Learning]**ì˜ ê°œë…ê³¼, ì´ê²ƒì´ ì „í†µì ì¸ íŒŒì´í”„ë¼ì¸ ë°©ì‹ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¹„êµ ë¶„ì„í•˜ë©° ì‹œì‘í•´ ë³´ê² ìŠµë‹ˆë‹¤. ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Transfer Learning:} ë¹…ë°ì´í„° ëª¨ë¸(Source)ì„ ì†ŒëŸ‰ ë°ì´í„° ë¬¸ì œ(Target)ì— ì¬í™œìš©í•œë‹¤.
    \item \textbf{Freeze:} ì´ˆê¸° í•™ìŠµ ì‹œì—ëŠ” Backboneì„ ê³ ì •í•˜ê³  Headë§Œ í•™ìŠµí•œë‹¤.
    \item \textbf{Fine-tune:} ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´ Backboneì˜ ì¼ë¶€ë¥¼ í’€ì–´ ë¯¸ì„¸ ì¡°ì •í•œë‹¤. (Low LR í•„ìˆ˜)
    \item \textbf{Multi-task:} ì—¬ëŸ¬ ì‘ì—…ì„ ë™ì‹œì— ë°°ìš°ë©´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{strategybox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§­ #1 (ì „ëµ ê°€ì´ë“œ)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Structuring Machine Learning Projects: \\ End-to-End Deep Learning}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-7.] ML Strategy \& Advanced Learning \textit{- Completed}
    \item[\textbf{Chapter 8.}] \textbf{End-to-End Deep Learning (Current Unit)}
    \begin{itemize}
        \item \textbf{8.1 Concept: Pipeline vs E2E}
        \begin{itemize}
            \item Definition: Direct Highway
            \item Pros: Let the Data Speak
            \item Cons: Data Hungry & Black Box
            \item Decision Checklist
        \end{itemize}
        \item 8.2 Application Examples (Speech, Vision)
    \end{itemize}
    \item[Chapter 9.] Convolutional Neural Networks (CNN) \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ê¹Œì§€ ìš°ë¦¬ëŠ” ì „ì´ í•™ìŠµê³¼ ë‹¤ì¤‘ ì‘ì—… í•™ìŠµì„ í†µí•´ ê¸°ì¡´ ì§€ì‹ì„ í™œìš©í•˜ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.
ì´ì œ ë”¥ëŸ¬ë‹ì´ ê°€ì ¸ì˜¨ ê°€ì¥ ê±°ëŒ€í•œ íŒ¨ëŸ¬ë‹¤ì„ì˜ ë³€í™”, **ì—”ë“œíˆ¬ì—”ë“œ(End-to-End) ë”¥ëŸ¬ë‹**ì— ëŒ€í•´ ì´ì•¼ê¸°í•  ì‹œê°„ì…ë‹ˆë‹¤.
ê³¼ê±°ì—ëŠ” ìŒì„± ì¸ì‹ì„ ìœ„í•´ ìŒí–¥í•™, ìŒì„±í•™ ë“± ìˆ˜ë§ì€ íŒŒì´í”„ë¼ì¸ì„ ì¡°ë¦½í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë”¥ëŸ¬ë‹ì€ ì´ ëª¨ë“  ë‹¨ê³„ë¥¼ ê±´ë„ˆë›°ê³ , **"ì…ë ¥ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ì§í–‰í•˜ëŠ” ê³ ì†ë„ë¡œ"**ë¥¼ ëš«ì–´ë²„ë ¸ìŠµë‹ˆë‹¤. ì´ê²ƒì´ ì–¸ì œë‚˜ ì •ë‹µì¼ê¹Œìš”?

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì¤‘ê°„ ë‹¨ê³„ë¥¼ ìƒëµí•˜ê³  ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµí•˜ëŠ” E2E ë”¥ëŸ¬ë‹ì˜ ì¥ë‹¨ì ì„ ë¶„ì„í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì •ì˜:} ì „í†µì ì¸ **íŒŒì´í”„ë¼ì¸(Pipeline)** ë°©ì‹ê³¼ **E2E** ë°©ì‹ì˜ êµ¬ì¡°ì  ì°¨ì´ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.
    \item \textbf{ì¥ì :} ì¸ê°„ì˜ í¸í–¥(Hand-designed components)ì„ ì œê±°í•˜ì—¬ ìµœì ì˜ ì„±ëŠ¥ì„ ë‚´ëŠ” ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ë‹¨ì :} ì™œ ë§‰ëŒ€í•œ ì–‘ì˜ ë°ì´í„°ê°€ í•„ìš”í•œì§€(Data Hungry), ì™œ ë””ë²„ê¹…ì´ ì–´ë ¤ìš´ì§€ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{ê²°ì •:} ì–¸ì œ E2Eë¥¼ ì“°ê³ , ì–¸ì œ íŒŒì´í”„ë¼ì¸ì„ ì¨ì•¼ í•˜ëŠ”ì§€ ê²°ì • ê¸°ì¤€ì„ ì„¸ì›ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ } \\ \hline
\textbf{End-to-End (E2E)} & ì…ë ¥ $\to$ ì¶œë ¥ìœ¼ë¡œ ì§í–‰í•˜ëŠ” ë‹¨ì¼ ëª¨ë¸ & ì§í•­ ë¹„í–‰ê¸° (ì¤‘ê°„ ê²½ìœ ì§€ ì—†ìŒ) \\ \hline
\textbf{Pipeline} & ì—¬ëŸ¬ ëª¨ë“ˆì„ ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°í•œ ì‹œìŠ¤í…œ & ê²½ìœ  ë¹„í–‰ê¸° (A $\to$ B $\to$ C $\to$ D) \\ \hline
\textbf{Hand-engineered} & ì‚¬ëŒì´ ì§ì ‘ ì„¤ê³„í•œ íŠ¹ì§•/ê·œì¹™ & ìˆ˜ì œì‘ ë¶€í’ˆ (ì¥ì¸ì˜ ì†ê¸¸) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì§í–‰ ê³ ì†ë„ë¡œ}

\subsection{1. Pipeline vs End-to-End}


\textbf{ì˜ˆì‹œ: ìŒì„± ì¸ì‹ (Speech Recognition)}
\begin{itemize}
    \item \textbf{Traditional Pipeline:}
    $$ \text{Audio} \to \text{MFCC(íŠ¹ì§• ì¶”ì¶œ)} \to \text{Phoneme(ìŒì†Œ)} \to \text{Word} \to \text{Text} $$
    ê° ë‹¨ê³„ë§ˆë‹¤ ì „ë¬¸ê°€ì˜ ì§€ì‹(ìŒì„±í•™ ë“±)ì´ í•„ìš”í•©ë‹ˆë‹¤.
    
    \item \textbf{End-to-End Deep Learning:}
    $$ \text{Audio} \to [\text{Deep Neural Network}] \to \text{Text} $$
    ì¤‘ê°„ ë‹¨ê³„(ìŒì†Œ ë“±)ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°€ë¥´ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„°ë§Œ ì¶©ë¶„í•˜ë©´ ì‹ ê²½ë§ì´ ì•Œì•„ì„œ ìµœì ì˜ ë‚´ë¶€ í‘œí˜„ì„ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
\end{itemize}

\subsection{2. The Key Idea: Let the Data Speak}
ì „í†µì  ë°©ì‹ì—ëŠ” "ìŒì†Œë¥¼ ë¨¼ì € ì°¾ì•„ì•¼ í•´"ë¼ëŠ” ì¸ê°„ì˜ ê°€ì •(Bias)ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë‹¤ë©´, ì‹ ê²½ë§ì€ ìŒì†Œë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ ìì‹ ë§Œì˜ ë°©ì‹ì„ ì°¾ì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. E2EëŠ” **ë°ì´í„°ê°€ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ ì²˜ë¦¬ ê³¼ì •ì„ ì„¤ê³„í•˜ë„ë¡ í—ˆìš©**í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

\section{Deep Dive: Pros \& Cons}

ë¬´ì¡°ê±´ E2Eê°€ ì¢‹ì€ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ëª…í™•í•œ íŠ¸ë ˆì´ë“œì˜¤í”„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.

\begin{strategybox}{End-to-End ì¥ë‹¨ì  ë¶„ì„}
\textbf{ì¥ì  (Pros):}
\begin{itemize}
    \item \textbf{Data-driven:} ì¸ê°„ì˜ ì„ ì…ê²¬ì— ê°‡íˆì§€ ì•Šê³  ë°ì´í„° íŒ¨í„´ ìì²´ë¥¼ í•™ìŠµí•˜ë¯€ë¡œ, ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ì„±ëŠ¥ ìƒí•œì„ ì´ ë†’ìŠµë‹ˆë‹¤.
    \item \textbf{Simplicity:} ë³µì¡í•œ íŒŒì´í”„ë¼ì¸ì„ ì„¤ê³„í•˜ê³  ìœ ì§€ë³´ìˆ˜í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹ ê²½ë§ í•˜ë‚˜ë§Œ ê´€ë¦¬í•˜ë©´ ë©ë‹ˆë‹¤.
\end{itemize}

\textbf{ë‹¨ì  (Cons):}
\begin{itemize}
    \item \textbf{Data Hungry:} $(x, y)$ ìŒ ë°ì´í„°ê°€ ì—„ì²­ë‚˜ê²Œ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. (íŒŒì´í”„ë¼ì¸ì€ ê° ëª¨ë“ˆë³„ë¡œ ì ì€ ë°ì´í„°ë¡œ í•™ìŠµ ê°€ëŠ¥).
    \item \textbf{Black Box:} ì™œ í‹€ë ¸ëŠ”ì§€ ì„¤ëª…í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. (íŒŒì´í”„ë¼ì¸ì€ "ìŒì†Œ ì¸ì‹ì—ì„œ í‹€ë ¸êµ°" í•˜ê³  ì•Œ ìˆ˜ ìˆìŒ).
\end{itemize}
\end{strategybox}

% --- 7. êµ¬í˜„ ì˜ˆì‹œ ---
\section{Example: Date Formatting}

ë‚ ì§œ í˜•ì‹ì„ ë³€í™˜í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì œë¡œ E2Eì˜ ì² í•™ì„ ë´…ë‹ˆë‹¤.
Input: "20th Jan. 2023" $\to$ Output: "2023-01-20"

\begin{lstlisting}[language=Python, caption=E2E Logic Overview]
# Traditional Approach (Rule-based)
def parse_date(date_str):
    # ìˆ˜ë§ì€ ê·œì¹™(Regex) ì‘ì„± í•„ìš”
    # "Jan" -> 01, "February" -> 02 ...
    # ì˜¤íƒ€ ì²˜ë¦¬, ì˜ˆì™¸ ì²˜ë¦¬ ë“± ë³µì¡í•¨
    pass

# End-to-End Approach
# ê·œì¹™ì„ ì§œëŠ” ê²Œ ì•„ë‹ˆë¼, ë°ì´í„°(x, y)ë¥¼ ë“¤ì´ë¶“ëŠ”ë‹¤.
x_data = ["25th Dec 2022", "December 25, 2022", "12/25/22"]
y_data = ["2022-12-25",    "2022-12-25",        "2022-12-25"]

# ë°ì´í„°ê°€ 10ë§Œ ê°œì¯¤ ìˆë‹¤ë©´, 
# ëª¨ë¸(Seq2Seq ë“±)ì€ "Dec"ê°€ "12"ë¼ëŠ” ê²ƒì„ ìŠ¤ìŠ¤ë¡œ ê¹¨ìš°ì¹©ë‹ˆë‹¤.
# ì½”ë“œëŠ” ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ë¿, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì€ ì—†ìŠµë‹ˆë‹¤.
model.fit(x_data, y_data)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ììœ¨ì£¼í–‰ë„ E2Eë¡œ í•˜ë‚˜ìš”?} \\
\textbf{A.} ì´ˆê¸°ì—ëŠ” ì‹œë„í–ˆì§€ë§Œ(NVIDIA ë“±), í˜„ì¬ëŠ” **ì•ˆì „** ë•Œë¬¸ì— íŒŒì´í”„ë¼ì¸ì„ ì„ í˜¸í•©ë‹ˆë‹¤. "ì´ë¯¸ì§€ $\to$ í•¸ë“¤ ì¡°í–¥"ìœ¼ë¡œ ë°”ë¡œ ê°€ë©´, ì‚¬ê³ ê°€ ë‚¬ì„ ë•Œ ì™œ í•¸ë“¤ì„ êº¾ì—ˆëŠ”ì§€ ì•Œ ìˆ˜ ì—†ì–´ ë””ë²„ê¹…ê³¼ ì±…ì„ ì†Œì¬ íŒŒì•…ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. (ìµœê·¼ì—” ë‹¤ì‹œ E2E ë¹„ì¤‘ì´ ëŠ˜ì–´ë‚˜ëŠ” ì¶”ì„¸ì´ê¸´ í•©ë‹ˆë‹¤.)

\textbf{Q. ë°ì´í„°ê°€ ì ì„ ë•Œ E2Eë¥¼ ì“°ë©´ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} ë„¤, ì„±ëŠ¥ì´ ì²˜ì°¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°ì´í„°ê°€ ì ì„ ë•ŒëŠ” ì¸ê°„ì˜ ì§€ì‹(Hand-engineered features)ì„ ì£¼ì…í•´ì£¼ëŠ” íŒŒì´í”„ë¼ì¸ ë°©ì‹ì´ í›¨ì”¬ íš¨ìœ¨ì ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ê²ƒìœ¼ë¡œ ì•¤ë“œë¥˜ ì‘ êµìˆ˜ì˜ **'ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ êµ¬ì¡°í™” ì „ëµ(Structuring ML Projects)'** íŒŒíŠ¸(Part 3)ë¥¼ ëª¨ë‘ ë§ˆì¹©ë‹ˆë‹¤.
ì´ì œ ì—¬ëŸ¬ë¶„ì€ ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆ ì´ë¡ ë¶€í„° ì„±ëŠ¥ í–¥ìƒ ê¸°ë²•, ê·¸ë¦¬ê³  í”„ë¡œì íŠ¸ë¥¼ ì§€íœ˜í•˜ëŠ” ì „ëµê¹Œì§€ ëª¨ë‘ ê°–ì·„ìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ë¶€í„°ëŠ” ë”¥ëŸ¬ë‹ì˜ ê½ƒì´ì ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë¶„ì•¼ì¸ **[Part 4. Convolutional Neural Networks (CNN)]**ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì»´í“¨í„°ì˜ ì‹œê°ì„ ì •ë³µí•´ë³´ê² ìŠµë‹ˆë‹¤. ê¸°ëŒ€í•˜ì‹­ì‹œì˜¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{E2E:} ì¤‘ê°„ ë‹¨ê³„ ì—†ì´ ì…ë ¥ì—ì„œ ì¶œë ¥ìœ¼ë¡œ ë°”ë¡œ ë§¤í•‘í•˜ëŠ” ë”¥ëŸ¬ë‹ ë°©ì‹.
    \item \textbf{Pros:} ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´ ì¸ê°„ë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ íŠ¹ì§•ì„ ì°¾ì•„ë‚¸ë‹¤.
    \item \textbf{Cons:} ë§‰ëŒ€í•œ ì–‘ì˜ ë¼ë²¨ë§ ë°ì´í„°ê°€ í•„ìš”í•˜ë‹¤. ì„¤ëª…ë ¥ì´ ë¶€ì¡±í•˜ë‹¤.
    \item \textbf{Decision:} ë°ì´í„° ì–‘ì´ ì ê±°ë‚˜ ì•ˆì „/ì„¤ëª…ì´ ì¤‘ìš”í•œ ë¶„ì•¼ëŠ” íŒŒì´í”„ë¼ì¸ì„ ì“´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ë§ŒëŠ¥ ê³µì‹)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ CNN Foundations}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item \textbf{9.1 CNN Foundations: Convolution, Padding, Strides}
        \begin{itemize}
            \item The Convolution Operation ($*$)
            \item Padding (Valid vs Same)
            \item Strides (Downsampling)
            \item \textbf{The Golden Formula} (Dimension Calculation)
        \end{itemize}
        \item 9.2 Pooling Layers (Max/Average) \textit{- Upcoming}
        \item 9.3 CNN Example: LeNet-5 \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸ë¥¼ ì „ëµì ìœ¼ë¡œ ì§€íœ˜í•˜ëŠ” ë²•(Part 3)ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ ë”¥ëŸ¬ë‹ì´ ê°€ì¥ ëˆˆë¶€ì‹  ì„±ê³¼ë¥¼ ë‚¸ **ì»´í“¨í„° ë¹„ì „(Part 4)**ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.
ë§Œì•½ ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¥¼ ê¸°ì¡´ì˜ FC(Fully Connected) Layerì— ë„£ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”? íŒŒë¼ë¯¸í„°ê°€ ìˆ˜ì‹­ì–µ ê°œë¡œ í­ë°œí•˜ì—¬ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•´ì§‘ë‹ˆë‹¤.
ìš°ë¦¬ì—ê² ì´ë¯¸ì§€ì˜ ì§€ì—­ì  íŠ¹ì§•ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” ìƒˆë¡œìš´ ë„êµ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë°”ë¡œ **í•©ì„±ê³±(Convolution)**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ CNNì„ êµ¬ì„±í•˜ëŠ” ê°€ì¥ ê¸°ì´ˆì ì¸ 'ë ˆê³  ë¸”ë¡' ì„¸ ê°€ì§€ë¥¼ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Convolution:} í•„í„°ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©° íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ê¸°ë³¸ ì—°ì‚°.
    \item \textbf{Padding:} ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ ì •ë³´ ì†ì‹¤ì„ ë§‰ê³  í¬ê¸°ë¥¼ ìœ ì§€í•˜ëŠ” ê¸°ë²•.
    \item \textbf{Strides:} í•„í„° ì´ë™ ê°„ê²©ì„ ì¡°ì ˆí•˜ì—¬ ì¶œë ¥ í¬ê¸°ë¥¼ ì¤„ì´ëŠ” ê¸°ë²•.
    \item \textbf{Formula:} ì…ë ¥ í¬ê¸°, í•„í„°, íŒ¨ë”©, ìŠ¤íŠ¸ë¼ì´ë“œê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì¶œë ¥ í¬ê¸°ë¥¼ ê³„ì‚°í•˜ëŠ” ê³µì‹ì„ ì•”ê¸°í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{ì„¤ëª…} \\ \hline
\textbf{Filter / Kernel} & $f \times f$ & ì´ë¯¸ì§€ë¥¼ í›‘ëŠ” ì‘ì€ ìœˆë„ìš°. í•™ìŠµ ëŒ€ìƒ íŒŒë¼ë¯¸í„°($W$). \\ \hline
\textbf{Padding} & $p$ & ì…ë ¥ ì´ë¯¸ì§€ í…Œë‘ë¦¬ì— ë§ëŒ€ëŠ” ê°€ì§œ í”½ì…€(0). \\ \hline
\textbf{Stride} & $s$ & í•„í„°ê°€ í•œ ë²ˆì— ì´ë™í•˜ëŠ” ì¹¸ ìˆ˜(ë³´í­). \\ \hline
\textbf{Feature Map} & - & í•©ì„±ê³± ì—°ì‚°ì˜ ê²°ê³¼ë¬¼(ì¶œë ¥ ì´ë¯¸ì§€). \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: CNNì˜ ë ˆê³  ë¸”ë¡}

\subsection{1. The Convolution Operation ($*$)}
FC Layerê°€ ì´ë¯¸ì§€ ì „ì²´ë¥¼ í•œ ë²ˆì— ë³¸ë‹¤ë©´, í•©ì„±ê³±ì€ ì‘ì€ **'ì†ì „ë“±'**ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ í›‘ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.



\begin{itemize}
    \item **ê³¼ì •:** $3 \times 3$ í•„í„°ë¥¼ ì´ë¯¸ì§€ ì¢Œì¸¡ ìƒë‹¨ì— ê²¹ì³ ë†“ê³ , ê²¹ì¹˜ëŠ” ìˆ«ìë¼ë¦¬ ê³±í•´ì„œ ë”í•©ë‹ˆë‹¤(ë‚´ì ). ê·¸ ê²°ê³¼ê°’ í•˜ë‚˜ê°€ ì¶œë ¥ì˜ í”½ì…€ í•˜ë‚˜ê°€ ë©ë‹ˆë‹¤. ì˜†ìœ¼ë¡œ í•œ ì¹¸ì”© ì´ë™í•˜ë©° ë°˜ë³µí•©ë‹ˆë‹¤.
    \item **ì˜ë¯¸:** í•„í„°ì˜ ê°’ì— ë”°ë¼ ìˆ˜ì§ì„ , ìˆ˜í‰ì„  ê°™ì€ **íŠ¹ì • íŒ¨í„´**ì´ ìˆëŠ” ìœ„ì¹˜ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
\end{itemize}

\subsection{2. Padding ($p$)}


í•©ì„±ê³±ì„ í•˜ë©´ ì´ë¯¸ì§€ê°€ ì ì  ì‘ì•„ì§‘ë‹ˆë‹¤ ($6 \times 6 \to 4 \times 4 \to \dots$). ë˜í•œ ê°€ì¥ìë¦¬ í”½ì…€ì€ í•„í„°ê°€ ëœ ì§€ë‚˜ê°€ì„œ ì •ë³´ê°€ ì†Œì‹¤ë©ë‹ˆë‹¤.
ì´ë¥¼ ë§‰ê¸° ìœ„í•´ í…Œë‘ë¦¬ì— 0ì„ ì±„ì›ë‹ˆë‹¤.
\begin{itemize}
    \item **Valid Padding ($p=0$):** íŒ¨ë”© ì—†ìŒ. í¬ê¸°ê°€ ì¤„ì–´ë“¬.
    \item **Same Padding:** ì…ë ¥ê³¼ ì¶œë ¥ì˜ í¬ê¸°ê°€ ê°™ì•„ì§€ë„ë¡ $p$ë¥¼ ì„¤ì •í•¨.
    $$ p = \frac{f-1}{2} \quad (\text{ë‹¨, } s=1) $$
\end{itemize}

\subsection{3. Strides ($s$)}
í•„í„°ë¥¼ í•œ ì¹¸ì”©($s=1$)ì´ ì•„ë‹ˆë¼ ë‘ ì¹¸ì”©($s=2$) ë“¬ì„±ë“¬ì„± ì´ë™í•©ë‹ˆë‹¤.
\begin{itemize}
    \item **íš¨ê³¼:** ì¶œë ¥ í¬ê¸°ê°€ ëŒ€ëµ $1/s$ ë°°ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤ (**Downsampling**).
    \item **ìš©ë„:** ê³„ì‚°ëŸ‰ì„ ì¤„ì´ê±°ë‚˜ ë„“ì€ ì˜ì—­ì„ ìš”ì•½í•´ì„œ ë³¼ ë•Œ ì”ë‹ˆë‹¤.
\end{itemize}

---

\section{Deep Dive: The Golden Formula (ë§ŒëŠ¥ ê³µì‹)}

ì´ ê³µì‹ì€ CNN ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ê±°ë‚˜ ë…¼ë¬¸ì„ ì½ì„ ë•Œ í•„ìˆ˜ì…ë‹ˆë‹¤. ë¬´ì¡°ê±´ ì•”ê¸°í•˜ì‹­ì‹œì˜¤.

\begin{formulabox}{ì¶œë ¥ í¬ê¸° ê³„ì‚° ê³µì‹}
ì…ë ¥ í¬ê¸°ê°€ $n \times n$ ì¼ ë•Œ, ì¶œë ¥ í¬ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
$$ n_{out} = \left\lfloor \frac{n + 2p - f}{s} + 1 \right\rfloor $$
\begin{itemize}
    \item $n$: ì…ë ¥ í¬ê¸°
    \item $p$: íŒ¨ë”© í¬ê¸°
    \item $f$: í•„í„° í¬ê¸°
    \item $s$: ìŠ¤íŠ¸ë¼ì´ë“œ
    \item $\lfloor \cdot \rfloor$: ë°”ë‹¥ í•¨ìˆ˜ (ì†Œìˆ˜ì  ë‚´ë¦¼)
\end{itemize}
\end{formulabox}

\begin{examplebox}{ê³„ì‚° ì˜ˆì œ}
\textbf{ìƒí™©:}
ì…ë ¥ $7 \times 7$ ($n=7$), í•„í„° $3 \times 3$ ($f=3$), íŒ¨ë”© ì—†ìŒ ($p=0$), ìŠ¤íŠ¸ë¼ì´ë“œ 2 ($s=2$).

\textbf{ê³„ì‚°:}
$$ n_{out} = \left\lfloor \frac{7 + 2(0) - 3}{2} + 1 \right\rfloor = \left\lfloor \frac{4}{2} + 1 \right\rfloor = \lfloor 3 \rfloor = 3 $$
\textbf{ê²°ê³¼:} ì¶œë ¥ì€ $3 \times 3$ í¬ê¸°ê°€ ë©ë‹ˆë‹¤.
\end{examplebox}

---

\section{Implementation Perspective: 3D Volume}

ì‹¤ì œ ì´ë¯¸ì§€ëŠ” ì»¬ëŸ¬(RGB)ì´ë¯€ë¡œ 3ì°¨ì›($H \times W \times C$)ì…ë‹ˆë‹¤.
\begin{itemize}
    \item **Rule:** í•„í„°ì˜ ì±„ë„ ìˆ˜(ê¹Šì´)ëŠ” ì…ë ¥ì˜ ì±„ë„ ìˆ˜ì™€ **í•­ìƒ ê°™ì•„ì•¼** í•©ë‹ˆë‹¤.
    \item **ì˜ˆì‹œ:** ì…ë ¥ì´ $6 \times 6 \times \mathbf{3}$ ì´ë©´, í•„í„°ëŠ” $3 \times 3 \times \mathbf{3}$ ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
    \item **ë‹¤ì¤‘ í•„í„°:** ë§Œì•½ ì´ëŸ° í•„í„°ë¥¼ 10ê°œ ì“´ë‹¤ë©´? ì¶œë ¥ì€ $4 \times 4 \times \mathbf{10}$ ì´ ë©ë‹ˆë‹¤.
\end{itemize}

\begin{warningbox}{í•„í„° ê°œìˆ˜ = ì¶œë ¥ ì±„ë„ ìˆ˜}
CNN ì¸µì„ ì§€ë‚œ ë’¤ ë°ì´í„°ì˜ ê¹Šì´(Depth)ëŠ” ì…ë ¥ì˜ ê¹Šì´ê°€ ì•„ë‹ˆë¼ **í•„í„°ì˜ ê°œìˆ˜**ì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤. ì´ê²ƒì´ ì±„ë„ ìˆ˜ë¥¼ ì¡°ì ˆí•˜ëŠ” í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤.
\end{warningbox}

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” CNNì˜ ê¸°ì´ˆ ì—°ì‚°(Conv, Padding, Stride)ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì´ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ë” ê³¼ê°í•˜ê²Œ ì¤„ì´ë©´ì„œë„ ì¤‘ìš”í•œ ì •ë³´(ìµœëŒ€ê°’)ë§Œ ë‚¨ê¸°ëŠ” **í’€ë§(Pooling)** ê³„ì¸µì´ í•„ìš”í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” Max Poolingê³¼ Average Poolingì— ëŒ€í•´ ë°°ìš°ê³ , ë“œë””ì–´ ì´ ëª¨ë“  ë¸”ë¡ì„ ì¡°ë¦½í•˜ì—¬ **ì²« ë²ˆì§¸ ì™„ì „í•œ CNN ëª¨ë¸(LeNet-5)**ì„ ë§Œë“¤ì–´ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Convolution:} í•„í„°ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©° ì§€ì—­ì  íŠ¹ì§•ì„ ì¶”ì¶œí•œë‹¤.
    \item \textbf{Padding:} ê°€ì¥ìë¦¬ ì†ì‹¤ì„ ë§‰ê³  í¬ê¸°ë¥¼ ìœ ì§€í•œë‹¤ (Same Padding).
    \item \textbf{Stride:} ì´ë™ ê°„ê²©ì„ ë„“í˜€ í¬ê¸°ë¥¼ ì¤„ì¸ë‹¤.
    \item \textbf{Formula:} $n_{out} = \lfloor \frac{n+2p-f}{s} + 1 \rfloor$.
\end{enumerate}
\end{summarybox}

\end{document}


\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{featurebox}[1]{
    colback=purple!5!white,
    colframe=purple!80!black,
    fonttitle=\bfseries,
    title=ğŸ” #1 (í•µì‹¬ íŠ¹ì§•)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ Pooling Layers}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1 CNN Foundations: Convolution, Padding, Strides \textit{- Completed}
        \item \textbf{9.2 Pooling Layers (Max/Average)}
        \begin{itemize}
            \item Concept: Downsampling \& Invariance
            \item Max Pooling vs Average Pooling
            \item Channel Independence Rule
            \item Implementation
        \end{itemize}
        \item 9.3 CNN Example: LeNet-5 \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” í•©ì„±ê³±(Conv) ì—°ì‚°ìœ¼ë¡œ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í•©ì„±ê³± ì¸µë§Œ ê³„ì† ìŒ“ìœ¼ë©´ ì—°ì‚°ëŸ‰ì´ ë„ˆë¬´ ë§ì•„ì§€ê³ , ëª¨ë¸ì´ ì´ë¯¸ì§€ì˜ ë¯¸ì„¸í•œ ë³€í™”(1í”½ì…€ ì´ë™ ë“±)ì— ë„ˆë¬´ ë¯¼ê°í•´ì§‘ë‹ˆë‹¤.
ìš°ë¦¬ëŠ” **"ì¤‘ìš”í•œ íŠ¹ì§•ë§Œ ë‚¨ê¸°ê³ , í¬ê¸°ëŠ” ì¤„ì—¬ì„œ íš¨ìœ¨ì ìœ¼ë¡œ"** ì²˜ë¦¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ì´ ë‘ ë§ˆë¦¬ í† ë¼ë¥¼ ì¡ëŠ” ê¸°ìˆ ì´ ë°”ë¡œ **í’€ë§(Pooling)**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ CNNì˜ í•µì‹¬ êµ¬ì„± ìš”ì†Œì¸ í’€ë§ ì¸µì˜ ì›ë¦¬ì™€ ì¢…ë¥˜ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë‹¤ìš´ìƒ˜í”Œë§:} ì´ë¯¸ì§€ í¬ê¸°($n_H, n_W$)ë¥¼ ì¤„ì—¬ ê³„ì‚°ëŸ‰ì„ ë‚®ì¶”ëŠ” ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ë¶ˆë³€ì„±:} í’€ë§ì´ ì–´ë–»ê²Œ í‰í–‰ ì´ë™ì— ëŒ€í•œ **ê°•ê±´í•¨(Invariance)**ì„ ì œê³µí•˜ëŠ”ì§€ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ë¹„êµ:} ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” **Max Pooling**ê³¼ ê³¼ê±°ì— ì“°ì˜€ë˜ **Average Pooling**ì„ ë¹„êµí•©ë‹ˆë‹¤.
    \item \textbf{íŠ¹ì§•:} í’€ë§ ì¸µì—ëŠ” **í•™ìŠµ íŒŒë¼ë¯¸í„°($W, b$)ê°€ ì—†ë‹¤**ëŠ” ì ì„ ëª…ì‹¬í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{í•µì‹¬ ì—­í• } \\ \hline
\textbf{Max Pooling} & ì˜ì—­ ë‚´ ìµœëŒ“ê°’ ì„ íƒ & ê°€ì¥ ê°•í•œ íŠ¹ì§•ë§Œ ë‚¨ê¹€ (ëŒ€ì„¸). \\ \hline
\textbf{Average Pooling} & ì˜ì—­ ë‚´ í‰ê· ê°’ ê³„ì‚° & ì •ë³´ë¥¼ ë¶€ë“œëŸ½ê²Œ ìš”ì•½í•¨. \\ \hline
\textbf{Invariant} & ë¶ˆë³€ì„± & ì…ë ¥ì´ ì¡°ê¸ˆ ë°”ë€Œì–´ë„ ì¶œë ¥ì€ ë³€í•˜ì§€ ì•ŠìŒ. \\ \hline
\textbf{Hyperparameters} & $f$(í•„í„° í¬ê¸°), $s$(ìŠ¤íŠ¸ë¼ì´ë“œ) & í’€ë§ ë™ì‘ì„ ê²°ì •í•˜ëŠ” ì„¤ì •ê°’. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìš”ì•½ì˜ ê¸°ìˆ }

\subsection{1. Max Pooling (ìµœëŒ€ í’€ë§)}


ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í•„í„° ì˜ì—­($f \times f$) ë‚´ì—ì„œ **ê°€ì¥ í° ìˆ«ì í•˜ë‚˜**ë§Œ ê³¨ë¼ëƒ…ë‹ˆë‹¤.
\begin{itemize}
    \item **ë™ì‘:** $2 \times 2$ ìœˆë„ìš°ë¡œ ì´ë¯¸ì§€ë¥¼ í›‘ìœ¼ë©° ìµœëŒ“ê°’ì„ ë½‘ìŠµë‹ˆë‹¤.
    \item **ì˜ë¯¸:** "ì´ êµ¬ì—­ì— ê³ ì–‘ì´ ëˆˆ(íŠ¹ì§•)ì´ ìˆëŠ”ê°€?" $\rightarrow$ **YES (ë†’ì€ ê°’)**. ì •í™•íˆ ì–´ë””(ì¢Œìƒë‹¨? ìš°í•˜ë‹¨?)ì— ìˆëŠ”ì§€ëŠ” ì¤‘ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì¡´ì¬ ìœ ë¬´ë§Œ ê°•ì¡°í•©ë‹ˆë‹¤.
\end{itemize}

\subsection{2. Average Pooling (í‰ê·  í’€ë§)}


í•„í„° ì˜ì—­ ë‚´ì˜ ìˆ«ìë¥¼ ëª¨ë‘ ë”í•´ í‰ê· ì„ ëƒ…ë‹ˆë‹¤.
\begin{itemize}
    \item **ì˜ë¯¸:** íŠ¹ì§•ì„ ë¶€ë“œëŸ½ê²Œ(Smoothing) ë§Œë“­ë‹ˆë‹¤.
    \item **ìš©ë„:** ê³¼ê±°ì—ëŠ” ë§ì´ ì¼ìœ¼ë‚˜ ìµœê·¼ì—ëŠ” ì˜ ì•ˆ ì”ë‹ˆë‹¤. ë‹¨, ëª¨ë¸ì˜ ë§¨ ë§ˆì§€ë§‰ë‹¨(Global Average Pooling)ì—ì„œëŠ” ì—¬ì „íˆ ìœ ìš©í•©ë‹ˆë‹¤.
\end{itemize}

---

\section{Deep Dive: Channel Independence}

ì´ ë¶€ë¶„ì´ í•©ì„±ê³±(Convolution)ê³¼ ê°€ì¥ í—·ê°ˆë¦¬ëŠ” ì§€ì ì…ë‹ˆë‹¤.

\begin{featurebox}{ì±„ë„ ë…ë¦½ì„±ì˜ ë²•ì¹™}
\begin{itemize}
    \item **Convolution:** ì…ë ¥ ì±„ë„(RGB 3ê°œ)ì„ **ëª¨ë‘ í•©ì³ì„œ(Sum)** í•˜ë‚˜ì˜ ìˆ«ìë¡œ ë§Œë“­ë‹ˆë‹¤. ì±„ë„ ìˆ˜ê°€ ë³€í•©ë‹ˆë‹¤.
    \item **Pooling:** ê° ì±„ë„ì„ **ë…ë¦½ì ìœ¼ë¡œ(Independently)** ì²˜ë¦¬í•©ë‹ˆë‹¤.
    \begin{itemize}
        \item ì…ë ¥: $32 \times 32 \times \mathbf{10}$
        \item í’€ë§: $16 \times 16 \times \mathbf{10}$
        \item \textbf{ê²°ê³¼:} ë†’ì´/ë„ˆë¹„ëŠ” ì¤„ì§€ë§Œ, \textbf{ì±„ë„ ìˆ˜(ê¹Šì´)ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€}ë©ë‹ˆë‹¤.
    \end{itemize}
\end{itemize}
\end{featurebox}

\begin{warningbox}{íŒŒë¼ë¯¸í„° ê°œìˆ˜ëŠ” ëª‡ ê°œ?}
í’€ë§ ì¸µì—ëŠ” í•™ìŠµí•´ì•¼ í•  ê°€ì¤‘ì¹˜($W$)ë‚˜ í¸í–¥($b$)ì´ ìˆì„ê¹Œìš”?
\textbf{ì •ë‹µ: 0ê°œì…ë‹ˆë‹¤.}
í’€ë§ì€ ìš°ë¦¬ê°€ ì •í•´ì¤€ ê·œì¹™($f, s$, Max/Avg)ëŒ€ë¡œë§Œ ê³„ì‚°í•˜ëŠ” ê³ ì •ëœ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì—­ì „íŒŒ ë•Œ ì—…ë°ì´íŠ¸ë  ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.
\end{warningbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: NumPy Pooling}

ì›ë¦¬ ì´í•´ë¥¼ ìœ„í•´ 4ì¤‘ ë£¨í”„ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ì ‘ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Max & Average Pooling Implementation]
import numpy as np

class Pooling:
    def __init__(self, f=2, s=2, mode='max'):
        self.f = f
        self.s = s
        self.mode = mode

    def forward(self, A_prev):
        """
        A_prev: (m, n_H, n_W, n_C)
        """
        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
        
        # ì¶œë ¥ í¬ê¸° ê³„ì‚° (ê³µì‹ ì ìš©)
        n_H = int((n_H_prev - self.f) / self.s) + 1
        n_W = int((n_W_prev - self.f) / self.s) + 1
        n_C = n_C_prev # ì±„ë„ ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ!
        
        A = np.zeros((m, n_H, n_W, n_C))
        
        for i in range(m):              # ë°ì´í„° ìƒ˜í”Œ
            for h in range(n_H):        # ì„¸ë¡œ ì´ë™
                for w in range(n_W):    # ê°€ë¡œ ì´ë™
                    for c in range(n_C):# ì±„ë„ (ë…ë¦½ì )
                        
                        # ìœˆë„ìš° ìŠ¬ë¼ì´ì‹±
                        vert_start = h * self.s
                        vert_end   = vert_start + self.f
                        horiz_start= w * self.s
                        horiz_end  = horiz_start + self.f
                        
                        slice_A = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]
                        
                        if self.mode == 'max':
                            A[i, h, w, c] = np.max(slice_A)
                        elif self.mode == 'average':
                            A[i, h, w, c] = np.mean(slice_A)
        return A

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    # 4x4 ì´ë¯¸ì§€, ì±„ë„ 1ê°œ
    img = np.array([[[[1],[3],[2],[1]],
                     [[2],[9],[1],[1]],
                     [[1],[3],[2],[3]],
                     [[5],[6],[1],[2]]]]) # shape (1,4,4,1)
    
    pool = Pooling(f=2, s=2, mode='max')
    out = pool.forward(img)
    
    print("Input:\n", img[0,:,:,0])
    print("Max Pool Output:\n", out[0,:,:,0])
    # ì˜ˆìƒ: [[9, 2], [6, 3]]
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. í’€ë§ì„ í•˜ë©´ ì •ë³´ê°€ ì‚¬ë¼ì§€ëŠ”ë° ê´œì°®ë‚˜ìš”?} \\
\textbf{A.} ë„¤, ê·¸ê²Œ ëª©ì ì…ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ë°°ê²½ì´ë‚˜ ë…¸ì´ì¦ˆëŠ” ë²„ë¦¬ê³ , **"ì—¬ê¸° íŠ¹ì§•ì´ ìˆë‹¤!"(Max Value)**ë¼ëŠ” í•µì‹¬ ì •ë³´ë§Œ ë‚¨ê²¨ì„œ ë‹¤ìŒ ì¸µìœ¼ë¡œ ì „ë‹¬í•˜ëŠ” ê²ƒì´ CNNì˜ ì¶”ìƒí™” ê³¼ì •ì…ë‹ˆë‹¤.

\textbf{Q. $f=3, s=2$ ê°™ì€ ê±´ ì–¸ì œ ì“°ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µì€ $f=2, s=2$ (í¬ê¸° ì ˆë°˜ ì¶•ì†Œ)ê°€ êµ­ë£°ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ê²¹ì¹˜ëŠ” ì˜ì—­ì„ ë‘ê³  ì‹¶ì„ ë•Œ(Overlapping Pooling) $f=3, s=2$ë¥¼ ì“°ê¸°ë„ í•©ë‹ˆë‹¤ (ì˜ˆ: AlexNet).

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” CNNì„ ë§Œë“œëŠ” 3ëŒ€ ìš”ì†Œì¸ **Convolution, ReLU(Activation), Pooling**ì„ ëª¨ë‘ ë°°ì› ìŠµë‹ˆë‹¤. ë ˆê³  ë¸”ë¡ì´ ë‹¤ ëª¨ì˜€ìŠµë‹ˆë‹¤.

ì´ì œ ì´ê²ƒë“¤ì„ ì–´ë–»ê²Œ ì¡°ë¦½í•´ì•¼ í• ê¹Œìš”?
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì´ ë¸”ë¡ë“¤ì„ ê²°í•©í•˜ì—¬ ìˆ«ì í•„ê¸°ì²´(MNIST)ë¥¼ ì¸ì‹í•˜ëŠ” ì „ì„¤ì ì¸ CNN ì•„í‚¤í…ì²˜, **[LeNet-5 Example]**ì„ í†µí•´ ì²« ë²ˆì§¸ ì™„ì „í•œ CNN ëª¨ë¸ì„ êµ¬ì¶•í•´ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Max Pooling:} ì˜ì—­ ë‚´ ìµœëŒ“ê°’ë§Œ ë‚¨ê²¨ íŠ¹ì§•ì„ ê°•ì¡°í•œë‹¤. (ê°€ì¥ ë§ì´ ì”€)
    \item \textbf{Dimension:} ê°€ë¡œ/ì„¸ë¡œëŠ” ì¤„ì–´ë“¤ì§€ë§Œ, **ì±„ë„ ìˆ˜($n_C$)ëŠ” ìœ ì§€ëœë‹¤.**
    \item \textbf{Parameters:} í•™ìŠµí•  ê°€ì¤‘ì¹˜($W$)ê°€ ì—†ë‹¤. (Parameter-free)
    \item \textbf{Effect:} ì—°ì‚°ëŸ‰ì„ ì¤„ì´ê³ , ì´ë™ ë¶ˆë³€ì„±(Invariance)ì„ ì–»ëŠ”ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ Classic Networks (LeNet, AlexNet, VGG)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.2 CNN Foundations \& Pooling \textit{- Completed}
        \item \textbf{9.3 Classic Networks}
        \begin{itemize}
            \item LeNet-5: The Pioneer
            \item AlexNet: The Game Changer
            \item VGG-16: The Standardizer ($3 \times 3$ Philosophy)
        \end{itemize}
        \item 9.4 ResNet (Residual Networks) \textit{- Upcoming}
        \item 9.5 Inception Network \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” CNNì˜ ë ˆê³  ë¸”ë¡(Convolution, Pooling, Activation)ì„ ë§ˆìŠ¤í„°í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ë¸”ë¡ë“¤ì„ ì¡°ë¦½í•˜ì—¬ **ê±°ëŒ€í•œ ì„±(Model)**ì„ ìŒ“ì„ ì‹œê°„ì…ë‹ˆë‹¤.
ë”¥ëŸ¬ë‹ ì—­ì‚¬ì—ëŠ” "ì´ ëª¨ë¸ ì´ì „ê³¼ ì´í›„ë¡œ ì„¸ìƒì´ ë‚˜ë‰˜ì—ˆë‹¤"ê³  í‰ê°€ë°›ëŠ” ì „ì„¤ì ì¸ ì•„í‚¤í…ì²˜ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ê·¸ ì—­ì‚¬ì˜ ì‹œì‘ì ì¸ **LeNet**, ë”¥ëŸ¬ë‹ ë¶ì„ ì¼ìœ¼í‚¨ **AlexNet**, ê·¸ë¦¬ê³  ê¹Šì€ ì‹ ê²½ë§ì˜ í‘œì¤€ì„ ì œì‹œí•œ **VGG**ë¥¼ í•´ë¶€í•©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í˜„ëŒ€ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì˜ ì¡°ìƒì´ ë˜ëŠ” ì„¸ ê°€ì§€ ê³ ì „ ëª¨ë¸ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{LeNet-5 (1998):} CNNì˜ ê¸°ë³¸ êµ¬ì¡°(Conv-Pool ë°˜ë³µ)ë¥¼ ì •ë¦½í•œ ì„ êµ¬ì.
    \item \textbf{AlexNet (2012):} ReLU, Dropout ë“±ì„ ë„ì…í•˜ì—¬ ë”¥ëŸ¬ë‹ ë¶ì„ ì¼ìœ¼í‚¨ ì£¼ì¸ê³µ.
    \item \textbf{VGG-16 (2014):} $3 \times 3$ í•„í„°ë§Œìœ¼ë¡œ ê¹Šì´ë¥¼ ìŒ“ëŠ” 'ë‹¨ìˆœí•¨ì˜ ë¯¸í•™'ì„ ì¦ëª…í•œ í‘œì¤€ ëª¨ë¸.
    \item \textbf{íŒ¨í„´:} ì±„ë„ì€ ëŠ˜ë¦¬ê³ ($\uparrow$), í¬ê¸°ëŠ” ì¤„ì´ëŠ”($\downarrow$) ê³µí†µì ì¸ ì„¤ê³„ íŒ¨í„´ì„ ìµí™ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ëª¨ë¸ ë¹„êµ ìš”ì•½ ---
\section{Model Summary Table}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{íŠ¹ì§•} & \textbf{LeNet-5 (1998)} & \textbf{AlexNet (2012)} & \textbf{VGG-16 (2014)} \\ \hline
\textbf{ì…ë ¥} & $32 \times 32$ (Gray) & $227 \times 227$ (RGB) & $224 \times 224$ (RGB) \\ \hline
\textbf{í•„í„°} & $5 \times 5$ & $11 \times 11, 5 \times 5$ & **Only $3 \times 3$** \\ \hline
\textbf{í™œì„±í™”} & Sigmoid / Tanh & **ReLU** & ReLU \\ \hline
\textbf{í’€ë§} & Average Pooling & Max Pooling & Max Pooling \\ \hline
\textbf{íŒŒë¼ë¯¸í„°} & ì•½ 6ë§Œ ê°œ & ì•½ 6,000ë§Œ ê°œ & **ì•½ 1ì–µ 3,800ë§Œ ê°œ** \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì „ì„¤ë“¤ì˜ ê³„ë³´}

\subsection{1. LeNet-5: The Pioneer}

ì–€ ë¥´ì¿¤(Yann LeCun) êµìˆ˜ê°€ ì€í–‰ ìˆ˜í‘œì˜ ì†ê¸€ì”¨ ìˆ«ì(MNIST) ì¸ì‹ì„ ìœ„í•´ ê°œë°œí–ˆìŠµë‹ˆë‹¤.
\begin{itemize}
    \item **êµ¬ì¡°:** Conv $\to$ Pool $\to$ Conv $\to$ Pool $\to$ FC ...
    \item **ì˜ì˜:** ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” ì¤„ì´ê³ ($32 \to 28 \to 14 \dots$), ì±„ë„ ìˆ˜ëŠ” ëŠ˜ë¦¬ëŠ”($1 \to 6 \to 16$) íŒ¨í„´ì„ ì²˜ìŒ ì •ë¦½í–ˆìŠµë‹ˆë‹¤.
\end{itemize}

\subsection{2. AlexNet: The Game Changer}

2012ë…„ ImageNet ëŒ€íšŒ ìš°ìŠ¹ì‘. ë”¥ëŸ¬ë‹ ì‹œëŒ€ë¥¼ ì—° ì¥ë³¸ì¸ì…ë‹ˆë‹¤.
\begin{itemize}
    \item **ReLU:** Sigmoidì˜ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
    \item **Dropout:** ê³¼ëŒ€ì í•©ì„ ë§‰ê¸° ìœ„í•´ FC ì¸µì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í–ˆìŠµë‹ˆë‹¤.
    \item **Multi-GPU:** ë‹¹ì‹œ GPU ì„±ëŠ¥ í•œê³„ë¡œ ëª¨ë¸ì„ ë‘ ê°œë¡œ ìª¼ê°œ í•™ìŠµí–ˆìŠµë‹ˆë‹¤.
\end{itemize}

\subsection{3. VGG-16: The Standardizer}

ì˜¥ìŠ¤í¼ë“œ ëŒ€í•™ VGG íŒ€ì´ ê°œë°œí–ˆìŠµë‹ˆë‹¤. **"í™”ë ¤í•œ ê¸°êµ(11x11 í•„í„° ë“±)ëŠ” í•„ìš” ì—†ë‹¤. ê¹Šì´(Depth)ë§Œì´ ì •ë‹µì´ë‹¤"**ë¥¼ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.

\begin{mathbox}{Why $3 \times 3$ filters?}
VGGëŠ” ëª¨ë“  ì¸µì—ì„œ $3 \times 3$ í•„í„°ë§Œ ì”ë‹ˆë‹¤. ì™œ í° í•„í„° í•œ ë²ˆ ëŒ€ì‹  ì‘ì€ í•„í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ì“¸ê¹Œìš”?

\textbf{1. íŒŒë¼ë¯¸í„° íš¨ìœ¨ (Parameter Efficiency)}
\begin{itemize}
    \item $5 \times 5$ í•„í„° 1ê°œ: $25 \times C^2$ íŒŒë¼ë¯¸í„°.
    \item $3 \times 3$ í•„í„° 2ê°œ: $2 \times (9 \times C^2) = 18 \times C^2$ íŒŒë¼ë¯¸í„°.
    \item **ê²°ë¡ :** ê°™ì€ ì˜ì—­(Receptive Field)ì„ ë³´ë©´ì„œë„ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ **28\% ì ˆì•½**í•©ë‹ˆë‹¤.
\end{itemize}

\textbf{2. ë¹„ì„ í˜•ì„± (Non-linearity)}
\begin{itemize}
    \item ì¸µì´ ë‘ ê°œë¼ëŠ” ê²ƒì€ ReLUë¥¼ ë‘ ë²ˆ í†µê³¼í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
    \item ë” ë³µì¡í•˜ê³  ì •êµí•œ í•¨ìˆ˜ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
\end{itemize}
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Load VGG16 with Keras}

ìµœì‹  í”„ë ˆì„ì›Œí¬ì—ì„œëŠ” ì´ ê±°ëŒ€í•œ ëª¨ë¸ì„ ë‹¨ í•œ ì¤„ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì „ì´ í•™ìŠµì˜ ê¸°ì´ˆê°€ ë©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Loading VGG16]
from tensorflow.keras.applications import VGG16

# ImageNet ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ VGG16 ëª¨ë¸ ë¡œë“œ
# include_top=True: ë§ˆì§€ë§‰ FC ë¶„ë¥˜ê¸°(1000ê°œ í´ë˜ìŠ¤)ê¹Œì§€ í¬í•¨
model = VGG16(weights='imagenet', include_top=True)

# ëª¨ë¸ êµ¬ì¡° ì¶œë ¥
model.summary()

# ì¶œë ¥ ì˜ˆì‹œ (íŒ¨í„´ í™•ì¸):
# Block 1: Conv(3x3) -> Conv(3x3) -> MaxPool
# Block 2: Conv(3x3) -> Conv(3x3) -> MaxPool
# ... (ì±„ë„ ìˆ˜: 64 -> 128 -> 256 -> 512)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. VGG-16ì˜ ë‹¨ì ì€ ì—†ë‚˜ìš”?} \\
\textbf{A.} **ë„ˆë¬´ ë¬´ê²ìŠµë‹ˆë‹¤.** íŒŒë¼ë¯¸í„° ìˆ˜ê°€ 1ì–µ 3,800ë§Œ ê°œë‚˜ ë˜ì–´ ë©”ëª¨ë¦¬ë¥¼ ì—„ì²­ë‚˜ê²Œ ì¡ì•„ë¨¹ìŠµë‹ˆë‹¤. ë˜í•œ 16ì¸µ, 19ì¸µê¹Œì§€ëŠ” ê´œì°®ì•˜ì§€ë§Œ, ê·¸ ì´ìƒ ìŒ“ìœ¼ë©´ ë‹¤ì‹œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¡œ í•™ìŠµì´ ì•ˆ ë©ë‹ˆë‹¤.

\textbf{Q. $1 \times 1$ Convolutionì€ ë­”ê°€ìš”? (Inception ì˜ˆê³ )} \\
\textbf{A.} í•„í„° í¬ê¸°ê°€ $1 \times 1$ì¸ í•©ì„±ê³±ì…ë‹ˆë‹¤. ê³µê°„ì  ì •ë³´($H, W$)ëŠ” ê±´ë“œë¦¬ì§€ ì•Šê³ , **ì±„ë„ ìˆ˜($C$)ë¥¼ ì¤„ì´ê±°ë‚˜ ëŠ˜ë¦¬ëŠ” ì—­í• **ì„ í•©ë‹ˆë‹¤. ì—°ì‚°ëŸ‰ì„ ì¤„ì´ëŠ” 'ë³‘ëª©(Bottleneck)' ê¸°ë²•ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
VGGëŠ” í›Œë¥­í–ˆì§€ë§Œ, ì¸µì´ 20ê°œë¥¼ ë„˜ì–´ê°€ë©´ ì„±ëŠ¥ì´ ì˜¤íˆë ¤ ë–¨ì–´ì§€ëŠ” í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. (Degradation Problem)
ì¸ê°„ì˜ ë‡ŒëŠ” ìˆ˜ë°± ì¸µì˜ ê¹Šì´ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ë„ 100ì¸µ, 1000ì¸µì„ ìŒ“ì„ ìˆ˜ ì—†ì„ê¹Œìš”?

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë”¥ëŸ¬ë‹ ì—­ì‚¬ìƒ ê°€ì¥ ì¤‘ìš”í•œ ë°œëª… ì¤‘ í•˜ë‚˜ì¸ **'ì”ì°¨ ì—°ê²°(Residual Connection)'**ì„ ë„ì…í•˜ì—¬ 152ì¸µì„ ìŒ“ì€ ê´´ë¬¼, **[ResNet (Residual Networks)]**ì„ ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{LeNet-5:} CNNì˜ ì¡°ìƒ. Conv-Pool íŒ¨í„´ì˜ ì‹œì´ˆ.
    \item \textbf{AlexNet:} ReLUì™€ Dropoutìœ¼ë¡œ ë”¥ëŸ¬ë‹ ì„±ëŠ¥ì„ ì…ì¦í•¨.
    \item \textbf{VGG-16:} $3 \times 3$ í•„í„°ë§Œ ì‚¬ìš©í•˜ì—¬ ê¹Šì´ë¥¼ ìŒ“ìŒ. (ë‹¨ìˆœí•¨, íŒŒë¼ë¯¸í„° íš¨ìœ¨)
    \item \textbf{Limit:} VGGì¡°ì°¨ë„ ë„ˆë¬´ ê¹Šì–´ì§€ë©´ í•™ìŠµì´ ì•ˆ ë˜ëŠ” í•œê³„ê°€ ìˆìŒ.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ ResNet (Residual Networks)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.3 CNN Basics \& Classic Networks \textit{- Completed}
        \item \textbf{9.4 ResNet (Residual Networks)}
        \begin{itemize}
            \item The Degradation Problem
            \item Skip Connections: The Shortcut
            \item Why $H(x) = F(x) + x$?
            \item Implementation: Identity \& Conv Blocks
        \end{itemize}
        \item 9.5 Inception Network \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ VGG-16ì€ 16ê°œ ì¸µìœ¼ë¡œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì§ˆë¬¸ì´ ìƒê¹ë‹ˆë‹¤.
**"ì¸µì„ 100ê°œ, 1000ê°œë¡œ ëŠ˜ë¦¬ë©´ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§€ì§€ ì•Šì„ê¹Œìš”?"**
ì´ë¡ ì ìœ¼ë¡œëŠ” ê·¸ë˜ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì¸µì´ 20ê°œë¥¼ ë„˜ì–´ê°€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ëŠ” **í‡´ë³´(Degradation)** í˜„ìƒì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê¹Šì€ ë§ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒ ìì²´ê°€ ë„ˆë¬´ ì–´ë ¤ì› ë˜ ê²ƒì…ë‹ˆë‹¤.
ì´ ë‚œì œë¥¼ í•´ê²°í•˜ê³  ë”¥ëŸ¬ë‹ ì—­ì‚¬ë¥¼ ìƒˆë¡œ ì“´ ëª¨ë¸ì´ ë°”ë¡œ **ResNet**ì…ë‹ˆë‹¤. í•µì‹¬ì€ **"ì§€ë¦„ê¸¸(Shortcut)"**ì„ ëš«ì–´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ 152ì¸µ ì´ìƒì˜ ì´ˆì‹¬ì¸µ ì‹ ê²½ë§ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•œ ResNetì˜ í•µì‹¬ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë¬¸ì œ:} ë§ì´ ê¹Šì–´ì§ˆìˆ˜ë¡ í•™ìŠµì´ ì•ˆ ë˜ëŠ” ì´ìœ (ê¸°ìš¸ê¸° ì†Œì‹¤ ë“±)ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{í•´ê²°:} ì…ë ¥ $x$ë¥¼ ì¶œë ¥ì— ë”í•´ì£¼ëŠ” **Skip Connection** êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{ì›ë¦¬:} ì”ì°¨ ë¸”ë¡ì´ ì–´ë–»ê²Œ í•­ë“± ë§¤í•‘($H(x)=x$)ì„ ì‰½ê²Œ í•™ìŠµí•˜ëŠ”ì§€ ìˆ˜ì‹ìœ¼ë¡œ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} Kerasë¡œ Identity Blockê³¼ Convolutional Blockì„ êµ¬í˜„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{í•µì‹¬} \\ \hline
\textbf{Skip Connection} & ì…ë ¥ì„ ëª‡ ì¸µ ê±´ë„ˆë›°ì–´ ì¶œë ¥ì— ë”í•˜ëŠ” ì—°ê²°ì„  & ì§€ë¦„ê¸¸ (Shortcut) \\ \hline
\textbf{Residual (ì”ì°¨)} & í•™ìŠµí•´ì•¼ í•  ì°¨ì´ ($F(x)$) & $H(x) - x$ \\ \hline
\textbf{Identity Mapping} & ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ê²ƒ ($H(x)=x$) & ê¸°ë³¸ì€ í•œë‹¤. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì§€ë¦„ê¸¸ì˜ ë§ˆë²•}

\subsection{1. The Degradation Problem (ì„±ëŠ¥ ì €í•˜)}

ì¼ë°˜ì ì¸ ë„¤íŠ¸ì›Œí¬(Plain Network)ì— ì¸µì„ ê³„ì† ì¶”ê°€í•˜ë©´, í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì—ëŸ¬ì¡°ì°¨ ë†’ì•„ì§‘ë‹ˆë‹¤. ê¸°ìš¸ê¸°(Gradient)ê°€ ì…ë ¥ì¸µê¹Œì§€ ë„ë‹¬í•˜ì§€ ëª»í•˜ê³  ì‚¬ë¼ì ¸ë²„ë¦¬ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

\subsection{2. Skip Connection (ì”ì°¨ ì—°ê²°)}


ResNetì˜ ì•„ì´ë””ì–´ëŠ” ê°„ë‹¨í•©ë‹ˆë‹¤. **"ì •ë³´ê°€ íë¥´ëŠ” ê³ ì†ë„ë¡œë¥¼ ëš«ì–´ì£¼ì."**
\begin{itemize}
    \item \textbf{Main Path:} í•©ì„±ê³± ì¸µì„ í†µê³¼í•˜ì—¬ $F(x)$ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    \item \textbf{Shortcut:} ì…ë ¥ $x$ë¥¼ ê·¸ëŒ€ë¡œ ê°€ì ¸ì™€ì„œ ë”í•©ë‹ˆë‹¤.
    \item \textbf{Output:} $H(x) = F(x) + x$
\end{itemize}

\begin{analogybox}{êµê³¼ì„œ ì•”ê¸° ë¹„ìœ }
\begin{itemize}
    \item \textbf{Plain:} "ë°±ì§€ìƒíƒœì—ì„œ êµê³¼ì„œ ì „ì²´($H(x)$)ë¥¼ ë‹¤ ì™¸ì›Œë¼." (ì–´ë µë‹¤)
    \item \textbf{ResNet:} "ë„ˆëŠ” ì´ë¯¸ $x$ë§Œí¼ ì•Œê³  ìˆìœ¼ë‹ˆ, êµê³¼ì„œ ë‚´ìš©ê³¼ ë„¤ ì§€ì‹ì˜ **ì°¨ì´($F(x)$)**ë§Œ ì¶”ê°€ë¡œ ê³µë¶€í•´ë¼." (ì‰½ë‹¤)
\end{itemize}
\end{analogybox}

---

\section{Deep Dive: Why does it work?}

\begin{mathbox}{Gradient Superhighway}
ì—­ì „íŒŒ ì‹œ ë¯¸ë¶„ê°’ì´ ì „ë‹¬ë˜ëŠ” ê³¼ì •ì„ ë´…ì‹œë‹¤.
$$ H(x) = F(x) + x $$
$$ \frac{\partial H}{\partial x} = \frac{\partial F(x)}{\partial x} + \mathbf{1} $$
\textbf{ì˜ë¯¸:} ë³µì¡í•œ í•©ì„±ê³± ê²½ë¡œ($F$)ì˜ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ì–´ë„(Vanishing), ì§€ë¦„ê¸¸ ê²½ë¡œ($+1$)ê°€ ì‚´ì•„ìˆìŠµë‹ˆë‹¤. ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ì§€ ì•Šê³  ë„¤íŠ¸ì›Œí¬ ì•ë‹¨ê¹Œì§€ ê·¸ëŒ€ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
\end{mathbox}

\subsection{3. Dimension Matching (ì°¨ì› ì¼ì¹˜)}
$F(x)$ì™€ $x$ë¥¼ ë”í•˜ë ¤ë©´ ë‘ í–‰ë ¬ì˜ í¬ê¸°ê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Identity Block:} ì…ì¶œë ¥ í¬ê¸°ê°€ ê°™ì„ ë•Œ. ê·¸ëƒ¥ ë”í•¨.
    \item \textbf{Convolutional Block:} í¬ê¸°ê°€ ë‹¤ë¥¼ ë•Œ(Pooling ë“±). $x$ì—ë„ $1 \times 1$ Convë¥¼ ì ìš©í•´ í¬ê¸°ë¥¼ ë§ì¶°ì¤€ ë’¤ ë”í•¨.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: ResNet Block}

Keras Functional APIë¥¼ ì‚¬ìš©í•œ êµ¬í˜„ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=ResNet Identity Block]
from tensorflow.keras import layers, models

def identity_block(X, f, filters):
    """
    X: ì…ë ¥ í…ì„œ
    f: ì»¤ë„ í¬ê¸° (ì¤‘ê°„ì¸µ)
    filters: í•„í„° ê°œìˆ˜ ë¦¬ìŠ¤íŠ¸ [F1, F2, F3]
    """
    F1, F2, F3 = filters
    X_shortcut = X # ì…ë ¥ ì €ì¥ (ì§€ë¦„ê¸¸ìš©)
    
    # --- Main Path (3ê°œì˜ Conv ì¸µ) ---
    # 1. 1x1 Conv (ì°¨ì› ì¶•ì†Œ/í™•ì¥ìš©)
    X = layers.Conv2D(filters=F1, kernel_size=(1, 1), padding='valid')(X)
    X = layers.BatchNormalization()(X)
    X = layers.Activation('relu')(X)
    
    # 2. fxf Conv (ë©”ì¸ ì—°ì‚°)
    X = layers.Conv2D(filters=F2, kernel_size=(f, f), padding='same')(X)
    X = layers.BatchNormalization()(X)
    X = layers.Activation('relu')(X)

    # 3. 1x1 Conv (ì°¨ì› ë³µì›)
    X = layers.Conv2D(filters=F3, kernel_size=(1, 1), padding='valid')(X)
    X = layers.BatchNormalization()(X)
    
    # --- Skip Connection (í•µì‹¬) ---
    # Main Path ê²°ê³¼ì™€ ì§€ë¦„ê¸¸(Original X)ì„ ë”í•¨
    X = layers.Add()([X, X_shortcut])
    
    # ë”í•œ ë’¤ì— ReLU ì ìš© (ì¤‘ìš”!)
    X = layers.Activation('relu')(X)
    
    return X
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ReLU ìœ„ì¹˜ ì£¼ì˜}
Skip Connectionì„ ë”í•˜ëŠ” `Add()` ì—°ì‚°ì€ ë§ˆì§€ë§‰ ReLU **ì´ì „**ì— ìˆ˜í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
(X + Shortcut) $\rightarrow$ ReLU. ìˆœì„œê°€ ë°”ë€Œë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ResNet-50ì˜ 'Bottleneck' êµ¬ì¡°ê°€ ë­”ê°€ìš”?} \\
\textbf{A.} $1 \times 1$, $3 \times 3$, $1 \times 1$ ìˆœì„œë¡œ ìŒ“ì€ ë¸”ë¡ì…ë‹ˆë‹¤.
$1 \times 1$ë¡œ ì±„ë„ì„ ì¤„ì˜€ë‹¤ê°€(ì••ì¶•), ì—°ì‚°í•˜ê³ , ë‹¤ì‹œ ëŠ˜ë¦½ë‹ˆë‹¤. ì—°ì‚°ëŸ‰ì„ ì¤„ì´ë©´ì„œ ê¹Šì´ë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•œ í…Œí¬ë‹‰ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ResNetì„ í†µí•´ ìš°ë¦¬ëŠ” ê¹Šì´ì— ëŒ€í•œ ë‘ë ¤ì›€ì„ ê·¹ë³µí–ˆìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° ë¹„ìŠ·í•œ ì‹œê¸°ì— êµ¬ê¸€ì—ì„œëŠ” ê¹Šì´ê°€ ì•„ë‹ˆë¼ **"ë„ˆë¹„(Width)"**ì™€ **"ë‹¤ì–‘ì„±"**ì— ì§‘ì¤‘í•œ ëª¨ë¸ì„ ë‚´ë†“ì•˜ìŠµë‹ˆë‹¤. $1 \times 1$, $3 \times 3$, $5 \times 5$ í•„í„°ë¥¼ í•œ ì¸µì—ì„œ ë™ì‹œì— ì“´ë‹¤ë©´ ì–´ë–¨ê¹Œìš”?

ë‹¤ìŒ ì‹œê°„ì—ëŠ” **Network within a Network**ë¼ê³  ë¶ˆë¦¬ëŠ” $1 \times 1$ Convolutionì˜ ë§ˆë²•ê³¼, ì´ë¥¼ í™œìš©í•œ **[Inception Network]**ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Problem:} ë„ˆë¬´ ê¹Šìœ¼ë©´ í•™ìŠµì´ ì•ˆ ëœë‹¤ (Degradation).
    \item \textbf{Solution:} Skip Connection ($H(x) = F(x) + x$).
    \item \textbf{Math:} ë¯¸ë¶„ ì‹œ $+1$ í•­ì´ ìƒê²¨ ê¸°ìš¸ê¸° ì†Œì‹¤ì„ ë§‰ëŠ”ë‹¤.
    \item \textbf{Block:} ì°¨ì›ì´ ê°™ìœ¼ë©´ Identity Block, ë‹¤ë¥´ë©´ Conv Blockì„ ì“´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ Inception Network \& 1x1 Convolutions}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.4 CNN Basics, Classic Nets, ResNet \textit{- Completed}
        \item \textbf{9.5 Inception Network}
        \begin{itemize}
            \item The Magic of $1 \times 1$ Convolution
            \item Inception Module (Naive vs Optimized)
            \item Bottleneck Layer (Computational Cost Reduction)
            \item Implementation with Keras
        \end{itemize}
        \item 9.6 Object Detection Introduction \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ResNetì„ í†µí•´ ì‹ ê²½ë§ì„ "ê¹Šê²Œ(Deep)" ìŒ“ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.
ê·¸ëŸ°ë° êµ¬ê¸€ ì—°êµ¬íŒ€ì€ ì „í˜€ ë‹¤ë¥¸ ì§ˆë¬¸ì„ ë˜ì¡ŒìŠµë‹ˆë‹¤.
**"í•„í„° í¬ê¸°ë¥¼ ê¼­ í•˜ë‚˜ë§Œ ê³¨ë¼ì•¼ í•˜ë‚˜? $1 \times 1$, $3 \times 3$, $5 \times 5$ë¥¼ ë‹¤ ì“°ë©´ ì•ˆ ë˜ë‚˜?"**
ì´ ë‹¨ìˆœí•˜ê³  ë¬´ì‹í•´ ë³´ì´ëŠ” ì•„ì´ë””ì–´ì—ì„œ ì¶œë°œí•˜ì—¬, ì—°ì‚° íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•œ ì•„í‚¤í…ì²˜ê°€ ë°”ë¡œ **Inception Network**ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ ê°€ëŠ¥í•˜ê²Œ ë§Œë“  ìˆ¨ì€ ê³µì‹ ì€ **$1 \times 1$ Convolution**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ CNNì˜ íš¨ìœ¨ì„± í˜ëª…ì„ ì´ëˆ Inception êµ¬ì¡°ì™€ í•µì‹¬ ê¸°ìˆ ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{1x1 Conv:} ì±„ë„ ìˆ˜ë¥¼ ì¡°ì ˆí•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì´ëŠ” **Network in Network** ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{Inception Module:} ë‹¤ì–‘í•œ í¬ê¸°ì˜ í•„í„°ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ê³  í•©ì¹˜ëŠ” êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{Bottleneck:} $1 \times 1$ í•©ì„±ê³±ì„ í†µí•´ ì—°ì‚° ë¹„ìš©ì„ **1/10 ìˆ˜ì¤€**ìœ¼ë¡œ ì¤„ì´ëŠ” ì›ë¦¬ë¥¼ ì¦ëª…í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬í˜„:} ë³µì¡í•œ ë¶„ê¸°(Branch) êµ¬ì¡°ë¥¼ Kerasë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{í•µì‹¬ ì—­í• } \\ \hline
\textbf{$1 \times 1$ Convolution} & $1 \times 1 \times C$ í•„í„° ì—°ì‚° & **ì±„ë„ ìˆ˜ ì¡°ì ˆ (Dimensionality Reduction)**. \\ \hline
\textbf{Bottleneck Layer} & ì…ë ¥ì„ ì••ì¶•í•˜ëŠ” ì¸µ & ì—°ì‚°ëŸ‰ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì„. \\ \hline
\textbf{Inception Module} & ë³‘ë ¬ ì—°ì‚° ë¸”ë¡ & ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ì˜ íŠ¹ì§•ì„ ë™ì‹œì— ì¶”ì¶œí•¨. \\ \hline
\textbf{Concatenate} & ì´ì–´ ë¶™ì´ê¸° & ë³‘ë ¬ë¡œ ë‚˜ì˜¨ ê²°ê³¼ë“¤ì„ ì±„ë„ ì¶•ìœ¼ë¡œ í•©ì¹¨. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìˆ˜ë„ê¼­ì§€ë¥¼ ì ê°€ë¼}

\subsection{1. The Magic of $1 \times 1$ Convolution}
"êµìˆ˜ë‹˜, $1 \times 1$ í•„í„°ë©´ ê·¸ëƒ¥ ìˆ«ì í•˜ë‚˜ ê³±í•˜ëŠ” ê±° ì•„ë‹™ë‹ˆê¹Œ?"
2D ì´ë¯¸ì§€ì—ì„œëŠ” ê·¸ë ‡ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì…ì²´ì ì¸ ë³¼ë¥¨($H \times W \times C$)ì—ì„œëŠ” ë‹¤ë¦…ë‹ˆë‹¤.



\begin{itemize}
    \item **ì—°ì‚°:** $1 \times 1 \times \mathbf{192}$ (ì…ë ¥ ì±„ë„ ìˆ˜) í¬ê¸°ì˜ í•„í„°ê°€ ì…ë ¥ì„ í›‘ìŠµë‹ˆë‹¤.
    \item **íš¨ê³¼:** ì±„ë„ ë°©í–¥ìœ¼ë¡œ FC Layerë¥¼ ì ìš©í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. í•„í„° ê°œìˆ˜ë¥¼ 32ê°œë¡œ ì„¤ì •í•˜ë©´, ì¶œë ¥ ì±„ë„ì€ 32ê°œê°€ ë©ë‹ˆë‹¤. **(192 $\to$ 32 ì••ì¶•)**
\end{itemize}

\subsection{2. Bottleneck Layer (ë¹„ìš© ì ˆê°ì˜ í•µì‹¬)}
ì´ ì„¹ì…˜ì´ ì˜¤ëŠ˜ ê°•ì˜ì˜ í•˜ì´ë¼ì´íŠ¸ì…ë‹ˆë‹¤. ìˆ«ìë¡œ ì¦ëª…í•©ë‹ˆë‹¤.

\begin{mathbox}{Computational Cost Analysis}
\textbf{ìƒí™©:} ì…ë ¥ $28 \times 28 \times 192$ $\to$ ì¶œë ¥ $28 \times 28 \times 32$ (using $5 \times 5$ conv).

\textbf{1. Naive Approach (ê·¸ëƒ¥ $5 \times 5$ ì‚¬ìš©):}
$$ \text{Cost} = (28 \times 28 \times 32) \times (5 \times 5 \times 192) \approx \mathbf{120,000,000} \text{ (1.2ì–µ)} $$

\textbf{2. Bottleneck Approach ($1 \times 1$ë¡œ ì¤„ì´ê³  $5 \times 5$ ì‚¬ìš©):}
(1) $1 \times 1$ë¡œ 192ch $\to$ 16ch ì••ì¶• (ì¤‘ê°„ ë‹¨ê³„)
$$ (28 \times 28 \times 16) \times (1 \times 1 \times 192) \approx 2,400,000 $$
(2) $5 \times 5$ë¡œ 16ch $\to$ 32ch í™•ì¥ (ìµœì¢… ë‹¨ê³„)
$$ (28 \times 28 \times 32) \times (5 \times 5 \times 16) \approx 10,000,000 $$
\textbf{ì´í•©:} $2.4M + 10M = \mathbf{12.4M}$

\textbf{ê²°ê³¼:} ì—°ì‚°ëŸ‰ì´ ì•½ **1/10**ë¡œ ì¤„ì—ˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ ì €í•˜ëŠ” ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.
\end{mathbox}

---

\section{Deep Dive: Inception Module Architecture}

êµ¬ê¸€ì€ ê³ ë¯¼í–ˆìŠµë‹ˆë‹¤. "$3 \times 3$ì„ ì“¸ê¹Œ, $5 \times 5$ë¥¼ ì“¸ê¹Œ?"
ê²°ë¡ ì€ **"ê·¸ëƒ¥ ë‹¤ í•˜ì(Do them all)"**ì˜€ìŠµë‹ˆë‹¤.



\begin{itemize}
    \item **êµ¬ì¡°:** $1 \times 1$, $3 \times 3$, $5 \times 5$, Max Poolingì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    \item **ë³‘í•©:** ë‚˜ì˜¨ ê²°ê³¼ë“¤ì˜ í¬ê¸°($28 \times 28$)ë¥¼ ë§ì¶”ê³ (Padding='same'), ì±„ë„ ì¶•ìœ¼ë¡œ ì´ì–´ ë¶™ì…ë‹ˆë‹¤(Concatenate).
    \item **ìµœì í™”:** $3 \times 3$ê³¼ $5 \times 5$ ì•ì—ëŠ” ë°˜ë“œì‹œ **$1 \times 1$ ë³‘ëª© ì¸µ**ì„ ë‘ì–´ ì—°ì‚°ëŸ‰ì„ ì¤„ì…ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Inception Block with Keras}

Keras Functional APIë¥¼ ì‚¬ìš©í•´ì•¼ ë³µì¡í•œ ë¶„ê¸°(Branch) êµ¬ì¡°ë¥¼ ì§¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Inception Module Implementation]
from tensorflow.keras import layers

def inception_module(x, filters):
    """
    x: ì…ë ¥ í…ì„œ
    filters: ê° ë¶„ê¸°ë³„ í•„í„° ê°œìˆ˜ ë”•ì…”ë„ˆë¦¬
    """
    
    # Branch 1: 1x1 Conv
    path1 = layers.Conv2D(filters['f1x1'], (1, 1), padding='same', activation='relu')(x)
    
    # Branch 2: 1x1 (Reduce) -> 3x3
    path2 = layers.Conv2D(filters['f3x3_reduce'], (1, 1), padding='same', activation='relu')(x)
    path2 = layers.Conv2D(filters['f3x3'], (3, 3), padding='same', activation='relu')(path2)
    
    # Branch 3: 1x1 (Reduce) -> 5x5
    path3 = layers.Conv2D(filters['f5x5_reduce'], (1, 1), padding='same', activation='relu')(x)
    path3 = layers.Conv2D(filters['f5x5'], (5, 5), padding='same', activation='relu')(path3)
    
    # Branch 4: MaxPool -> 1x1
    path4 = layers.MaxPooling2D((3, 3), strides=(1, 1), padding='same')(x)
    path4 = layers.Conv2D(filters['pool_proj'], (1, 1), padding='same', activation='relu')(path4)
    
    # ë³‘í•© (Concatenate)
    output = layers.concatenate([path1, path2, path3, path4], axis=3)
    
    return output
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ì •ë³´ë¥¼ ì••ì¶•í–ˆë‹¤ê°€ ëŠ˜ë ¤ë„ ì†ì‹¤ì´ ì—†ë‚˜ìš”?} \\
\textbf{A.} ë„¤, ê´œì°®ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ ë°ì´í„°ì—ëŠ” **ì¤‘ë³µì„±(Redundancy)**ì´ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. $1 \times 1$ ì¸µì€ í•„ìš”í•œ í•µì‹¬ ì •ë³´ë§Œ ì••ì¶•(Linear Combination)í•´ì„œ ë‹¤ìŒ ì¸µì— ë„˜ê²¨ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

\textbf{Q. Max Pooling ë’¤ì— ì™œ $1 \times 1$ Convë¥¼ ë¶™ì´ë‚˜ìš”?} \\
\textbf{A.} í’€ë§ì€ ì±„ë„ ìˆ˜ë¥¼ ì¤„ì´ì§€ ëª»í•©ë‹ˆë‹¤. ì¸ì…‰ì…˜ ëª¨ë“ˆì„ ê±°ì¹  ë•Œë§ˆë‹¤ ì±„ë„ì´ ê³„ì† ëŠ˜ì–´ë‚˜ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´, í’€ë§ ë’¤ì— $1 \times 1$ì„ ë¶™ì—¬ ì±„ë„ ìˆ˜ë¥¼ ê°•ì œë¡œ ì¤„ì—¬ì¤ë‹ˆë‹¤(Projection).

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” ì´ì œ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜(Classification)í•˜ëŠ” ìµœê³ ì˜ ì•„í‚¤í…ì²˜ë“¤ì„ ëª¨ë‘ ì„­ë µí–ˆìŠµë‹ˆë‹¤ (VGG, ResNet, Inception).

í•˜ì§€ë§Œ í˜„ì‹¤ ì„¸ê³„ì—ì„œëŠ” ì´ë¯¸ì§€ì— ë¬´ì—‡ì´ ìˆëŠ”ì§€ë§Œ ì•„ëŠ” ê²ƒìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. **"ê·¸ ë¬¼ì²´ê°€ ì–´ë””ì— ìˆëŠ”ì§€(ìœ„ì¹˜)"**ë„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì»´í“¨í„° ë¹„ì „ì˜ ê½ƒ, **[Object Detection]**ìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. ê·¸ì¤‘ì—ì„œë„ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ í˜ëª…, **YOLO (You Only Look Once)** ì•Œê³ ë¦¬ì¦˜ì„ í–¥í•œ ì—¬ì •ì„ ì‹œì‘í•´ ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{$1 \times 1$ Conv:} ì±„ë„ ìˆ˜ë¥¼ ì¤„ì—¬ ì—°ì‚°ëŸ‰ì„ ì•„ë¼ëŠ” í•µì‹¬ ë„êµ¬.
    \item \textbf{Inception:} ì—¬ëŸ¬ í•„í„°ë¥¼ ë³‘ë ¬ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ íŠ¹ì§•ì„ ë™ì‹œì— ì¡ëŠ”ë‹¤.
    \item \textbf{Bottleneck:} í° í•„í„° ì•ì— $1 \times 1$ì„ ë‘ì–´ ì…ë ¥ì„ ì••ì¶•í•œë‹¤. (ë¹„ìš© 1/10 ì ˆê°)
    \item \textbf{Concatenate:} ë³‘ë ¬ ì—°ì‚° ê²°ê³¼ë¥¼ ì±„ë„ ì¶•ìœ¼ë¡œ í•©ì¹œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ Object Detection \& Sliding Windows}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.5 CNN Basics, Classic Nets, ResNet, Inception \textit{- Completed}
        \item \textbf{9.6 Object Detection Introduction}
        \begin{itemize}
            \item Classification vs Detection
            \item Sliding Windows Algorithm (The Old Way)
            \item \textbf{Convolutional Implementation (The Fast Way)}
        \end{itemize}
        \item 9.7 YOLO Algorithm (You Only Look Once) \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ "ì´ ì´ë¯¸ì§€ê°€ ê³ ì–‘ì´ì¸ê°€?"ë¥¼ ë§ì¶”ëŠ” **ë¶„ë¥˜(Classification)** ë¬¸ì œë¥¼ í’€ì—ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ììœ¨ì£¼í–‰ì°¨ë¼ë©´ ì–´ë–¨ê¹Œìš”? "ì „ë°©ì— ì°¨ê°€ ìˆë‹¤"ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. **"ì „ë°© 50m ì™¼ìª½ ì°¨ì„ ì— ìˆë‹¤"**ëŠ” ìœ„ì¹˜ ì •ë³´ê°€ í•„ìš”í•˜ë©°, ë™ì‹œì— ë³´í–‰ì, ì‹ í˜¸ë“±ë„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
ì´ê²ƒì´ **ê°ì²´ íƒì§€(Object Detection)**ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ê·¸ ì‹œì´ˆì¸ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì•Œê³ ë¦¬ì¦˜ê³¼, ì´ë¥¼ íšê¸°ì ìœ¼ë¡œ ê°€ì†í™”í•œ í•©ì„±ê³± êµ¬í˜„ë²•ì„ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ê°ì²´ íƒì§€ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì†ë„ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” í•µì‹¬ ê¸°ìˆ ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê°œë…:} ë¶„ë¥˜(Classification)ì™€ íƒì§€(Detection)ì˜ ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ê³ ì „:} ìœˆë„ìš°ë¥¼ ì´ë™ì‹œí‚¤ë©° ì°¾ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ì˜ í•œê³„ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{í˜ì‹ :} FC ì¸µì„ Conv ì¸µìœ¼ë¡œ ë³€í™˜í•˜ì—¬, **ë‹¨ í•œ ë²ˆì˜ ì—°ì‚°**ìœ¼ë¡œ ëª¨ë“  ìœˆë„ìš°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì„ ìµí™ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì§ˆë¬¸} & \textbf{ì¶œë ¥ ì˜ˆì‹œ} \\ \hline
\textbf{Classification} & ë¬´ì—‡ì¸ê°€? & Cat (1) \\ \hline
\textbf{Localization} & ë¬´ì—‡ì´ê³  ì–´ë””ì— ìˆëŠ”ê°€? (ë‹¨ì¼ ê°ì²´) & Cat, $b_x, b_y, b_h, b_w$ \\ \hline
\textbf{Detection} & ë¬´ì—‡ë“¤ì´ ê°ê° ì–´ë””ì— ìˆëŠ”ê°€? (ë‹¤ì¤‘ ê°ì²´) & Cat(x,y..), Dog(x,y..) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì°¾ì„ ë•Œê¹Œì§€ ë’¤ì§„ë‹¤}

\subsection{1. Sliding Windows Algorithm (Basic)}


ê°€ì¥ ì›ì‹œì ì¸ ë°©ë²•ì…ë‹ˆë‹¤.
1. ì´ë¯¸ì§€ ì™¼ìª½ ìƒë‹¨ë¶€í„° ì‘ì€ ìœˆë„ìš°ë¥¼ ì˜ë¼ëƒ…ë‹ˆë‹¤.
2. ì˜ë¼ë‚¸ ì´ë¯¸ì§€ë¥¼ ConvNetì— ë„£ì–´ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
3. ì˜†ìœ¼ë¡œ í•œ ì¹¸ ì´ë™(Stride)í•´ì„œ ë°˜ë³µí•©ë‹ˆë‹¤.
4. ë‹¤ ëë‚˜ë©´ ìœˆë„ìš° í¬ê¸°ë¥¼ í‚¤ì›Œì„œ ë‹¤ì‹œ ì²˜ìŒë¶€í„° í•©ë‹ˆë‹¤.

\begin{warningbox}{ì¹˜ëª…ì  ë‹¨ì : ì†ë„}
ì´ ë°©ì‹ì€ ê³„ì‚° ë¹„ìš©ì´ í­ë°œí•©ë‹ˆë‹¤.
ì‘ì€ ìŠ¤íŠ¸ë¼ì´ë“œ $\rightarrow$ ìˆ˜ë§Œ ë²ˆ ConvNet ì‹¤í–‰ $\rightarrow$ **ë„ˆë¬´ ëŠë¦¼.**
í° ìŠ¤íŠ¸ë¼ì´ë“œ $\rightarrow$ ë“¬ì„±ë“¬ì„± ë´„ $\rightarrow$ **ì •í™•ë„ í•˜ë½.**
\end{warningbox}

---

\section{Deep Dive: Convolutional Implementation}

"ì–´ë–»ê²Œ í•˜ë©´ for-loop ì—†ì´ í•œ ë²ˆì— ì²˜ë¦¬í• ê¹Œ?"
ì´ ì„¹ì…˜ì´ ì˜¤ëŠ˜ ê°•ì˜ì˜ í•˜ì´ë¼ì´íŠ¸ì…ë‹ˆë‹¤.

\subsection{1. Turning FC into Conv layers}
ì „í†µì ì¸ CNNì˜ ë§ˆì§€ë§‰ì€ í‰íƒ„í™”(Flatten) í›„ FC ì¸µì´ì—ˆìŠµë‹ˆë‹¤. ì´ë¥¼ **$1 \times 1$ Conv ì¸µ**ìœ¼ë¡œ ë°”ê¿‰ë‹ˆë‹¤.

\begin{itemize}
    \item **ê¸°ì¡´:** $5 \times 5 \times 16 \xrightarrow{Flatten} 400 \xrightarrow{FC} 400$
    \item **ë³€í™˜:** $5 \times 5 \times 16 \xrightarrow{Conv(5\times5, 400)} \mathbf{1 \times 1 \times 400}$
\end{itemize}
ìˆ˜í•™ì ìœ¼ë¡œ ê°’ì€ ì™„ë²½íˆ ë™ì¼í•˜ì§€ë§Œ, ì´ì œëŠ” **ê³µê°„ì  ìœ„ì¹˜ ì •ë³´**ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.

\subsection{2. Running on the Whole Image}
ì´ì œ ì´ë¯¸ì§€ë¥¼ ìë¥´ì§€ ì•Šê³  í†µì§¸ë¡œ ë„£ìŠµë‹ˆë‹¤.

\begin{mathbox}{ì—°ì‚° ê³µìœ ì˜ ë§ˆë²•}
í•™ìŠµ ëª¨ë¸ ì…ë ¥ì´ $14 \times 14$ë¼ê³  ê°€ì •í•©ì‹œë‹¤.
í…ŒìŠ¤íŠ¸ ë•Œ $16 \times 16$ ì´ë¯¸ì§€ë¥¼ í†µì§¸ë¡œ ë„£ìœ¼ë©´ ì–´ë–»ê²Œ ë ê¹Œìš”?

\begin{itemize}
    \item **ê¸°ì¡´:** 4ë²ˆ ì˜ë¼ì„œ 4ë²ˆ ì‹¤í–‰.
    \item **Conv ë°©ì‹:** $16 \times 16$ ì´ë¯¸ì§€ê°€ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µê³¼í•˜ë©´, ìµœì¢… ì¶œë ¥ì´ **$2 \times 2$ í¬ê¸°**ë¡œ ë‚˜ì˜µë‹ˆë‹¤.
    \item **í•´ì„:** $(0,0)$ì€ ì¢Œìƒë‹¨ ìœˆë„ìš° ê²°ê³¼, $(0,1)$ì€ ìš°ìƒë‹¨ ìœˆë„ìš° ê²°ê³¼ì…ë‹ˆë‹¤.
    \item **ê²°ê³¼:** **ê³µí†µ ì˜ì—­ì˜ ì—°ì‚°ì„ ê³µìœ **í•˜ë¯€ë¡œ ì†ë„ê°€ ìˆ˜ì‹­ ë°° ë¹¨ë¼ì§‘ë‹ˆë‹¤.
\end{itemize}
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Fully Convolutional Network}

Kerasë¥¼ ì´ìš©í•´ FC ì¸µì„ Conv ì¸µìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Fully Convolutional Model]
import tensorflow as tf
from tensorflow.keras import layers, models

def create_fcn_model(input_shape, num_classes):
    inputs = layers.Input(shape=input_shape)
    
    # Feature Extractor
    x = layers.Conv2D(16, (5, 5), activation='relu')(inputs)
    x = layers.MaxPooling2D((2, 2), strides=2)(x)
    x = layers.Conv2D(32, (5, 5), activation='relu')(x)
    x = layers.MaxPooling2D((2, 2), strides=2)(x)
    
    # --- í•µì‹¬ ë³€í™˜ ë¶€ë¶„ ---
    # Flatten ëŒ€ì‹  Conv2D ì‚¬ìš©
    # ë§ˆì§€ë§‰ íŠ¹ì„±ë§µ í¬ê¸°ê°€ 5x5ë¼ê³  ê°€ì •í•  ë•Œ, 5x5 ì»¤ë„ ì‚¬ìš©
    x = layers.Conv2D(256, (5, 5), activation='relu')(x) 
    
    # ë§ˆì§€ë§‰ ë¶„ë¥˜ê¸° (1x1 Conv)
    outputs = layers.Conv2D(num_classes, (1, 1), activation='softmax')(x)
    
    model = models.Model(inputs, outputs)
    return model

# --- ì‹¤í–‰ ---
if __name__ == "__main__":
    # 1. í•™ìŠµìš© (ì‘ì€ ì´ë¯¸ì§€)
    train_model = create_fcn_model((28, 28, 3), 4)
    print(train_model.output_shape) # (None, 1, 1, 4)
    
    # 2. í…ŒìŠ¤íŠ¸ìš© (í° ì´ë¯¸ì§€ í†µì§¸ë¡œ ì…ë ¥)
    test_input = tf.random.normal((1, 32, 32, 3))
    test_output = train_model(test_input)
    print(test_output.shape) # (1, 2, 2, 4) -> 4ê°œ ìœˆë„ìš° ê²°ê³¼ ë™ì‹œ ì¶œë ¥
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ì´ë ‡ê²Œ í•˜ë©´ ë°”ìš´ë”© ë°•ìŠ¤ ìœ„ì¹˜ê°€ ì •í™•í•œê°€ìš”?} \\
\textbf{A.} **ì•„ë‹ˆìš”.** ìœˆë„ìš°ê°€ ê³ ì •ëœ ê°„ê²©(Stride)ìœ¼ë¡œë§Œ ì›€ì§ì´ê¸° ë•Œë¬¸ì—, ê°ì²´ê°€ ìœˆë„ìš° ì‚¬ì´ì— ê±¸ì³ ìˆê±°ë‚˜ í¬ê¸°ê°€ ì•ˆ ë§ìœ¼ë©´ ì •í™•íˆ ì¡ì•„ë‚´ì§€ ëª»í•©ë‹ˆë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒ ì‹œê°„ì— ë°°ìš¸ **YOLO**ê°€ í•„ìš”í•©ë‹ˆë‹¤.

\textbf{Q. ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°ê°€ ê³„ì† ë°”ë€Œì–´ë„ ë˜ë‚˜ìš”?} \\
\textbf{A.} ë„¤, Fully Convolutional NetworkëŠ” ê³ ì •ëœ í¬ê¸°ì˜ FC ì¸µì´ ì—†ìœ¼ë¯€ë¡œ ì…ë ¥ í¬ê¸°ì— ì œí•œì´ ì—†ìŠµë‹ˆë‹¤. ì…ë ¥ì´ ì»¤ì§€ë©´ ì¶œë ¥ ë§µ($H \times W$)ë„ ì»¤ì§ˆ ë¿ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ë¥¼ ë¹ ë¥´ê²Œ ë§Œë“œëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ì „íˆ **"ì •í™•í•œ ë°•ìŠ¤ ìœ„ì¹˜"**ë¥¼ ì¡ì§€ ëª»í•˜ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.

ê°ì²´ê°€ ì–´ë””ì— ìˆë“  ì •í™•í•˜ê²Œ ë°•ìŠ¤ë¥¼ ì³ì£¼ê³ (Regression), ì‹¬ì§€ì–´ í•˜ë‚˜ì˜ ì…€ì—ì„œ ì—¬ëŸ¬ ê°ì²´ë¥¼ ë™ì‹œì— ì°¾ì•„ë‚´ëŠ” ì‹¤ì‹œê°„ íƒì§€ì˜ ëíŒì™•.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” **[YOLO (You Only Look Once)]** ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ **IOU**ì™€ **Non-max Suppression**ì˜ ê°œë…ì„ ì •ë³µí•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Detection:} ë¬´ì—‡ì´(What) ì–´ë””ì—(Where) ìˆëŠ”ì§€ ì°¾ëŠ” ë¬¸ì œ.
    \item \textbf{Sliding Window:} ìœˆë„ìš°ë¥¼ ì´ë™í•˜ë©° ì°¾ìŒ. ë„ˆë¬´ ëŠë¦¼.
    \item \textbf{Conv Implementation:} FC ì¸µì„ Conv ì¸µìœ¼ë¡œ ë°”ê¾¸ë©´ ì—°ì‚°ì„ ê³µìœ í•  ìˆ˜ ìˆë‹¤.
    \item \textbf{Result:} í° ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆë§Œ í†µê³¼ì‹œí‚¤ë©´(One pass) ëª¨ë“  ìœˆë„ìš° ê²°ê³¼ê°€ ë‚˜ì˜¨ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì •ì˜)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ YOLO Algorithm (Object Detection)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.6 CNN Basics \& Sliding Windows \textit{- Completed}
        \item \textbf{9.7 YOLO Algorithm (You Only Look Once)}
        \begin{itemize}
            \item The Grid System & Bounding Box Regression
            \item IoU (Intersection over Union) Metric
            \item Non-max Suppression (Removing Duplicates)
            \item Anchor Boxes (Handling Overlap)
        \end{itemize}
    \end{itemize}
    \item[Chapter 10.] Sequence Models (RNN) \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ëŠ” í•©ì„±ê³± êµ¬í˜„ìœ¼ë¡œ ì†ë„ëŠ” ë¹¨ë¼ì¡Œì§€ë§Œ, ì—¬ì „íˆ **"ë°•ìŠ¤ ìœ„ì¹˜ê°€ ë¶€ì •í™•í•˜ë‹¤"**ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìœˆë„ìš°ê°€ ê³ ì •ëœ ê°„ê²©ìœ¼ë¡œë§Œ ì›€ì§ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
"ê°ì²´ì˜ ì¤‘ì‹¬ì„ ì°¾ê³ , ê·¸ ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ ë°•ìŠ¤ í¬ê¸°ë¥¼ ì˜ˆì¸¡í•˜ë©´ ì–´ë–¨ê¹Œ?"
ì´ ì•„ì´ë””ì–´ë¡œ íƒ„ìƒí•œ ê²ƒì´ **YOLO**ì…ë‹ˆë‹¤. ì´ë¦„ì²˜ëŸ¼ ì´ë¯¸ì§€ë¥¼ ë‹¨ í•œ ë²ˆë§Œ ë³´ê³ (Look Once), ëª¨ë“  ê°ì²´ì˜ ìœ„ì¹˜ì™€ ì¢…ë¥˜ë¥¼ ë™ì‹œì— ì°¾ì•„ë‚´ëŠ” í˜ì‹ ì ì¸ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ í‘œì¤€ì¸ YOLO ì•Œê³ ë¦¬ì¦˜ì˜ ì›ë¦¬ë¥¼ ì™„ë²½íˆ ì´í•´í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê·¸ë¦¬ë“œ:} ì´ë¯¸ì§€ë¥¼ ê²©ìë¡œ ë‚˜ëˆ„ê³ , ê°ì²´ ì¤‘ì‹¬ì ì´ ì†í•œ ì…€ì´ ì±…ì„ì„ ì§€ëŠ” êµ¬ì¡°ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{IoU:} ë‘ ë°•ìŠ¤ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” í‰ê°€ ì§€í‘œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    \item \textbf{NMS:} ì¤‘ë³µëœ ë°•ìŠ¤ë¥¼ ì œê±°í•˜ëŠ” ë¹„ìµœëŒ€ ì–µì œ(Non-max Suppression) ì•Œê³ ë¦¬ì¦˜ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{ì•µì»¤:} ê²¹ì¹œ ë¬¼ì²´ë¥¼ ë¶„ë¦¬í•˜ëŠ” ì•µì»¤ ë°•ìŠ¤(Anchor Box) ê°œë…ì„ íŒŒì•…í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì•½ì–´} & \textbf{ì„¤ëª…} \\ \hline
\textbf{Grid Cell} & - & ì´ë¯¸ì§€ë¥¼ $S \times S$ë¡œ ë‚˜ëˆˆ ì‘ì€ êµ¬ì—­. \\ \hline
\textbf{IoU} & Intersection over Union & êµì§‘í•© ì˜ì—­ / í•©ì§‘í•© ì˜ì—­. (ì¼ì¹˜ë„) \\ \hline
\textbf{NMS} & Non-max Suppression & ê°€ì¥ í™•ì‹¤í•œ ë°•ìŠ¤ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì§€ì›€. \\ \hline
\textbf{Anchor Box} & - & ë¯¸ë¦¬ ì •ì˜ëœ ë°•ìŠ¤ ëª¨ì–‘. (ê¸¸ì­‰í•œ ì‚¬ëŒ, ë„“ì€ ì°¨ ë“±) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë‹¨ í•œ ë²ˆì˜ ì¶”ë¡ }

\subsection{1. Bounding Box Predictions (ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ)}


YOLOëŠ” ì´ë¯¸ì§€ë¥¼ $S \times S$ ê·¸ë¦¬ë“œ(ë³´í†µ $19 \times 19$)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
ê° ì…€ì€ ë‹¤ìŒ ë²¡í„° $y$ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
$$ y = [p_c, b_x, b_y, b_h, b_w, c_1, c_2, \dots]^T $$
\begin{itemize}
    \item $p_c$: ê°ì²´ê°€ ìˆì„ í™•ë¥  (Confidence).
    \item $b_x, b_y$: ë°•ìŠ¤ ì¤‘ì‹¬ ì¢Œí‘œ (ì…€ ë‚´ ìƒëŒ€ ìœ„ì¹˜, 0~1).
    \item $b_h, b_w$: ë°•ìŠ¤ ë†’ì´/ë„ˆë¹„ (ì „ì²´ ì´ë¯¸ì§€ ëŒ€ë¹„ ë¹„ìœ¨).
    \item $c_i$: í´ë˜ìŠ¤ í™•ë¥  (ì°¨, ì‚¬ëŒ ë“±).
\end{itemize}

\subsection{2. IoU (Intersection over Union)}


ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë°•ìŠ¤ê°€ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ í‰ê°€í•˜ëŠ” ì²™ë„ì…ë‹ˆë‹¤.
\begin{formulabox}{IoU ìˆ˜ì‹}
$$ \text{IoU} = \frac{\text{êµì§‘í•© ì˜ì—­ (Intersection)}}{\text{í•©ì§‘í•© ì˜ì—­ (Union)}} $$
\begin{itemize}
    \item ë³´í†µ $\text{IoU} \ge 0.5$ ì´ë©´ "ì˜¬ë°”ë¥¸ íƒì§€"ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
    \item 1ì´ë©´ ì™„ë²½í•˜ê²Œ ì¼ì¹˜, 0ì´ë©´ ì „í˜€ ê²¹ì¹˜ì§€ ì•ŠìŒ.
\end{itemize}
\end{formulabox}

\subsection{3. Anchor Boxes (ê²¹ì¹œ ë¬¼ì²´ í•´ê²°)}
í•œ ì…€ì˜ ì¤‘ì‹¬ì— ì‚¬ëŒê³¼ ì°¨ê°€ ê²¹ì³ ìˆë‹¤ë©´?
ê¸°ì¡´ ë²¡í„°ë¡œëŠ” í•˜ë‚˜ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ì–‘(ì•µì»¤)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
$$ y = [\text{Anchor 1}, \text{Anchor 2}] $$
\begin{itemize}
    \item **Anchor 1 (ì„¸ë¡œë¡œ ê¸´):** ì‚¬ëŒ ë‹´ë‹¹.
    \item **Anchor 2 (ê°€ë¡œë¡œ ë„“ì€):** ìë™ì°¨ ë‹´ë‹¹.
\end{itemize}

---

\section{Deep Dive: Non-max Suppression (NMS)}

YOLOëŠ” í•˜ë‚˜ì˜ ê°ì²´ì— ëŒ€í•´ ì—¬ëŸ¬ ì…€ì´ "ë‚´ê°€ ì°¾ì•˜ë‹¤!"ë©° ë°•ìŠ¤ë¥¼ ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ë³µì„ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item **Filter:** $p_c < 0.6$ ì¸ ë°•ìŠ¤(í™•ì‹  ì—†ëŠ” ê²ƒ)ëŠ” ëª¨ë‘ ë²„ë¦½ë‹ˆë‹¤.
    \item **Select:** ë‚¨ì€ ë°•ìŠ¤ ì¤‘ $p_c$ê°€ ê°€ì¥ ë†’ì€ ê²ƒì„ ì„ íƒí•©ë‹ˆë‹¤ (Best Box).
    \item **Suppress:** ì„ íƒëœ ë°•ìŠ¤ì™€ $\text{IoU} \ge 0.5$ ì¸(ë§ì´ ê²¹ì¹œ) ë‹¤ë¥¸ ë°•ìŠ¤ë“¤ì€ "ê°™ì€ ë¬¼ì²´ë¥¼ ì¤‘ë³µ íƒì§€í•œ ê²ƒ"ìœ¼ë¡œ ë³´ê³  ì§€ì›ë‹ˆë‹¤.
    \item **Repeat:** ë°•ìŠ¤ê°€ ë‹¤ ì •ë¦¬ë  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
\end{enumerate}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: IoU Calculation}

IoU ê³„ì‚°ì€ ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ì™€ NMS êµ¬í˜„ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=IoU Calculation Function]
def calculate_iou(box1, box2):
    """
    box: (x1, y1, x2, y2)ì¢Œí‘œ (ì¢Œìƒë‹¨, ìš°í•˜ë‹¨)
    """
    (b1_x1, b1_y1, b1_x2, b1_y2) = box1
    (b2_x1, b2_y1, b2_x2, b2_y2) = box2
    
    # 1. êµì§‘í•©(Intersection) ì¢Œí‘œ ê³„ì‚°
    xi1 = max(b1_x1, b2_x1)
    yi1 = max(b1_y1, b2_y1)
    xi2 = min(b1_x2, b2_x2)
    yi2 = min(b1_y2, b2_y2)
    
    # ë„“ì´ (ìŒìˆ˜ë©´ 0)
    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    
    # 2. í•©ì§‘í•©(Union) ë„“ì´ ê³„ì‚°
    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union_area = b1_area + b2_area - inter_area
    
    # 3. IoU
    return inter_area / (union_area + 1e-6)

# --- í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    box_a = (1, 1, 3, 3) # ë©´ì  4
    box_b = (2, 2, 4, 4) # ë©´ì  4, êµì§‘í•© 1
    # í•©ì§‘í•© = 4 + 4 - 1 = 7
    # IoU = 1/7 = 0.1428...
    
    print(f"IoU: {calculate_iou(box_a, box_b):.4f}")
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ì¢Œí‘œê³„ ì£¼ì˜}
YOLOì˜ ì¶œë ¥ $b_x, b_y$ëŠ” **ê·¸ë¦¬ë“œ ì…€ ë‚´ë¶€ì—ì„œì˜ ìƒëŒ€ ìœ„ì¹˜(0~1)**ì…ë‹ˆë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ ìœ„ì— ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ë ¤ë©´, ì…€ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤)ë¥¼ ë”í•˜ê³  ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ê³±í•´ì£¼ëŠ” ë³€í™˜ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì•µì»¤ ë°•ìŠ¤ í¬ê¸°ëŠ” ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µ í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” ê°ì²´ë“¤ì˜ ì‹¤ì œ ë°•ìŠ¤ í¬ê¸°ë¥¼ ëª¨ì•„ì„œ **K-Means í´ëŸ¬ìŠ¤í„°ë§**ì„ ëŒë¦½ë‹ˆë‹¤. ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ëŒ€í‘œì ì¸ ëª¨ì–‘ 5~9ê°œë¥¼ ì„ ì •í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤ (YOLO v2ë¶€í„° ì ìš©).

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ê²ƒìœ¼ë¡œ ì»´í“¨í„° ë¹„ì „(CNN) íŒŒíŠ¸ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ì€ ì •ì§€ëœ ì´ë¯¸ì§€ì—ì„œ ì‚¬ë¬¼ì„ ë¶„ë¥˜í•˜ê³  ìœ„ì¹˜ê¹Œì§€ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ ì„ ìŠµë“í–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ì„¸ìƒì€ ë©ˆì¶° ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ íŠœë¸Œ ì˜ìƒ, ìŒì„± ì¸ì‹, ì£¼ê°€ ì˜ˆì¸¡, ë²ˆì—­ ë“±ì€ **ì‹œê°„ì˜ íë¦„(Sequence)**ì´ ìˆëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ë¶€í„°ëŠ” **[Part 5. Sequence Models]**ì˜ ì„¸ê³„ë¡œ ë– ë‚©ë‹ˆë‹¤. ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹ ê²½ë§, **RNN (Recurrent Neural Networks)**ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{YOLO:} ê·¸ë¦¬ë“œ ì…€ë§ˆë‹¤ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ íšŒê·€(Regression)ë¡œ ì§ì ‘ ì˜ˆì¸¡í•œë‹¤.
    \item \textbf{IoU:} êµì§‘í•©/í•©ì§‘í•©. ë°•ìŠ¤ ìœ„ì¹˜ ì •í™•ë„ì˜ ì²™ë„ì´ì NMSì˜ ê¸°ì¤€.
    \item \textbf{NMS:} ì¤‘ë³µëœ ë°•ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ ê°ì²´ë‹¹ í•˜ë‚˜ì˜ ë°•ìŠ¤ë§Œ ë‚¨ê¸´ë‹¤.
    \item \textbf{Anchor:} ë‹¤ì–‘í•œ ë¹„ìœ¨ì˜ ê°ì²´ë¥¼ ì¡ê¸° ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë°•ìŠ¤ ëª¨ì–‘ì„ ì“´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì¦ëª…)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Special Applications: \\ Face Recognition \& One-shot Learning}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] CNN Foundations \& Architectures \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Special Applications (Current Unit)}
    \begin{itemize}
        \item \textbf{10.1 Face Recognition}
        \begin{itemize}
            \item The One-shot Learning Problem
            \item Siamese Network Architecture
            \item Triplet Loss Function
            \item Binary Classification Alternative
        \end{itemize}
        \item 10.2 Neural Style Transfer \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ê¹Œì§€ ìš°ë¦¬ëŠ” YOLOë¥¼ í†µí•´ ê°ì²´ì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.
ì˜¤ëŠ˜ì€ ê·¸ë³´ë‹¤ ë” ê¹Œë‹¤ë¡œìš´ ë¬¸ì œì¸ **"ì´ ì‚¬ëŒì´ ëˆ„êµ¬ì¸ê°€?"**ë¥¼ êµ¬ë³„í•˜ëŠ” ì–¼êµ´ ì¸ì‹ì— ë„ì „í•©ë‹ˆë‹¤.
ìš°ë¦¬ëŠ” ìŠ¤ë§ˆíŠ¸í°ì„ ì‚´ ë•Œ ì–¼êµ´ì„ í•œ ë²ˆë§Œ ë“±ë¡í•©ë‹ˆë‹¤. ê·¸ëŸ°ë° ë”¥ëŸ¬ë‹ì€ ë³´í†µ ìˆ˜ì²œ ì¥ì˜ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì–´ë–»ê²Œ ë‹¨ í•œ ì¥ì˜ ì‚¬ì§„ë§Œìœ¼ë¡œ ì£¼ì¸ì„ ì•Œì•„ë³¼ê¹Œìš”? ê¸°ì¡´ ìƒì‹ì„ ê¹¨ëŠ” **One-shot Learning**ê³¼ **Siamese Network**ì˜ ë¹„ë°€ì„ íŒŒí—¤ì³ ë´…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë°ì´í„°ê°€ ê·¹ë„ë¡œ ì ì€ ìƒí™©ì—ì„œ ì‘ë™í•˜ëŠ” ì–¼êµ´ ì¸ì‹ ì‹œìŠ¤í…œì˜ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{One-shot Learning:} ë‹¨ í•œ ì¥ì˜ ë°ì´í„°ë¡œ í•™ìŠµí•˜ëŠ” ë¬¸ì œë¥¼ 'ë¶„ë¥˜'ê°€ ì•„ë‹Œ 'ìœ ì‚¬ë„ ì¸¡ì •'ìœ¼ë¡œ í’‰ë‹ˆë‹¤.
    \item \textbf{Siamese Network:} ë‘ ì´ë¯¸ì§€ë¥¼ ê°™ì€ ë„¤íŠ¸ì›Œí¬ì— í†µê³¼ì‹œì¼œ ê±°ë¦¬(Distance)ë¥¼ ê³„ì‚°í•˜ëŠ” êµ¬ì¡°ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{Triplet Loss:} Anchor, Positive, Negative ì„¸ ì¥ì˜ ì‚¬ì§„ì„ ì´ìš©í•œ í•™ìŠµ ë°©ë²•ì„ ìˆ˜í•™ì ìœ¼ë¡œ ìœ ë„í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{í•µì‹¬ ì—­í• } \\ \hline
\textbf{One-shot Learning} & í•œ ë²ˆë§Œ ë³´ê³  ë°°ìš°ê¸° & ë°ì´í„°ê°€ ì ì€ ë¬¸ì œ í•´ê²°. \\ \hline
\textbf{Siamese Network} & ìƒ´ ë„¤íŠ¸ì›Œí¬ & ë‘ ì…ë ¥ì´ ê°™ì€ ê°€ì¤‘ì¹˜($W$)ë¥¼ ê³µìœ í•¨. \\ \hline
\textbf{Encoding} & $f(x)$ & ì´ë¯¸ì§€ë¥¼ 128ì°¨ì› ë“±ì˜ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜. \\ \hline
\textbf{Triplet Loss} & ì„¸ ìŒ ì†ì‹¤ í•¨ìˆ˜ & A-PëŠ” ê°€ê¹ê²Œ, A-Nì€ ë©€ê²Œ ë§Œë“¦. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë¶„ë¥˜ê°€ ì•„ë‹ˆë¼ ë¹„êµë‹¤}

\subsection{1. The Challenge (One-shot Learning)}
ì§ì› 1,000ëª…ì˜ ì¶œì… í†µì œ ì‹œìŠ¤í…œì„ ë§Œë“ ë‹¤ê³  ê°€ì •í•©ì‹œë‹¤.
\begin{itemize}
    \item **Softmax (ì‹¤íŒ¨):** 1,000ê°œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜. ì‹ ì… ì‚¬ì›ì´ ì˜¤ë©´ ë„¤íŠ¸ì›Œí¬ ì „ì²´ë¥¼ ì¬í•™ìŠµí•´ì•¼ í•©ë‹ˆë‹¤. (í™•ì¥ì„± 0ì )
    \item **Similarity (ì„±ê³µ):** ë‘ ì‚¬ì§„ì„ ë¹„êµí•˜ì—¬ **ê±°ë¦¬(Distance) $d$**ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    \begin{itemize}
        \item $d(\text{img1}, \text{img2}) \le \tau$: ê°™ì€ ì‚¬ëŒ (ë¬¸ ì—´ë¦¼)
        \item $d(\text{img1}, \text{img2}) > \tau$: ë‹¤ë¥¸ ì‚¬ëŒ (ê±°ë¶€)
    \end{itemize}
    ì‹ ì… ì‚¬ì›ì´ ì˜¤ë©´ ì‚¬ì§„ë§Œ DBì— ì¶”ê°€í•˜ë©´ ë©ë‹ˆë‹¤. ì¬í•™ìŠµì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
\end{itemize}

\subsection{2. Siamese Network (ìƒ´ ë„¤íŠ¸ì›Œí¬)}
[Image of Siamese network architecture with two shared CNNs feeding into distance calculation]

ë‘ ê°œì˜ ë˜‘ê°™ì€ ë„¤íŠ¸ì›Œí¬ê°€ ë¨¸ë¦¬(ê°€ì¤‘ì¹˜)ë¥¼ ê³µìœ í•©ë‹ˆë‹¤.
\begin{enumerate}
    \item ë‘ ì´ë¯¸ì§€ $x^{(1)}, x^{(2)}$ë¥¼ ê°ê° CNNì— ë„£ìŠµë‹ˆë‹¤.
    \item ë§ˆì§€ë§‰ FC ì¸µì—ì„œ ë‚˜ì˜¨ ë²¡í„°(ì¸ì½”ë”©) $f(x^{(1)}), f(x^{(2)})$ë¥¼ ì–»ìŠµë‹ˆë‹¤.
    \item ë‘ ë²¡í„° ì‚¬ì´ì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    $$ d(x^{(1)}, x^{(2)}) = || f(x^{(1)}) - f(x^{(2)}) ||^2 $$
\end{enumerate}

---

\section{Deep Dive: Triplet Loss (íŠ¸ë¦¬í”Œë › ì†ì‹¤)}

ìƒ´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¬ê¹Œìš”? ì„¸ ì¥ì˜ ì‚¬ì§„ì„ í•œ ì„¸íŠ¸(Triplet)ë¡œ ë¬¶ì–´ í•™ìŠµí•©ë‹ˆë‹¤.

\begin{itemize}
    \item **Anchor (A):** ê¸°ì¤€ì´ ë˜ëŠ” ë‚´ ì‚¬ì§„.
    \item **Positive (P):** ë‚˜ì™€ ê°™ì€ ì‚¬ëŒì˜ ë‹¤ë¥¸ ì‚¬ì§„.
    \item **Negative (N):** ë‚˜ì™€ ë‹¤ë¥¸ ì‚¬ëŒ(ì˜í¬)ì˜ ì‚¬ì§„.
\end{itemize}

\begin{mathbox}{Loss Function Derivation}
ìš°ë¦¬ì˜ ëª©í‘œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.
$$ ||f(A) - f(P)||^2 \le ||f(A) - f(N)||^2 $$
(Aì™€ P ì‚¬ì´ì˜ ê±°ë¦¬ê°€ Aì™€ N ì‚¬ì´ì˜ ê±°ë¦¬ë³´ë‹¤ ì‘ì•„ì•¼ í•œë‹¤.)

í•˜ì§€ë§Œ ì‹ ê²½ë§ì´ $f(x)=0$ (ëª¨ë“  ì¶œë ¥ì„ 0ìœ¼ë¡œ)ìœ¼ë¡œ í•™ìŠµí•´ë²„ë¦¬ë©´, $0 \le 0$ì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤(Trivial Solution). ì´ë¥¼ ë§‰ê¸° ìœ„í•´ **ë§ˆì§„($\alpha$)**ì„ ë‘¡ë‹ˆë‹¤.

$$ ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha \le 0 $$

ìµœì¢… ì†ì‹¤ í•¨ìˆ˜ (ReLU í˜•íƒœ):
$$ L(A, P, N) = \max(0, ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 + \alpha) $$
\end{mathbox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Triplet Loss}

TensorFlow/Keras ìŠ¤íƒ€ì¼ì˜ ì†ì‹¤ í•¨ìˆ˜ êµ¬í˜„ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Triplet Loss Function]
import tensorflow as tf

def triplet_loss(y_true, y_pred, alpha=0.2):
    """
    y_pred: [Anchor, Positive, Negative] ì„ë² ë”©ì´ ì—°ê²°ëœ í…ì„œ
    """
    # 1. ì„ë² ë”© ë²¡í„° ë¶„ë¦¬ (ì „ì²´ ê¸¸ì´ì˜ 1/3ì”©)
    total_len = y_pred.shape[1]
    emb_size = total_len // 3
    
    anchor = y_pred[:, 0:emb_size]
    positive = y_pred[:, emb_size:2*emb_size]
    negative = y_pred[:, 2*emb_size:]
    
    # 2. ê±°ë¦¬ ê³„ì‚° (ì œê³±í•©)
    # axis=-1: ë²¡í„°ì˜ ê° ì„±ë¶„ë³„ í•©
    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)
    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)
    
    # 3. Loss ê³„ì‚° (Margin ì¶”ê°€)
    basic_loss = pos_dist - neg_dist + alpha
    
    # 4. max(0, loss)
    loss = tf.maximum(basic_loss, 0.0)
    
    return tf.reduce_mean(loss)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Hard Triplet Mining (í•™ìŠµ ë°ì´í„° ì„ ì •)}
ëœë¤í•˜ê²Œ A, P, Nì„ ê³ ë¥´ë©´ í•™ìŠµì´ ì˜ ì•ˆ ë©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ëœë¤í•œ ë‘ ì‚¬ëŒ(A, N)ì€ ì´ë¯¸ ì¶©ë¶„íˆ ë‹¤ë¥´ê²Œ ìƒê²¼ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ($Loss=0$)
í•™ìŠµ íš¨ìœ¨ì„ ìœ„í•´ **"Aì™€ Nì´ ê½¤ ë‹®ì€ ê²½ìš°(Hard Triplet)"**ë¥¼ ê³¨ë¼ë‚´ì–´ í•™ìŠµì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì–¼êµ´ ë§ê³  ë‹¤ë¥¸ ê±°ì—ë„ ì“¸ ìˆ˜ ìˆë‚˜ìš”?} \\
\textbf{A.} ë„¤! ì„œëª… ì¸ì‹, ì§€ë¬¸ ì¸ì‹, ì‹¬ì§€ì–´ **ì´ë¯¸ì§€ ê²€ìƒ‰(ì‡¼í•‘ëª°ì—ì„œ ë¹„ìŠ·í•œ ì˜· ì°¾ê¸°)** ë“± "ìœ ì‚¬í•œ ê²ƒì„ ì°¾ëŠ”" ëª¨ë“  ë¶„ì•¼ì— ì“°ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì–¼êµ´ ì¸ì‹ì€ CNNì´ ì¶”ì¶œí•œ **'ë‚´ìš©(Content)'**ì„ ë¹„êµí•˜ëŠ” ê¸°ìˆ ì´ì—ˆìŠµë‹ˆë‹¤.
ê·¸ë ‡ë‹¤ë©´ CNNì´ ì¶”ì¶œí•œ **'í™”í’(Style)'**ë§Œ ë”°ë¡œ ë–¼ì–´ë‚¼ ìˆ˜ë„ ìˆì„ê¹Œìš”?

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë°˜ ê³ íì˜ í™”í’ì„ ë‚´ ì‚¬ì§„ì— ì…íˆëŠ” ì˜ˆìˆ ì ì¸ AI, **[Neural Style Transfer]**ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤. CNNì˜ ê¹Šì€ ì¸µì´ ë¬´ì—‡ì„ ë³´ê³  ìˆëŠ”ì§€ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ” í¥ë¯¸ë¡œìš´ ì‹œê°„ì´ ë  ê²ƒì…ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Similarity:} ë¶„ë¥˜ê°€ ì•„ë‹Œ ê±°ë¦¬ ì¸¡ì • ë¬¸ì œë¡œ ì ‘ê·¼í•œë‹¤.
    \item \textbf{Siamese Network:} ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ëŠ” ìŒë‘¥ì´ ë„¤íŠ¸ì›Œí¬.
    \item \textbf{Triplet Loss:} $(A, P, N)$ êµ¬ì¡°. A-PëŠ” ë‹¹ê¸°ê³ , A-Nì€ ë¯¼ë‹¤.
    \item \textbf{Margin $\alpha$:} ëª¨ë¸ì´ ëª¨ë“  ì¶œë ¥ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ” ê¼¼ìˆ˜ë¥¼ ë°©ì§€í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì •ì˜)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Convolutional Neural Networks: \\ YOLO Algorithm (Object Detection)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-8.] Deep Learning Strategy \& Architecture \textit{- Completed}
    \item[\textbf{Chapter 9.}] \textbf{Convolutional Neural Networks (Current Part)}
    \begin{itemize}
        \item 9.1-9.6 CNN Basics \& Sliding Windows \textit{- Completed}
        \item \textbf{9.7 YOLO Algorithm (You Only Look Once)}
        \begin{itemize}
            \item The Grid System & Bounding Box Regression
            \item IoU (Intersection over Union) Metric
            \item Non-max Suppression (Removing Duplicates)
            \item Anchor Boxes (Handling Overlap)
        \end{itemize}
    \end{itemize}
    \item[Chapter 10.] Sequence Models (RNN) \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ ìŠ¬ë¼ì´ë”© ìœˆë„ìš°ëŠ” í•©ì„±ê³± êµ¬í˜„ìœ¼ë¡œ ì†ë„ëŠ” ë¹¨ë¼ì¡Œì§€ë§Œ, ì—¬ì „íˆ **"ë°•ìŠ¤ ìœ„ì¹˜ê°€ ë¶€ì •í™•í•˜ë‹¤"**ëŠ” í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤. ìœˆë„ìš°ê°€ ê³ ì •ëœ ê°„ê²©ìœ¼ë¡œë§Œ ì›€ì§ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
"ê°ì²´ì˜ ì¤‘ì‹¬ì„ ì°¾ê³ , ê·¸ ì¤‘ì‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ ë°•ìŠ¤ í¬ê¸°ë¥¼ ì˜ˆì¸¡í•˜ë©´ ì–´ë–¨ê¹Œ?"
ì´ ì•„ì´ë””ì–´ë¡œ íƒ„ìƒí•œ ê²ƒì´ **YOLO**ì…ë‹ˆë‹¤. ì´ë¦„ì²˜ëŸ¼ ì´ë¯¸ì§€ë¥¼ ë‹¨ í•œ ë²ˆë§Œ ë³´ê³ (Look Once), ëª¨ë“  ê°ì²´ì˜ ìœ„ì¹˜ì™€ ì¢…ë¥˜ë¥¼ ë™ì‹œì— ì°¾ì•„ë‚´ëŠ” í˜ì‹ ì ì¸ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ì˜ í‘œì¤€ì¸ YOLO ì•Œê³ ë¦¬ì¦˜ì˜ ì›ë¦¬ë¥¼ ì™„ë²½íˆ ì´í•´í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ê·¸ë¦¬ë“œ:} ì´ë¯¸ì§€ë¥¼ ê²©ìë¡œ ë‚˜ëˆ„ê³ , ê°ì²´ ì¤‘ì‹¬ì ì´ ì†í•œ ì…€ì´ ì±…ì„ì„ ì§€ëŠ” êµ¬ì¡°ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{IoU:} ë‘ ë°•ìŠ¤ê°€ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” í‰ê°€ ì§€í‘œë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.
    \item \textbf{NMS:} ì¤‘ë³µëœ ë°•ìŠ¤ë¥¼ ì œê±°í•˜ëŠ” ë¹„ìµœëŒ€ ì–µì œ(Non-max Suppression) ì•Œê³ ë¦¬ì¦˜ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{ì•µì»¤:} ê²¹ì¹œ ë¬¼ì²´ë¥¼ ë¶„ë¦¬í•˜ëŠ” ì•µì»¤ ë°•ìŠ¤(Anchor Box) ê°œë…ì„ íŒŒì•…í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì•½ì–´} & \textbf{ì„¤ëª…} \\ \hline
\textbf{Grid Cell} & - & ì´ë¯¸ì§€ë¥¼ $S \times S$ë¡œ ë‚˜ëˆˆ ì‘ì€ êµ¬ì—­. \\ \hline
\textbf{IoU} & Intersection over Union & êµì§‘í•© ì˜ì—­ / í•©ì§‘í•© ì˜ì—­. (ì¼ì¹˜ë„) \\ \hline
\textbf{NMS} & Non-max Suppression & ê°€ì¥ í™•ì‹¤í•œ ë°•ìŠ¤ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì§€ì›€. \\ \hline
\textbf{Anchor Box} & - & ë¯¸ë¦¬ ì •ì˜ëœ ë°•ìŠ¤ ëª¨ì–‘. (ê¸¸ì­‰í•œ ì‚¬ëŒ, ë„“ì€ ì°¨ ë“±) \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë‹¨ í•œ ë²ˆì˜ ì¶”ë¡ }

\subsection{1. Bounding Box Predictions (ê·¸ë¦¬ë“œ ì‹œìŠ¤í…œ)}


YOLOëŠ” ì´ë¯¸ì§€ë¥¼ $S \times S$ ê·¸ë¦¬ë“œ(ë³´í†µ $19 \times 19$)ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
ê° ì…€ì€ ë‹¤ìŒ ë²¡í„° $y$ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
$$ y = [p_c, b_x, b_y, b_h, b_w, c_1, c_2, \dots]^T $$
\begin{itemize}
    \item $p_c$: ê°ì²´ê°€ ìˆì„ í™•ë¥  (Confidence).
    \item $b_x, b_y$: ë°•ìŠ¤ ì¤‘ì‹¬ ì¢Œí‘œ (ì…€ ë‚´ ìƒëŒ€ ìœ„ì¹˜, 0~1).
    \item $b_h, b_w$: ë°•ìŠ¤ ë†’ì´/ë„ˆë¹„ (ì „ì²´ ì´ë¯¸ì§€ ëŒ€ë¹„ ë¹„ìœ¨).
    \item $c_i$: í´ë˜ìŠ¤ í™•ë¥  (ì°¨, ì‚¬ëŒ ë“±).
\end{itemize}

\subsection{2. IoU (Intersection over Union)}


ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë°•ìŠ¤ê°€ ì •ë‹µê³¼ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ í‰ê°€í•˜ëŠ” ì²™ë„ì…ë‹ˆë‹¤.
\begin{formulabox}{IoU ìˆ˜ì‹}
$$ \text{IoU} = \frac{\text{êµì§‘í•© ì˜ì—­ (Intersection)}}{\text{í•©ì§‘í•© ì˜ì—­ (Union)}} $$
\begin{itemize}
    \item ë³´í†µ $\text{IoU} \ge 0.5$ ì´ë©´ "ì˜¬ë°”ë¥¸ íƒì§€"ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.
    \item 1ì´ë©´ ì™„ë²½í•˜ê²Œ ì¼ì¹˜, 0ì´ë©´ ì „í˜€ ê²¹ì¹˜ì§€ ì•ŠìŒ.
\end{itemize}
\end{formulabox}

\subsection{3. Anchor Boxes (ê²¹ì¹œ ë¬¼ì²´ í•´ê²°)}
í•œ ì…€ì˜ ì¤‘ì‹¬ì— ì‚¬ëŒê³¼ ì°¨ê°€ ê²¹ì³ ìˆë‹¤ë©´?
ê¸°ì¡´ ë²¡í„°ë¡œëŠ” í•˜ë‚˜ë§Œ ì˜ˆì¸¡ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ëª¨ì–‘(ì•µì»¤)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
$$ y = [\text{Anchor 1}, \text{Anchor 2}] $$
\begin{itemize}
    \item **Anchor 1 (ì„¸ë¡œë¡œ ê¸´):** ì‚¬ëŒ ë‹´ë‹¹.
    \item **Anchor 2 (ê°€ë¡œë¡œ ë„“ì€):** ìë™ì°¨ ë‹´ë‹¹.
\end{itemize}

---

\section{Deep Dive: Non-max Suppression (NMS)}

YOLOëŠ” í•˜ë‚˜ì˜ ê°ì²´ì— ëŒ€í•´ ì—¬ëŸ¬ ì…€ì´ "ë‚´ê°€ ì°¾ì•˜ë‹¤!"ë©° ë°•ìŠ¤ë¥¼ ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ë³µì„ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item **Filter:** $p_c < 0.6$ ì¸ ë°•ìŠ¤(í™•ì‹  ì—†ëŠ” ê²ƒ)ëŠ” ëª¨ë‘ ë²„ë¦½ë‹ˆë‹¤.
    \item **Select:** ë‚¨ì€ ë°•ìŠ¤ ì¤‘ $p_c$ê°€ ê°€ì¥ ë†’ì€ ê²ƒì„ ì„ íƒí•©ë‹ˆë‹¤ (Best Box).
    \item **Suppress:** ì„ íƒëœ ë°•ìŠ¤ì™€ $\text{IoU} \ge 0.5$ ì¸(ë§ì´ ê²¹ì¹œ) ë‹¤ë¥¸ ë°•ìŠ¤ë“¤ì€ "ê°™ì€ ë¬¼ì²´ë¥¼ ì¤‘ë³µ íƒì§€í•œ ê²ƒ"ìœ¼ë¡œ ë³´ê³  ì§€ì›ë‹ˆë‹¤.
    \item **Repeat:** ë°•ìŠ¤ê°€ ë‹¤ ì •ë¦¬ë  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
\end{enumerate}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: IoU Calculation}

IoU ê³„ì‚°ì€ ê°ì²´ íƒì§€ ì„±ëŠ¥ í‰ê°€ì™€ NMS êµ¬í˜„ì˜ í•µì‹¬ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=IoU Calculation Function]
def calculate_iou(box1, box2):
    """
    box: (x1, y1, x2, y2)ì¢Œí‘œ (ì¢Œìƒë‹¨, ìš°í•˜ë‹¨)
    """
    (b1_x1, b1_y1, b1_x2, b1_y2) = box1
    (b2_x1, b2_y1, b2_x2, b2_y2) = box2
    
    # 1. êµì§‘í•©(Intersection) ì¢Œí‘œ ê³„ì‚°
    xi1 = max(b1_x1, b2_x1)
    yi1 = max(b1_y1, b2_y1)
    xi2 = min(b1_x2, b2_x2)
    yi2 = min(b1_y2, b2_y2)
    
    # ë„“ì´ (ìŒìˆ˜ë©´ 0)
    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)
    
    # 2. í•©ì§‘í•©(Union) ë„“ì´ ê³„ì‚°
    b1_area = (b1_x2 - b1_x1) * (b1_y2 - b1_y1)
    b2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1)
    union_area = b1_area + b2_area - inter_area
    
    # 3. IoU
    return inter_area / (union_area + 1e-6)

# --- í…ŒìŠ¤íŠ¸ ---
if __name__ == "__main__":
    box_a = (1, 1, 3, 3) # ë©´ì  4
    box_b = (2, 2, 4, 4) # ë©´ì  4, êµì§‘í•© 1
    # í•©ì§‘í•© = 4 + 4 - 1 = 7
    # IoU = 1/7 = 0.1428...
    
    print(f"IoU: {calculate_iou(box_a, box_b):.4f}")
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ì¢Œí‘œê³„ ì£¼ì˜}
YOLOì˜ ì¶œë ¥ $b_x, b_y$ëŠ” **ê·¸ë¦¬ë“œ ì…€ ë‚´ë¶€ì—ì„œì˜ ìƒëŒ€ ìœ„ì¹˜(0~1)**ì…ë‹ˆë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ ìœ„ì— ë°•ìŠ¤ë¥¼ ê·¸ë¦¬ë ¤ë©´, ì…€ì˜ ìœ„ì¹˜(ì¸ë±ìŠ¤)ë¥¼ ë”í•˜ê³  ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ê³±í•´ì£¼ëŠ” ë³€í™˜ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì•µì»¤ ë°•ìŠ¤ í¬ê¸°ëŠ” ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} ë³´í†µ í›ˆë ¨ ë°ì´í„°ì— ìˆëŠ” ê°ì²´ë“¤ì˜ ì‹¤ì œ ë°•ìŠ¤ í¬ê¸°ë¥¼ ëª¨ì•„ì„œ **K-Means í´ëŸ¬ìŠ¤í„°ë§**ì„ ëŒë¦½ë‹ˆë‹¤. ê°€ì¥ ë¹ˆë²ˆí•˜ê²Œ ë“±ì¥í•˜ëŠ” ëŒ€í‘œì ì¸ ëª¨ì–‘ 5~9ê°œë¥¼ ì„ ì •í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤ (YOLO v2ë¶€í„° ì ìš©).

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ê²ƒìœ¼ë¡œ ì»´í“¨í„° ë¹„ì „(CNN) íŒŒíŠ¸ë¥¼ ë§ˆì¹©ë‹ˆë‹¤. ì´ì œ ì—¬ëŸ¬ë¶„ì€ ì •ì§€ëœ ì´ë¯¸ì§€ì—ì„œ ì‚¬ë¬¼ì„ ë¶„ë¥˜í•˜ê³  ìœ„ì¹˜ê¹Œì§€ ì°¾ì•„ë‚´ëŠ” ê¸°ìˆ ì„ ìŠµë“í–ˆìŠµë‹ˆë‹¤.

í•˜ì§€ë§Œ ì„¸ìƒì€ ë©ˆì¶° ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ íŠœë¸Œ ì˜ìƒ, ìŒì„± ì¸ì‹, ì£¼ê°€ ì˜ˆì¸¡, ë²ˆì—­ ë“±ì€ **ì‹œê°„ì˜ íë¦„(Sequence)**ì´ ìˆëŠ” ë°ì´í„°ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ë¶€í„°ëŠ” **[Part 5. Sequence Models]**ì˜ ì„¸ê³„ë¡œ ë– ë‚©ë‹ˆë‹¤. ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ì‹ ê²½ë§, **RNN (Recurrent Neural Networks)**ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{YOLO:} ê·¸ë¦¬ë“œ ì…€ë§ˆë‹¤ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ íšŒê·€(Regression)ë¡œ ì§ì ‘ ì˜ˆì¸¡í•œë‹¤.
    \item \textbf{IoU:} êµì§‘í•©/í•©ì§‘í•©. ë°•ìŠ¤ ìœ„ì¹˜ ì •í™•ë„ì˜ ì²™ë„ì´ì NMSì˜ ê¸°ì¤€.
    \item \textbf{NMS:} ì¤‘ë³µëœ ë°•ìŠ¤ë¥¼ ì œê±°í•˜ì—¬ ê°ì²´ë‹¹ í•˜ë‚˜ì˜ ë°•ìŠ¤ë§Œ ë‚¨ê¸´ë‹¤.
    \item \textbf{Anchor:} ë‹¤ì–‘í•œ ë¹„ìœ¨ì˜ ê°ì²´ë¥¼ ì¡ê¸° ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë°•ìŠ¤ ëª¨ì–‘ì„ ì“´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Special Applications: \\ Neural Style Transfer (NST)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Special Applications (Current Unit)}
    \begin{itemize}
        \item 10.1 Face Recognition \textit{- Completed}
        \item \textbf{10.2 Neural Style Transfer}
        \begin{itemize}
            \item What are we optimizing? (Pixel vs Weights)
            \item Content Cost Function ($J_{content}$)
            \item Style Cost Function ($J_{style}$): Gram Matrix
            \item Implementation Strategy
        \end{itemize}
    \end{itemize}
    \item[Chapter 11.] Sequence Models (RNN) \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ìƒ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ CNNì´ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ë²¡í„°ë¡œ ì••ì¶•í•˜ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.
ê·¸ë ‡ë‹¤ë©´, ì´ íŠ¹ì§•ì„ ë¶„ë¦¬í•´ì„œ ì¡°ì‘í•  ìˆ˜ëŠ” ì—†ì„ê¹Œìš”? ì´ë¯¸ì§€ì˜ **'ë‚´ìš©(Content)'**ì€ ë‚´ ì‚¬ì§„ì¸ë°, **'í™”í’(Style)'**ì€ ë°˜ ê³ íì˜ ê·¸ë¦¼ì²˜ëŸ¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤ë©´ ì–´ë–¨ê¹Œìš”?
ì´ê²ƒì´ ë°”ë¡œ AI ì˜ˆìˆ ì˜ ì‹œì´ˆ, **Neural Style Transfer(NST)**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ CNNì˜ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë‘ ì´ë¯¸ì§€ë¥¼ í•©ì„±í•˜ëŠ” NST ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì „í™˜:} ê°€ì¤‘ì¹˜($W$)ê°€ ì•„ë‹Œ **ì…ë ¥ ì´ë¯¸ì§€($G$)ì˜ í”½ì…€**ì„ í•™ìŠµí•œë‹¤ëŠ” ì°¨ì´ì ì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì½˜í…ì¸ :} ê¹Šì€ ì¸µì˜ í™œì„±í™” ë§µì„ ë¹„êµí•˜ì—¬ ë‚´ìš©ì˜ ìœ ì‚¬ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
    \item \textbf{ìŠ¤íƒ€ì¼:} **ê·¸ëŒ í–‰ë ¬(Gram Matrix)**ì„ í†µí•´ ì´ë¯¸ì§€ì˜ ì§ˆê°ê³¼ ìƒê´€ê´€ê³„ë¥¼ ìˆ˜ì¹˜í™”í•©ë‹ˆë‹¤.
    \item \textbf{í†µí•©:} $J(G) = \alpha J_{content} + \beta J_{style}$ì„ ìµœì†Œí™”í•˜ì—¬ ì˜ˆìˆ  ì‘í’ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ì´ë¯¸ì§€} & \textbf{ê¸°í˜¸} & \textbf{ì—­í• } \\ \hline
\textbf{Content Image} & $C$ & ë‚´ìš©ì˜ ê¸°ì¤€ (ì˜ˆ: ë‚´ ì‚¬ì§„). \\ \hline
\textbf{Style Image} & $S$ & í™”í’ì˜ ê¸°ì¤€ (ì˜ˆ: ê³ í ê·¸ë¦¼). \\ \hline
\textbf{Generated Image} & $G$ & ìš°ë¦¬ê°€ ë§Œë“¤ ê²°ê³¼ë¬¼ (ì²˜ìŒì—” ë…¸ì´ì¦ˆ). \\ \hline
\textbf{Gram Matrix} & $G_{kk'}$ & íŠ¹ì„±ë§µ ì±„ë„ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë¬´ì—‡ì„ í•™ìŠµí•˜ëŠ”ê°€?}

\subsection{1. The Big Picture}
NSTì˜ ê°€ì¥ í° íŠ¹ì§•ì€ í•™ìŠµì˜ ëŒ€ìƒì´ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{ê¸°ì¡´ CNN í•™ìŠµ:} ì´ë¯¸ì§€ ê³ ì • $\rightarrow$ ê°€ì¤‘ì¹˜ $W$ ì—…ë°ì´íŠ¸.
    \item \textbf{NST í•™ìŠµ:} ê°€ì¤‘ì¹˜ $W$ ê³ ì •(Pre-trained) $\rightarrow$ **ìƒì„± ì´ë¯¸ì§€ $G$ì˜ í”½ì…€ê°’ ì—…ë°ì´íŠ¸.**
\end{itemize}

\subsection{2. Content Cost Function ($J_{content}$)}
"ì´ë¯¸ì§€ $G$ê°€ ì´ë¯¸ì§€ $C$ì™€ ë¹„ìŠ·í•œ ë‚´ìš©ì„ ë‹´ê³  ìˆëŠ”ê°€?"
\begin{itemize}
    \item **ì›ë¦¬:** CNNì˜ ê¹Šì€ ì¸µ(Deep Layer)ì€ ì‚¬ë¬¼ì˜ ë°°ì¹˜ë‚˜ êµ¬ì¡° ê°™ì€ ê³ ì°¨ì› ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.
    \item **ìˆ˜ì‹:** íŠ¹ì • ì¸µ $l$ì—ì„œ ë‘ ì´ë¯¸ì§€ì˜ í™œì„±í™” ë§µ($a^{[l]}$) ê°„ì˜ ì°¨ì´(MSE)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    $$ J_{content}(C, G) = \frac{1}{2} \| a^{[l](C)} - a^{[l](G)} \|^2 $$
\end{itemize}

\subsection{3. Style Cost Function ($J_{style}$) - [í•µì‹¬]}
"ì´ë¯¸ì§€ $G$ê°€ ì´ë¯¸ì§€ $S$ì˜ í™”í’(ì§ˆê°)ì„ ë‹´ê³  ìˆëŠ”ê°€?"
ìŠ¤íƒ€ì¼ì€ "ì–´ë””ì— ìˆëŠ”ê°€"ê°€ ì•„ë‹ˆë¼ **"ë¬´ì—‡ì´ í•¨ê»˜ ë‚˜íƒ€ë‚˜ëŠ”ê°€(Correlation)"**ì…ë‹ˆë‹¤.

\begin{analogybox}{ê·¸ëŒ í–‰ë ¬ì˜ ì§ê´€}
ì–´ë–¤ ì¸µì— ë‘ ê°œì˜ í•„í„°(ì±„ë„)ê°€ ìˆë‹¤ê³  ê°€ì •í•©ì‹œë‹¤.
\begin{itemize}
    \item **í•„í„° A:** ìˆ˜ì§ì„ ì„ ì°¾ìŒ.
    \item **í•„í„° B:** ì£¼í™©ìƒ‰ì„ ì°¾ìŒ.
\end{itemize}
ì´ ë‘ í•„í„°ê°€ ì´ë¯¸ì§€ì˜ ê°™ì€ ìœ„ì¹˜ì—ì„œ ë™ì‹œì— í™œì„±í™”ëœë‹¤ë©´(ìƒê´€ê´€ê³„ ë†’ìŒ), "ì´ í™”í’ì€ ìˆ˜ì§ì„ ê³¼ ì£¼í™©ìƒ‰ì´ í•¨ê»˜ ë‹¤ë‹ˆëŠ” ìŠ¤íƒ€ì¼ì´ë‹¤"ë¼ê³  ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì„ ìˆ˜ì¹˜í™”í•œ ê²ƒì´ **ê·¸ëŒ í–‰ë ¬(Gram Matrix)**ì…ë‹ˆë‹¤.
\end{analogybox}

\begin{formulabox}{Gram Matrix ($G^{[l]}$)}
$$ G_{kk'}^{[l]} = \sum_{i} \sum_{j} a_{i,j,k}^{[l]} \cdot a_{i,j,k'}^{[l]} $$
(ì±„ë„ $k$ì™€ ì±„ë„ $k'$ì˜ í™œì„±í™” ë§µ ë‚´ì )
\end{formulabox}

ìŠ¤íƒ€ì¼ ë¹„ìš©ì€ ë‘ ì´ë¯¸ì§€ì˜ ê·¸ëŒ í–‰ë ¬ ì°¨ì´ì…ë‹ˆë‹¤.
$$ J_{style}^{[l]}(S, G) = \| G^{[l](S)} - G^{[l](G)} \|^2 $$

---

\section{Implementation: Optimization Loop}

TensorFlow/Kerasë¥¼ ì‚¬ìš©í•œ êµ¬í˜„ì˜ í•µì‹¬ êµ¬ì¡°ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Neural Style Transfer Logic]
import tensorflow as tf

# 1. ëª¨ë¸ ì¤€ë¹„ (VGG19, ê°€ì¤‘ì¹˜ ê³ ì •)
vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')
vgg.trainable = False 

# 2. ìƒì„± ì´ë¯¸ì§€(G) ì´ˆê¸°í™” (í•™ìŠµ ëŒ€ìƒ!)
# ì²˜ìŒì—ëŠ” ëœë¤ ë…¸ì´ì¦ˆë¡œ ì‹œì‘í•˜ê±°ë‚˜ Content ì´ë¯¸ì§€ë¡œ ì‹œì‘
generated_image = tf.Variable(content_image) 

# 3. ìµœì í™” ë£¨í”„
optimizer = tf.optimizers.Adam(learning_rate=0.02)

@tf.function
def train_step(image):
    with tf.GradientTape() as tape:
        # ëª¨ë¸ í†µê³¼ -> í™œì„±í™” ë§µ ì¶”ì¶œ
        outputs = get_vgg_layers(image)
        
        # Loss ê³„ì‚°
        loss_c = calculate_content_loss(outputs, content_targets)
        loss_s = calculate_style_loss(outputs, style_targets) # Gram Matrix ì‚¬ìš©
        
        total_loss = alpha * loss_c + beta * loss_s
        
    # í•µì‹¬: ê°€ì¤‘ì¹˜ê°€ ì•„ë‹Œ 'ì…ë ¥ ì´ë¯¸ì§€'ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°
    grad = tape.gradient(total_loss, image)
    
    # ì´ë¯¸ì§€ í”½ì…€ ì—…ë°ì´íŠ¸
    optimizer.apply_gradients([(grad, image)])
    
    # í”½ì…€ê°’ í´ë¦¬í•‘ (0~1 ì‚¬ì´ ìœ ì§€)
    image.assign(tf.clip_by_value(image, 0.0, 1.0))
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ì™œ ê·¸ëŒ í–‰ë ¬ì¸ê°€ìš”?}
ê·¸ëŒ í–‰ë ¬ì€ ê³µê°„ ì •ë³´(Spatial Information)ë¥¼ í•©ì³ë²„ë¦½ë‹ˆë‹¤($\sum_{i,j}$). ì¦‰, "ì–´ë””ì—" ìˆëŠ”ì§€ëŠ” ë¬´ì‹œí•˜ê³  "ì–´ë–¤ íŠ¹ì§•ë“¤ì´ í†µê³„ì ìœ¼ë¡œ ê³µì¡´í•˜ëŠ”ê°€"ë§Œ ë‚¨ê¸°ê¸° ë•Œë¬¸ì— **ìŠ¤íƒ€ì¼(ì§ˆê°, íŒ¨í„´)**ì„ í‘œí˜„í•˜ê¸°ì— ì í•©í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. $\alpha$ì™€ $\beta$ëŠ” ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} ì·¨í–¥ì…ë‹ˆë‹¤. $\alpha$(ì½˜í…ì¸ )ë¥¼ ë†’ì´ë©´ ì›ë³¸ ì‚¬ì§„ê³¼ ë¹„ìŠ·í•´ì§€ê³ , $\beta$(ìŠ¤íƒ€ì¼)ë¥¼ ë†’ì´ë©´ ê·¸ë¦¼ í™”í’ì´ ê°•í•´ì§‘ë‹ˆë‹¤. ë³´í†µ $\beta$ë¥¼ í›¨ì”¬ í¬ê²Œ ì„¤ì •í•©ë‹ˆë‹¤(ìˆ«ì ìŠ¤ì¼€ì¼ ì°¨ì´ ë•Œë¬¸).

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” CNNì„ ì´ìš©í•´ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ê³ , íƒì§€í•˜ê³ , ì‹¬ì§€ì–´ ì˜ˆìˆ  ì‘í’ˆìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ê¹Œì§€ ë°°ì› ìŠµë‹ˆë‹¤. ì´ë¡œì¨ **ì»´í“¨í„° ë¹„ì „(Computer Vision)** íŒŒíŠ¸ë¥¼ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

ì´ì œ ì‹œê° ì •ë³´ê°€ ì•„ë‹Œ, **ì‹œê°„ì˜ íë¦„ì´ ìˆëŠ” ë°ì´í„°(Sequence Data)**ì˜ ì„¸ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. 
ë‹¤ìŒ ì‹œê°„ë¶€í„°ëŠ” ìŒì„±, ì–¸ì–´, ì£¼ê°€ ë“± ì—°ì†ì ì¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” **[Part 5. Sequence Models]**ë¥¼ ì‹œì‘í•˜ë©°, ê·¸ ì²« ë²ˆì§¸ ì£¼ìì¸ **RNN (Recurrent Neural Networks)**ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{NST:} ì‚¬ì „ í•™ìŠµëœ CNNì„ ì´ìš©í•´ ì½˜í…ì¸ ì™€ ìŠ¤íƒ€ì¼ì„ í•©ì„±í•œë‹¤.
    \item \textbf{Learning:} ê°€ì¤‘ì¹˜ëŠ” ê³ ì •í•˜ê³ , **ì…ë ¥ ì´ë¯¸ì§€ì˜ í”½ì…€**ì„ í•™ìŠµ(ì—…ë°ì´íŠ¸)í•œë‹¤.
    \item \textbf{Content:} ê¹Šì€ ì¸µì˜ í™œì„±í™” ë§µ ì°¨ì´ë¥¼ ì¤„ì¸ë‹¤.
    \item \textbf{Style:} **ê·¸ëŒ í–‰ë ¬(Gram Matrix)**ì˜ ì°¨ì´ë¥¼ ì¤„ì—¬ ì§ˆê° ìƒê´€ê´€ê³„ë¥¼ ëª¨ë°©í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì •ì˜)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Recurrent Neural Networks (RNN)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item \textbf{10.1 Recurrent Neural Networks (RNN)}
        \begin{itemize}
            \item Why Sequence Models? (Time \& Order)
            \item RNN Architecture (Unrolled View)
            \item Forward Propagation Formulas
            \item BPTT (Backpropagation Through Time)
        \end{itemize}
        \item 10.2 GRU & LSTM (Gated Units) \textit{- Upcoming}
        \item 10.3 NLP & Word Embeddings \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ê¸ˆê¹Œì§€ ê³ ì •ëœ í¬ê¸°ì˜ ì´ë¯¸ì§€($H \times W$)ë¥¼ ì²˜ë¦¬í•˜ëŠ” CNNì„ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì„¸ìƒì˜ ë§ì€ ë°ì´í„°ëŠ” **'ìˆœì„œ(Sequence)'**ì™€ **'ì‹œê°„(Time)'**ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.
"ë‚˜ëŠ” í”„ë‘ìŠ¤ì— ê°€ì„œ..."ë¼ëŠ” ë§ì„ ë“¤ìœ¼ë©´, ë’¤ì— "í”„ë‘ìŠ¤ì–´"ë¼ëŠ” ë§ì´ ë‚˜ì˜¬ í™•ë¥ ì´ ë†’ë‹¤ëŠ” ê²ƒì„ ìš°ë¦¬ëŠ” ì••ë‹ˆë‹¤. ì´ëŠ” ì•ì„  ë‹¨ì–´ë“¤ì˜ **ë¬¸ë§¥(Context)**ì„ ê¸°ì–µí•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
ê¸°ì¡´ ì‹ ê²½ë§ì€ ì´ 'ê¸°ì–µ' ëŠ¥ë ¥ì´ ì—†ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ê¸°ì–µì„ ê°€ì§„ ì‹ ê²½ë§, **RNN**ì˜ ì„¸ê³„ë¡œ ë“¤ì–´ê°‘ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” RNNì˜ ê¸°ë³¸ ì›ë¦¬ì™€ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ì¡°:} ì€ë‹‰ ìƒíƒœ(Hidden State)ë¥¼ í†µí•´ ê³¼ê±° ì •ë³´ë¥¼ í˜„ì¬ë¡œ ì „ë‹¬í•˜ëŠ” ë£¨í”„ êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜ì‹:} $a^{\langle t \rangle} = g(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle})$ ê³µì‹ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.
    \item \textbf{ê³µìœ :} ëª¨ë“  ì‹œê°„ ë‹¨ê³„ì—ì„œ **íŒŒë¼ë¯¸í„°($W$)ë¥¼ ê³µìœ **í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ì›ë¦¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{í•™ìŠµ:} ì‹œê°„ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ëŠ” ì—­ì „íŒŒ, **BPTT**ì˜ ê°œë…ì„ ìµí™ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{ê¸°í˜¸} & \textbf{ìš©ì–´} & \textbf{ì„¤ëª…} \\ \hline
$x^{\langle t \rangle}$ & Input & ì‹œê°„ $t$ì—ì„œì˜ ì…ë ¥ (ì˜ˆ: $t$ë²ˆì§¸ ë‹¨ì–´). \\ \hline
$a^{\langle t \rangle}$ & Hidden State & ì‹œê°„ $t$ì—ì„œì˜ ì€ë‹‰ ìƒíƒœ. **(ê¸°ì–µ)** \\ \hline
$y^{\langle t \rangle}$ & Output & ì‹œê°„ $t$ì—ì„œì˜ ì¶œë ¥ (ì˜ˆ: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡). \\ \hline
$W_{aa}$ & Weight (Hidden) & ê³¼ê±° ê¸°ì–µì„ í˜„ì¬ë¡œ ê°€ì ¸ì˜¤ëŠ” ê°€ì¤‘ì¹˜. \\ \hline
$W_{ax}$ & Weight (Input) & í˜„ì¬ ì…ë ¥ì„ ë°›ì•„ë“¤ì´ëŠ” ê°€ì¤‘ì¹˜. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìˆœí™˜ì˜ ë§ˆë²•}

\subsection{1. RNN Architecture (Unrolled)}
RNNì€ ìì‹ ì„ ê°€ë¦¬í‚¤ëŠ” í™”ì‚´í‘œ(Loop)ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì´ë¥¼ ì‹œê°„ì¶•ìœ¼ë¡œ í¼ì¹˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.



\begin{itemize}
    \item **ì…ë ¥:** ë§¤ ì‹œì  $t$ë§ˆë‹¤ $x^{\langle t \rangle}$ê°€ ë“¤ì–´ì˜µë‹ˆë‹¤.
    \item **ì „ë‹¬:** ì´ì „ ì‹œì ì˜ ê¸°ì–µ $a^{\langle t-1 \rangle}$ì´ í˜„ì¬ ì‹œì  $t$ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
    \item **ì¶œë ¥:** ë‘ ì •ë³´ë¥¼ í•©ì³ì„œ $y^{\langle t \rangle}$ë¥¼ ì¶œë ¥í•˜ê³ , ë‹¤ìŒ ì‹œì  $t+1$ë¡œ ê¸°ì–µ $a^{\langle t \rangle}$ë¥¼ ë„˜ê¹ë‹ˆë‹¤.
\end{itemize}

\subsection{2. Forward Propagation Formulas}
RNNì˜ í•µì‹¬ì€ **"í˜„ì¬ ì…ë ¥ê³¼ ê³¼ê±° ê¸°ì–µì„ ì„ì–´ì„œ ìƒˆë¡œìš´ ê¸°ì–µì„ ë§Œë“ ë‹¤"**ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{formulabox}{ì€ë‹‰ ìƒíƒœ ì—…ë°ì´íŠ¸ (ê¸°ì–µ ê°±ì‹ )}
$$ a^{\langle t \rangle} = \tanh(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle} + b_a) $$
\begin{itemize}
    \item $W_{aa}a^{\langle t-1 \rangle}$: ê³¼ê±°ì˜ ê¸°ì–µ ë°˜ì˜.
    \item $W_{ax}x^{\langle t \rangle}$: í˜„ì¬ì˜ ì •ë³´ ë°˜ì˜.
    \item $\tanh$: ê°’ì„ -1 ~ 1 ì‚¬ì´ë¡œ ì••ì¶•í•˜ì—¬ í­ë°œ ë°©ì§€ (ì£¼ë¡œ ì‚¬ìš©).
\end{itemize}
\end{formulabox}

\begin{formulabox}{ì¶œë ¥ ê³„ì‚°}
$$ \hat{y}^{\langle t \rangle} = \text{softmax}(W_{ya}a^{\langle t \rangle} + b_y) $$
\begin{itemize}
    \item í˜„ì¬ì˜ ê¸°ì–µ($a^{\langle t \rangle}$)ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
\end{itemize}
\end{formulabox}

\textbf{Key Point (Parameter Sharing):}
$t=1$ì´ë“  $t=100$ì´ë“ , $W_{aa}, W_{ax}, W_{ya}$ëŠ” **ëª¨ë‘ ë˜‘ê°™ì€ í–‰ë ¬**ì„ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ê²ƒì´ RNNì´ ê¸¸ì´ ì œí•œ ì—†ì´ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë¹„ê²°ì…ë‹ˆë‹¤.

---

\section{Deep Dive: Backpropagation Through Time (BPTT)}

RNNì˜ í•™ìŠµì€ ì‹œê°„ì„ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°‘ë‹ˆë‹¤.

\begin{itemize}
    \item **Loss:** ì „ì²´ ì†ì‹¤ $L$ì€ ê° ì‹œê°„ ë‹¨ê³„ë³„ ì†ì‹¤ì˜ í•©ì…ë‹ˆë‹¤. $L = \sum L^{\langle t \rangle}$.
    \item **Gradient:** $t=100$ ì‹œì ì˜ ì˜¤ì°¨ë¥¼ ìˆ˜ì •í•˜ë ¤ë©´, $t=99, 98, \dots, 1$ ì‹œì ì˜ ìƒíƒœê¹Œì§€ ì˜í–¥ì„ ë¯¸ì³ì•¼ í•©ë‹ˆë‹¤.
    \item **Problem:** ë¯¸ë¶„ê°’ì´ ê³„ì† ê³±í•´ì§€ë©´ì„œ($W_{aa}^{100}$), ê°’ì´ 0ìœ¼ë¡œ ì‚¬ë¼ì§€ê±°ë‚˜(Vanishing) ë¬´í•œëŒ€ë¡œ ì»¤ì§€ëŠ”(Exploding) ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ê¸°ë³¸ RNNì€ **ì¥ê¸° ì˜ì¡´ì„±(Long-term Dependency)**ì„ í•™ìŠµí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: RNN Step & Loop}

NumPyë¡œ RNNì˜ ë‚´ë¶€ ë™ì‘ì„ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=RNN Forward Pass Implementation]
import numpy as np

def rnn_cell_forward(xt, a_prev, parameters):
    """
    ë‹¨ì¼ íƒ€ì„ ìŠ¤í… (t) ì²˜ë¦¬
    """
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]

    # 1. ë‹¤ìŒ ì€ë‹‰ ìƒíƒœ ê³„ì‚° (Tanh ì‚¬ìš©)
    # a_next = tanh(Waa*a_prev + Wax*xt + ba)
    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)
    
    # 2. í˜„ì¬ ì¶œë ¥ ì˜ˆì¸¡ (Softmax ê°€ì •)
    yt_pred = softmax(np.dot(Wya, a_next) + by)
    
    return a_next, yt_pred

def rnn_forward(x, a0, parameters):
    """
    ì „ì²´ ì‹œí€€ìŠ¤ ì²˜ë¦¬ (Time Loop)
    """
    n_x, m, T_x = x.shape  # T_x: ì‹œê°„ ê¸¸ì´ (Timesteps)
    n_y, n_a = parameters["Wya"].shape
    
    a = np.zeros((n_a, m, T_x))      # ëª¨ë“  ê¸°ì–µ ì €ì¥ìš©
    y_pred = np.zeros((n_y, m, T_x)) # ëª¨ë“  ì¶œë ¥ ì €ì¥ìš©
    
    a_next = a0 # ì´ˆê¸° ê¸°ì–µ
    
    # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë£¨í”„ (RNNì˜ í•µì‹¬)
    for t in range(T_x):
        xt = x[:, :, t] # të²ˆì§¸ ì…ë ¥
        
        # ì…€ ì—…ë°ì´íŠ¸
        a_next, yt_pred = rnn_cell_forward(xt, a_next, parameters)
        
        # ì €ì¥
        a[:, :, t] = a_next
        y_pred[:, :, t] = yt_pred
        
    return a, y_pred

def softmax(x):
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ì™œ ReLU ëŒ€ì‹  Tanhë¥¼ ì“°ë‚˜ìš”?} \\
\textbf{A.} RNNì€ ê°™ì€ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì‹­, ìˆ˜ë°± ë²ˆ ë°˜ë³µí•´ì„œ ê³±í•©ë‹ˆë‹¤. ReLUë¥¼ ì“°ë©´ ê°’ì´ ê³„ì† ì»¤ì ¸ì„œ ë°œì‚°(Exploding)í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. TanhëŠ” ê°’ì„ -1~1 ì‚¬ì´ë¡œ ë¬¶ì–´ë‘ì–´(Bounding) ì•ˆì •ì ì¸ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤.

\textbf{Q. RNNì€ ë³‘ë ¬ ì²˜ë¦¬ê°€ ì•ˆ ë˜ë‚˜ìš”?} \\
\textbf{A.} ë„¤, êµ¬ì¡°ì ìœ¼ë¡œ ì–´ë µìŠµë‹ˆë‹¤. $t$ ì‹œì ì˜ ê³„ì‚°ì„ í•˜ë ¤ë©´ ë°˜ë“œì‹œ $t-1$ ì‹œì ì˜ ê²°ê³¼ê°€ ë‚˜ì™€ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤(Sequential). ì´ê²ƒì´ íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)ê°€ ë“±ì¥í•˜ê²Œ ëœ ë°°ê²½ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ê¸°ë³¸ RNNì€ ì´ë¡ ì ìœ¼ë¡œ í›Œë¥­í•˜ì§€ë§Œ, ë¬¸ì¥ì´ ì¡°ê¸ˆë§Œ ê¸¸ì–´ì ¸ë„(10ë‹¨ì–´ ì´ìƒ) ì•ë¶€ë¶„ ë‚´ìš©ì„ ê¹Œë¨¹ëŠ” **ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ**ê°€ ìˆìŠµë‹ˆë‹¤.

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë”¥ëŸ¬ë‹ ì—°êµ¬ìë“¤ì€ **"ê¸°ì–µì„ ì–¼ë§ˆë‚˜ ì˜¤ë˜ ìœ ì§€í• ì§€ ìŠ¤ìŠ¤ë¡œ ê²°ì •í•˜ëŠ” ê²Œì´íŠ¸(Gate)"**ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” í˜„ëŒ€ NLPì˜ ê·¼ê°„ì´ ëœ **[GRU (Gated Recurrent Unit)]**ì™€ **[LSTM (Long Short-Term Memory)]**ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Structure:} ì…ë ¥($x$) + ì´ì „ ê¸°ì–µ($a$) $\rightarrow$ ìƒˆ ê¸°ì–µ $\rightarrow$ ì¶œë ¥($y$).
    \item \textbf{Sharing:} ëª¨ë“  ì‹œì ì—ì„œ ë™ì¼í•œ íŒŒë¼ë¯¸í„°($W_{ax}, W_{aa}$)ë¥¼ ì“´ë‹¤.
    \item \textbf{Formula:} $a^{\langle t \rangle} = \tanh(W_{aa}a^{\langle t-1 \rangle} + W_{ax}x^{\langle t \rangle} + b_a)$.
    \item \textbf{Limit:} ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ì–´(Vanishing Gradient) ì´ˆê¸° ê¸°ì–µì„ ìƒëŠ”ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}
\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{architecturebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ—ï¸ #1 (êµ¬ì¡° ë¶„ì„)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Various RNN Architectures}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Unit)}
    \begin{itemize}
        \item 10.1 Recurrent Neural Networks (RNN Basics) \textit{- Completed}
        \item \textbf{10.2 Various RNN Architectures}
        \begin{itemize}
            \item One-to-Many (Music Generation)
            \item Many-to-One (Sentiment Analysis)
            \item Many-to-Many (Synced vs Async)
            \item Encoder-Decoder Structure
        \end{itemize}
        \item 10.3 Language Model & Sequence Generation \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” RNNì˜ ê¸°ë³¸ êµ¬ì¡°(Basic RNN Cell)ë¥¼ ë°°ì› ìŠµë‹ˆë‹¤. ë‹¹ì‹œì—ëŠ” ì…ë ¥ ê¸¸ì´($T_x$)ì™€ ì¶œë ¥ ê¸¸ì´($T_y$)ê°€ ê°™ì€ ê²½ìš°ë§Œ ê°€ì •í–ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ í˜„ì‹¤ì€ ë‹¤ë¦…ë‹ˆë‹¤. **"I love you"(3ë‹¨ì–´)**ë¥¼ ë²ˆì—­í•˜ë©´ **"Je t'aime"(2ë‹¨ì–´)**ê°€ ë©ë‹ˆë‹¤. ìŒì•… ìƒì„±ì€ **ì¥ë¥´ í•˜ë‚˜(1)**ë¥¼ ì£¼ë©´ **ê³¡ ì „ì²´(ìˆ˜ë°±)**ë¥¼ ë§Œë“­ë‹ˆë‹¤.
RNNì˜ ê°•ë ¥í•¨ì€ ë°”ë¡œ ì´ **ì…ì¶œë ¥ êµ¬ì¡°ì˜ ìœ ì—°ì„±(Flexibility)**ì—ì„œ ë‚˜ì˜µë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì…ë ¥($T_x$)ê³¼ ì¶œë ¥($T_y$)ì˜ ì¡°í•©ì— ë”°ë¥¸ 5ê°€ì§€ RNN ì•„í‚¤í…ì²˜ë¥¼ ë¶„ë¥˜í•˜ê³  ì´í•´í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{One-to-Many:} í•˜ë‚˜ì˜ ì…ë ¥ìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ìƒì„± (ìŒì•…, ìº¡ì…”ë‹).
    \item \textbf{Many-to-One:} ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ê²°ê³¼ë¡œ ìš”ì•½ (ê°ì„± ë¶„ì„).
    \item \textbf{Many-to-Many (Same):} ì…ë ¥ë§ˆë‹¤ ì¦‰ì‹œ ì¶œë ¥ (ê°œì²´ëª… ì¸ì‹).
    \item \textbf{Many-to-Many (Diff):} ë‹¤ ë“£ê³  ë§í•˜ê¸° (**ê¸°ê³„ ë²ˆì—­, Encoder-Decoder**).
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|c|l|}
\hline
\textbf{êµ¬ì¡°} & \textbf{ì…ë ¥ : ì¶œë ¥} & \textbf{ëŒ€í‘œ ì˜ˆì‹œ} \\ \hline
\textbf{One-to-One} & 1 : 1 & ì¼ë°˜ì ì¸ ì‹ ê²½ë§ (ì´ë¯¸ì§€ ë¶„ë¥˜). \\ \hline
\textbf{One-to-Many} & 1 : N & ìŒì•… ìƒì„±, ì´ë¯¸ì§€ ìº¡ì…”ë‹. \\ \hline
\textbf{Many-to-One} & N : 1 & ê°ì„± ë¶„ì„ (ë¦¬ë·° $\to$ ë³„ì ). \\ \hline
\textbf{Many-to-Many} & N : N & ê°œì²´ëª… ì¸ì‹ (NER). \\ \hline
\textbf{Seq2Seq} & N : M & ê¸°ê³„ ë²ˆì—­ (ë²ˆì—­ì€ ë¬¸ì¥ ëê¹Œì§€ ë“¤ì–´ì•¼ í•¨). \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: êµ¬ì¡°ì˜ ë‹¤ì–‘ì„±}

\subsection{1. One-to-Many ($T_x=1, T_y>1$)}

í•˜ë‚˜ì˜ ì”¨ì•—(Seed) ì •ë³´ë¥¼ ì£¼ë©´ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
\begin{itemize}
    \item **ë™ì‘:** ì²« íƒ€ì„ ìŠ¤í…ì—ì„œ ì…ë ¥ $x$ë¥¼ ë°›ìŠµë‹ˆë‹¤. ê·¸ ì´í›„ì—ëŠ” ì „ ë‹¨ê³„ì˜ ì¶œë ¥ $\hat{y}^{\langle t-1 \rangle}$ì„ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ìœ¼ë¡œ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤ (Auto-regressive).
    \item **ì˜ˆì‹œ:** ìŒì•… ìƒì„± (ì¥ë¥´ $\to$ ë©œë¡œë””), ì´ë¯¸ì§€ ìº¡ì…”ë‹ (ì´ë¯¸ì§€ $\to$ "ê³ ì–‘ì´ê°€ ì”ë‹¤").
\end{itemize}

\subsection{2. Many-to-One ($T_x>1, T_y=1$)}

ê¸´ ì‹œí€€ìŠ¤ë¥¼ ì½ì–´ì„œ í•˜ë‚˜ì˜ ê²°ë¡ ì„ ë‚´ë¦½ë‹ˆë‹¤.
\begin{itemize}
    \item **ë™ì‘:** ì‹œí€€ìŠ¤ë¥¼ ëê¹Œì§€ ì½ìœ¼ë©° ì€ë‹‰ ìƒíƒœ($a$)ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì¶œë ¥ì€ **ë§ˆì§€ë§‰ íƒ€ì„ ìŠ¤í…**ì—ì„œë§Œ ë‚˜ì˜µë‹ˆë‹¤.
    \item **ì˜ˆì‹œ:** ì˜í™” ë¦¬ë·° ê°ì„± ë¶„ì„ (í…ìŠ¤íŠ¸ $\to$ ê¸ì •/ë¶€ì •).
\end{itemize}

---

\section{Deep Dive: Many-to-Many (The Tricky Part)}

Many-to-ManyëŠ” ë‹¤ì‹œ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì´ ì°¨ì´ê°€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

\subsection{1. Synced Many-to-Many ($T_x = T_y$)}

ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê¸¸ì´ê°€ ê°™ê³ , íƒ€ì´ë°ì´ ì¼ì¹˜í•©ë‹ˆë‹¤.
\begin{itemize}
    \item **ë™ì‘:** ì…ë ¥ì´ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ì¦‰ì‹œ ì¶œë ¥ì„ ë±‰ìŠµë‹ˆë‹¤.
    \item **ì˜ˆì‹œ:** ë¹„ë””ì˜¤ í”„ë ˆì„ ë¶„ë¥˜, ê°œì²´ëª… ì¸ì‹(NER).
\end{itemize}

\subsection{2. Asynchronous Many-to-Many ($T_x \neq T_y$) - Seq2Seq}

ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ê¸°ê³„ ë²ˆì—­ì˜ í‘œì¤€ì¸ **Encoder-Decoder** êµ¬ì¡°ì…ë‹ˆë‹¤.
\begin{itemize}
    \item **Encoder (Many-to-One):** ì…ë ¥ ë¬¸ì¥ ì „ì²´ë¥¼ ì½ì–´ì„œ í•˜ë‚˜ì˜ **ë¬¸ë§¥ ë²¡í„°(Context Vector)**ë¡œ ì••ì¶•í•©ë‹ˆë‹¤.
    \item **Decoder (One-to-Many):** ì••ì¶•ëœ ë²¡í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²ˆì—­ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.
    \item **ì´ìœ :** "ë‚˜ëŠ” ë„ˆë¥¼ ì‚¬ë‘í•´"ë¥¼ ë²ˆì—­í•˜ë ¤ë©´ ë¬¸ì¥ ëê¹Œì§€ ë“¤ì–´ë´ì•¼ ì–´ìˆœì„ ë§ì¶œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
\end{itemize}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Encoder-Decoder Structure}

Kerasë¥¼ ì´ìš©í•´ Seq2Seq ëª¨ë¸ì˜ ê°œë…ì  êµ¬ì¡°ë¥¼ êµ¬í˜„í•´ ë´…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Encoder-Decoder Implementation]
from tensorflow.keras.layers import Input, LSTM, Dense
from tensorflow.keras.models import Model

def build_seq2seq(n_x, n_y, Tx, Ty, n_a):
    """
    n_x, n_y: ì…/ì¶œë ¥ ë‹¨ì–´ ì‚¬ì „ í¬ê¸°
    Tx, Ty: ì…/ì¶œë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´
    n_a: ì€ë‹‰ ìƒíƒœ í¬ê¸°
    """
    
    # --- 1. Encoder (ì…ë ¥ ì••ì¶•) ---
    encoder_inputs = Input(shape=(Tx, n_x))
    
    # return_state=True: ë§ˆì§€ë§‰ ìƒíƒœ(h, c)ë¥¼ ë°˜í™˜ë°›ìŒ
    # return_sequences=False: ì¤‘ê°„ ì¶œë ¥ì€ í•„ìš” ì—†ìŒ (ì••ì¶•ì´ ëª©ì )
    encoder = LSTM(n_a, return_state=True)
    
    _, state_h, state_c = encoder(encoder_inputs)
    
    # ì´ 'encoder_states'ê°€ ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ ë‹´ì€ 'ë¬¸ë§¥'
    encoder_states = [state_h, state_c]
    
    # --- 2. Decoder (ë²ˆì—­ ìƒì„±) ---
    decoder_inputs = Input(shape=(Ty, n_y))
    
    # Encoderì˜ ìƒíƒœë¥¼ Decoderì˜ ì´ˆê¸° ìƒíƒœ(initial_state)ë¡œ ì£¼ì…!
    decoder_lstm = LSTM(n_a, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, 
                                         initial_state=encoder_states)
    
    # ë‹¨ì–´ ì˜ˆì¸¡ (Softmax)
    decoder_dense = Dense(n_y, activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    
    # ëª¨ë¸ ì •ì˜
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    return model
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ì…ë ¥ ë¬¸ì¥ì´ ë„ˆë¬´ ê¸¸ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?} \\
\textbf{A.} Encoderê°€ ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ì— ìš±ì—¬ë„£ì–´ì•¼ í•˜ë¯€ë¡œ, ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´(ì˜ˆ: 50ë‹¨ì–´ ì´ìƒ) ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‚˜ì¤‘ì— **Attention Mechanism**ì´ ë“±ì¥í•©ë‹ˆë‹¤.

\textbf{Q. ì´ë¯¸ì§€ ìº¡ì…”ë‹ì€ ì–´ë–¤ êµ¬ì¡°ì¸ê°€ìš”?} \\
\textbf{A.} **One-to-Many**ì…ë‹ˆë‹¤. ì…ë ¥ì€ ì´ë¯¸ì§€(CNNì„ í†µê³¼í•œ 1ê°œì˜ ë²¡í„°), ì¶œë ¥ì€ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤(RNN)ì…ë‹ˆë‹¤. CNNì´ Encoder, RNNì´ Decoder ì—­í• ì„ í•˜ëŠ” ì…ˆì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
RNNì€ ì´ë ‡ê²Œ ë‹¤ì–‘í•œ êµ¬ì¡°ë¡œ ë³€ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì–´ë–¤ êµ¬ì¡°ë¥¼ ì“°ë“ , ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§€ë©´ ì•ì˜ ë‚´ìš©ì„ ìŠì–´ë²„ë¦¬ëŠ” **'ê¸°ìš¸ê¸° ì†Œì‹¤'**ì´ë¼ëŠ” ê³ ì§ˆë³‘ì€ ì—¬ì „í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **"ê¸°ì–µì„ ì €ì¥í•˜ëŠ” ê¸ˆê³ (Cell State)"**ì™€ **"ë¬¸ì˜ ì—´ì‡ (Gate)"**ë¥¼ ë„ì…í•œ í˜„ëŒ€ì  RNNì˜ í‘œì¤€, **[GRU]**ì™€ **[LSTM]**ì„ í•´ë¶€í•˜ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Flexibility:} $T_x$ì™€ $T_y$ê°€ ë‹¬ë¼ë„ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤.
    \item \textbf{Many-to-One:} ê°ì„± ë¶„ì„ (ì •ë³´ ìš”ì•½).
    \item \textbf{Many-to-Many:} ê°œì²´ëª… ì¸ì‹ (ì¦‰ì‹œ ì¶œë ¥) vs ê¸°ê³„ ë²ˆì—­ (ë‹¤ ë“£ê³  ì¶œë ¥).
    \item \textbf{Seq2Seq:} Encoderê°€ ì••ì¶•í•˜ê³  Decoderê°€ ìƒì„±í•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ìˆ˜ì‹)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Vanishing Gradients \& GRU}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1 RNN Basics \textit{- Completed}
        \item 10.2 Various RNN Architectures \textit{- Completed}
        \item \textbf{10.3 Vanishing Gradients \& Gated Units}
        \begin{itemize}
            \item The Problem: Vanishing Gradients
            \item The Solution: Gating Mechanism
            \item GRU (Gated Recurrent Unit)
            \item Implementation: Update \& Reset Gates
        \end{itemize}
        \item 10.4 LSTM (Long Short-Term Memory) \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ ê¸°ë³¸ RNNì€ ì´ë¡ ì ìœ¼ë¡œëŠ” ì™„ë²½í•´ ë³´ì…ë‹ˆë‹¤. ê³¼ê±° ì •ë³´ë¥¼ í˜„ì¬ë¡œ ì „ë‹¬í•˜ë‹ˆê¹Œìš”.
í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë¬¸ì¥ì´ 10ë‹¨ì–´ë§Œ ë„˜ì–´ê°€ë„ ì•ë¶€ë¶„ ë‚´ìš©ì„ ê¹Œë§£ê²Œ ìŠì–´ë²„ë¦½ë‹ˆë‹¤.
**"The cat, which ate ..., was full."**
RNNì€ ì¤‘ê°„ ìˆ˜ì‹ì–´ê°€ ê¸¸ì–´ì§€ë©´ ì£¼ì–´ 'cat(ë‹¨ìˆ˜)'ì„ ìŠì–´ë²„ë¦¬ê³  ë™ì‚¬ 'was'ë¥¼ ì˜ˆì¸¡í•˜ì§€ ëª»í•©ë‹ˆë‹¤.
ì´ê²ƒì´ ë°”ë¡œ **ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)** ë¬¸ì œì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì´ ë‚œì œë¥¼ í•´ê²°í•œ **GRU**ë¥¼ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ RNNì˜ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²Œì´íŠ¸(Gate) ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì›ì¸:} ì—­ì „íŒŒ ì‹œ ê¸°ìš¸ê¸°ê°€ ì§€ìˆ˜ì ìœ¼ë¡œ ì‘ì•„ì ¸ì„œ ì´ˆê¸° ê¸°ì–µì´ ì‚¬ë¼ì§€ëŠ” ìˆ˜í•™ì  ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{í•´ê²°:} ì •ë³´ë¥¼ "ì–¼ë§ˆë‚˜ ìœ ì§€í•˜ê³  ë²„ë¦´ì§€" ê²°ì •í•˜ëŠ” **ê²Œì´íŠ¸(Gate)** ê°œë…ì„ ë„ì…í•©ë‹ˆë‹¤.
    \item \textbf{ëª¨ë¸:} LSTMì˜ ê°„ì†Œí™” ë²„ì „ì¸ **GRU**ì˜ êµ¬ì¡°ì™€ ìˆ˜ì‹ì„ ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ì—­í• } \\ \hline
\textbf{Vanishing Gradient} & ê¸°ìš¸ê¸° ì†Œì‹¤ & ê¹Šì€ ì‹ ê²½ë§ í•™ìŠµì„ ë°©í•´í•˜ëŠ” ì£¼ë²”. \\ \hline
\textbf{Gate ($\Gamma$)} & ë¬¸ì§€ê¸° (0~1) & ì •ë³´ íë¦„ì„ ì œì–´í•˜ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜. \\ \hline
\textbf{Update Gate ($\Gamma_u$)} & ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸ & ê³¼ê±° ê¸°ì–µì„ ì–¼ë§ˆë‚˜ ìœ ì§€í• ì§€ ê²°ì •. \\ \hline
\textbf{Reset Gate ($\Gamma_r$)} & ë¦¬ì…‹ ê²Œì´íŠ¸ & ê³¼ê±° ê¸°ì–µì„ ì–¼ë§ˆë‚˜ ë¬´ì‹œí• ì§€ ê²°ì •. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ê¸°ì–µì˜ ë³´ì¡´}

\subsection{1. The Problem: Vanishing Gradients}
RNNì—ì„œ $t=100$ ì‹œì ì˜ ì˜¤ì°¨ë¥¼ $t=1$ ì‹œì ê¹Œì§€ ì „íŒŒí•˜ë ¤ë©´ ê°€ì¤‘ì¹˜ í–‰ë ¬ $W$ë¥¼ 100ë²ˆ ê³±í•´ì•¼ í•©ë‹ˆë‹¤.

\begin{analogybox}{ë³µì‚¬ë³¸ì˜ ë³µì‚¬ë³¸}
ë¬¸ì„œë¥¼ ë³µì‚¬í•˜ê³ , ê·¸ ë³µì‚¬ë³¸ì„ ë˜ ë³µì‚¬í•˜ëŠ” ê³¼ì •ì„ 100ë²ˆ ë°˜ë³µí•œë‹¤ê³  ìƒìƒí•´ ë³´ì„¸ìš”.
ì¡°ê¸ˆì´ë¼ë„ íë¦¿í•´ì§€ë©´($W < 1$), 100ë²ˆì§¸ ë³µì‚¬ë³¸ì€ ë°±ì§€ê°€ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤.
ë°˜ëŒ€ë¡œ ì§„í•´ì§€ë©´($W > 1$), ê²€ì€ìƒ‰ ì‰í¬ ë©ì–´ë¦¬ê°€ ë©ë‹ˆë‹¤(Exploding).
\end{analogybox}

\subsection{2. The Solution: Gated Recurrent Unit (GRU)}
í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **"ê¸°ì–µì„ ìœ„í•œ ì „ìš© ê¸ˆê³ (Memory Cell)ë¥¼ ë§Œë“¤ê³  ë¬¸ì§€ê¸°ë¥¼ ì„¸ìš°ì"**ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{formulabox}{GRUì˜ í•µì‹¬ ìˆ˜ì‹}
$$ c^{\langle t \rangle} = \Gamma_u \cdot \tilde{c}^{\langle t \rangle} + (1 - \Gamma_u) \cdot c^{\langle t-1 \rangle} $$
\begin{itemize}
    \item $\Gamma_u$ (Update Gate): 0ì´ë©´ ë¬¸ì„ ë‹«ìŠµë‹ˆë‹¤.
    \item $\Gamma_u \approx 0$ ì¼ ë•Œ: $c^{\langle t \rangle} \approx c^{\langle t-1 \rangle}$.
    \item **ì˜ë¯¸:** ê³¼ê±°ì˜ ê¸°ì–µ($c^{\langle t-1 \rangle}$)ì´ ì•„ë¬´ëŸ° ë³€í˜• ì—†ì´(í–‰ë ¬ ê³±ì…ˆ ì—†ì´) ê·¸ëŒ€ë¡œ ë³µì‚¬ë˜ì–´ ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. **ê¸°ìš¸ê¸° ì†Œì‹¤ ì—†ì´ ê³ ì†ë„ë¡œì²˜ëŸ¼ ì „ë‹¬**ë©ë‹ˆë‹¤.
\end{itemize}
\end{formulabox}

---

\section{Deep Dive: GRU Gates Detail}

GRUëŠ” ë‘ ê°œì˜ ê²Œì´íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

\begin{enumerate}
    \item **Update Gate ($\Gamma_u$):** "ì´ ê¸°ì–µì„ ê³„ì† ê°€ì ¸ê°ˆê¹Œ?" (ë‹¨ìˆ˜/ë³µìˆ˜ ì •ë³´ ìœ ì§€)
    $$ \Gamma_u = \sigma(W_u[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u) $$
    
    \item **Reset Gate ($\Gamma_r$):** "ì´ì œ ê³¼ê±°ëŠ” ìŠì„ê¹Œ?" (ë¬¸ì¹¨í‘œê°€ ë‚˜ì˜¤ë©´ ë¦¬ì…‹)
    $$ \Gamma_r = \sigma(W_r[c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_r) $$
    
    \item **Candidate Memory ($\tilde{c}$):** ìƒˆë¡œìš´ ê¸°ì–µ í›„ë³´
    $$ \tilde{c}^{\langle t \rangle} = \tanh(W_c[\Gamma_r \cdot c^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c) $$
\end{enumerate}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: GRU Cell}

\begin{lstlisting}[language=Python, caption=GRU Cell Forward Implementation]
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def gru_cell_forward(xt, a_prev, parameters):
    """
    xt: í˜„ì¬ ì…ë ¥
    a_prev: ê³¼ê±° ì€ë‹‰ ìƒíƒœ (GRUì—ì„  Memory Cell cì™€ ë™ì¼)
    """
    # íŒŒë¼ë¯¸í„° ë¡œë“œ
    W_u = parameters["Wu"] # Update Gate Weights
    W_r = parameters["Wr"] # Reset Gate Weights
    W_c = parameters["Wc"] # Candidate Memory Weights
    
    # ì…ë ¥ ê²°í•© (Concatenate)
    concat = np.concatenate((a_prev, xt), axis=0)
    
    # 1. Update Gate (Gamma_u)
    Gamma_u = sigmoid(np.dot(W_u, concat) + parameters["bu"])
    
    # 2. Reset Gate (Gamma_r)
    Gamma_r = sigmoid(np.dot(W_r, concat) + parameters["br"])
    
    # 3. Candidate Memory (c_tilde)
    # ê³¼ê±° ê¸°ì–µì— Reset Gate ì ìš©
    concat_reset = np.concatenate((Gamma_r * a_prev, xt), axis=0)
    c_candidate = np.tanh(np.dot(W_c, concat_reset) + parameters["bc"])
    
    # 4. Final Memory Update (The Magic Formula)
    # uê°€ 0ì´ë©´ ê³¼ê±° ê¸°ì–µ ë³´ì¡´, 1ì´ë©´ ìƒˆ ê¸°ì–µìœ¼ë¡œ ë®ì–´ì”€
    c_next = Gamma_u * c_candidate + (1 - Gamma_u) * a_prev
    
    return c_next
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{Gradient Clipping (ê¸°ìš¸ê¸° ìë¥´ê¸°)}
ê¸°ìš¸ê¸° ì†Œì‹¤ì˜ ë°˜ëŒ€ëŠ” **ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient)**ì…ë‹ˆë‹¤. ìˆ«ìê°€ ë„ˆë¬´ ì»¤ì ¸ì„œ `NaN`ì´ ëœ¹ë‹ˆë‹¤.
í•´ê²°ì±…ì€ ê°„ë‹¨í•©ë‹ˆë‹¤. ê¸°ìš¸ê¸° ë²¡í„°ì˜ í¬ê¸°(Norm)ê°€ íŠ¹ì • ê°’(ì˜ˆ: 5)ì„ ë„˜ìœ¼ë©´ ê°•ì œë¡œ ì¤„ì—¬ë²„ë¦¬ëŠ” **Gradient Clipping**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. GRUì™€ LSTM ì¤‘ ë­ê°€ ë” ì¢‹ë‚˜ìš”?} \\
\textbf{A.} **ì •ë‹µì€ ì—†ìŠµë‹ˆë‹¤.** GRUëŠ” êµ¬ì¡°ê°€ ë‹¨ìˆœí•´ì„œ(ê²Œì´íŠ¸ 2ê°œ) ì—°ì‚°ì´ ë¹ ë¥´ê³  ë°ì´í„°ê°€ ì ì„ ë•Œ ìœ ë¦¬í•©ë‹ˆë‹¤. LSTMì€ êµ¬ì¡°ê°€ ë³µì¡í•˜ì§€ë§Œ(ê²Œì´íŠ¸ 3ê°œ) í‘œí˜„ë ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤. ë³´í†µ LSTMì„ ê¸°ë³¸ìœ¼ë¡œ ì“°ê³ , ì†ë„ê°€ ì¤‘ìš”í•˜ë©´ GRUë¥¼ ì”ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
GRUëŠ” í›Œë¥­í•˜ê³  ë‹¨ìˆœí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë•Œë¡œëŠ” ë” ì„¬ì„¸í•œ ì œì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤. GRUë³´ë‹¤ ì¡°ê¸ˆ ë” ë³µì¡í•˜ì§€ë§Œ, ë” ê°•ë ¥í•˜ê³  ë„ë¦¬ ì“°ì´ëŠ” í˜•ë‹˜ì´ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” GRUì˜ í™•ì¥íŒì´ì RNNì˜ ì‚¬ì‹¤ìƒ í‘œì¤€(De facto Standard), **[LSTM (Long Short-Term Memory)]**ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤. 3ê°œì˜ ê²Œì´íŠ¸(Forget, Input, Output)ê°€ ì–´ë–»ê²Œ ê¸°ì–µì„ ìˆ˜ìˆ í•˜ë“¯ ì •êµí•˜ê²Œ ë‹¤ë£¨ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Vanishing Gradient:} ì‹œí€€ìŠ¤ê°€ ê¸¸ì–´ì§€ë©´ ì´ˆê¸° ì •ë³´ê°€ ì‚¬ë¼ì§€ëŠ” ë¬¸ì œ.
    \item \textbf{Gate:} ì‹œê·¸ëª¨ì´ë“œë¥¼ ì‚¬ìš©í•´ ì •ë³´ì˜ í†µê³¼ ì—¬ë¶€(0~1)ë¥¼ ê²°ì •í•˜ëŠ” ë°¸ë¸Œ.
    \item \textbf{GRU:} Update/Reset ê²Œì´íŠ¸ë¥¼ ì‚¬ìš©í•´ ê¸°ì–µì„ ë³´ì¡´í•˜ê±°ë‚˜ ê°±ì‹ í•œë‹¤.
    \item \textbf{Key:} $\Gamma_u=0$ì¼ ë•Œ ê³¼ê±° ê¸°ì–µì´ ì†ì‹¤ ì—†ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬ëœë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ìˆ˜ì‹)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ LSTM (Long Short-Term Memory)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.3 RNN Basics \& GRU \textit{- Completed}
        \item \textbf{10.4 LSTM (Long Short-Term Memory)}
        \begin{itemize}
            \item Difference: Cell State vs Hidden State
            \item The Three Gates (Forget, Update, Output)
            \item Mathematical Equations
            \item GRU vs LSTM Comparison
        \end{itemize}
        \item 10.5 NLP & Word Embeddings \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ GRUëŠ” **'ê¸°ìš¸ê¸° ì†Œì‹¤'**ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ê²Œì´íŠ¸ë¥¼ ë„ì…í•œ í›Œë¥­í•œ ëª¨ë¸ì´ì—ˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ GRUëŠ” 2014ë…„ì— ë‚˜ì˜¨ ëª¨ë¸ì´ê³ , ê·¸ë³´ë‹¤ í›¨ì”¬ ì „ì¸ 1997ë…„ì— ì œì•ˆë˜ì–´ ì§€ê¸ˆê¹Œì§€ **ì‹œí€€ìŠ¤ ëª¨ë¸ì˜ ì œì™•(Standard)**ìœ¼ë¡œ êµ°ë¦¼í•˜ê³  ìˆëŠ” ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤.
ë°”ë¡œ **LSTM**ì…ë‹ˆë‹¤. GRUê°€ 2ê°œì˜ ê²Œì´íŠ¸ë¡œ íš¨ìœ¨ì„±ì„ ì¶”êµ¬í–ˆë‹¤ë©´, LSTMì€ 3ê°œì˜ ê²Œì´íŠ¸ë¡œ ê¸°ì–µì„ ë”ìš± ì •êµí•˜ê²Œ ì œì–´í•©ë‹ˆë‹¤. ì•¤ë“œë¥˜ ì‘ êµìˆ˜ë‹˜ì€ ë§í•©ë‹ˆë‹¤.
**"ë¬´ì—‡ì„ ì“¸ì§€ ëª¨ë¥´ê² ë‹¤ë©´, ì¼ë‹¨ LSTMë¶€í„° ì‹œì‘í•˜ì‹­ì‹œì˜¤."**

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì‹œí€€ìŠ¤ ëª¨ë¸ì˜ ì—…ê³„ í‘œì¤€ì¸ LSTMì˜ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬ë¥¼ íŒŒí—¤ì¹©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ì¡°:} **Cell State($c$)**ì™€ **Hidden State($a$)**ê°€ ë¶„ë¦¬ëœ ì´ì¤‘ íŠ¸ë™ êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ê²Œì´íŠ¸:} Forget, Update, Outputì´ë¼ëŠ” 3ê°œì˜ ê²Œì´íŠ¸ ì—­í• ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{ìˆ˜ì‹:} ê³¼ê±°ë¥¼ ìŠê³ ($\Gamma_f$) ìƒˆë¡œìš´ ê¸°ì–µì„ ë”í•˜ëŠ”($\Gamma_u$) ë…ë¦½ì  ì œì–´ ê³µì‹ì„ ìµí™ë‹ˆë‹¤.
    \item \textbf{ë¹„êµ:} GRUì™€ LSTMì˜ ì¥ë‹¨ì ì„ ë¹„êµí•˜ê³  ì„ íƒ ê¸°ì¤€ì„ ì„¸ì›ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ê¸°í˜¸} & \textbf{ì—­í• } \\ \hline
\textbf{Cell State} & $c^{\langle t \rangle}$ & **ì¥ê¸° ê¸°ì–µ ê³ ì†ë„ë¡œ.** ë‚´ë¶€ì—ì„œë§Œ ìˆœí™˜í•˜ë©° ì •ë³´ë¥¼ ë³´ì¡´í•¨. \\ \hline
\textbf{Hidden State} & $a^{\langle t \rangle}$ & **ë‹¨ê¸° ìƒíƒœ ë° ì¶œë ¥.** Cell Stateë¥¼ ê°€ê³µí•˜ì—¬ ì™¸ë¶€ë¡œ ë‚´ë³´ëƒ„. \\ \hline
\textbf{Forget Gate} & $\Gamma_f$ & ê³¼ê±°ì˜ ê¸°ì–µì„ ì‚­ì œí•˜ëŠ” ë¹„ìœ¨ (0~1). \\ \hline
\textbf{Update Gate} & $\Gamma_u$ & ìƒˆë¡œìš´ ê¸°ì–µì„ ì €ì¥í•˜ëŠ” ë¹„ìœ¨ (0~1). \\ \hline
\textbf{Output Gate} & $\Gamma_o$ & í˜„ì¬ ìƒíƒœë¥¼ ë‹¤ìŒ ì¸µìœ¼ë¡œ ë‚´ë³´ë‚´ëŠ” ë¹„ìœ¨ (0~1). \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ê¸°ì–µì˜ ì •êµí•œ ì œì–´}

\subsection{1. The Key Difference: $c$ and $a$}
GRUëŠ” ê¸°ì–µ($c$)ê³¼ ì¶œë ¥($a$)ì´ í†µí•©ë˜ì–´ ìˆì—ˆìŠµë‹ˆë‹¤. ë°˜ë©´ LSTMì€ ì´ë¥¼ ì—„ê²©íˆ ë¶„ë¦¬í•©ë‹ˆë‹¤.



\begin{itemize}
    \item **Cell State ($c^{\langle t \rangle}$):** ê¸°ì–µì˜ í•µì‹¬ì…ë‹ˆë‹¤. ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ì§€ ì•Šë„ë¡ ë³´í˜¸ë°›ëŠ” ê²½ë¡œì…ë‹ˆë‹¤.
    \item **Hidden State ($a^{\langle t \rangle}$):** Cell Stateì— $\tanh$ë¥¼ ì”Œìš°ê³  Output Gateë¥¼ í†µê³¼ì‹œì¼œ ë§Œë“ , "ì§€ê¸ˆ ë‹¹ì¥ í•„ìš”í•œ ì •ë³´"ì…ë‹ˆë‹¤.
\end{itemize}

\subsection{2. The Three Gates (3ê°œì˜ ë¬¸ì§€ê¸°)}
LSTMì€ ê¸°ì–µì„ ê´€ë¦¬í•˜ê¸° ìœ„í•´ 3ë‹¨ê³„ ê²€ë¬¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

\begin{analogybox}{LSTMì˜ ê¸°ì–µ ê´€ë¦¬ ì‹œìŠ¤í…œ}
\begin{itemize}
    \item **Forget Gate ($\Gamma_f$): [ì“°ë ˆê¸°í†µ]** "ì´ì „ ê¸°ì–µ ì¤‘ ì“¸ëª¨ì—†ëŠ” ê±´ ë²„ë ¤ë¼." (ì˜ˆ: ë¬¸ë‹¨ì´ ë°”ë€Œì—ˆìœ¼ë‹ˆ ì´ì „ ì£¼ì œ ì‚­ì œ)
    \item **Update Gate ($\Gamma_u$): [ê¸°ë¡ì¥]** "ìƒˆë¡œìš´ ì •ë³´ ì¤‘ ì¤‘ìš”í•œ ê²ƒë§Œ ì ì–´ë¼." (ì˜ˆ: ìƒˆë¡œìš´ ì£¼ì–´ ë“±ë¡)
    \item **Output Gate ($\Gamma_o$): [ìŠ¤í”¼ì»¤]** "ì§€ê¸ˆ ë‹¹ì¥ í•„ìš”í•œ ì •ë³´ë§Œ ë§í•´ë¼." (ì˜ˆ: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì— í•„ìš”í•œ ì •ë³´ë§Œ ë°œì„¤)
\end{itemize}
\end{analogybox}

---

\section{Deep Dive: LSTM Equations}

ì´ ìˆ˜ì‹ë“¤ì´ LSTMì˜ ì‘ë™ ì›ë¦¬ë¥¼ ë³´ì—¬ì£¼ëŠ” ì§€ë„ì…ë‹ˆë‹¤.


\subsection{1. Gate Calculation}
ì´ì „ ìƒíƒœ($a^{\langle t-1 \rangle}$)ì™€ í˜„ì¬ ì…ë ¥($x^{\langle t \rangle}$)ì„ ë³´ê³  ê²Œì´íŠ¸ë¥¼ ì–¼ë§ˆë‚˜ ì—´ì§€ ê²°ì •í•©ë‹ˆë‹¤.
$$ \Gamma_f = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f) $$
$$ \Gamma_u = \sigma(W_u[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_u) $$
$$ \Gamma_o = \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o) $$

\subsection{2. Memory Update (í•µì‹¬)}
\begin{formulabox}{Cell State Update}
$$ c^{\langle t \rangle} = \underbrace{\Gamma_f \cdot c^{\langle t-1 \rangle}}_{\text{ê³¼ê±° ê¸°ì–µ ì‚­ì œ}} + \underbrace{\Gamma_u \cdot \tilde{c}^{\langle t \rangle}}_{\text{ìƒˆ ê¸°ì–µ ì¶”ê°€}} $$
\begin{itemize}
    \item **GRUì™€ì˜ ì°¨ì´:** GRUëŠ” $\Gamma_u$ í•˜ë‚˜ë¡œ ê³¼ê±°ì™€ í˜„ì¬ì˜ ë¹„ìœ¨ì„ ì‹œì†Œì²˜ëŸ¼ ì¡°ì ˆí–ˆìŠµë‹ˆë‹¤ ($1-\Gamma_u$).
    \item **LSTM:** $\Gamma_f$ì™€ $\Gamma_u$ê°€ **ë…ë¦½ì **ì…ë‹ˆë‹¤. ê³¼ê±°ë¥¼ ê¸°ì–µí•˜ë©´ì„œ($\Gamma_f=1$) ë™ì‹œì— ìƒˆë¡œìš´ ì¤‘ìš”í•œ ì •ë³´ë„ ì¶”ê°€($\Gamma_u=1$)í•  ìˆ˜ ìˆì–´ í‘œí˜„ë ¥ì´ ë” í’ë¶€í•©ë‹ˆë‹¤.
\end{itemize}
\end{formulabox}

\subsection{3. Output Generation}
$$ a^{\langle t \rangle} = \Gamma_o \cdot \tanh(c^{\langle t \rangle}) $$

---

\section{Comparison: GRU vs LSTM}

\begin{center}
\begin{tabular}{|c|c|c|}
\hline
\textbf{íŠ¹ì§•} & \textbf{GRU} & \textbf{LSTM} \\ \hline
\textbf{ê²Œì´íŠ¸ ìˆ˜} & 2ê°œ (Update, Reset) & 3ê°œ (Forget, Update, Output) \\ \hline
\textbf{ë³µì¡ë„} & ë‹¨ìˆœí•¨ (Simpler) & ë³µì¡í•¨ (Powerful) \\ \hline
\textbf{ë°ì´í„° ì–‘} & ì ì„ ë•Œ ìœ ë¦¬ & ë§ì„ ë•Œ ìœ ë¦¬ (ëŒ€ìš©ëŸ‰ í•™ìŠµ) \\ \hline
\textbf{ìœ„ìƒ} & ê²½ëŸ‰í™” ëª¨ë¸ & **ì—…ê³„ í‘œì¤€ (Default Choice)** \\ \hline
\end{tabular}
\end{center}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: LSTM Cell}

\begin{lstlisting}[language=Python, caption=LSTM Cell Forward Implementation]
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def lstm_cell_forward(xt, a_prev, c_prev, parameters):
    """
    xt: í˜„ì¬ ì…ë ¥
    a_prev: ì´ì „ ì€ë‹‰ ìƒíƒœ (ë‹¨ê¸°)
    c_prev: ì´ì „ ì…€ ìƒíƒœ (ì¥ê¸°)
    """
    # íŒŒë¼ë¯¸í„° ë¡œë“œ (Wf, Wu, Wc, Wo ë“±)
    concat = np.concatenate((a_prev, xt), axis=0)
    
    # 1. Gates ê³„ì‚°
    ft = sigmoid(np.dot(parameters["Wf"], concat) + parameters["bf"]) # Forget
    it = sigmoid(np.dot(parameters["Wu"], concat) + parameters["bu"]) # Update(Input)
    ot = sigmoid(np.dot(parameters["Wo"], concat) + parameters["bo"]) # Output
    
    # 2. ìƒˆë¡œìš´ ê¸°ì–µ í›„ë³´ (Candidate)
    c_tilde = np.tanh(np.dot(parameters["Wc"], concat) + parameters["bc"])
    
    # 3. Cell State ì—…ë°ì´íŠ¸ (í•µì‹¬!)
    # ê³¼ê±°ë¥¼ ìŠì„ ê±´ ìŠê³ (ft), ìƒˆê²ƒì„ ë”í•¨(it)
    c_next = ft * c_prev + it * c_tilde
    
    # 4. Hidden State ì—…ë°ì´íŠ¸ (ì¶œë ¥)
    a_next = ot * np.tanh(c_next)
    
    return a_next, c_next
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. Peephole Connectionì´ë€ ë­”ê°€ìš”?} \\
\textbf{A.} ê¸°ë³¸ LSTMì—ì„œ ê²Œì´íŠ¸($\Gamma$)ëŠ” $a^{\langle t-1 \rangle}$ì™€ $x^{\langle t \rangle}$ë§Œ ë´…ë‹ˆë‹¤. Peephole ë³€í˜•ì€ ê²Œì´íŠ¸ê°€ **Cell State($c^{\langle t-1 \rangle}$)ë„ í›”ì³ë³´ê³ (Peep)** ê²°ì •ì„ ë‚´ë¦¬ê²Œ í•©ë‹ˆë‹¤. "ê¸°ì–µí†µì´ ì–¼ë§ˆë‚˜ ì°¼ëŠ”ì§€ ë³´ê³  ë¬¸ì„ ì—´ì§€ ë§ì§€ ì •í•œë‹¤"ëŠ” ê°œë…ì…ë‹ˆë‹¤.

\textbf{Q. ì™œ LSTMì´ ê¸°ìš¸ê¸° ì†Œì‹¤ì— ê°•í•œê°€ìš”?} \\
\textbf{A.} $c^{\langle t \rangle} = c^{\langle t-1 \rangle} + \dots$ í˜•íƒœì˜ **ë§ì…ˆ ì—°ì‚°** ë•Œë¬¸ì…ë‹ˆë‹¤. ì—­ì „íŒŒ ì‹œ ë§ì…ˆì€ ê¸°ìš¸ê¸°ë¥¼ ê·¸ëŒ€ë¡œ($\times 1$) ì „ë‹¬í•˜ëŠ” íŠ¹ì„±ì´ ìˆì–´, ê¹Šì€ ì‹œê°„ê¹Œì§€ ì˜¤ì°¨ê°€ ì˜ ì „ë‹¬ë©ë‹ˆë‹¤. (ResNetê³¼ ìœ ì‚¬ ì›ë¦¬)

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°€ì¥ ê°•ë ¥í•œ ì—”ì§„(LSTM)ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ì—”ì§„ì— ë„£ì„ **ì—°ë£Œ(ë°ì´í„°)**ë¥¼ ê°€ê³µí•  ì°¨ë¡€ì…ë‹ˆë‹¤.

ì»´í“¨í„°ëŠ” 'ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜'ë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ìˆ«ìë¡œ ë°”ê¿”ì•¼ í•˜ëŠ”ë°, ë‹¨ìˆœíˆ $1, 2$ë¡œ ë°”ê¾¸ë©´ ë‹¨ì–´ ê°„ ì˜ë¯¸ ê´€ê³„ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ì—¬ **"ì™• - ë‚¨ì + ì—¬ì = ì—¬ì™•"**ì´ë¼ëŠ” ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” ë§ˆë²•, **[Word Embedding (Word2Vec, GloVe)]**ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{LSTM:} 3ê°œì˜ ê²Œì´íŠ¸ë¡œ ê¸°ì–µì„ ê´€ë¦¬í•˜ëŠ” ì‹œí€€ìŠ¤ ëª¨ë¸ì˜ í‘œì¤€.
    \item \textbf{Structure:} ì¥ê¸° ê¸°ì–µ($c$)ê³¼ ë‹¨ê¸° ìƒíƒœ($a$)ë¥¼ ë¶„ë¦¬í•˜ì—¬ ìš´ì˜í•œë‹¤.
    \item \textbf{Gates:} Forget(ì‚­ì œ), Update(ì¶”ê°€), Output(ì¶œë ¥).
    \item \textbf{Strategy:} ì¼ë‹¨ LSTMì„ ê¸°ë³¸ìœ¼ë¡œ ì“°ê³ , ê²½ëŸ‰í™”ê°€ í•„ìš”í•˜ë©´ GRUë¥¼ ê³ ë ¤í•˜ë¼.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Word Representation (Embedding)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.4 RNN, GRU, LSTM \textit{- Completed}
        \item \textbf{10.5 Word Representation}
        \begin{itemize}
            \item One-hot Encoding (The Old Way)
            \item Word Embedding (The New Way)
            \item Analogy Reasoning (King - Man + Woman)
            \item Embedding Matrix & Lookup Table
        \end{itemize}
        \item 10.6 Word2Vec & GloVe \textit{- Upcoming}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ ìš°ë¦¬ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ê°•ë ¥í•œ ì—”ì§„ì¸ **LSTM**ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ì—”ì§„ì— ì—°ë£Œë¥¼ ë„£ì„ ì°¨ë¡€ì…ë‹ˆë‹¤.
ì»´í“¨í„°ëŠ” 'ì‚¬ê³¼'ë‚˜ 'ì™•'ì´ë¼ëŠ” ê¸€ìë¥¼ ì´í•´í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì˜¤ì§ ìˆ«ìë§Œ ì´í•´í•˜ì£ . ê·¸ë ‡ë‹¤ë©´ ë‹¨ì–´ë¥¼ ì–´ë–»ê²Œ ìˆ«ìë¡œ ë°”ê¿”ì•¼ í• ê¹Œìš”?
ë‹¨ìˆœíˆ ë²ˆí˜¸ë¥¼ ë§¤ê¸°ë©´(1ë²ˆ ì‚¬ê³¼, 2ë²ˆ ë°°) ì—‰ëš±í•œ ìˆ˜í•™ì  ê´€ê³„ê°€ ìƒê¹ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë‹¨ì–´ì˜ **ì˜ë¯¸(Meaning)**ë¥¼ ìˆ«ì ì†ì— ë‹´ê³  ì‹¶ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ NLPì˜ í˜ëª…, **ë‹¨ì–´ ì„ë² ë”©(Word Embedding)**ì„ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì»´í“¨í„°ê°€ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì´í•´í•˜ëŠ” ì²™í•˜ê²Œ ë§Œë“  í•µì‹¬ ê¸°ìˆ ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{One-hot:} ì „í†µì ì¸ ë°©ì‹ì˜ í¬ì†Œì„±(Sparsity)ê³¼ ì˜ë¯¸ ë¶€ì¬ ë¬¸ì œë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{Embedding:} ë‹¨ì–´ë¥¼ ì‹¤ìˆ˜ ë²¡í„°(Dense Vector)ë¡œ í‘œí˜„í•˜ì—¬ ì˜ë¯¸ë¥¼ ë‹´ëŠ” ì›ë¦¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{Analogy:} ë²¡í„° ì—°ì‚°ì„ í†µí•´ "ë‚¨ì - ì—¬ì = ì™• - ì—¬ì™•" ê´€ê³„ë¥¼ ë„ì¶œí•´ë´…ë‹ˆë‹¤.
    \item \textbf{Matrix:} ì„ë² ë”© ì¸µì´ ì‹¤ì œë¡œëŠ” ê±°ëŒ€í•œ ë£©ì—… í…Œì´ë¸”(Lookup Table)ì„ì„ ì´í•´í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{í˜•íƒœ} & \textbf{íŠ¹ì§•} \\ \hline
\textbf{One-hot Encoding} & $[0, 0, 1, 0, \dots]$ & í¬ì†Œí•¨. ë‹¨ì–´ ê°„ ê±°ë¦¬ê°€ ëª¨ë‘ ê°™ìŒ (ê´€ê³„ ì—†ìŒ). \\ \hline
\textbf{Word Embedding} & $[0.1, -0.5, 0.9, \dots]$ & ë°€ì§‘í•¨. ë¹„ìŠ·í•œ ë‹¨ì–´ë¼ë¦¬ ê±°ë¦¬ê°€ ê°€ê¹Œì›€. \\ \hline
\textbf{Embedding Matrix} & $E \in \mathbb{R}^{V \times D}$ & ëª¨ë“  ë‹¨ì–´ì˜ ë²¡í„°ë¥¼ ë‹´ê³  ìˆëŠ” í–‰ë ¬. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìˆ«ìì— ì˜ë¯¸ë¥¼ ë‹´ë‹¤}

\subsection{1. The Old Way: One-hot Encoding}
ë‹¨ì–´ ì‚¬ì „ í¬ê¸°ê°€ 10,000ê°œë¼ë©´ 10,000ì°¨ì› ë²¡í„°ë¥¼ ë§Œë“­ë‹ˆë‹¤.
\begin{itemize}
    \item **Man (5391ë²ˆ):** $[0, \dots, 1(\text{5391ë²ˆì§¸}), \dots, 0]$
    \item **Woman (9853ë²ˆ):** $[0, \dots, 1(\text{9853ë²ˆì§¸}), \dots, 0]$
    \item **ë¬¸ì œì :** ë‘ ë²¡í„°ì˜ ë‚´ì (Dot Product)ì€ 0ì…ë‹ˆë‹¤. ì»´í“¨í„° ì…ì¥ì—ì„œ Manì€ Womanê³¼ë„ ë‹¤ë¥´ê³  Appleê³¼ë„ ë‹¤ë¦…ë‹ˆë‹¤. **ìœ ì‚¬ì„±ì„ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.**
\end{itemize}

\subsection{2. The New Way: Word Embedding}
ë‹¨ì–´ë¥¼ í›¨ì”¬ ì‘ì€ ì°¨ì›(ì˜ˆ: 300ì°¨ì›)ì˜ **ì‹¤ìˆ˜ ë²¡í„°(Real-valued Vector)**ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ê° ì°¨ì›ì€ ìš°ë¦¬ê°€ ì•Œ ìˆ˜ ì—†ëŠ”(í˜¹ì€ ì¶”ìƒì ì¸) 'íŠ¹ì§•(Feature)'ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

\begin{analogybox}{ê°€ìƒì˜ íŠ¹ì§•í‘œ (Featurized Representation)}
\begin{center}
\begin{tabular}{l|c|c|c|c}
\hline
\textbf{Feature} & \textbf{Man} & \textbf{Woman} & \textbf{King} & \textbf{Queen} \\ \hline
Gender & -1 & 1 & -0.95 & 0.97 \\ 
Royal & 0.01 & 0.02 & 0.93 & 0.95 \\ 
Age & 0.03 & 0.02 & 0.70 & 0.60 \\ \hline
\end{tabular}
\end{center}
ì´ì œ "Man"ê³¼ "Woman" ë²¡í„°ë¥¼ ë¹„êµí•˜ë©´, Genderë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ìˆ˜ì¹˜ë“¤ì´ ë§¤ìš° ë¹„ìŠ·í•©ë‹ˆë‹¤. ë‚´ì ì„ í•˜ë©´ ê°’ì´ í¬ê²Œ ë‚˜ì˜µë‹ˆë‹¤. **ìœ ì‚¬ì„±ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
\end{analogybox}

\subsection{3. Analogy Reasoning (ìœ ì¶” ì¶”ë¡ )}


ë‹¨ì–´ ì„ë² ë”©ì˜ ê°€ì¥ ìœ ëª…í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.
$$ e_{Man} - e_{Woman} \approx \begin{bmatrix} -2 \\ 0 \\ 0 \end{bmatrix} \quad (\text{Gender ì°¨ì´ë§Œ ë‚¨ìŒ}) $$
$$ e_{King} - e_{Queen} \approx \begin{bmatrix} -2 \\ 0 \\ 0 \end{bmatrix} \quad (\text{Gender ì°¨ì´ë§Œ ë‚¨ìŒ}) $$
ë”°ë¼ì„œ ë‹¤ìŒ ë“±ì‹ì´ ì„±ë¦½í•©ë‹ˆë‹¤.
$$ e_{Man} - e_{Woman} \approx e_{King} - e_{Queen} $$
$$ e_{King} - e_{Man} + e_{Woman} \approx e_{Queen} $$
ì»´í“¨í„°ì—ê²Œ "ì™• - ë‚¨ì + ì—¬ì"ë¥¼ ê³„ì‚°í•˜ë¼ë©´, ë†€ëê²Œë„ **"ì—¬ì™•"**ì— í•´ë‹¹í•˜ëŠ” ë²¡í„°ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.

---

\section{Deep Dive: Implementation Details}

\subsection{Embedding Layer as a Lookup Table}
ìˆ˜í•™ì ìœ¼ë¡œëŠ” ì›-í•« ë²¡í„° $O_j$ì™€ ì„ë² ë”© í–‰ë ¬ $E$ë¥¼ ê³±í•˜ëŠ” ê²ƒ($E \cdot O_j$)ì…ë‹ˆë‹¤.
í•˜ì§€ë§Œ $O_j$ëŠ” í•˜ë‚˜ë§Œ 1ì´ê³  ë‚˜ë¨¸ì§€ê°€ 0ì´ë¯€ë¡œ, ì´ëŠ” ê²°êµ­ í–‰ë ¬ $E$ì˜ **$j$ë²ˆì§¸ ì—´(Column)ì„ êº¼ë‚´ì˜¤ëŠ” ê²ƒ**ê³¼ ê°™ìŠµë‹ˆë‹¤.
ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ëŠ” ì´ë¥¼ í–‰ë ¬ ê³±ì…ˆì´ ì•„ë‹Œ, **ì¸ë±ìŠ¤ ì ‘ê·¼(Lookup)**ìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ ì†ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Keras Embedding}

\begin{lstlisting}[language=Python, caption=Embedding Layer Usage]
import tensorflow as tf
from tensorflow.keras.layers import Embedding

# ì„¤ì •
vocab_size = 10000  # ë‹¨ì–´ ì‚¬ì „ í¬ê¸° (V)
embedding_dim = 300 # ì„ë² ë”© ì°¨ì› (D)
input_length = 10   # ì…ë ¥ ë¬¸ì¥ ê¸¸ì´ (T)

# ëª¨ë¸ ì •ì˜
model = tf.keras.Sequential()

# Embedding Layer
# ì´ ì¸µì€ (10000, 300) í¬ê¸°ì˜ í–‰ë ¬ Eë¥¼ ê°€ì§‘ë‹ˆë‹¤.
# í•™ìŠµ ì´ˆë°˜ì—ëŠ” ëœë¤ ê°’ì´ì§€ë§Œ, ì—­ì „íŒŒë¥¼ í†µí•´ 'ì˜ë¯¸'ë¥¼ í•™ìŠµí•´ê°‘ë‹ˆë‹¤.
model.add(Embedding(input_dim=vocab_size, 
                    output_dim=embedding_dim, 
                    input_length=input_length))

# ëª¨ë¸ ìš”ì•½
model.summary()

# --- ì…ë ¥ ë°ì´í„° ì˜ˆì‹œ ---
# "I love you" -> [1, 539, 23, 0, 0...] (ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜)
# ì…ë ¥ shape: (Batch, 10)
# ì¶œë ¥ shape: (Batch, 10, 300) -> ê° ë‹¨ì–´ê°€ 300ì°¨ì› ë²¡í„°ë¡œ ë³€í•¨
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ì„ë² ë”© ê°’ì€ ì–´ë–»ê²Œ í•™ìŠµí•˜ë‚˜ìš”?} \\
\textbf{A.} ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.
1. **End-to-End:** ë‚´ ë¬¸ì œ(ì˜ˆ: ê°ì„± ë¶„ì„)ë¥¼ í’€ë©´ì„œ ì„ë² ë”© ì¸µë„ ê°™ì´ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ë°ì´í„°ê°€ ë§ì•„ì•¼ í•©ë‹ˆë‹¤.
2. **Pre-trained:** **Word2Vec**ì´ë‚˜ **GloVe**ì²˜ëŸ¼ ìœ„í‚¤í”¼ë””ì•„ ê°™ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¡œ ë¯¸ë¦¬ í•™ìŠµëœ ì„ë² ë”© í–‰ë ¬ì„ ê°€ì ¸ì™€ì„œ ì”ë‹ˆë‹¤. (ì „ì´ í•™ìŠµ, ì¶”ì²œ!)

\textbf{Q. ì„ë² ë”© ì°¨ì›(300 ë“±)ì€ ì–´ë–»ê²Œ ì •í•˜ë‚˜ìš”?} \\
\textbf{A.} í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ë³´í†µ 50, 100, 300 ë“±ì„ ë§ì´ ì”ë‹ˆë‹¤. ì°¨ì›ì´ í´ìˆ˜ë¡ ë” ì •êµí•œ ì˜ë¯¸ë¥¼ ë‹´ì„ ìˆ˜ ìˆì§€ë§Œ, ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ëŸ‰ì´ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” 'ê°œë…'ì„ ì•Œì•˜ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ ë²¡í„° ê°’(ìˆ«ìë“¤)ì€ ë„ëŒ€ì²´ ì–´ë–»ê²Œ êµ¬í•˜ëŠ” ê±¸ê¹Œìš”? ì‚¬ëŒì´ ì¼ì¼ì´ ì…ë ¥í•  ìˆ˜ëŠ” ì—†ìŠµë‹ˆë‹¤.

ì»´í“¨í„°ê°€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ ì½ìœ¼ë©´ì„œ **"ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ê·¸ ì£¼ë³€ ë‹¨ì–´ë“¤ì— ì˜í•´ ê²°ì •ëœë‹¤"**ëŠ” ì›ë¦¬ë¥¼ ì´ìš©í•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê²Œ í•´ì•¼ í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì„ë² ë”© í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì˜ ì–‘ëŒ€ ì‚°ë§¥, **[Word2Vec (Skip-gram, CBOW)]**ê³¼ **[GloVe]**ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{One-hot:} ë‹¨ì–´ ê°„ ìœ ì‚¬ì„±ì„ í‘œí˜„í•˜ì§€ ëª»í•˜ëŠ” í¬ì†Œ ë²¡í„°.
    \item \textbf{Embedding:} ë‹¨ì–´ ê°„ ìœ ì‚¬ì„±ì„ ë‚´ì (ê±°ë¦¬)ìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë°€ì§‘ ë²¡í„°.
    \item \textbf{Analogy:} ë²¡í„° ì—°ì‚°ìœ¼ë¡œ ë‹¨ì–´ ê´€ê³„(ì„±ë³„, ì‹œì œ ë“±)ë¥¼ ì¶”ë¡  ê°€ëŠ¥í•¨.
    \item \textbf{Layer:} ì„ë² ë”© ì¸µì€ ê±°ëŒ€í•œ ë£©ì—… í…Œì´ë¸”(Lookup Table)ì´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Word2Vec \& GloVe}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.5 RNNs, LSTM, Word Representation \textit{- Completed}
        \item \textbf{10.6 Word Embedding Algorithms}
        \begin{itemize}
            \item The Philosophy: Distributional Hypothesis
            \item Word2Vec: Skip-gram \& Negative Sampling
            \item Word2Vec: CBOW (Continuous Bag of Words)
            \item GloVe (Global Vectors)
        \end{itemize}
        \item 10.7 Sentiment Classification \textit{- Upcoming}
    \end{itemize}
    \item[Chapter 11.] Attention Mechanism \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” **ì„ë² ë”©**ì˜ ê°œë…ì„ ë°°ì› ìŠµë‹ˆë‹¤. 'ì™•'ê³¼ 'ë‚¨ì'ê°€ ê°€ê¹ë‹¤ëŠ” ê²ƒì„ ì•Œì•˜ì£ .
ê·¸ë ‡ë‹¤ë©´ ì´ ë§ˆë²• ê°™ì€ ë²¡í„° ê°’ë“¤ì€ ì–´ë–»ê²Œ êµ¬í• ê¹Œìš”? ì–¸ì–´í•™ì ì¡´ í¼ìŠ¤ëŠ” ë§í–ˆìŠµë‹ˆë‹¤.
**"ë‹¨ì–´ì˜ ì˜ë¯¸ëŠ” ê·¸ ë‹¨ì–´ì˜ ì¹œêµ¬ë“¤(ì£¼ë³€ ë‹¨ì–´)ì„ ë³´ë©´ ì•Œ ìˆ˜ ìˆë‹¤."**
ì˜¤ëŠ˜ ë°°ìš¸ ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ì´ ì² í•™ì„ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ë¥¼ ì½ìœ¼ë©° ë‹¨ì–´ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ê³  ì˜ë¯¸ë¥¼ í•™ìŠµí•˜ëŠ” **Word2Vec**ê³¼ **GloVe**ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ë‹¨ì–´ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ëŒ€í‘œì ì¸ ë‘ ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Skip-gram:} ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë©° í•™ìŠµí•˜ëŠ” ë°©ì‹ì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{Negative Sampling:} ê±°ëŒ€í•œ Softmax ì—°ì‚°ì„ í”¼í•˜ê³  ì†ë„ë¥¼ ë†’ì´ëŠ” ìµœì í™” ê¸°ë²•ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{CBOW:} ì£¼ë³€ ë‹¨ì–´ë¡œ ì¤‘ì‹¬ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ê³¼ Skip-gramì˜ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
    \item \textbf{GloVe:} ì „ì²´ ë§ë­‰ì¹˜ì˜ ë™ì‹œ ë“±ì¥ í–‰ë ¬ì„ í™œìš©í•˜ëŠ” í†µê³„ì  ë°©ë²•ì„ íŒŒì•…í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ì•Œê³ ë¦¬ì¦˜} & \textbf{ë°©ì‹} & \textbf{íŠ¹ì§•} \\ \hline
\textbf{Skip-gram} & ì¤‘ì‹¬ $\rightarrow$ ì£¼ë³€ ì˜ˆì¸¡ & í¬ê·€ ë‹¨ì–´ í•™ìŠµì— ìœ ë¦¬í•¨ (ë„ë¦¬ ì“°ì„). \\ \hline
\textbf{CBOW} & ì£¼ë³€ $\rightarrow$ ì¤‘ì‹¬ ì˜ˆì¸¡ & í•™ìŠµ ì†ë„ê°€ ë¹ ë¦„. \\ \hline
\textbf{Negative Sampling} & ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë³€í™˜ & 10ë§Œ ê°œ í´ë˜ìŠ¤ ë¶„ë¥˜ë¥¼ O/X ë¬¸ì œë¡œ ë°”ê¿ˆ. \\ \hline
\textbf{GloVe} & í–‰ë ¬ ë¶„í•´ (Matrix Factorization) & ì „ì²´ í†µê³„ ì •ë³´ë¥¼ ì§ì ‘ í™œìš©í•¨. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì¹œêµ¬ë¥¼ ë³´ë©´ ë„ˆë¥¼ ì•ˆë‹¤}

\subsection{1. Word2Vec: Skip-gram Model}


ìš°ë¦¬ëŠ” ë¼ë²¨ì´ ì—†ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì§€ë„ í•™ìŠµ(Supervised Learning) ë¬¸ì œë¡œ ë°”ê¿‰ë‹ˆë‹¤.
\textbf{ë¬¸ì¥:} "I want a glass of \textbf{orange} \underline{juice} to drink."
\begin{itemize}
    \item **Input (Context):** orange (ì¤‘ì‹¬ ë‹¨ì–´)
    \item **Target:** juice (ì£¼ë³€ ë‹¨ì–´)
    \item **Task:** "orangeê°€ ë‚˜ì™”ì„ ë•Œ, ì£¼ë³€ì— juiceê°€ ë‚˜ì˜¬ í™•ë¥ ì€?"
\end{itemize}

\subsection{2. The Problem with Softmax}
$$ P(t | c) = \frac{e^{\theta_t^T e_c}}{\sum_{j=1}^{V} e^{\theta_j^T e_c}} $$
ë¶„ëª¨ì˜ í•©($\sum$)ì„ êµ¬í•˜ë ¤ë©´ ì‚¬ì „(Vocabulary)ì— ìˆëŠ” 10ë§Œ ê°œ ë‹¨ì–´ë¥¼ ë‹¤ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. í•™ìŠµ í•œ ë²ˆ í•  ë•Œë§ˆë‹¤ 10ë§Œ ë²ˆ ì—°ì‚°? **ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤.**

\subsection{3. The Solution: Negative Sampling}
êµ¬ê¸€ ì—°êµ¬íŒ€ì€ ë¬¸ì œë¥¼ ë°”ê¿¨ìŠµë‹ˆë‹¤. "10ë§Œ ê°œ ì¤‘ í•˜ë‚˜ ë§ì¶”ê¸°" $\rightarrow$ **"ì´ê²Œ ì§„ì§œ ìŒ(Pair)ì´ëƒ ê°€ì§œëƒ(Binary)?"**

\begin{itemize}
    \item **Positive Pair (ì§„ì§œ):** (orange, juice) $\rightarrow$ Label 1
    \item **Negative Pair (ê°€ì§œ):** (orange, king), (orange, book) ... $\rightarrow$ Label 0
\end{itemize}
ì´ì œ **1ê°œì˜ ì§„ì§œì™€ $k$ê°œì˜ ê°€ì§œ(ë³´í†µ 5~20ê°œ)**ë§Œ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤. ì—°ì‚°ëŸ‰ì´ 1/10000ë¡œ ì¤„ì–´ë“­ë‹ˆë‹¤. ì´ê²ƒì´ Word2Vecì˜ í•µì‹¬ì…ë‹ˆë‹¤.

---

\section{GloVe (Global Vectors)}

Word2Vecì€ ìœˆë„ìš°ë¥¼ ìŠ¬ë¼ì´ë”©í•˜ë©° êµ­ì†Œì (Local) ì •ë³´ë§Œ ë´…ë‹ˆë‹¤. GloVeëŠ” **ì „ì²´ í†µê³„(Global Statistics)**ë¥¼ ë´…ë‹ˆë‹¤.

\begin{formulabox}{Co-occurrence Matrix (ë™ì‹œ ë“±ì¥ í–‰ë ¬)}
$$ X_{ij} = \text{ë‹¨ì–´ } i \text{ ì£¼ë³€ì— ë‹¨ì–´ } j \text{ê°€ ë“±ì¥í•œ íšŸìˆ˜} $$
GloVeì˜ ëª©í‘œëŠ” ì„ë² ë”© ë²¡í„°ì˜ ë‚´ì ì´ ì´ íšŸìˆ˜ì˜ ë¡œê·¸ê°’ê³¼ ë¹„ìŠ·í•´ì§€ëŠ” ê²ƒì…ë‹ˆë‹¤.
$$ w_i^T w_j + b_i + b_j \approx \log(X_{ij}) $$
\end{formulabox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Gensim Word2Vec}

ì‹¤ë¬´ì—ì„œëŠ” ì§ì ‘ êµ¬í˜„í•˜ê¸°ë³´ë‹¤ ìµœì í™”ëœ ë¼ì´ë¸ŒëŸ¬ë¦¬ `Gensim`ì„ ì”ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Word2Vec Training with Gensim]
from gensim.models import Word2Vec

# 1. í•™ìŠµ ë°ì´í„° (í† í°í™”ëœ ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸)
sentences = [
    ['I', 'love', 'machine', 'learning'],
    ['deep', 'learning', 'is', 'fun'],
    ['machine', 'learning', 'is', 'hard'],
    ['orange', 'juice', 'is', 'delicious']
]

# 2. ëª¨ë¸ í•™ìŠµ
# vector_size: ì„ë² ë”© ì°¨ì› (100~300)
# window: ì£¼ë³€ ë‹¨ì–´ ë²”ìœ„ (5)
# sg: 1=Skip-gram, 0=CBOW
model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, sg=1)

# 3. ê²°ê³¼ í™•ì¸
vector = model.wv['machine']
print(f"Vector:\n{vector}")

# 4. ìœ ì‚¬ë„ í™•ì¸ (í•µì‹¬)
similar = model.wv.most_similar('deep')
print(f"Similar to 'deep': {similar}")
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ë°ì´í„° ì–‘ì´ ì¤‘ìš”í•©ë‹ˆë‹¤}
ìœ„ì˜ ì˜ˆì œì²˜ëŸ¼ ë¬¸ì¥ 4ê°œë¡œëŠ” ì•„ë¬´ê²ƒë„ ëª» ë°°ì›ë‹ˆë‹¤. Word2Vecì´ ì œëŒ€ë¡œ ì‘ë™í•˜ë ¤ë©´ ìœ„í‚¤í”¼ë””ì•„ ì „ì²´ ê°™ì€ **ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸**ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ë³´í†µ êµ¬ê¸€ì´ë‚˜ í˜ì´ìŠ¤ë¶ì´ ë¯¸ë¦¬ í•™ìŠµí•´ë‘” ëª¨ë¸(Pre-trained)ì„ ë‹¤ìš´ë°›ì•„ ì”ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. Word2Vecê³¼ GloVe ì¤‘ ë­ê°€ ë” ì¢‹ë‚˜ìš”?} \\
\textbf{A.} **ë¹„ìŠ·í•©ë‹ˆë‹¤.** ì–´ë–¤ ë°ì´í„°ì…‹ì—ì„œëŠ” Word2Vecì´, ì–´ë–¤ ê³³ì—ì„œëŠ” GloVeê°€ ë‚«ìŠµë‹ˆë‹¤. ë³´í†µì€ ë‘˜ ë‹¤ ì¨ë³´ê³  ì„±ëŠ¥ ì¢‹ì€ ê±¸ íƒí•˜ê±°ë‚˜, ìµœê·¼ì—ëŠ” BERT ê°™ì€ ë¬¸ë§¥ ê¸°ë°˜ ì„ë² ë”©ì„ ì”ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë‹¨ì–´ í•˜ë‚˜í•˜ë‚˜ë¥¼ ì˜ë¯¸ ìˆëŠ” ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” ë²•ì„ ì•Œì•˜ìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ "I ate an apple"ê³¼ "An apple was eaten by me"ëŠ” ë‹¨ì–´ ìˆœì„œê°€ ë‹¤ë¥´ì§€ë§Œ ì˜ë¯¸ëŠ” ê°™ìŠµë‹ˆë‹¤. ë°˜ë©´ RNNì€ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©´ ì•ì„ ìŠì–´ë²„ë¦¬ëŠ” ë¬¸ì œê°€ ì—¬ì „í•©ë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ ì „ì²´ë¥¼ ë³´ê³  ë²ˆì—­í•˜ê±°ë‚˜ ìš”ì•½í•˜ëŠ” **[Seq2Seq]** ëª¨ë¸ê³¼, ê¸´ ë¬¸ì¥ì—ì„œë„ ì¤‘ìš”í•œ ë‹¨ì–´ì— ì§‘ì¤‘í•˜ê²Œ ë§Œë“œëŠ” **[Attention Mechanism]**ì„ ë°°ìš°ê² ìŠµë‹ˆë‹¤. ì´ê²ƒì´ í˜„ëŒ€ AIì˜ ì •ì ì¸ **Transformer**ë¡œ ê°€ëŠ” ë§ˆì§€ë§‰ ê´€ë¬¸ì…ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Skip-gram:} ì¤‘ì‹¬ ë‹¨ì–´ë¡œ ì£¼ë³€ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•œë‹¤. (í¬ê·€ ë‹¨ì–´ì— ê°•í•¨)
    \item \textbf{Negative Sampling:} ì „ì²´ Softmax ëŒ€ì‹  O/X ë¬¸ì œë¡œ ë°”ê¿” ì†ë„ë¥¼ ë†’ì¸ë‹¤.
    \item \textbf{GloVe:} ì „ì²´ ë§ë­‰ì¹˜ì˜ í†µê³„(ë™ì‹œ ë“±ì¥ íšŸìˆ˜)ë¥¼ ì§ì ‘ í™œìš©í•œë‹¤.
    \item \textbf{Tip:} ë°ì´í„°ê°€ ì ì„ ë• Pre-trained ëª¨ë¸ì„ ì¨ë¼.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ìˆ˜ì‹)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Debiasing Word Embeddings}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.6 RNN, LSTM, Word2Vec, GloVe \textit{- Completed}
        \item \textbf{10.7 Debiasing Word Embeddings}
        \begin{itemize}
            \item The Problem: "Man is to Computer Programmer..."
            \item Geometry of Bias: Identifying the Bias Axis
            \item Neutralize Algorithm (Projection)
            \item Equalize Algorithm (Equidistance)
        \end{itemize}
    \end{itemize}
    \item[Chapter 11.] Attention Mechanism \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ Word2Vecì€ ë†€ë¼ì› ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ 2016ë…„, ì—°êµ¬ìë“¤ì€ ì¶©ê²©ì ì¸ ì‚¬ì‹¤ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.
**"ë‚¨ìì—ê²Œ í”„ë¡œê·¸ë˜ë¨¸ê°€ ìˆë‹¤ë©´, ì—¬ìì—ê²ŒëŠ” ëˆ„ê°€ ìˆëŠ”ê°€?"**
ëª¨ë¸ì˜ ëŒ€ë‹µì€ **"ê°€ì •ì£¼ë¶€(Homemaker)"**ì˜€ìŠµë‹ˆë‹¤.
ì´ëŠ” ëª¨ë¸ì˜ ì˜ëª»ì´ ì•„ë‹™ë‹ˆë‹¤. ëª¨ë¸ì€ ì¸ê°„ì´ ì“´ í…ìŠ¤íŠ¸ì˜ í¸í–¥(Gender Bias)ì„ ê·¸ëŒ€ë¡œ í•™ìŠµí–ˆì„ ë¿ì…ë‹ˆë‹¤. ì˜¤ëŠ˜ì€ AI ìœ¤ë¦¬ì˜ í•µì‹¬, **ì„ë² ë”© í¸í–¥ ì œê±°**ë¥¼ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í•™ìŠµëœ ì„ë² ë”© ë²¡í„°ì—ì„œ ì‚¬íšŒì  í¸í–¥ì„ ìˆ˜í•™ì ìœ¼ë¡œ ì œê±°í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì‹ë³„:} í¸í–¥ ë°©í–¥(Gender Axis)ì„ ë²¡í„° ê³µê°„ì—ì„œ ì°¾ì•„ëƒ…ë‹ˆë‹¤.
    \item \textbf{ì¤‘í™” (Neutralize):} ì˜ì‚¬, ê°„í˜¸ì‚¬ ë“± ì¤‘ë¦½ ë‹¨ì–´ì—ì„œ í¸í–¥ ì„±ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤ (íˆ¬ì˜).
    \item \textbf{ê· í˜•í™” (Equalize):} í• ë¨¸ë‹ˆ, í• ì•„ë²„ì§€ ë“± ì„±ë³„ ë‹¨ì–´ê°€ ì¤‘ë¦½ ë‹¨ì–´ì™€ ê°™ì€ ê±°ë¦¬ë¥¼ ê°–ë„ë¡ ì¡°ì •í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ì˜ˆì‹œ} \\ \hline
\textbf{Bias Axis} & ì„±ë³„ì„ ë‚˜íƒ€ë‚´ëŠ” ë²¡í„° ë°©í–¥. & $\vec{he} - \vec{she}$ ì˜ í‰ê· . \\ \hline
\textbf{Neutral Words} & ì„±ë³„ê³¼ ë¬´ê´€í•´ì•¼ í•˜ëŠ” ë‹¨ì–´. & Doctor, Nurse, Teacher. \\ \hline
\textbf{Gender Specific} & ì„±ë³„ì´ ì •ì˜ìƒ í•„ìš”í•œ ë‹¨ì–´. & Grandfather, Queen, Girl. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í¸í–¥ì˜ ê¸°í•˜í•™}

\subsection{1. Identifying Bias (í¸í–¥ ì‹ë³„)}
ì„ë² ë”© ê³µê°„ì—ì„œ ì„±ë³„ í¸í–¥ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.
$$ Man : Woman :: King : Queen \quad (\text{ì ì ˆí•¨}) $$
$$ Man : Woman :: Doctor : \textbf{Nurse} \quad (\text{ë¶€ì ì ˆí•¨ - ê³ ì •ê´€ë…}) $$

\subsection{2. The Debiasing Algorithm (3ë‹¨ê³„)}


\subsubsection{Step 1: Identify Bias Direction (ì¶• ì°¾ê¸°)}
ì„±ë³„ì„ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ ìŒë“¤ì˜ ì°¨ì´ë¥¼ í‰ê·  ë‚´ì–´ **ì„±ë³„ ì¶•(Gender Axis) $g$**ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
$$ g \approx \text{Average}(\vec{he}-\vec{she}, \vec{male}-\vec{female}, \dots) $$

\subsubsection{Step 2: Neutralize (ì¤‘í™”)}
"Doctor" ê°™ì€ ì¤‘ë¦½ ë‹¨ì–´ëŠ” ì„±ë³„ ì¶• ì„±ë¶„ì„ ê°€ì ¸ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.
**Action:** ë‹¨ì–´ ë²¡í„° $w$ë¥¼ ì„±ë³„ ì¶• $g$ì— íˆ¬ì˜(Project)í•˜ì—¬ í¸í–¥ ì„±ë¶„ì„ ëºë‹ˆë‹¤.

\begin{formulabox}{ì¤‘í™” ê³µì‹ (Projection)}
$$ w_{debiased} = w - \frac{w \cdot g}{\|g\|^2} g $$
ë²¡í„° $w$ì—ì„œ ì„±ë³„ ì¶• ë°©í–¥ì˜ **ê·¸ë¦¼ì(ì„±ë¶„)**ë¥¼ ì œê±°í•˜ì—¬, ì„±ë³„ ì¶•ê³¼ ìˆ˜ì§ì´ ë˜ê²Œ ë§Œë“­ë‹ˆë‹¤.
\end{formulabox}

\subsubsection{Step 3: Equalize (ê· í˜•í™”)}
"Grandmother"ì™€ "Grandfather"ëŠ” ì„±ë³„ ì •ë³´ë¥¼ ê°€ì ¸ì•¼ í•˜ë¯€ë¡œ ì¤‘í™”í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ "Babysitter"ì™€ì˜ ê±°ë¦¬ëŠ” ê³µí‰í•´ì•¼ í•©ë‹ˆë‹¤.
**Action:** ë‘ ë‹¨ì–´ê°€ ì¤‘ë¦½ ì¶•ì—ì„œ **ë“±ê±°ë¦¬(Equidistant)**ì— ìœ„ì¹˜í•˜ë„ë¡ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.

---

\section{Implementation: Neutralize Function}

ë²¡í„° íˆ¬ì˜ì„ í†µí•´ í¸í–¥ì„ ì œê±°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Neutralize Implementation]
import numpy as np

def neutralize(curr_embedding, g):
    """
    curr_embedding: ì¤‘í™”í•  ë‹¨ì–´ ë²¡í„° (ì˜ˆ: doctor)
    g: í¸í–¥ ë°©í–¥ ë²¡í„° (bias axis)
    """
    # 1. í¸í–¥ ì„±ë¶„ ê³„ì‚° (Projection)
    # (w . g / ||g||^2) * g
    norm_sq = np.linalg.norm(g) ** 2
    projection = (np.dot(curr_embedding, g) / norm_sq) * g
    
    # 2. ì›ë³¸ì—ì„œ í¸í–¥ ì œê±°
    debiased_embedding = curr_embedding - projection
    
    return debiased_embedding

# --- ê²€ì¦ ---
if __name__ == "__main__":
    g = np.array([1.0, 0.0]) # xì¶•ì´ ì„±ë³„ ì¶•ì´ë¼ê³  ê°€ì •
    doctor = np.array([2.0, 4.0]) # ì„±ë³„ ì„±ë¶„(2.0)ì´ í¬í•¨ë¨
    
    # ì¤‘í™”
    doctor_debiased = neutralize(doctor, g)
    
    print(f"Original: {doctor}")
    print(f"Debiased: {doctor_debiased}") 
    # ì˜ˆìƒ: [0.0, 4.0] (ì„±ë³„ ì„±ë¶„ì´ 0ì´ ë¨)
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{ëª¨ë“  ë‹¨ì–´ë¥¼ ì¤‘í™”í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤!}
"King", "Queen", "Actor", "Actress" ê°™ì€ ë‹¨ì–´ëŠ” ì„±ë³„ ì •ë³´ê°€ ê·¸ ë‹¨ì–´ì˜ í•µì‹¬ ì˜ë¯¸ì…ë‹ˆë‹¤. ì´ëŸ° ë‹¨ì–´ë“¤ì„ ì¤‘í™”í•´ë²„ë¦¬ë©´ "King"ê³¼ "Queen"ì´ ë˜‘ê°™ì€ ë‹¨ì–´ê°€ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤.
ë”°ë¼ì„œ **ì„±ë³„ ì •ì˜ ë‹¨ì–´(Gender Definition Words)**ë¥¼ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì´ ì„ í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì´ ë°©ë²•ìœ¼ë¡œ ëª¨ë“  í¸í–¥ì´ ì‚¬ë¼ì§€ë‚˜ìš”?} \\
\textbf{A.} ì™„ë²½í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤. 'Hard Debiasing'ì€ ë²¡í„°ì˜ ë°©í–¥ë§Œ ìˆ˜ì •í•˜ì§€ë§Œ, ë‹¨ì–´ë“¤ì´ ë­‰ì³ ìˆëŠ” í´ëŸ¬ìŠ¤í„°ë§ êµ¬ì¡° ë“± ê°„ì ‘ì ì¸ í¸í–¥ì€ ë‚¨ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë§¤ìš° íš¨ê³¼ì ì¸ ì²«ê±¸ìŒì…ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ ìœ¤ë¦¬ì ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë²•ê¹Œì§€ ë°°ì› ìŠµë‹ˆë‹¤. ì¤€ë¹„ëŠ” ëë‚¬ìŠµë‹ˆë‹¤.

ì´ì œ ë‹¨ì–´ë“¤ì„ ì¡°í•©í•´ ë¬¸ì¥ì„ í†µì§¸ë¡œ ë²ˆì—­í•˜ê±°ë‚˜ ìš”ì•½í•˜ëŠ” ê±°ëŒ€í•œ ëª¨ë¸ì„ ë§Œë“¤ì–´ ë´…ì‹œë‹¤.
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì••ì¶•í–ˆë‹¤ê°€ ë‹¤ì‹œ í’€ì–´ë‚´ëŠ” **[Seq2Seq Model]**ê³¼, ê¸´ ë¬¸ì¥ ì²˜ë¦¬ì— í•„ìˆ˜ì ì¸ **[Beam Search]** ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ë‹¤ë£¨ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Bias:} í•™ìŠµ ë°ì´í„°ì˜ ì‚¬íšŒì  í¸í–¥ì´ ì„ë² ë”© ë²¡í„°ì— ë°˜ì˜ëœë‹¤.
    \item \textbf{Neutralize:} ì¤‘ë¦½ ë‹¨ì–´(ì§ì—… ë“±)ë¥¼ ì„±ë³„ ì¶•ì— ìˆ˜ì§ì´ ë˜ë„ë¡ íˆ¬ì˜í•œë‹¤.
    \item \textbf{Equalize:} ì„±ë³„ ë‹¨ì–´(í• ë¨¸ë‹ˆ/í• ì•„ë²„ì§€)ê°€ ì¤‘ë¦½ ë‹¨ì–´ì™€ ë“±ê±°ë¦¬ì— ìˆê²Œ í•œë‹¤.
    \item \textbf{Ethics:} AI ëª¨ë¸ ë°°í¬ ì „ í¸í–¥ ì œê±°ëŠ” í•„ìˆ˜ì ì¸ ì „ì²˜ë¦¬ ê³¼ì •ì´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{architecturebox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ—ï¸ #1 (êµ¬ì¡° ë¶„ì„)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Sequence-to-Sequence (Seq2Seq)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.7 RNN, Embedding, Debiasing \textit{- Completed}
        \item \textbf{10.8 Sequence-to-Sequence (Seq2Seq)}
        \begin{itemize}
            \item The Problem: Different Lengths ($T_x \neq T_y$)
            \item Encoder-Decoder Architecture
            \item Context Vector (The Information Bottleneck)
            \item Teacher Forcing (Training Trick)
        \end{itemize}
        \item 10.9 Beam Search \textit{- Upcoming}
    \end{itemize}
    \item[Chapter 11.] Attention Mechanism \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ê¹Œì§€ ìš°ë¦¬ëŠ” ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë°”ê¾¸ëŠ” ë²•(Embedding)ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì´ì œ ë‹¨ì–´ë“¤ì„ ì¡°ë¦½í•´ ë¬¸ì¥ì„ ë§Œë“¤ê³ , ë²ˆì—­í•˜ëŠ” ì‹œìŠ¤í…œì„ ë§Œë“¤ ì°¨ë¡€ì…ë‹ˆë‹¤.
ê·¸ëŸ°ë° ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.
**ì…ë ¥:** "I love you" (3ë‹¨ì–´) $\rightarrow$ **ì¶œë ¥:** "Je t'aime" (2ë‹¨ì–´).
ê¸¸ì´ê°€ ë‹¤ë¦…ë‹ˆë‹¤. ê¸°ì¡´ RNNì€ ì´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ êµ¬ê¸€ì´ ì œì•ˆí•œ í˜ëª…ì ì¸ ì•„í‚¤í…ì²˜, **Seq2Seq (Encoder-Decoder)**ë¥¼ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ì…ë ¥ê³¼ ì¶œë ¥ì˜ ê¸¸ì´ê°€ ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ë³€í™˜ ëª¨ë¸ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{êµ¬ì¡°:} ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì••ì¶•í•˜ëŠ” **ì¸ì½”ë”**ì™€, ì´ë¥¼ í’€ì–´ë‚´ëŠ” **ë””ì½”ë”** êµ¬ì¡°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ì••ì¶•:} ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœê°€ ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ë‹´ì€ **ë¬¸ë§¥ ë²¡í„°(Context Vector)**ê°€ ë¨ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{ì „ë‹¬:} ë¬¸ë§¥ ë²¡í„°ê°€ ë””ì½”ë”ì˜ ì´ˆê¸° ìƒíƒœë¡œ ì „ë‹¬ë˜ëŠ” íë¦„ì„ ì½”ë“œë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
    \item \textbf{í•œê³„:} ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´ ì •ë³´ê°€ ì†ì‹¤ë˜ëŠ” **ë³‘ëª©(Bottleneck)** í˜„ìƒì„ ì¸ì§€í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì—­í• } & \textbf{ë¹„ìœ } \\ \hline
\textbf{Encoder} & ì…ë ¥ ë¬¸ì¥ì„ ì½ê³  ì´í•´í•¨. & ë¬¸ì¥ì„ ì½ëŠ” ë²ˆì—­ê°€. \\ \hline
\textbf{Decoder} & ì´í•´í•œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë¬¸ì¥ì„ ìƒì„±í•¨. & ë²ˆì—­ë¬¸ì„ ì“°ëŠ” ë²ˆì—­ê°€. \\ \hline
\textbf{Context Vector} & ì¸ì½”ë”ê°€ ë„˜ê²¨ì£¼ëŠ” í•µì‹¬ ì •ë³´ (ë§ˆì§€ë§‰ $h$). & ë¨¸ë¦¿ì†ì— ì •ë¦¬ëœ ë¬¸ì¥ì˜ ì˜ë¯¸. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ë‹¤ ë“£ê³  ë§í•˜ê¸°}

\subsection{1. The Architecture: Encoder-Decoder}
Seq2SeqëŠ” ë‘ ê°œì˜ RNNì„ ì´ì–´ ë¶™ì¸ êµ¬ì¡°ì…ë‹ˆë‹¤.



\begin{itemize}
    \item **Encoder:** ë¬¸ì¥ $x^{\langle 1 \rangle} \dots x^{\langle T_x \rangle}$ë¥¼ ìˆœì„œëŒ€ë¡œ ì½ìŠµë‹ˆë‹¤. ì¶œë ¥ì„ ë‚´ì§€ ì•Šê³  ì€ë‹‰ ìƒíƒœ($h$)ë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ ìƒíƒœ $h^{\langle T_x \rangle}$ì— ëª¨ë“  ì •ë³´ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤.
    \item **Decoder:** ì¸ì½”ë”ê°€ ì¤€ ë¬¸ë§¥ ë²¡í„°ë¥¼ **ìì‹ ì˜ ì´ˆê¸° ìƒíƒœ($h^{\langle 0 \rangle}_{dec}$)**ë¡œ ë°›ìŠµë‹ˆë‹¤. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë²ˆì—­ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
\end{itemize}

\begin{analogybox}{ë²ˆì—­ê°€ì˜ ì‘ì—… ë°©ì‹}
\begin{itemize}
    \item **Read (Encoder):** ë¬¸ì¥ì„ ëê¹Œì§€ ë‹¤ ì½ìŠµë‹ˆë‹¤. ì¤‘ê°„ì— ë²ˆì—­í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¸ë¦¿ì†ì— í•µì‹¬ ì˜ë¯¸(Context)ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.
    \item **Write (Decoder):** ë¨¸ë¦¿ì† ì˜ë¯¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ í”„ë‘ìŠ¤ì–´ ë¬¸ì¥ì„ ì²˜ìŒë¶€í„° ì¨ ë‚´ë ¤ê°‘ë‹ˆë‹¤.
\end{itemize}
\end{analogybox}

\subsection{2. Training Trick: Teacher Forcing}
í•™ìŠµí•  ë•ŒëŠ” ë””ì½”ë”ê°€ ì‹¤ìˆ˜ë¡œ ì—‰ëš±í•œ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í–ˆë”ë¼ë„, ë‹¤ìŒ ìŠ¤í… ì…ë ¥ìœ¼ë¡œëŠ” **ì •ë‹µ ë‹¨ì–´(Ground Truth)**ë¥¼ ë„£ì–´ì¤ë‹ˆë‹¤. ì´ë¥¼ **êµì‚¬ ê°•ìš”(Teacher Forcing)**ë¼ê³  í•©ë‹ˆë‹¤. (ì´ˆë°˜ í•™ìŠµ ì•ˆì •í™”ìš©)

---

\section{Implementation: Keras Functional API}

ì¸ì½”ë”ì˜ ìƒíƒœë¥¼ ë””ì½”ë”ë¡œ ë„˜ê²¨ì£¼ëŠ” í•µì‹¬ ì½”ë“œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Seq2Seq Model Definition]
import tensorflow as tf
from tensorflow.keras.layers import Input, LSTM, Dense, Embedding
from tensorflow.keras.models import Model

def build_seq2seq(vocab_size, embedding_dim, latent_dim):
    
    # --- 1. Encoder ---
    enc_inputs = Input(shape=(None,))
    enc_emb = Embedding(vocab_size, embedding_dim)(enc_inputs)
    
    # return_state=True: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ(h)ì™€ ì…€ ìƒíƒœ(c)ë¥¼ ë°˜í™˜
    encoder_lstm = LSTM(latent_dim, return_state=True)
    _, state_h, state_c = encoder_lstm(enc_emb)
    
    # ë¬¸ë§¥ ë²¡í„° (Context Vector): ë¬¸ì¥ì˜ ëª¨ë“  ì •ë³´ë¥¼ ì••ì¶•í•œ ê²ƒ
    encoder_states = [state_h, state_c]
    
    # --- 2. Decoder ---
    dec_inputs = Input(shape=(None,))
    dec_emb = Embedding(vocab_size, embedding_dim)(dec_inputs)
    
    # return_sequences=True: ëª¨ë“  íƒ€ì„ ìŠ¤í… ì¶œë ¥ (ë²ˆì—­ë¬¸ ìƒì„±)
    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)
    
    # í•µì‹¬: initial_stateì— ì¸ì½”ë”ì˜ ìƒíƒœë¥¼ ì£¼ì…!
    dec_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)
    
    decoder_dense = Dense(vocab_size, activation='softmax')
    dec_outputs = decoder_dense(dec_outputs)
    
    # ëª¨ë¸ ì •ì˜
    model = Model([enc_inputs, dec_inputs], dec_outputs)
    return model
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\begin{warningbox}{The Bottleneck Problem (ë³‘ëª© í˜„ìƒ)}
Seq2Seqì˜ ì¹˜ëª…ì  ë‹¨ì ì€ ì¸ì½”ë”ê°€ ë¬¸ì¥ì´ ê¸¸ë“  ì§§ë“  **ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„° í•˜ë‚˜**ì— ëª¨ë“  ì •ë³´ë¥¼ ìš°ê²¨ ë„£ì–´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´(50ë‹¨ì–´ ì´ìƒ) ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•´ ë²ˆì—­ í’ˆì§ˆì´ ê¸‰ê²©íˆ ë–¨ì–´ì§‘ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **Attention Mechanism**ì´ ë“±ì¥í–ˆìŠµë‹ˆë‹¤.
\end{warningbox}

\textbf{Q. ì¶”ë¡ (Inference) ë•ŒëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?} \\
\textbf{A.} í•™ìŠµ ë•Œì™€ ë‹¬ë¦¬ ì •ë‹µì„ ëª¨ë¥´ë¯€ë¡œ, ë””ì½”ë”ê°€ ë°©ê¸ˆ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ë‹¤ìŒ ìŠ¤í…ì˜ ì…ë ¥ìœ¼ë¡œ ë„£ì–´ì£¼ëŠ” ë£¨í”„(Loop)ë¥¼ ì§ì ‘ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤. (Auto-regressive)

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì´ì œ ëª¨ë¸ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ë””ì½”ë”ê°€ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ, ë§¤ ìˆœê°„ ê°€ì¥ í™•ë¥  ë†’ì€ ë‹¨ì–´ í•˜ë‚˜ë§Œ(Greedy) ê³ ë¥´ë©´ ìµœê³ ì˜ ë¬¸ì¥ì´ ë ê¹Œìš”?
"The" ë’¤ì— "nice"ê°€ ì˜¬ í™•ë¥ ì´ ë†’ë‹¤ê³  ê³¨ëëŠ”ë°, ë‚˜ì¤‘ì— ë³´ë‹ˆ "The huge building..."ì´ ë” ì¢‹ì€ ë²ˆì—­ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë²ˆì—­ í’ˆì§ˆì„ ê²°ì •í•˜ëŠ” ê²°ì •ì  íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜, **[Beam Search]**ì— ëŒ€í•´ ì•Œì•„ë³´ê³ , Seq2Seqì˜ ë³‘ëª© ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” **[Attention Mechanism]**ìœ¼ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Structure:} Encoderê°€ ì••ì¶•í•˜ê³  Decoderê°€ í‘¼ë‹¤.
    \item \textbf{Context:} ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ìƒíƒœê°€ ë¬¸ì¥ì˜ í•µì‹¬ ì˜ë¯¸ë¥¼ ë‹´ì€ ë§¤ê°œì²´ë‹¤.
    \item \textbf{Transfer:} $h_{enc}$ë¥¼ $h_{dec}$ì˜ `initial_state`ë¡œ ì „ë‹¬í•œë‹¤.
    \item \textbf{Limit:} ê¸´ ë¬¸ì¥ì„ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì••ì¶•í•˜ë©´ ì •ë³´ ì†ì‹¤ì´ ë°œìƒí•œë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ìˆ˜ì‹)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Beam Search (Optimization)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.8 RNN, Embedding, Seq2Seq \textit{- Completed}
        \item \textbf{10.9 Beam Search}
        \begin{itemize}
            \item Greedy Search vs Beam Search
            \item Beam Width Parameter ($B$)
            \item Refinements: Log Likelihood & Length Normalization
        \end{itemize}
    \end{itemize}
    \item[Chapter 11.] Attention Mechanism \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” Seq2Seq ëª¨ë¸ë¡œ ë²ˆì—­ê¸°ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ë§‰ìƒ ëŒë ¤ë³´ë‹ˆ ì—‰ëš±í•œ ë¬¸ì¥ì´ ë‚˜ì˜µë‹ˆë‹¤.
ì´ìœ ëŠ” ìš°ë¦¬ê°€ **'ë‹¹ì¥ì˜ ìµœì„ (Greedy)'**ë§Œ ì„ íƒí–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 1ë“±ë§Œ ë½‘ì•„ì„œ ë¬¸ì¥ì„ ì´ì—ˆë”ë‹ˆ ì „ì²´ ë¬¸ë§¥ì€ ì—‰ë§ì´ ëœ ê²ƒì´ì£ .
ì¸ìƒê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, "ë‹¹ì¥ì˜ ìµœì„ ì´ ê²°ê³¼ì ì¸ ìµœì„ ì€ ì•„ë‹ ìˆ˜" ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ê°€ëŠ¥ì„±ì„ ë™ì‹œì— íƒìƒ‰í•˜ëŠ” **ë¹” ì„œì¹˜(Beam Search)**ê°€ í•„ìš”í•©ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²°ì •ì§“ëŠ” íƒìƒ‰ ì•Œê³ ë¦¬ì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Greedy:} ë§¤ ìˆœê°„ 1ë“±ë§Œ ë½‘ëŠ” ë°©ì‹ì˜ í•œê³„(Local Optima)ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{Beam:} ìƒìœ„ $B$ê°œì˜ ê°€ëŠ¥ì„±ì„ ì‚´ë ¤ë‘ë©° íƒìƒ‰í•˜ëŠ” ì›ë¦¬ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{Refinement:} ë¡œê·¸ ìš°ë„(Log Likelihood)ì™€ ê¸¸ì´ ì •ê·œí™”(Length Normalization)ë¥¼ í†µí•´ ìˆ˜í•™ì  ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{íŠ¹ì§•} \\ \hline
\textbf{Greedy Search} & ë§¤ë²ˆ í™•ë¥  1ë“± ë‹¨ì–´ë§Œ ì„ íƒ. & ë¹ ë¥´ì§€ë§Œ ìµœì í•´ ë³´ì¥ ëª»í•¨. \\ \hline
\textbf{Beam Search} & ë§¤ë²ˆ ìƒìœ„ $B$ê°œë¥¼ ìœ ì§€í•˜ë©° íƒìƒ‰. & ëŠë¦¬ì§€ë§Œ ë” ì¢‹ì€ ë¬¸ì¥ ìƒì„±. \\ \hline
\textbf{Beam Width ($B$)} & ìœ ì§€í•  í›„ë³´ ê°œìˆ˜ (ë³´í†µ 3~10). & $B=1$ì´ë©´ Greedyì™€ ê°™ìŒ. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì—¬ëŸ¬ ê¸¸ì„ ë™ì‹œì— ê°€ë¼}

\subsection{1. The Problem: Greedy Search}

ë§¤ ìŠ¤í…ì—ì„œ í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ë‹¨ì–´ 1ê°œë§Œ ê³ ë¥´ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.
\begin{analogybox}{ë¯¸ë¡œ ì°¾ê¸°}
ê°ˆë¦¼ê¸¸ì—ì„œ ë¬´ì¡°ê±´ "ì¶œêµ¬ì™€ ê°€ê¹Œì›Œ ë³´ì´ëŠ” ìª½"ìœ¼ë¡œë§Œ ê°€ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ê°€ë‹¤ ë³´ë‹ˆ ë§‰ë‹¤ë¥¸ ê¸¸ì¼ ìˆ˜ ìˆì§€ë§Œ, ë˜ëŒì•„ê°ˆ ìˆ˜ ì—†ìŠµë‹ˆë‹¤(No Backtracking).
\end{analogybox}

\subsection{2. The Solution: Beam Search}

ë§¤ ìˆœê°„ ìƒìœ„ $B$ê°œì˜ ê°€ëŠ¥ì„±(Hypothesis)ì„ ìœ ì§€í•©ë‹ˆë‹¤. ($B=3$ ê°€ì •)

\begin{itemize}
    \item **Step 1:** ì²« ë‹¨ì–´ í›„ë³´ ì¤‘ ìƒìœ„ 3ê°œë¥¼ ë½‘ìŠµë‹ˆë‹¤. (ì˜ˆ: "in", "the", "a")
    \item **Step 2:** ì‚´ì•„ë‚¨ì€ 3ê°œì˜ ë‹¨ì–´ ê°ê°ì— ëŒ€í•´, ë‹¤ìŒ ë‹¨ì–´ í™•ë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    \item **Step 3:** ì´ 30,000ê°œ(3 $\times$ ë‹¨ì–´ì¥) ì¡°í•© ì¤‘ ë‹¤ì‹œ ìƒìœ„ 3ë“±ê¹Œì§€ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ë²„ë¦½ë‹ˆë‹¤.
    \item **Repeat:** ë¬¸ì¥ì´ ëë‚  ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤.
\end{itemize}

---

\section{Deep Dive: Refinements (ì •ë°€í™”)}

ë‹¨ìˆœ í™•ë¥  ê³±ì…ˆì—ëŠ” ìˆ˜í•™ì  ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ ë³´ì •í•´ì•¼ í•©ë‹ˆë‹¤.

\subsection{1. Log Likelihood (ë¡œê·¸ ìš°ë„)}
í™•ë¥ (0.1, 0.05...)ì„ ìˆ˜ì‹­ ë²ˆ ê³±í•˜ë©´ ê°’ì´ $0.0000\dots$ì´ ë˜ì–´ ì»´í“¨í„°ê°€ 0ìœ¼ë¡œ ì¸ì‹í•´ë²„ë¦½ë‹ˆë‹¤ (**Underflow**).
í•´ê²°ì±…ì€ **ë¡œê·¸($\log$)**ë¥¼ ì·¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê³±ì…ˆì´ ë§ì…ˆìœ¼ë¡œ ë³€í•©ë‹ˆë‹¤.

\begin{formulabox}{Score Function}
$$ \sum_{t=1}^{T_y} \log P(y^{\langle t \rangle} | y^{\langle 1 \dots t-1 \rangle}, x) $$
ë¡œê·¸ í•©ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì€ í™•ë¥  ê³±ì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒê³¼ ìˆ˜í•™ì ìœ¼ë¡œ ë™ì¼í•©ë‹ˆë‹¤.
\end{formulabox}

\subsection{2. Length Normalization (ê¸¸ì´ ì •ê·œí™”)}
ë¡œê·¸ í™•ë¥ (ìŒìˆ˜)ì„ ê³„ì† ë”í•˜ë©´, ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ ì ìˆ˜ëŠ” ê³„ì† ë‚®ì•„ì§‘ë‹ˆë‹¤. ëª¨ë¸ì´ **"ì§§ì€ ë¬¸ì¥"**ì„ ê³¼ë„í•˜ê²Œ ì„ í˜¸í•˜ê²Œ ë©ë‹ˆë‹¤.
í•´ê²°ì±…ì€ ì ìˆ˜ë¥¼ ë¬¸ì¥ ê¸¸ì´($T_y$)ë¡œ ë‚˜ëˆ„ì–´ **í‰ê· **ì„ ë‚´ëŠ” ê²ƒì…ë‹ˆë‹¤.

\begin{formulabox}{Normalized Score}
$$ \text{Score} = \frac{1}{T_y^\alpha} \sum \log P(\dots) $$
\begin{itemize}
    \item $\alpha$: ë³´ì • ê³„ìˆ˜ (ë³´í†µ 0.7).
    \item ê¸´ ë¬¸ì¥ì— ëŒ€í•œ í˜ë„í‹°ë¥¼ ì™„í™”í•´ì¤ë‹ˆë‹¤.
\end{itemize}
\end{formulabox}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. $B$ë¥¼ ë¬´ì¡°ê±´ í¬ê²Œ í•˜ë©´ ì¢‹ì€ê°€ìš”?} \\
\textbf{A.} ì•„ë‹™ë‹ˆë‹¤. $B$ê°€ í¬ë©´ ë” ì¢‹ì€ ë¬¸ì¥ì„ ì°¾ì„ í™•ë¥ ì€ ë†’ì§€ë§Œ, ê³„ì‚°ëŸ‰ê³¼ ë©”ëª¨ë¦¬ê°€ $B$ë°°ë¡œ ëŠ˜ì–´ë‚©ë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” ë³´í†µ $B=3 \sim 10$ ì •ë„ë¥¼ ì“°ë©°, ì•„ì£¼ ì •ë°€í•œ ë²ˆì—­ì´ í•„ìš”í•  ë•Œë§Œ ë” í‚¤ì›ë‹ˆë‹¤.

\textbf{Q. ë¹” ì„œì¹˜ëŠ” ìµœì í•´(Global Optima)ë¥¼ ë³´ì¥í•˜ë‚˜ìš”?} \\
\textbf{A.} ì•„ë‹ˆìš”. $B$ê°€ ë¬´í•œëŒ€(BFS)ê°€ ì•„ë‹Œ ì´ìƒ, íœ´ë¦¬ìŠ¤í‹± íƒìƒ‰ì´ë¯€ë¡œ ì™„ë²½í•œ ìµœì í•´ë¥¼ ë³´ì¥í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Greedyë³´ë‹¤ëŠ” í›¨ì”¬ ë‚«ìŠµë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ë¹” ì„œì¹˜ë¡œ ë¬¸ì¥ ìƒì„± ëŠ¥ë ¥ì€ ì¢‹ì•„ì¡ŒìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—¬ì „íˆ ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ë‚¨ì•˜ìŠµë‹ˆë‹¤.
Seq2Seqì˜ ì¸ì½”ë”ëŠ” ì•„ë¬´ë¦¬ ê¸´ ë¬¸ì¥ì´ë¼ë„ **í•˜ë‚˜ì˜ ê³ ì •ëœ ë²¡í„°(Context Vector)**ì— ìš±ì—¬ë„£ì–´ì•¼ í•œë‹¤ëŠ” **'ë³‘ëª©(Bottleneck)'** í˜„ìƒì…ë‹ˆë‹¤.

**"ë²ˆì—­í•  ë•Œ ë¬¸ì¥ ì „ì²´ë¥¼ ì™¸ìš°ê³  í•˜ëŠ” ì‚¬ëŒì€ ì—†ìŠµë‹ˆë‹¤. í•„ìš”í•œ ë‹¨ì–´ë¥¼ ê·¸ë•Œê·¸ë•Œ ë‹¤ì‹œ ë³´ë©´ì„œ(Attention) í•˜ì£ ."**
ë‹¤ìŒ ì‹œê°„ì—ëŠ” ë”¥ëŸ¬ë‹ ì—­ì‚¬ìƒ ê°€ì¥ ìœ„ëŒ€í•œ ë°œëª… ì¤‘ í•˜ë‚˜ì¸ **[Attention Mechanism]**ì„ í†µí•´ ì´ ë³‘ëª©ì„ ë¶€ìˆ˜ê³  Transformerë¡œ ê°€ëŠ” ë¬¸ì„ ì—´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Greedy:} ë¹ ë¥´ì§€ë§Œ ì‹œì•¼ê°€ ì¢ì•„ ìµœì  ë¬¸ì¥ì„ ë†“ì¹œë‹¤.
    \item \textbf{Beam Search:} $B$ê°œì˜ ìœ ë§ì£¼ë¥¼ ëê¹Œì§€ í‚¤ìš´ë‹¤.
    \item \textbf{Log:} ì–¸ë”í”Œë¡œìš° ë°©ì§€ë¥¼ ìœ„í•´ í™•ë¥  ê³± ëŒ€ì‹  ë¡œê·¸ í•©ì„ ì“´ë‹¤.
    \item \textbf{Normalize:} ì§§ì€ ë¬¸ì¥ í¸í–¥ì„ ë§‰ê¸° ìœ„í•´ ê¸¸ì´ë¡œ ë‚˜ëˆˆë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}
\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ìˆ˜ì‹)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ BLEU Score (Evaluation Metric)}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.9 RNN, Seq2Seq, Beam Search \textit{- Completed}
        \item \textbf{10.10 BLEU Score}
        \begin{itemize}
            \item The Multiple Reference Problem
            \item Modified Precision (Clipping)
            \item N-grams (Unigram, Bigram, Trigram)
            \item Brevity Penalty (Length Normalization)
        \end{itemize}
    \end{itemize}
    \item[Chapter 11.] Attention Mechanism \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë¹” ì„œì¹˜ë¡œ ìµœì ì˜ ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ê·¼ë³¸ì ì¸ ì§ˆë¬¸ì´ ë‚¨ìŠµë‹ˆë‹¤.
**"ë²ˆì—­ì´ ì˜ ëëŠ”ì§€ ì»´í“¨í„°ê°€ ì–´ë–»ê²Œ ì±„ì í• ê¹Œ?"**
ì´ë¯¸ì§€ ë¶„ë¥˜ëŠ” ì •ë‹µì´ í•˜ë‚˜(ê³ ì–‘ì´)ì§€ë§Œ, ë²ˆì—­ì€ ì •ë‹µì´ ì—¬ëŸ¬ ê°œì…ë‹ˆë‹¤. ("ë‚˜ëŠ” í•™êµì— ê°„ë‹¤", "í•™êµì— ê°€ëŠ” ì¤‘ì´ë‹¤" ë“±)
ì‚¬ëŒì´ ì¼ì¼ì´ ì±„ì í•˜ê¸°ì—” ë„ˆë¬´ ë¹„ìŒ‰ë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ IBMì´ ì œì•ˆí•œ ìë™í™”ëœ ì ìˆ˜, **BLEU Score**ë¥¼ ë°°ì›ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ê¸°ê³„ ë²ˆì—­ í‰ê°€ì˜ í‘œì¤€ì¸ BLEU Scoreì˜ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë‹¤ì¤‘ ì •ë‹µ:} ì—¬ëŸ¬ ê°œì˜ ì°¸ê³  ë¬¸ì¥(Reference)ì„ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    \item \textbf{ì •ë°€ë„:} ë‹¨ìˆœ ì¼ì¹˜ê°€ ì•„ë‹Œ **í´ë¦¬í•‘ëœ ì •ë°€ë„(Clipped Precision)**ë¥¼ ì‚¬ìš©í•´ "the the the" ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.
    \item \textbf{N-gram:} ë‹¨ì–´ ë¬¶ìŒì„ ë¹„êµí•˜ì—¬ ë¬¸ë§¥ê³¼ ì–´ìˆœì˜ ì •í™•ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤.
    \item \textbf{í˜ë„í‹°:} ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì— í˜ë„í‹°(BP)ë¥¼ ì£¼ì–´ ê¼¼ìˆ˜ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ì—­í• } \\ \hline
\textbf{Reference} & ì •ë‹µ ë¬¸ì¥(ë“¤). & ì±„ì  ê¸°ì¤€. (ì‚¬ëŒì´ ë²ˆì—­í•œ ê²ƒ) \\ \hline
\textbf{Candidate} & ëª¨ë¸ì´ ìƒì„±í•œ ë¬¸ì¥. & ì±„ì  ëŒ€ìƒ. \\ \hline
\textbf{Modified Precision} & ë¹ˆë„ìˆ˜ ì œí•œ ì •ë°€ë„. & ì¤‘ë³µ ë‹¨ì–´ ë‚¨ë°œ ë°©ì§€. \\ \hline
\textbf{Brevity Penalty} & ê¸¸ì´ ë¶ˆì´ìµ. & ì •ë³´ ëˆ„ë½(ì§§ì€ ë¬¸ì¥) ë°©ì§€. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ì •ë°€ë„ì˜ í•¨ì •}

\subsection{1. The Problem with Standard Precision}


\textbf{Candidate:} "the the the the the" (5ë‹¨ì–´)
\textbf{Reference:} "The cat is on the mat."
\begin{itemize}
    \item **ë‹¨ìˆœ ì •ë°€ë„:** 5ê°œ ëª¨ë‘ ì •ë‹µì— ìˆìŒ("the"). $\rightarrow$ 5/5 = 100ì ? (ë§ë„ ì•ˆ ë¨)
    \item **í•´ê²°ì±… (Clipping):** ì •ë‹µ ë¬¸ì¥ì— "the"ê°€ ìµœëŒ€ ëª‡ ë²ˆ ë‚˜ì˜¤ëŠ”ì§€ ì…‰ë‹ˆë‹¤. (2ë²ˆ). ë¶„ìë¥¼ 2ë¡œ ì œí•œí•©ë‹ˆë‹¤. $\rightarrow$ 2/5 = 40ì .
\end{itemize}

\subsection{2. N-gram Analysis (ì–´ìˆœ í‰ê°€)}
ë‹¨ì–´ë§Œ ë‹¤ ìˆë‹¤ê³  ë¬¸ì¥ì´ ì•„ë‹™ë‹ˆë‹¤.
"The cat the mat on is" (ë‹¨ì–´ëŠ” ë§ì§€ë§Œ ì—‰í„°ë¦¬)
ì´ë¥¼ ì¡ê¸° ìœ„í•´ **N-gram(ë‹¨ì–´ ë¬¶ìŒ)**ì„ ë´…ë‹ˆë‹¤.
\begin{itemize}
    \item **Unigram (1-gram):** ë‹¨ì–´ ì¡´ì¬ ì—¬ë¶€ (ë‚´ìš© ì¶©ì‹¤ë„).
    \item **Bigram (2-gram):** "The cat", "cat is" ... (ì–´ìˆœ/ìœ ì°½ì„±).
\end{itemize}

\subsection{3. Brevity Penalty (BP)}
ì •ë°€ë„ ê¸°ë°˜ í‰ê°€ëŠ” **"ì§§ê²Œ ë§í•˜ë©´ ìœ ë¦¬í•˜ë‹¤"**ëŠ” ì•½ì ì´ ìˆìŠµë‹ˆë‹¤.
\textbf{Cand:} "The cat" (2ë‹¨ì–´ ë‹¤ ë§ìŒ. ì •ë°€ë„ 100%)
í•˜ì§€ë§Œ ì •ë³´ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ë¬¸ì¥ì´ ì •ë‹µë³´ë‹¤ ì§§ìœ¼ë©´ ì ìˆ˜ë¥¼ ê¹ìŠµë‹ˆë‹¤.

---

\section{Deep Dive: The Formula}

\begin{formulabox}{BLEU Formula}
$$ \text{BLEU} = BP \times \exp \left( \frac{1}{4} \sum_{n=1}^{4} p_n \right) $$
\begin{itemize}
    \item $p_n$: n-gramì˜ ë³´ì •ëœ ì •ë°€ë„.
    \item $BP$: Brevity Penalty (ê¸¸ì´ í˜ë„í‹°).
    \item $\exp(\dots)$: ê¸°í•˜ í‰ê· (Geometric Mean)ì„ êµ¬í•˜ëŠ” ë°©ì‹.
\end{itemize}
\end{formulabox}

% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: NLTK Library}

íŒŒì´ì¬ NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=BLEU Score Calculation]
from nltk.translate.bleu_score import sentence_bleu

# 1. ë°ì´í„° ì¤€ë¹„
# ReferenceëŠ” ì—¬ëŸ¬ ê°œì¼ ìˆ˜ ìˆìŒ (ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸)
references = [
    ['the', 'cat', 'is', 'on', 'the', 'mat'],       # Ref 1
    ['there', 'is', 'a', 'cat', 'on', 'the', 'mat'] # Ref 2
]

# CandidateëŠ” í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸
candidate = ['the', 'cat', 'is', 'on', 'the', 'mat']

# 2. BLEU ê³„ì‚°
# weights: 1-gram ~ 4-gram ê°€ì¤‘ì¹˜ (ë³´í†µ ê· ë“±í•˜ê²Œ 0.25)
score = sentence_bleu(references, candidate, weights=(0.25, 0.25, 0.25, 0.25))

print(f"BLEU Score: {score:.4f}")
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. BLEU ì ìˆ˜ê°€ ë†’ìœ¼ë©´ ì™„ë²½í•œ ë²ˆì—­ì¸ê°€ìš”?} \\
\textbf{A.} **ì•„ë‹ˆìš”.** BLEUëŠ” ë‹¨ì–´ ì¼ì¹˜ë„ë§Œ ë´…ë‹ˆë‹¤. ì˜ë¯¸ëŠ” í†µí•˜ëŠ”ë° ë‹¨ì–´ê°€ ë‹¤ë¥¸ ì˜ì—­(Paraphrasing)ì´ë‚˜, ë¯¸ë¬˜í•œ ë‰˜ì•™ìŠ¤ ì°¨ì´ëŠ” ì¡ì•„ë‚´ì§€ ëª»í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ "ìµœì†Œí•œ ì´ ì •ë„ëŠ” í•œë‹¤"ëŠ” ì§€í‘œë¡œëŠ” ê°€ì¥ í›Œë¥­í•©ë‹ˆë‹¤.

\textbf{Q. ë§Œì•½ 4-gramì´ í•˜ë‚˜ë„ ì•ˆ ë§ìœ¼ë©´ìš”?} \\
\textbf{A.} $p_4 = 0$ì´ ë˜ë©´ ê¸°í•˜ í‰ê·  íŠ¹ì„±ìƒ ì „ì²´ ì ìˆ˜ê°€ 0ì´ ë˜ì–´ë²„ë¦½ë‹ˆë‹¤. ì´ë¥¼ ë§‰ê¸° ìœ„í•´ ì•„ì£¼ ì‘ì€ ê°’($\epsilon$)ì„ ë”í•´ì£¼ëŠ” **Smoothing** ê¸°ë²•ì„ ì”ë‹ˆë‹¤.

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ìš°ë¦¬ëŠ” ë¹” ì„œì¹˜ë¡œ ë¬¸ì¥ì„ ë§Œë“¤ê³ , BLEUë¡œ í‰ê°€ê¹Œì§€ í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ Seq2Seqì—ëŠ” ì—¬ì „íˆ **"ê¸´ ë¬¸ì¥ì˜ ë³‘ëª© í˜„ìƒ"**ì´ë¼ëŠ” ê±°ëŒ€í•œ ë²½ì´ ë‚¨ì•„ ìˆìŠµë‹ˆë‹¤.
ì¸ì½”ë”ê°€ 100ë‹¨ì–´ë¥¼ ë²¡í„° í•˜ë‚˜ì— ì••ì¶•í•˜ëŠ” ê²ƒì€ ë¬´ë¦¬ì…ë‹ˆë‹¤.

**"ë²ˆì—­í•  ë•Œ ë¬¸ì¥ ì „ì²´ë¥¼ ì™¸ìš°ì§€ ì•Šê³ , í•„ìš”í•œ ë‹¨ì–´ë§Œ ê·¸ë•Œê·¸ë•Œ ë‹¤ì‹œ ë³´ë©´ì„œ(Attend) ë²ˆì—­í•  ìˆ˜ëŠ” ì—†ì„ê¹Œ?"**
ë‹¤ìŒ ì‹œê°„, ë”¥ëŸ¬ë‹ ì—­ì‚¬ìƒ ê°€ì¥ ìœ„ëŒ€í•œ ë°œëª… ì¤‘ í•˜ë‚˜ì¸ **[Attention Mechanism]**ì„ í†µí•´ ì´ ë²½ì„ ë¶€ìˆ˜ê² ìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ Transformerì™€ GPTë¡œ ê°€ëŠ” ë§ˆì§€ë§‰ ì—´ì‡ ì…ë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Modified Precision:} ì •ë‹µì— ë“±ì¥í•˜ëŠ” íšŸìˆ˜ë§Œí¼ë§Œ ì¸ì •í•˜ì—¬ ì¤‘ë³µ ê¼¼ìˆ˜ë¥¼ ë§‰ëŠ”ë‹¤.
    \item \textbf{N-gram:} 1~4ë‹¨ì–´ ë¬¶ìŒì„ ë¹„êµí•˜ì—¬ ë¬¸ë§¥ê³¼ ìœ ì°½ì„±ì„ í‰ê°€í•œë‹¤.
    \item \textbf{BP:} ë¬¸ì¥ì´ ì§§ìœ¼ë©´ ì ìˆ˜ë¥¼ ê¹ì•„ ì •ë³´ ëˆ„ë½ì„ ë°©ì§€í•œë‹¤.
    \item \textbf{Standard:} ê¸°ê³„ ë²ˆì—­ ì„±ëŠ¥ í‰ê°€ì˜ ì—…ê³„ í‘œì¤€ì´ë‹¤.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (ìˆ˜í•™ì  ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Attention Mechanism}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.10 RNN, LSTM, Word2Vec, BLEU \textit{- Completed}
        \item \textbf{10.11 Attention Mechanism}
        \begin{itemize}
            \item The Bottleneck Problem in Seq2Seq
            \item Dynamic Context Vector ($c^{\langle t \rangle}$)
            \item Alignment Scores \& Attention Weights ($\alpha$)
            \item Implementation: Bahdanau Attention
        \end{itemize}
    \end{itemize}
    \item[Chapter 11.] Transformer Network \textit{- Next Part}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ Seq2Seq ëª¨ë¸ì€ í˜ì‹ ì ì´ì—ˆì§€ë§Œ, ë¬¸ì¥ì´ ê¸¸ì–´ì§€ë©´ ì„±ëŠ¥ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§€ëŠ” **'ë³‘ëª©(Bottleneck)'** í˜„ìƒì„ ê²ªì—ˆìŠµë‹ˆë‹¤. ì¸ì½”ë”ê°€ 100ë‹¨ì–´ì§œë¦¬ ê¸´ ë¬¸ì¥ì„ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„° í•˜ë‚˜ì— ì–µì§€ë¡œ ì••ì¶•í•´ì•¼ í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.

ìƒê°í•´ ë´…ì‹œë‹¤. ì—¬ëŸ¬ë¶„ì´ ê¸´ ë¬¸ì¥ì„ ë²ˆì—­í•  ë•Œ, ë¬¸ì¥ ì „ì²´ë¥¼ í•œ ë²ˆ ì½ê³  ì™¸ìš´ ë‹¤ìŒ ëˆˆì„ ê°ê³  ë²ˆì—­í•©ë‹ˆê¹Œ? ì•„ë‹™ë‹ˆë‹¤. **"ì§€ê¸ˆ ë²ˆì—­í•˜ë ¤ëŠ” ë‹¨ì–´ì™€ ê´€ë ¨ëœ ì›ë¬¸ ë¶€ë¶„ì„ ê·¸ë•Œê·¸ë•Œ ë‹¤ì‹œ ì³ë‹¤ë³´ë©´ì„œ(Attend)"** ë²ˆì—­í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ êµ¬í˜„í•œ ê²ƒì´ ë°”ë¡œ **ì–´í…ì…˜(Attention)**ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ ê¸´ ì‹œí€€ìŠ¤ ì •ë³´ ì†ì‹¤ì„ í•´ê²°í•˜ê³  ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ëŠ” ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ë™ì  ë¬¸ë§¥:} ë§¤ íƒ€ì„ ìŠ¤í…ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ê³„ì‚°ë˜ëŠ” **ë¬¸ë§¥ ë²¡í„° $c^{\langle t \rangle}$**ì˜ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
    \item \textbf{ê°€ì¤‘ì¹˜:} ì¸ì½”ë”ì˜ íŠ¹ì • ë‹¨ì–´ì— ì–¼ë§ˆë‚˜ ì§‘ì¤‘í• ì§€ ë‚˜íƒ€ë‚´ëŠ” **ì–´í…ì…˜ ê°€ì¤‘ì¹˜($\alpha$)**ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    \item \textbf{êµ¬ì¡°:} ì¸ì½”ë”ì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœë¥¼ ê°€ì¤‘ í•©(Weighted Sum)í•˜ì—¬ ì •ë³´ë¥¼ ì·¨í•©í•˜ëŠ” ì›ë¦¬ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{í•™ìŠµ:} ê°€ì¤‘ì¹˜ë¥¼ ê²°ì •í•˜ëŠ” ì •ë ¬ ëª¨ë¸(Alignment Model)ë„ ì—­ì „íŒŒë¡œ í•¨ê»˜ í•™ìŠµë¨ì„ íŒŒì•…í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ë¹„ìœ } \\ \hline
\textbf{Attention Weight} & $\alpha^{\langle t, t' \rangle}$: $t$ë²ˆì§¸ ì¶œë ¥ ì‹œ $t'$ë²ˆì§¸ ì…ë ¥ì˜ ì¤‘ìš”ë„. & ë‹ë³´ê¸°ë¡œ ë¹„ì¶”ëŠ” ë¹›ì˜ ì„¸ê¸°. \\ \hline
\textbf{Context Vector} & $c^{\langle t \rangle}$: ì¸ì½”ë” ìƒíƒœë“¤ì˜ ê°€ì¤‘ í‰ê· . & ë²ˆì—­ ì¤‘ì¸ ë‹¨ì–´ì˜ ì—°ê´€ ì°¸ê³  ìë£Œ. \\ \hline
\textbf{Alignment Score} & ë‹¨ì–´ ê°„ì˜ ìœ ì‚¬ë„/ê´€ë ¨ì„± ì ìˆ˜. & ë‘ ë‹¨ì–´ê°€ ì–¼ë§ˆë‚˜ 'ì°°ë–¡ê¶í•©'ì¸ê°€. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: í•„ìš”í•œ ê³³ë§Œ ì³ë‹¤ë³´ê¸°}

\subsection{1. The Intuition (ì§ê´€)}


ê¸°ì¡´ Seq2SeqëŠ” ì¸ì½”ë”ì˜ **ë§ˆì§€ë§‰ ì •ë³´** í•˜ë‚˜ë§Œ ë””ì½”ë”ì—ê²Œ ë˜ì ¸ì¤ë‹ˆë‹¤. ë°˜ë©´, ì–´í…ì…˜ ëª¨ë¸ì€ ë””ì½”ë”ê°€ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œë§ˆë‹¤ ì¸ì½”ë”ì˜ **ëª¨ë“  ì€ë‹‰ ìƒíƒœ**($a^{\langle 1 \rangle}, a^{\langle 2 \rangle}, \dots$)ë¥¼ ë‹¤ ì°¸ê³ í•©ë‹ˆë‹¤. ë‹¨, ì¤‘ìš”í•œ ê²ƒì€ ì§„í•˜ê²Œ(ë†’ì€ ê°€ì¤‘ì¹˜), ì•ˆ ì¤‘ìš”í•œ ê²ƒì€ íë¦¬ê²Œ ë´…ë‹ˆë‹¤.

\begin{analogybox}{ë²ˆì—­ê°€ì˜ íë”ê±°ë¦¬ê¸°}
\begin{itemize}
    \item **ê¸°ì¡´:** ë¬¸ì¥ì„ í•œ ë²ˆ ì½ê³  ì±…ì„ ë®ì€ ë’¤ ì•”ê¸°í•´ì„œ ë²ˆì—­í•¨. (ë³‘ëª© ë°œìƒ)
    \item **ì–´í…ì…˜:** ë²ˆì—­ë¬¸ì„ ì“°ë©´ì„œ ì›ë¬¸ì—ì„œ ì§€ê¸ˆ ë‹¨ì–´ì™€ ê´€ë ¨ëœ ë¶€ë¶„(ì˜ˆ: ì£¼ì–´, ëª©ì ì–´)ì„ ê³„ì† **íë”íë”** ë‹¤ì‹œ ë³´ë©° ë²ˆì—­í•¨.
\end{itemize}
\end{analogybox}

\subsection{2. The Architecture: Context Vector $c^{\langle t \rangle}$}
ë¬¸ë§¥ ë²¡í„° $c^{\langle t \rangle}$ëŠ” ì¸ì½”ë”ì˜ ëª¨ë“  ì€ë‹‰ ìƒíƒœ $a^{\langle t' \rangle}$ì˜ **ê°€ì¤‘ í‰ê· **ì…ë‹ˆë‹¤.

\begin{formulabox}{Context Vector ê³µì‹}
$$ c^{\langle t \rangle} = \sum_{t'} \alpha^{\langle t, t' \rangle} a^{\langle t' \rangle} $$
\begin{itemize}
    \item $\alpha^{\langle t, t' \rangle}$: ë””ì½”ë” $t$ ì‹œì ì— ì¸ì½”ë” $t'$ ì‹œì ì„ ë³´ëŠ” ë¹„ì¤‘.
    \item $\sum_{t'} \alpha^{\langle t, t' \rangle} = 1$ (í™•ë¥ ê°’).
\end{itemize}
\end{formulabox}

---

\section{Deep Dive: Computing Attention (ìˆ˜í•™ì  ì›ë¦¬)}

ê·¸ë ‡ë‹¤ë©´ ê°€ì¥ ì¤‘ìš”í•œ $\alpha$ (ê°€ì¤‘ì¹˜)ëŠ” ëˆ„ê°€ ì •í• ê¹Œìš”? ì‚¬ëŒì´ ì •í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ì´ì¡°ì°¨ **ì‘ì€ ì‹ ê²½ë§(Alignment Model)**ì´ í•™ìŠµí•©ë‹ˆë‹¤.

\begin{enumerate}
    \item \textbf{Alignment Score ($e^{\langle t, t' \rangle}$):} ë””ì½”ë”ì˜ ì´ì „ ìƒíƒœ $s^{\langle t-1 \rangle}$ì™€ ì¸ì½”ë” ìƒíƒœ $a^{\langle t' \rangle}$ê°€ ì–¼ë§ˆë‚˜ ì–´ìš¸ë¦¬ëŠ”ì§€ ì ìˆ˜ë¥¼ ë§¤ê¹ë‹ˆë‹¤.
    $$ e^{\langle t, t' \rangle} = \text{dense\_layer}([s^{\langle t-1 \rangle}, a^{\langle t' \rangle}]) $$
    
    \item \textbf{Softmax (Weights):} ì ìˆ˜ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    $$ \alpha^{\langle t, t' \rangle} = \frac{\exp(e^{\langle t, t' \rangle})}{\sum_{k=1}^{T_x} \exp(e^{\langle t, k \rangle})} $$
\end{enumerate}



% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Bahdanau Attention}

TensorFlow/Kerasì˜ ì„œë¸Œí´ë˜ì‹± ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Bahdanau Attention Layer]
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense

class BahdanauAttention(Layer):
    def __init__(self, units):
        super().__init__()
        self.W1 = Dense(units) # Decoder state weight
        self.W2 = Dense(units) # Encoder state weight
        self.V = Dense(1)      # Score weight

    def call(self, query, values):
        # query: ë””ì½”ë” ì€ë‹‰ ìƒíƒœ (Batch, hidden)
        # values: ì¸ì½”ë” ì€ë‹‰ ìƒíƒœë“¤ (Batch, Tx, hidden)
        
        query_with_time = tf.expand_dims(query, 1)
        
        # 1. ì ìˆ˜ ê³„ì‚° (Additive Attention)
        score = self.V(tf.nn.tanh(self.W1(query_with_time) + self.W2(values)))
        
        # 2. ê°€ì¤‘ì¹˜ ê³„ì‚° (Softmax)
        attention_weights = tf.nn.softmax(score, axis=1)
        
        # 3. ë¬¸ë§¥ ë²¡í„° ê³„ì‚° (Weighted Sum)
        context_vector = attention_weights * values
        context_vector = tf.reduce_sum(context_vector, axis=1)
        
        return context_vector, attention_weights
\end{lstlisting}

% --- 8. FAQ ---
\section{FAQ & Pitfalls}

\textbf{Q. ì–´í…ì…˜ì„ ì“°ë©´ ëª¨ë¸ì´ ë¬´ê±°ì›Œì§€ì§€ ì•Šë‚˜ìš”?} \\
\textbf{A.} ë„¤, ì¸ì½”ë”ì˜ ëª¨ë“  íƒ€ì„ ìŠ¤í…ì„ ë§¤ë²ˆ ì—°ì‚°í•´ì•¼ í•˜ë¯€ë¡œ $T_x \times T_y$ì˜ ì—°ì‚°ëŸ‰ì´ ì¶”ê°€ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì„±ëŠ¥ í–¥ìƒ í­ì´ ì›Œë‚™ ì»¤ì„œ ê°ìˆ˜í•  ë§Œí•œ ë¹„ìš©ì…ë‹ˆë‹¤.

\textbf{Q. ì–´í…ì…˜ì€ ë²ˆì—­ ë§ê³  ì–´ë””ì— ì“°ì´ë‚˜ìš”?} \\
\textbf{A.} **ì´ë¯¸ì§€ ìº¡ì…”ë‹**ì—ì„œë„ ì“°ì…ë‹ˆë‹¤. ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œë§ˆë‹¤ ì´ë¯¸ì§€ì˜ íŠ¹ì • êµ¬ì—­ì„ ì³ë‹¤ë³´ëŠ” ì‹ì´ì£ .

% --- 9. ë‹¤ìŒ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ë‹¤ìŒ ë‹¨ê³„ (Next Step)}
ì–´í…ì…˜ì€ RNNì˜ í•œê³„ë¥¼ ë¶€ìˆ˜ê³  ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‚¬ëŒë“¤ì€ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤.
**"ì–´í…ì…˜ì´ ì´ë ‡ê²Œ ê°•ë ¥í•˜ë‹¤ë©´, êµ³ì´ ëŠë¦° ìˆœì°¨ ë°©ì‹(RNN)ì„ ì¨ì•¼ í• ê¹Œ? ê·¸ëƒ¥ ì–´í…ì…˜ë§Œ ì“°ë©´ ì•ˆ ë˜ë‚˜?"**

ì—¬ê¸°ì„œ ë”¥ëŸ¬ë‹ ì—­ì‚¬ë¥¼ ë°”ê¾¼ ë…¼ë¬¸, **"Attention Is All You Need"**ê°€ ë“±ì¥í•©ë‹ˆë‹¤.
ë‹¤ìŒ ì‹œê°„, RNNê³¼ CNNì„ ëª¨ë‘ ë²„ë¦¬ê³  ì˜¤ì§ ì–´í…ì…˜ë§Œìœ¼ë¡œ ë¬´ì¥í•œ í˜„ëŒ€ AIì˜ ì‹¬ì¥, **[Transformer Network]**ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

\vspace{0.5cm}

\begin{summarybox}{ë‹¨ì› ìš”ì•½ (Cheat Sheet)}
\begin{enumerate}
    \item \textbf{Problem:} Seq2Seq ì¸ì½”ë”ì˜ ê³ ì • í¬ê¸° ë²¡í„° ë³‘ëª© í˜„ìƒ.
    \item \textbf{Solution:} ë””ì½”ë”ê°€ ë§¤ ìˆœê°„ ì¸ì½”ë”ì˜ ëª¨ë“  ë¶€ë¶„ì„ ì—°ê´€ì„±ì— ë”°ë¼ ì°¸ê³ í•œë‹¤.
    \item \textbf{Formula:} $c^{\langle t \rangle} = \sum \alpha^{\langle t, t' \rangle} a^{\langle t' \rangle}$ (ê°€ì¤‘ í‰ê· ).
    \item \textbf{Benefit:} ê¸´ ë¬¸ì¥ ë²ˆì—­ ì„±ëŠ¥ ë¹„ì•½ì  ìƒìŠ¹ + ì‹œê°ì  í•´ì„ ê°€ëŠ¥.
\end{enumerate}
\end{summarybox}

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{formulabox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ Transformers \& Self-Attention}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.11 RNN, LSTM, Attention Mechanism \textit{- Completed}
        \item \textbf{10.12 Transformers \& Modern Attention}
        \begin{itemize}
            \item Self-Attention: Understanding Relationships
            \item The Q, K, V Framework (Query, Key, Value)
            \item Multi-Head Attention: Parallel Perspectives
            \item Beyond Transformers: LLM and ViT
        \end{itemize}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ì§€ë‚œ ì‹œê°„ì— ë°°ìš´ 'ì–´í…ì…˜'ì´ RNNì´ë¼ëŠ” ì—”ì§„ì˜ ì„±ëŠ¥ì„ ë†’ì—¬ì£¼ëŠ” ë³´ì¡° ì¥ì¹˜ì˜€ë‹¤ë©´, ì˜¤ëŠ˜ ë°°ìš¸ **íŠ¸ëœìŠ¤í¬ë¨¸(Transformer)**ëŠ” ê·¸ ì—”ì§„ ìì²´ë¥¼ í†µì§¸ë¡œ ê°ˆì•„ì¹˜ìš´ í˜ëª…ì…ë‹ˆë‹¤. 
êµ¬ê¸€ì˜ 2017ë…„ ë…¼ë¬¸ **"Attention Is All You Need"**ëŠ” ìˆœì°¨ì  ì²˜ë¦¬(RNN)ë¥¼ ë²„ë¦¬ê³  ì˜¤ì§ ì–´í…ì…˜ë§Œìœ¼ë¡œ ë¬¸ì¥ì„ ì´í•´í•  ìˆ˜ ìˆìŒì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ ChatGPTì™€ ëª¨ë“  ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸(LLM)ì˜ íƒ„ìƒ ì§€ì ì…ë‹ˆë‹¤.

% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í˜„ëŒ€ AI ì‹œìŠ¤í…œì˜ ì¤‘ì¶”ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ ì›ë¦¬ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Self-Attention:} ë¬¸ì¥ ë‚´ ë‹¨ì–´ë“¤ì´ ì„œë¡œ ì–´ë–¤ ê´€ê³„ë¥¼ ë§ºê³  ìˆëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ëŠ” ì›ë¦¬ë¥¼ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{Q, K, V:} ì •ë³´ ê²€ìƒ‰ì˜ ê´€ì ì—ì„œ ì–´í…ì…˜ì„ ê³„ì‚°í•˜ëŠ” ì„¸ ê°€ì§€ êµ¬ì„± ìš”ì†Œ(Query, Key, Value)ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{Multi-Head:} ì—¬ëŸ¬ ê°œì˜ ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ ë‹¤ê°ì ì¸ ë¬¸ë§¥ì„ ì¶”ì¶œí•˜ëŠ” ì´ìœ ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.
    \item \textbf{ë³‘ë ¬ì„±:} RNNê³¼ ë‹¬ë¦¬ ë¬¸ì¥ ì „ì²´ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•¨ìœ¼ë¡œì¨ ì–»ëŠ” ê³„ì‚° íš¨ìœ¨ì„±ì„ ì´í•´í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. ìš©ì–´ ì •ë¦¬ ---
\section{Essential Terminology: The QKV Framework}
\begin{center}
\begin{tabular}{|c|l|l|}
\hline
\textbf{ìš©ì–´} & \textbf{ì˜ë¯¸} & \textbf{ì—­í• } \\ \hline
\textbf{Query (Q)} & ì§ˆë¬¸ & "ì§€ê¸ˆ ë‚´ê°€ ì°¾ê³  ì‹¶ì€ ì •ë³´ëŠ” ë¬´ì—‡ì¸ê°€?" \\ \hline
\textbf{Key (K)} & ì¸ë±ìŠ¤/ìƒ‰ì¸ & "ë‚´ê°€ ê°€ì§„ ì •ë³´ì˜ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€?" \\ \hline
\textbf{Value (V)} & ë‚´ìš© & "í‚¤ì›Œë“œì— í•´ë‹¹í•˜ëŠ” ì‹¤ì œ ê°’ì€ ë¬´ì—‡ì¸ê°€?" \\ \hline
\textbf{Scaled Dot-Product} & ìœ ì‚¬ë„ ê³„ì‚° & Qì™€ Kë¥¼ ê³±í•´ ì ìˆ˜ë¥¼ ë‚´ê³  ë£¨íŠ¸ ì°¨ì›ìœ¼ë¡œ ë‚˜ëˆ”. \\ \hline
\end{tabular}
\end{center}

% --- 6. í•µì‹¬ ê°œë… ìƒì„¸ ì„¤ëª… ---
\section{Core Concepts: ìê¸° ìì‹ ì„ ëŒì•„ë³´ê¸°}

\subsection{1. Self-Attention: ëŒ€ëª…ì‚¬ í•´ê²°}
RNNì€ ë‹¨ì–´ë¥¼ ìˆœì„œëŒ€ë¡œ ì½ì—ˆì§€ë§Œ, íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë¬¸ì¥ ì•ˆì˜ ëª¨ë“  ë‹¨ì–´ ìŒ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ **í•œ ë²ˆì—** ê³„ì‚°í•©ë‹ˆë‹¤.

\begin{analogybox}{"it"ì€ ëˆ„êµ¬ì¸ê°€?}
\textbf{"The animal didn't cross the street because it was too tired."} \\
Self-Attentionì€ "it"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ë•Œ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ í›‘ìŠµë‹ˆë‹¤.
ëª¨ë¸ì€ "animal"ê³¼ì˜ ì–´í…ì…˜ ì ìˆ˜ë¥¼ ê°€ì¥ ë†’ê²Œ ì£¼ì–´, **"it = animal"**ì„ì„ ìŠ¤ìŠ¤ë¡œ ê¹¨ë‹«ê²Œ ë©ë‹ˆë‹¤.
\end{analogybox}



\subsection{2. Multi-Head Attention}
í•˜ë‚˜ì˜ ì–´í…ì…˜ë§Œìœ¼ë¡œëŠ” ë³µì¡í•œ ë¬¸ë§¥ì„ ë‹¤ ë‹´ê¸° ì–´ë µìŠµë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ì—¬ëŸ¬ ê°œì˜ 'í—¤ë“œ'ë¥¼ ë³‘ë ¬ë¡œ ìš´ì˜í•©ë‹ˆë‹¤.

\begin{itemize}
    \item **Head 1:** ì£¼ì–´ì™€ ë™ì‚¬ì˜ ê´€ê³„ì— ì§‘ì¤‘
    \item **Head 2:** í˜•ìš©ì‚¬ì™€ ëª…ì‚¬ì˜ ê´€ê³„ì— ì§‘ì¤‘
    \item **Head 3:** ëŒ€ëª…ì‚¬ì™€ ì„ í–‰ì‚¬ì˜ ê´€ê³„ì— ì§‘ì¤‘
\end{itemize}
ì´ ë‹¤ì–‘í•œ ê´€ì ë“¤ì„ ë‚˜ì¤‘ì— í•˜ë‚˜ë¡œ í•©ì³(**Concatenate**) í’ë¶€í•œ ì´í•´ë ¥ì„ ê°–ê²Œ ë©ë‹ˆë‹¤.

---

\section{Deep Dive: The Attention Math}

íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì‹¬ì¥ì¸ Scaled Dot-Product Attentionì˜ ìˆ˜ì‹ì…ë‹ˆë‹¤.

\begin{formulabox}{Scaled Dot-Product Attention}
$$ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V $$
\begin{itemize}
    \item $QK^T$: Queryì™€ Key ì‚¬ì´ì˜ ìœ ì‚¬ë„ ì ìˆ˜.
    \item $\sqrt{d_k}$: ì°¨ì›ì´ ì»¤ì§ˆ ë•Œ ì ìˆ˜ê°€ ë„ˆë¬´ ì»¤ì ¸ Softmax ê¸°ìš¸ê¸°ê°€ ì†Œì‹¤ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” **Scaling** ê³„ìˆ˜.
    \item $V$: ìµœì¢…ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ ì •ë³´ë¥¼ ì·¨í•©í•  ì‹¤ì œ ê°’.
\end{itemize}
\end{formulabox}



% --- 7. êµ¬í˜„ ì½”ë“œ ---
\section{Implementation: Conceptual Logic}

TensorFlowë¥¼ ì´ìš©í•œ ì–´í…ì…˜ì˜ í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Scaled Dot-Product Attention]
import tensorflow as tf

def scaled_dot_product_attention(q, k, v, mask=None):
    # 1. ìœ ì‚¬ë„ Score ê³„ì‚°
    matmul_qk = tf.matmul(q, k, transpose_b=True)

    # 2. Scaling (ë£¨íŠ¸ ì°¨ì›ë§Œí¼ ë‚˜ëˆ„ê¸°)
    dk = tf.cast(tf.shape(k)[-1], tf.float32)
    scaled_logits = matmul_qk / tf.math.sqrt(dk)

    # 3. Softmaxë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ ê²°ì •
    attention_weights = tf.nn.softmax(scaled_logits, axis=-1)

    # 4. Valueë¥¼ ê°€ì¤‘ í•©í•˜ì—¬ ìµœì¢… ì¶œë ¥
    output = tf.matmul(attention_weights, v)

    return output, attention_weights
\end{lstlisting}

% --- 8. í˜„ëŒ€ì  íŠ¸ë Œë“œ ---
\section{Modern Trends: Beyond Transformers}

íŠ¸ëœìŠ¤í¬ë¨¸ ì´í›„ AIì˜ íŒ¨ëŸ¬ë‹¤ì„ì€ í¬ê²Œ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.

\begin{itemize}
    \item \textbf{LLM (ê±°ëŒ€ ì–¸ì–´ ëª¨ë¸):} GPT, Llama ë“± íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ ìˆ˜ë°± ê°œ ìŒ“ì•„ ì¸ê°„ ìˆ˜ì¤€ì˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    \item \textbf{ViT (Vision Transformer):} ì´ë¯¸ì§€ë„ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì–´í…ì…˜ì„ ì ìš©í•©ë‹ˆë‹¤. ì´ì œ ì»´í“¨í„° ë¹„ì „ì—ì„œë„ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ëŒ€ì„¸ì…ë‹ˆë‹¤.
    \item \textbf{Efficiency:} ë¬¸ì¥ì´ ê¸¸ì–´ì§ˆ ë•Œ ê³„ì‚°ëŸ‰ì´ $N^2$ìœ¼ë¡œ í­ë°œí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ FlashAttention ë“±ì´ í™œë°œíˆ ì—°êµ¬ë©ë‹ˆë‹¤.
\end{itemize}



% --- 9. ìš”ì•½ ë° ë§ˆë¬´ë¦¬ ---
\section*{ğŸ Course Conclusion}
\begin{summarybox}{ìµœì¢… ìš”ì•½}
\begin{enumerate}
    \item \textbf{Self-Attention:} ë‹¨ì–´ ê°„ì˜ ìœ ê¸°ì  ê´€ê³„ë¥¼ í•œ ë²ˆì— íŒŒì•…í•˜ëŠ” í˜ì‹ ì  ë°©ì‹.
    \item \textbf{No More RNN:} ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•´ì ¸ ëª¨ë¸ì˜ ê±°ëŒ€í™”ê°€ ê°€ëŠ¥í•´ì§.
    \item \textbf{Versatility:} í…ìŠ¤íŠ¸ë¥¼ ë„˜ì–´ ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ëª¨ë“  ë„ë©”ì¸ìœ¼ë¡œ í™•ì¥ ì¤‘.
\end{enumerate}
\end{summarybox}

ì¶•í•˜í•©ë‹ˆë‹¤! Seq2Seqì—ì„œ ì‹œì‘í•´ ì–´í…ì…˜ì„ ê±°ì³, ì„¸ìƒì„ ë°”ê¾¸ê³  ìˆëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ì›ë¦¬ê¹Œì§€ ëª¨ë‘ ë§ˆìŠ¤í„°í•˜ì…¨ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ì´ì œ í˜„ëŒ€ ë”¥ëŸ¬ë‹ì˜ ì •ì ì— ì„œ ìˆìŠµë‹ˆë‹¤.

\end{document}

\documentclass[a4paper, 11pt]{article}

% --- íŒ¨í‚¤ì§€ ì„¤ì • ---
\usepackage{kotex} % í•œê¸€ ì§€ì›
\usepackage{geometry} % ì—¬ë°± ì„¤ì •
\geometry{left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage{amsmath, amssymb, amsfonts} % ìˆ˜ì‹ íŒ¨í‚¤ì§€
\usepackage{graphicx} % ì´ë¯¸ì§€ ì‚½ì…
\usepackage{hyperref} % í•˜ì´í¼ë§í¬
\usepackage{xcolor} % ìƒ‰ìƒ ì§€ì›
\usepackage{listings} % ì½”ë“œ ë¸”ë¡
\usepackage[most]{tcolorbox} % ë°•ìŠ¤ ë””ìì¸
\usepackage{enumitem} % ë¦¬ìŠ¤íŠ¸ ìŠ¤íƒ€ì¼
\usepackage{booktabs} % í‘œ ë””ìì¸
\usepackage{array} % í‘œ ì •ë ¬

% --- ìƒ‰ìƒ ì •ì˜ ---
\definecolor{conceptblue}{RGB}{60, 100, 160}
\definecolor{analogygreen}{RGB}{80, 160, 100}
\definecolor{alertred}{RGB}{200, 60, 60}
\definecolor{exampleorange}{RGB}{230, 120, 30}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

% --- ì½”ë“œ ìŠ¤íƒ€ì¼ ì„¤ì • ---
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{analogygreen},
    keywordstyle=\color{conceptblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{exampleorange},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    frame=single
}
\lstset{style=mystyle}

% --- ë°•ìŠ¤ ìŠ¤íƒ€ì¼ ì •ì˜ ---
\newtcolorbox{summarybox}[1]{
    colback=conceptblue!5!white,
    colframe=conceptblue!80!black,
    fonttitle=\bfseries,
    title=ğŸ“Œ #1
}

\newtcolorbox{analogybox}[1]{
    colback=analogygreen!5!white,
    colframe=analogygreen!80!black,
    fonttitle=\bfseries,
    title=ğŸ’¡ #1 (ì§ê´€ì  ë¹„ìœ )
}

\newtcolorbox{warningbox}[1]{
    colback=alertred!5!white,
    colframe=alertred!80!black,
    fonttitle=\bfseries,
    title=âš ï¸ #1 (ì˜¤í•´ ë°©ì§€ ê°€ì´ë“œ)
}

\newtcolorbox{mathbox}[1]{
    colback=exampleorange!5!white,
    colframe=exampleorange!80!black,
    fonttitle=\bfseries,
    title=ğŸ§® #1 (í•µì‹¬ ì›ë¦¬)
}

% --- ë¬¸ì„œ ì •ë³´ ---
\title{\textbf{[CS230] Sequence Models: \\ The Transformer Architecture}}
\author{Lecturer: Gemini (Integrated Editor)}
\date{}

\begin{document}

\maketitle

% --- 1. ì „ì²´ ëª©ì°¨ (TOC) ---
\section*{ğŸ“š Course Table of Contents}
\begin{itemize}
    \item[Chapter 1-9.] Deep Learning Fundamentals \& CNNs \textit{- Completed}
    \item[\textbf{Chapter 10.}] \textbf{Sequence Models (Current Part)}
    \begin{itemize}
        \item 10.1-10.11 RNN, LSTM, Self-Attention \textit{- Completed}
        \item \textbf{10.12 The Transformer Architecture}
        \begin{itemize}
            \item Positional Encoding: Giving Order to Sets
            \item Encoder Block: Deep Understanding
            \item Decoder Block: Sequential Generation
            \item BERT vs GPT: Architectural Philosophy
        \end{itemize}
    \end{itemize}
\end{itemize}

\vspace{0.5cm}
\hrule
\vspace{0.5cm}

% --- 3. ì´ì „ ë‹¨ì› ì—°ê²° ---
\section*{ğŸ”— ì§€ë‚œ ì‹œê°„ ë³µìŠµ ë° ì—°ê²°}
ìš°ë¦¬ëŠ” ì§€ë‚œ ì‹œê°„ì— Self-Attentionì´ë¼ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì˜ í•µì‹¬ ì—”ì§„ì„ ë°°ì› ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ì€ ì´ ì—”ì§„ë“¤ì„ ì¡°ë¦½í•´ì„œ ì–´ë–»ê²Œ BERTë‚˜ GPT ê°™ì€ ê±°ëŒ€ ëª¨ë¸ì˜ ë¼ˆëŒ€ê°€ ë˜ëŠ” **íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜(Transformer Architecture)**ê°€ ì™„ì„±ë˜ëŠ”ì§€ í•´ë¶€í•´ ë³´ê² ìŠµë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” í¬ê²Œ ì •ë³´ë¥¼ ì½ì–´ ë“¤ì´ëŠ” **ì¸ì½”ë”(Encoder)**ì™€ ì •ë³´ë¥¼ ìƒì„±í•˜ëŠ” **ë””ì½”ë”(Decoder)** ë‘ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.



% --- 4. ê°œìš” ---
\section{Unit Overview}
\begin{summarybox}{í•µì‹¬ ëª©í‘œ}
ì´ ë‹¨ì›ì€ í˜„ëŒ€ AI ì‹œìŠ¤í…œì˜ ì„¤ê³„ë„ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ì˜ êµ¬ì¡°ë¥¼ ì™„ë²½íˆ ì´í•´í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{ì „ì²´ êµ¬ì¡°:} ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì—°ê²° ë©”ì»¤ë‹ˆì¦˜ì„ íŒŒì•…í•©ë‹ˆë‹¤.
    \item \textbf{ìˆœì„œ ì¸ì‹:} RNN ì—†ì´ ìˆœì„œë¥¼ íŒŒì•…í•˜ëŠ” **Positional Encoding**ì„ ë°°ì›ë‹ˆë‹¤.
    \item \textbf{ì•ˆì •ì„±:} ê¹Šì€ ì¸µì„ ìŒ“ê¸° ìœ„í•œ **Add \& Norm** (ì”ì°¨ ì—°ê²° ë° ì •ê·œí™”) ì¥ì¹˜ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    \item \textbf{ì² í•™ì˜ ì°¨ì´:} ì¸ì½”ë” ê¸°ë°˜ì˜ **BERT**ì™€ ë””ì½”ë” ê¸°ë°˜ì˜ **GPT** ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
\end{itemize}
\end{summarybox}

% --- 5. í•µì‹¬ êµ¬ì„± ìš”ì†Œ ---
\section{Core Components: ì„¤ê³„ë„ í•´ë¶€}

\subsection{1. Positional Encoding (ìœ„ì¹˜ ì •ë³´ ì£¼ì…)}
íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” ë‹¨ì–´ë¥¼ í•œêº¼ë²ˆì— ë³‘ë ¬ë¡œ ì…ë ¥ë°›ê¸° ë•Œë¬¸ì—, "I love you"ì™€ "You love me"ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•©ë‹ˆë‹¤. 

\begin{mathbox}{ìœ„ì¹˜ ì‹ í˜¸ ì£¼ì…}
ê° ë‹¨ì–´ ë²¡í„°ì— ê³ ìœ í•œ ìœ„ì¹˜ ê°’ì„ ë”í•´ì¤ë‹ˆë‹¤. ì£¼ê¸° í•¨ìˆ˜ì¸ Sineê³¼ Cosineì„ í™œìš©í•˜ì—¬ ê³ ì°¨ì› ê³µê°„ì—ì„œ ë‹¨ì–´ì˜ ìƒëŒ€ì /ì ˆëŒ€ì  ìœ„ì¹˜ë¥¼ ì…í™ë‹ˆë‹¤.
$$ PE_{(pos, 2i)} = \sin(pos/10000^{2i/d_{model}}) $$
$$ PE_{(pos, 2i+1)} = \cos(pos/10000^{2i/d_{model}}) $$
\end{mathbox}



\subsection{2. Encoder Block (ì¸ì½”ë”: ì´í•´ì˜ ì˜ì—­)}
ì¸ì½”ë”ëŠ” ì…ë ¥ ë¬¸ì¥ì„ í†µì§¸ë¡œ ë³´ê³  ë¬¸ë§¥ì„ íŒŒì•…í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Multi-Head Self-Attention:} ë‹¨ì–´ ì‚¬ì´ì˜ ìœ ê¸°ì  ê´€ê³„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    \item \textbf{Add \& Norm:} ì”ì°¨ ì—°ê²°ë¡œ ê¸°ìš¸ê¸° ì†Œì‹¤ì„ ë§‰ê³  Layer Normìœ¼ë¡œ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤.
    \item \textbf{BERT:} ì´ ì¸ì½”ë”ë¥¼ ìŒ“ì•„ ë§Œë“  ëª¨ë¸ë¡œ, ë¬¸ë§¥ì„ ì–‘ë°©í–¥(Bi-directional)ìœ¼ë¡œ ì´í•´í•©ë‹ˆë‹¤.
\end{itemize}

\subsection{3. Decoder Block (ë””ì½”ë”: ìƒì„±ì˜ ì˜ì—­)}
ë””ì½”ë”ëŠ” ë¶„ì„ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¨ì–´ë¥¼ í•˜ë‚˜ì”© ìƒì„±í•©ë‹ˆë‹¤.
\begin{itemize}
    \item \textbf{Masked Self-Attention:} ë‹¨ì–´ ìƒì„± ì‹œ ë¯¸ë˜ ë‹¨ì–´ë¥¼ ë¯¸ë¦¬ ë³´ê³  ì •ë‹µì„ ìœ ì¶”í•˜ì§€ ëª»í•˜ë„ë¡ ë§ˆìŠ¤í‚¹ì„ ì ìš©í•©ë‹ˆë‹¤.
    \item \textbf{Encoder-Decoder Attention:} ë‹¨ì–´ë¥¼ ë±‰ì„ ë•Œë§ˆë‹¤ ì¸ì½”ë”ê°€ ë¶„ì„í•œ ì›ë¬¸ ì •ë³´ë¥¼ ë‹¤ì‹œ ì°¸ê³ í•©ë‹ˆë‹¤.
    \item \textbf{GPT:} ì´ ë””ì½”ë”ë¥¼ ìŒ“ì•„ ë§Œë“  ëª¨ë¸ë¡œ, ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡(Auto-regressive)ì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
\end{itemize}



---

\section{Deep Dive: BERT vs GPT}

ê°™ì€ íŠ¸ëœìŠ¤í¬ë¨¸ ë¿Œë¦¬ë¥¼ ë‘ì§€ë§Œ, ë¶€í’ˆ ì„ íƒì— ë”°ë¼ ì„±ê²©ì´ ì™„ì „íˆ ë‹¬ë¼ì§‘ë‹ˆë‹¤.

\begin{center}
\begin{tabular}{>{\raggedright\arraybackslash}p{2.5cm} p{5cm} p{5cm}}
\toprule
\textbf{íŠ¹ì§•} & \textbf{BERT} & \textbf{GPT} \\ \midrule
\textbf{ê¸°ë°˜ êµ¬ì¡°} & íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” & íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë” \\
\textbf{í•™ìŠµ ë°©í–¥} & ì–‘ë°©í–¥ (Bi-directional) & ë‹¨ë°©í–¥ (Left-to-Right) \\
\textbf{ì£¼ìš” ëª©ì } & ë¬¸ì¥ ë¶„ë¥˜, ì§ˆì˜ì‘ë‹µ & ë¬¸ì¥ ìƒì„±, ëŒ€í™”, ì°½ì‘ \\
\textbf{ë¹„ìœ } & ë¹ˆì¹¸ ì±„ìš°ê¸° ì˜í•˜ëŠ” ëª¨ë²”ìƒ & ì´ì•¼ê¸°ë¥¼ ì§€ì–´ë‚´ëŠ” ì†Œì„¤ê°€ \\ \bottomrule
\end{tabular}
\end{center}

% --- 7. êµ¬í˜„ ê´€ì  ---
\section{Implementation: Hugging Face Transformers}

í˜„ëŒ€ ê°œë°œìë“¤ì€ ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ë¯¸ì„¸ ì¡°ì •(Fine-tuning)í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.

\begin{lstlisting}[language=Python, caption=Loading Pre-trained Transformers]
from transformers import BertModel, GPT2Model

# BERT: ì˜ë¯¸ ì¶”ì¶œ ë° ë¬¸ì¥ ë¶„ë¥˜ ì‹œ
bert = BertModel.from_pretrained('bert-base-uncased')

# GPT: í…ìŠ¤íŠ¸ ìƒì„± ë° ì±—ë´‡ êµ¬í˜„ ì‹œ
gpt = GPT2Model.from_pretrained('gpt2')
\end{lstlisting}

% --- 8. ìš”ì•½ ë° ë§ˆë¬´ë¦¬ ---
\section*{ğŸ Summary \& Next Step}
\begin{enumerate}
    \item \textbf{Transformer:} RNNì„ ëŒ€ì²´í•œ ì™„ë²½í•œ ë³‘ë ¬ ì²˜ë¦¬ ì•„í‚¤í…ì²˜.
    \item \textbf{Positional Encoding:} ì–´í…ì…˜ì— 'ìˆœì„œ'ë¼ëŠ” ìƒëª…ë ¥ì„ ë¶ˆì–´ë„£ëŠ” ì¥ì¹˜.
    \item \textbf{Functional Split:} ì´í•´ë¥¼ ì›í•˜ë©´ ì¸ì½”ë”(BERT), ìƒì„±ì„ ì›í•˜ë©´ ë””ì½”ë”(GPT).
\end{enumerate}

ì´ì œ ì—¬ëŸ¬ë¶„ì€ í˜„ëŒ€ ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ”ì§€ ê·¸ ë°‘ë°”ë‹¥ ì„¤ê³„ë„ë¥¼ ë³´ì…¨ìŠµë‹ˆë‹¤. ì´ êµ¬ì¡°ëŠ” ì´ì œ ì–¸ì–´ë¥¼ ë„˜ì–´ ì´ë¯¸ì§€(ViT), ìŒì„±(Whisper) ë“±ìœ¼ë¡œ ë¬´í•œíˆ í™•ì¥ë˜ê³  ìˆìŠµë‹ˆë‹¤.

\vspace{0.5cm}
\begin{summarybox}{ìƒê°í•´ë³¼ ê±°ë¦¬}
ë§ˆìŠ¤í‚¹(Masking)ì´ ì—†ë‹¤ë©´ ë””ì½”ë”ëŠ” ì™œ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í• ê¹Œìš”? ì”ì°¨ ì—°ê²°(Residual Connection)ì´ ê¹Šì€ íŠ¸ëœìŠ¤í¬ë¨¸ ì¸µì—ì„œ ì™œ í•„ìˆ˜ì ì¼ê¹Œìš”? ê¶ê¸ˆí•œ ì ì´ ìˆë‹¤ë©´ ì–¸ì œë“  ì§ˆë¬¸í•´ ì£¼ì„¸ìš”!
\end{summarybox}

\end{document}