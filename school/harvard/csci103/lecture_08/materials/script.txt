103 day 8 - YouTube
https://www.youtube.com/watch?v=0CWRgBkhMTc

Transcript:
(00:01) All right. Um, welcome everyone. Today we'll um look at uh how to create uh reproducible machine learning. So we'll start off with a review from from some of the concepts from last week and look at the role of the data engineer in the um for machine learning. look at the um model life cycle also um talk about challenges in ML um development.
(00:40) I look at um ML pipelines and feature and feature stores and as well as uh ML ops and um do a intro into ML flow which is a way of um creating automated uh ML pipelines within the data bricks framework and then for lab last part of the class will be lab and we'll look at um ML flow for model life cycle management. Okay.
(01:18) So starting with some questions um scale out refers to starting with a review of from last week. So when we think about scaling up, we um what are we adding more of? Are we adding more workers to the cluster? Adding or adding more nodes to the cluster for um more compute. And then uh autoscale uses And it's kind of a reference to the previous question.
(02:07) Dynamic number of nodes well or um or we could say just scale out. So um the ability to add more nodes through scale out and um ability to use the what would be interesting is what is the other form of scaling. So you have scale out and what's the uh what's a complimentary scaling technique? Uh we can vertically we can just get like size small to medium to large to x large. What is it called it? Scale up. Scale uh horizontal vertically.
(02:47) Scale up. Yes. Vert vertic vertical scaling versus horizontal scaling. Mhm. And then ability to use a service any time is referred to as uh serverless. Well, serless um helps helps with this, but it um what sort of non-functional requirement does this uh does this refer to? And I'll give you a hint.
(03:20) Um Amazon had a problem with this yesterday. High availability. Exactly. Availability. Yeah. Amazon if for um yeah they they suffered a major outage yesterday with their um I guess US East one region. Okay. Um planning and which will affect their um availability numbers when they compute them at the end of the year.
(03:54) Um planning to switch to a different region or outage in response to or as a way of supporting what and this also relates to what happened yesterday with Amazon multiple availability zones. Yeah. Um but what what is it trying to address? One is no disaster recovery.
(04:25) So um like okay well US East one is down but maybe we can switch over to US East 2 or something. Ail ability to grow shrink nodes refers to elasticity. That's right. And then example of a bridge pattern in Spark. It's a a pattern that helps us um connect to different data sources. anyone and con connecting to data sources is the clue. It's it's the connector pattern that we looked at.
(05:15) And then what what are some of the advantages of the multihop pattern? Uh we also refer to this as the medallion pattern sometimes. What does what does it help us with? I distributing the load. Yeah. Anyone else? Sorry. you can process certain parts of the medallion part without having to redo everything from scratch. Yeah. Well, let's just look.
(06:10) So it's um supports different types of users and different SLAs's like the um silver curated level can support ML where the um gold or or final um layer can support business BI and BA. Um so here multi multiop pattern refers to bronze, silver, gold. Yeah. Okay. Sorry. sometimes referred to as medallion and you can think of the Olympics um and the gold and the medals that they provide to help remind remember remember that and also it's it's modular so you're you're not trying to do everything at once you've you've broken it up into different stages so it helps helps um enforce modularity in your um data pipeline and it's also more robust
(06:55) if one part fails maybe the other up to two parts have completed su successfully and you just need to rerun maybe the gold layer versus all all everything that came before. So very important and as especially as we're learning how to create um automated data pipelines remembering wh why um why the modularity is important. Uh okay, BI versus BA.
(07:27) Um business intelligence is just asking like what what happened to summarizing to make decisions and analytics is asking like why did it happen or like what's what's forecasting like what's what's going to happen next for diagnostics. Perfect. Yeah. Very good.
(07:52) So um looking BA is more looking into the future and and trying to predict um make predictions and uh okay two KPIs that um are important for PI tools. Think about price and speed. I'm sorry, what was the first? Price and speed. Cost is important and speed. Speed is one of the or latency is one of the the um answers I'm looking for. What about um ability to scale up to more users? like how many users it can support. So latency I'm sorry I go ahead.
(08:49) Oh no I was asking is it like parallel parallelism but concurrency I guess. Yeah concurrency. So um being able to support instead of one one um analyst maybe a 100 or a thousand analyst architecture paradigm that supports AI plus BI. And we we looked at this last week. Lakehouse. Yeah, Lakehouse.
(09:18) So, Lakehouse combines data lake with um the ability to do efficient um data warehousing as well. All all within one one platform. Okay, good. So, just a quick review of um data science. So um so data science is um combination of um a process skills and and also follows the scientific method. Um and so it it has the name data science and um and it includes um different um different um skill sets or um uh information applied um statistics and mathematics.
(10:11) uh domain knowledge about a particular domain whether it be marketing or or um oil production or or some other domain is important and then uh also a background in computer science and software engineering. Um and then the uh just um we and the purpose of data science is to um solve uh real world problems and um help um improve improve business the world through better understanding of of our environment which matches with um the scientific method which is a a process that allows continual advancement of human understanding of the natural world and uh uh using empirical evidence uh
(11:02) objectivity and um also critical thinking to refine and expand our knowledge. So, so um and regarding the the scientific method, the scientific method, as you know, starts with a question. Uh um the question could be something like, well, why why are sales declining? Uh business might want to ask that question.
(11:34) And then um and then and then that's followed by a hypothesis while maybe the sales is declining because of our user engagement is is is is less. And so that's a that could be a hypothesis. And then once we have a hypothesis, we want to do some experimentation to try to understand whether or not that hypothesis is correct or not.
(12:02) Is that is that actually the right answer to the question? And it could be. It it may not be. The experimentation is designed to to um understand whether the hypothesis is correct or not. Once we've done our experimentation, we can interpret the results and and come up with a answer to whether or not the hypo hypothesis was was um correct or not. And then um and and both are important.
(12:29) It's it's it's good to know whether or not it is or it isn't. If it's if it's if that's not the reason, then probably we need to continue to hypothesize and and come up with other experiments to to test those um theories. And then finally, um once we've completed our interpretation, we can um share the results with the broader community.
(12:58) That may be the the scientific community or it may be just a community within our company that we we want to share the results with and so that everybody can learn and and and um benefit from the from the learnings that we've um accomplished through the um um scientific method. Any questions about about this? I think it's interesting that the the use of the scientific method in and data science.
(13:38) Okay. So um there's rule-based um so for machine learning uh there's um rule-based uh um algorithms that can help us and then there's um AI based or machine learning uh algorithms that can help us um uh answer questions and um rule-based um is is based on a set of um facts And then we apply rules to them which are generally if then else statements.
(14:17) If um if this then that and then um and that that can of often be um useful for helping identify you know some some source of a problem or helping to um determine some uh answer to some question that we may have. But um rule-based systems tend to break down um if there's too many rules, it gets very um complicated to manage and understand and and also um maintain.
(14:47) So um where AI based systems are are based on algorithms they're um generally probabilistic and um and but more scale they require more data but um but they're also more scalable because um you're not managing a huge set of of rules. So um we can um use um columns to create features and then based on features do predictions.
(15:20) Um more data um generates more wisdom and better better answers and um more data also requires um efficient algorithms. As we as we grow the data set then the algorithms need to be computationally efficient to be able to handle that large amount of data. Um we can start with um data exploration uh data wrangling.
(15:51) We've seen this before where we we take um different data sets, combine them together, maybe do some data wrangling where we improve the data, fill in um missing missing data, um get rid of duplicates, um do some filtering and then um from from that data, the curated data, we can do feature engineering.
(16:16) Features are um aggregates generally aggregates of data. For example, um the average transaction amount for um a customer could be a feature that we might compute um um based on a data set for sales. And then once we have the features, then we can use those features as input to our models and um and then do evaluation, tuning and selection of models.
(16:44) And then finally um we uh deploy the model uh and and use it for in real time inferencing uh to answer um various business questions. So that's the that's the basic flow of of model development. And um this is important that um once we've deployed the model we're not done. we generally need to go back and um maybe do more model development to adjust for um model drift or um or maybe just basically improve the model.
(17:23) I can also go back and um maybe come up with ideas for new features that may help um improve the model performance. Uh do more data wrangling and also incorporate more um data sets into our um model pipeline. So um this combines the um a picture of the scientific method with um our ML pipeline development.
(18:01) And we can see that um the um the model basically um the machine learning model pipeline that we create can help us with um experimentation. We can use the model to test out different um uh theories or hypotheses and um and also the um the model can help us with interpreting the results as well as um sharing the uh results through um model deployment.
(18:32) Um now with um chat GPT uh you could even consider um machine learning helping us with um coming up with the right questions to ask and as well as the right maybe um potential hypotheses that we could um uh investigate as part of the scientific method. So um as as as systems like um LMS like chat GPT get chat GPT get more um capable um we're able to use them for maybe um expanded role in the um um scientific um process.
(19:20) Okay. So just a quick review of AI versus um machine machine learning versus deep learning. So AI is a superset of machine learning and um deep learning. And it basically anything that can um um perform a task that a human might perform is considered AI. And then um it it machine learning is a process of uh learning from data without being explicitly programmed.
(19:53) So um and the counter to that is rule-based systems that required uh someone to construct if then else statements to help drive the um decision process. Machine learning is be able to do it based on probability and and algorithms. Um and then deep learning uses uh is a subset of machine learning and it uses complex neural networks to um detect patterns and large unstructured data sets.
(20:26) So deep learning is of course um what's behind um things like LLMs and um and uh a self-driving cars and and um uh computers that can play go and so have had uh dramatic success recently. uh just a timeline showing the um development of AI and then and up to the 80s and then machine learning up to the 2010 and then deep learning after that.
(21:02) So, um like the first LLMs were able to um detect whether or not there was a cat in a picture um um became the go champion one won at Jeopardy and um and so and and now there's self-driving cars driving around cities uh all thanks to deep learning and um and that that continues to um grow quickly and capab ability. Um so the the um the the key um key thing that's that's happened to enable deep learning is really uh the ability to have lots of data.
(21:50) Uh deep learning is very data hungry and also um uh compute. It's also requires a lot of compute to create these um uh large language models and um but now um thanks to platforms like uh AWS and and data bricks and and others um the compute is available and um fairly uh affordable so that it it's it's um cost effective to produce these models.
(22:26) So just um and and just a picture of how Spark has um helped um help support the platform. Remembering we've seen this picture before uh the actual generation of the machine learning models just one one very small part of the puzzle and in producing machine learning models.
(22:51) There's a lot of um data configuration and and um feature feature extraction and and other other things that go on besides just generating the machine learning model and the um that that these platforms uh especially things like Spark have really helped um enable the ability to manage this complexity. So um there's uh just a review of ML frameworks. There's um classical and statistical algorithms uh neural net and deep learning um and then generative AI using LLMs and GAN or generative adversial networks for um image analysis and um and then to um to support these different types of workflows we have um something referred to as AutoML that's helps automate the process of coming up
(23:50) with um machine learning um pipelines and um it's easier and faster but also rec um provides fewer um controls and is less customizable where no notebook based um is more powerful but but also more complicated and and maybe more difficult requires a higher skill set to be able to do that.
(24:22) But um AutoML is important because it helps um enable like the C citizen data scientist um or citizen machine learning um person to be able to tackle these things. And maybe a single individual to do it versus an an entire team of people working together to um produce more customized um frameworks. So we we have um again more review um basically we have um different types of ML uh we can we can categorize ML as discrete or continuous and also um supervised learning versus unsupervised learning.
(25:10) So, um I'll ask you um like where which which bucket does um the question how much rain how much will it rain tomorrow fall into? Anyone supervised and continuous? Yeah. So um yeah that's correct. And then um will it rain tomorrow? That's discrete and supervised. Yeah. Classification. Yeah.
(25:55) And then um which days of the week um uh can be grouped based on weather patterns? That's right. clustering. So discrete and unsupervised. Discrete unsupervised. Yeah. Very good. And then um key factors that influence consumer purchasing emission reduction continuous unsupervised discrete and unsupervised or um or maybe continuous and unsupervised where dimensional reduction where we're we may have lots of choices but we're trying to um basically reduce the um number of variables to a few that that are more easy to um understand and contemplate. So key factors for influencing consumer
(26:55) purchasing. There may be many things that that affect that. But um maybe the most important things are price and um availability and um functionality or um brand awareness. things like that could be um versus other things like maybe the weather could affect um purchasing. So we we've um talked about the ability to predict the future as as being uh very important for businesses to be able to understand what's going to happen going in the future so that we can um better make better business decisions and um with those uh future um conditions in mind so that we're ready
(27:48) if if the um market is going to increase and and maybe we expand production or if it's going to contract, we want to maybe reduce um production. Uh so um being able to do that accurately is really important and and and if we get it wrong, it could, you know, um cause cause damage for the business. So um and then one heruristic um is that the um that's true is is that there more data is better better than more um complex algorithms.
(28:34) So, um you can you can improve your um ability to predict with with additional data more than um maybe um more complex algorithms. So, that's something that's important for us to remember is that data data often has a much bigger impact than um tweaking algorithms in terms of uh getting um good results. This is a view of different um machine learning types here. Um regression, classification.
(29:07) So regression is is um you being able to predict um housing prices for example. Classification might be um uh looking at uh medical images and detecting whether or not um there's cancer in the um image. uh clustering uh being able to segment customers into different groups. Um association um basically um associating like the classic example is associating diapers with beer and shopping carts.
(29:41) Um and then uh classification text classification um for example is this spam or is this email spam or or legitimate email? uh something that we use every day in our our mail applications. Clustering um determining um which lanes are are on the highway based on uh the behavior of uh vehicles on the road.
(30:13) Uh classification, optimized marketing like um being able to um suggest um ads or or products to to users. And then um and then reinforcement learning with control um with uh with where the target variables are not available. Um that's um something like a driverless car where there's no um there's no like right or wrong answer at any one point, but you over time like you want to um guide the vehicle to um have fewer accidents and drive safely and and um and and uh you know follow um traffic traffic regulations and and speed limits and things like that. So, um there you're um there's no no
(31:06) specific um yes or right or wrong answer at any one point, but there's uh generally um a set of criteria that could be used to gauge whether or not the um the self-driving vehicle is performing well or not and and to improve improve the performance over time. All right.
(31:41) So, um the uh the data engineering role um basically in in the um in the in the development of ML pipelines. We have the data engineer helping with the in ingesting, storing, validating, cleaning and um and standardizing um the curated data that's then passed over to um and maybe this curated data winds up in the this silver or curated level layer that we were talking about before um in the medallion architecture and then a data scientist takes over um maybe uh computes features uh uh selects the features that he thinks are valuable for input to the model um does some
(32:27) coding and and some training to train the model and then um some sort of validation and then um the ML engineer takes over from there helps validate evaluate and and then package um for distribution of the model helps package the model uh categorize um containerize the model and deploy the model and then and serve it and then and then then applications can then consume the model and then um the the flow happens um again.
(33:03) So kind of a continuous um uh cycle of um ingesting data and finally outputting models that then can be used to um support the creation of more more models. So um I'll just um review some of the aspirations of the ML platform and then hand it over to Anandita to continue. But um we basically want our ML platform to support the development of um with a a diverse set of tools um including libraries, languages and collab and different ways of collaboration um support different deployment strategies uh uh different um types of um compute
(33:58) um single node and distributed algorithms including CPUs, GPUs um support higher efficiencies like lower cost, faster um and the uh for training, tuning, tracking and and inferencing uh be able to support a feature store for the um aggregates that the data scientist comes up with.
(34:28) we need to be able to manage those features and and be able to compute them easily and then store the results. So, um feature store helps with that. And then um AutoML for uh that can help simplify the creation of ML pipelines for for citizen data scientists. And then um be able to monitor drift of the model. Is the um is the incoming data um different from what the model was trained on? Is and is the performance of the model degrading? And that could be an indication that we need to um retrain the model on more recent data.
(35:09) Um operationalization of the ML pipeline basically deploying it and making it accessible for people to to use and and um inference against and then it should be reproducible. So um if we've if we've you know once we've created the model we should be able to maybe recreate the model um based on the same data and get the same results.
(35:36) So it should be um a a process that's that's coded and and managed that um it can be reproduced on demand. All right with that I'll um I'll hand it over to Anadita to continue with ML life cycle and challenges. All right. Thanks, Eric. Thank you. And Anadita, your your camera is not shared right now. Okay. This is so sensitive. Is it better now? Yes. Okay.
(36:23) So hi everyone by show of hands how many of you do this for a day-to-day job you come from the data science um track and so are you in the process of um training models uh or managing models let's see you can come off mute and um let's like level that as to um you know what are some of the operations that you typically perform? No, it can't be this bad.
(37:16) Now if you train models like score models or package models I train models for assignments. Okay. Um but for in the data science track I would imagine that this might be a little familiar. If not yeah let's go through it. uh we've kind of gone through it in class that um the raw data has to be made ready to feed into the models which is why we do the ETL we do feature extraction and we'll talk a little bit more about what feature engineering is all about because um a lot of times this is done by data engineers and not so much by uh ML folks
(37:57) and then the actual feeding of the data into the model is a very small piece but it gets over glorified then you need to be able to get it into a state in which you serve the model. So when new data comes in, you're able to score it. Uh it's also referred to as inference. Uh and you can do it in batch, you can do it in real time or you can do it um uh at scale as HTTP request.
(38:24) There are three main ways by which uh you can score. Um then that it needs to be monitored as well to be um able to detect a drift and anytime you uh detect some degradation either in your serving or in your training or you will have to go back to some previous levels. But this is like the chain of things.
(38:52) Data comes you cleanse it you featurize it you train it you serve it you monitor it. uh from here maybe you go back here or maybe you have to go back here because you need new features your model is not behaving properly this is at an abstract level the highest level irrespective of what tools you're using you have to do this now let's look at why ML becomes um very complex very quickly and um you know um it requires a level of specialization because here the ecosystem is extremely fragmented um like we talked about um say Python or Spark or R or you know some libraries to be able to do so. It's not that data engineering is not fragmented but it is
(39:33) perhaps less fragmented than the ML ecosystem. Uh look at all these uh frameworks like if a organization decides to standardize on one of them and you are not trained on it your productivity will start to go down. Uh you would not be able to work as fast. So ideally um you if a platform were to enable you to work on any one of them to be able to get to your uh end results uh in the most optimum manner in the most performant manner that would be great.
(40:06) Then the model is it's one thing to just fit the data into the model. The there's another thing about tuning it. So that is that whole hyperparameter optimization that might take several several cycles. And then deployment is also non-trivial. Many times ML projects are built um and shown around and people like it but they just can't take it into production and then you have to manage the model because model is a very living breathing asset and it's not like do it once and forget it. It needs a lot of TLC. It needs care.
(40:38) It needs uh um it needs refinement and it needs justification because these models are not built for fun. They are contributing to a very key area of your business. So the data and the model are getting put together and these insights are driving decisions that your executives or your business stakeholders are doing which is why this becomes like the data suddenly become extremely valuable.
(41:01) You have taken it from raw which was like um in a very um loose state and you have refined refined refined. So when there was that question about the medallion architecture one of the main reasons why we are moving from left to right is we are curating the data.
(41:19) Every time you refine the data, you add value to the data. So, think of raw gold and think of a piece of um like a gold um um jewelry, right? The the cost gets so much more. Think of a piece of carbon and then your final uh diamond. So, all that work that has gone into it is what makes it valuable. Otherwise, you know, how much do we pay for coal anyway? So along with this you need to be able to collaborate because this is not a one-man job.
(41:46) There are lots of different types of data personas. We saw three of them in the previous one but there are lots more as well with whom you have to do hand on and hand off. Scale is important. Something that you build on your laptop u uh needs to be able to handle the test of um volume that uh is going to um that it's going to face once it's put onto production. Um and then the model management of the governance of course.
(42:12) So just like we talked about unity catalog um deciding who has access to what data and what kind of access whether it's read whether it's read write similarly models could be trained with sensitive data. The insights that they generate could be sensitive. So they need to be treated uh with similar care as well.
(42:31) So that's all about governance of the model. underneath it. Um when you create these features, you need a repository to store it. When you are doing your um experimentation upon you know is this good, is this model good, is this set of parameters good, you need to be able to track it just like uh when in school you were doing your science experiments and writing that you added this to this and you got this.
(42:56) Similarly, this this process needs to be automated. It should not be manual because every time there's something manual people forget and then you would lose the traceability to be able to reproduce that uh experiment which is why today's lecture is called reproducible ML. It's great that we can all do ML but to do reproducible ML is just a little harder.
(43:16) Then there are tools in the ecosystem which try to simplify the job of a data scientist in which you do not perhaps need to have a PhD or statistical background. These models are very very robust and um they cater to a wide breath of things. So you just fit in the input and get the output.
(43:36) You don't care about a lot of them. So that is called AutoML. There are companies like data robot who were like really forerunners here. They thought about this a while back. Um but uh it's good to get started with these things. So if you do not have enough control to be able to tune it and refine it over time, then you're kind of stuck.
(43:53) you do not know how to do. You got a very nice shiny toy but uh you've tired of playing with it. You want a slightly sophisticated you want to tweak it a little. You can't. Which is why data bricks invests in what was called as um glass open glass autoML in which um you whatever it has generated all of it all of that code is available to you.
(44:21) At some point we'll show you how you can uh use the AutoML that is uh in your workspaces. uh there are very limited algorithms though. So you have um a little bit of forecasting, a little bit of regression, a little bit of classification. So the number of algorithms is limited. But uh for the breadand butter type of scenarios, a a little bit of prediction or to even test the hypothesis that this data is good enough that a model should be created out of it, it plays its part.
(44:46) Um cloud execution. Yes, we are all in uh cloud ecosystems. uh most of the time there's project management, there's model exchange. Sometimes the models you create, you might have to give it to somebody else to do to to use uh AB testing.
(45:07) This is hard because this is where you'll have to make a decision that even though your intuition says that this is the right model, but your data and your inferencing and your results and your metrics are saying otherwise. And so you when you have an existing model um and a new model comes up initially you put 20% of your incoming traffic onto the new model and the 80% continues then you kind of vet it see whether it is good or not it's performing it is uh acceptable people like the results then you'll slowly start to put 50% of your data 100% of your data and phase out this model. So the older model was originally the
(45:38) champion. Now the new model is referred to as the challenger and the challenger has to prove his worth and replace the champion if it has the merit. Right? That's what it AB testing is about. Um CICD um uh Jenkins. So you know you have to integrate this with the rest of your code with your um ecosystem of where all your assets are uh orchestration.
(46:03) So if when you remember when we did data bricks jobs the workflows that we created you could have machine learning as part of the node inferencing at master of the node EIBI dashboard as part of the node life cycle management going round and round data drift and model drift and we'll talk a little bit about it any questions here all right um so from data ingestion to model deployment this is basically exactly the same diagram diagram which says that we are in a lake which means we get all kinds of data not just structured data but we get semi-structured and unstructured. So a
(46:42) lot of text lot of images video audio you've already done an assignment in which you've played with um um unstructured data as well. We'll see a few more later on. And these are uh if you use the ML runtime these are some of the out ofbox um um libraries and frameworks that are available.
(47:05) So in the classic compute you had the data engineering mode and you had the ML runtime mode. Now of course with um serverless you do not have all those variations and in fact database is kind of catching up. So some of the ML stuff may or may not you know be ready there but um this I'm talking about the traditional um development ML development deploy anywhere and at scale. So all these clouds are also available.
(47:30) Now can anyone tell me why this out of the box environment um you know all these libraries coming together is um relevant is and why why should we care that uh somebody has actually packaged it all and made it available. Any guesses? I guess makes it easier to deploy and then you have sometimes also faster deployment times um faster deployment but Daniel was also bringing up a very good point which is that each of these have been developed independently.
(48:06) So can you imagine the library hell of ensuring that all of them are kind of cooperating. How many of you have come across when you you were doing a pip install of one package and then you did a pip install of another and it said that oh no you can't you can't work with this version of pandas or this versions of numpy I want you to upgrade to this particular version.
(48:24) So that is the dependency hell and it's very very hard to do when you have couple of libraries it's okay but when you have as many disparate uh libraries so um hardening them and making sure that they are working well with each other uh in a CPU and a GPU environment is actually making the job of a uh ML or a data scientist much easier. So all you have to do instead of spending like 5 hours figuring out which library what version you just go you want to use this library you go ahead and start coding and focus on your problem. You need an additional library something which is a little more esoteric you still have the ability to
(48:57) layer that in. in data prep which is the first half of the class we talked about um the value of data versioning as well because sometimes when data comes in and it's not very uh good you might have to roll back to a previous version.
(49:19) So think of how the problem is magnified when it comes to model building uh if every time the data changes and each one of the data scientists is pointing to a table they will get really uh varied results. So working off version 200 of the table or 23 of the table it means that all of the people are experimenting.
(49:39) Maybe I'm working with XG boost you are working with a deep learning model somebody else but the data is the same and the results that we are going to all get and compare against one another is going to be apples to apples otherwise I built it on Monday you built it on Wednesday. we were all looking at different views of the data. Working off a versioned uh data set, building your model and then testing it out with a different version is a like a slightly different problem.
(50:02) So baselining um is uh better if all of the experiments are uh looking at the same value of the data. Today the data characteristics may change. So the model that you're building will be very different and then you're comparing with something else which doesn't even matter anymore.
(50:21) Featurizing uh featurization is important um model building and training of course uh tracking and registry. So just like uh when you develop code you go to git and you put your code there so that uh it's safe somebody else wants to take a look at it they can it's versioned.
(50:40) Uh the same thing is needed for models also because it's constantly changing. You can't look at a model and say how it was built. You can't say what kind of data was used what kind of parameters were there what metrics it had provided. It's important because tomorrow when you're going to retire the model or you're going to promote the model, you're going to be accountable for why you did it.
(50:56) So you have to uh justify and why is this especially important is because uh imagine I go to the website, I given some characteristics and I get a loan approved. Somebody else with similar characteristics but few differences, they go and they put the information and they get rejected. We can sue the company on grounds of discrimination if like you know our bigger factors were exactly the same but there might be some some um regional factors or some very narrow uh profile characteristics which is why I got it and maybe the other person didn't. So you have to be careful and you have to justify and that explanability of the model and the
(51:32) fairness of the model is important. Uh NIC for instance is coming up with the model AI law like there isn't that many laws out there. Europe is probably a little better at it. Um, in which you have to explain exactly what you're doing with the data and what kind of models you are.
(51:53) Some companies are um, expected by regulation to reveal all that but but not everybody. Model serving and model monitoring. We talked about governance. Now we are going to introduce all these ops words. When I started in software engineering, we heard DevOps like DevOps is the way in which your uh, code gets into production. Then we came to data ops. Data ops is your data pipelines.
(52:16) Your pipelines will are running uh curating your data, producing your gold tables. Then we had model ops. Model ops is all the thing that is necessary for your uh uh trained model, your registered model to be made available so that scoring can be done on real data. Um and then the replacement of the model and so on. Um so a pipeline is ML pipeline is very similar to your data pipeline.
(52:40) So let's say if we start by reading some data that's in a data frame and you have a series of transformers um and some estimators. So your transformers obviously uh are going to take the data and um um you know create another version of it like some maybe add some tables made some manipulations but your estimator is where it's learning.
(53:01) So learning happens here and there are parameters being fed to each one of them. In the end you do care about um an evaluator and a metric. So if you cannot measure what it is then you would never be able to make improvements on it. So that is why having these evaluators and metrics are important to continuously inch forward. Uh you had a baseline model then you make it better then you make it better.
(53:27) If you didn't have any metrics or evaluation uh guidance, you would not know whether your the needle is moving to the right or to the left. So transformers um are are of different kinds. You might see string indexes. Um so when you have a string field, you might have to change it into index or you might have heard categorical uh variables go through one encoding.
(53:47) So when you have like limited number of categories, that's the technique that is used. A standard scaler would scale the data um to some um unit of standard deviation because you are normalizing it at that point. Imputer um we talked about it when we were dealing with data quality.
(54:08) It um completes missing values in a data set. So without it it might not be possible for a model to um score. And so imputtors are important. Uh you may of course drop the data but how much can you drop? at some point you're going to uh make up for it. And then a vector assembler is going to combine all of the features um or a list of columns into a single vector column which would then be fed into some of the models.
(54:32) So your estimators are basically when you every time you call a fit function it's going to learn. So the input here is a data frame but the output here is a model. Here the input is a data frame. The output is also a data frame. We talked about metrics. they are um like evaluation for uh checking the performance of a model.
(55:02) So what are some examples of metrics that you may have uh come across? Accuracy. Accuracy. Very good. Yes. F1 score. Yeah, you can have um F1 scores. You can have all of this and then you have evaluators to go with it to kind of um um see how this can be automated on each uh run. Um now confusion matrix is also a very important thing. Of course it is in the context of classification algorithms uh primarily.
(55:30) So you will predict certain values and you will have um in reality there are certain values. So you compare uh whether what you predicted was actually positive or what you predicted as negative was actually negative. So these are the true positives and the true negatives and these are the false positives and the false negatives.
(55:52) The reason why having all four values is important is because in some domains just saying oh I predicted and you know 85% of the time it's doing so great. Uh that's a fantastic model. Uh and what about these values? If you know a lot of times it's actually not able to do it like and very common example of that would be when you have very few occurrences like say you've gone for your regular medical scan and um you have asked ML and ML says oh wow this person has got blah blah disease and the probability is very high and an automated report has already been sent to the client to come back and
(56:26) get reviewed and all of that stuff that's very very expensive and so in that case not only having the true positives but reducing all these false positives and false negatives is also important. Now feature engineering so uh feature engineering is an important aspects um mainly for um ML but could be interesting for uh the data engineering aspects as well.
(56:57) So you've got your different sources um you're you're cleansing and transforming them into features and there's a difference between just your regular columns in your gold tables and the features that you have created. Um and these features are being used by models maybe the you know all the columns in the table may not be relevant the output that you get are your insights.
(57:16) So this is where we get a little more into domain knowledge of how to create these features and make these machine learning algorithms uh work. So anytime that you are just creating um some simple um value uh that may or may not be a feature.
(57:36) Typically creation of a feature is a compute inensive task which is why you don't want to do it over and over again. You want to do it once and you want to use it at training time and you want to use it at um inference time with the same values so that you know uh there is no drift in your inference when you're computing the features separately. Secondly, uh some feature computation is expensive.
(57:58) So you want to use it uh across use cases like say a user profile feature. Well, why would you want to do it multiple times even though the groups are uh different? They sit in different parts of the organization but user profile needs to be computed once and should be used for all use cases. Um and let's take some examples.
(58:18) If there is um a user table with zip code, payment methods etc. Um you might do some transformation and do some category encoding out of it. Now if you have description, category etc. which is like a little longish um and people are buying things um maybe you may want to uh see if there is a particular day of the week where um the context is important for making a sale.
(58:44) Um may maybe like Thanksgiving could be um um a heavy um day for uh sales or maybe Saturday morning at 9:00 is a time when people are free and they there is a particular pattern to it. There could be uh other raw data like the purchases that they people have made. So this user, this item, this date, this quantity, this price. Um you might um have feature augmentation.
(59:10) So I I I think I should take back one step. There's transformations. There are context features like weekday is something that you can add. So that's an additional context. Feature augmentation weather on that day. So even though it was a Tuesday but it was raining very heavily and so maybe that is why your sales is down and preomputed uh features.
(59:36) So when you're trying to do customer lifetime value when you're trying to predict what is the next purchase the customer would make and within what time span you want to understand what are the rolling values like I buy a shoe a running shoe maybe uh every year or every 2 years and so when once that feature has been computed I'm not going to get coupons for running shoes within that period as much but when it is close to that uh the the uh lifetime value will indicate that there's a good time that this person may buy a shoe, give the coupon now and uh uh maybe the chances of a conversion is going to be higher. So what were the last 7 days, 14 days, 21 days, month, year type of um purchase history of the
(1:00:16) user and these are very expensive features to compute. Maybe the category encoding was okay. Maybe slapping in a weekday was okay, but getting going out getting the weather um keeping track of the rolling um um purchase history, these are expensive and that is why uh feature engineering is a field to itself and these values are going to feed into the model to make the prediction.
(1:00:42) So I might say okay I I will purchase uh a roller with a probability of 0.58 in the next 1 week or blah blah because I'm doing some homework u housework and you can see some patterns around it coming up with features is difficult and timeconuming it requires expert knowledge by here expert knowledge refers to the domain knowledge uh and applied machine learning is basically feature engineering so I'm sure all of you at some point or the other would heard some inspirational talk by Andrew Eni. I really enjoy um his uh uh videos.
(1:01:20) Uh now let's look a little more into what is um ML Ops. So data feature pipeline developing a model train and deploy. So you have your data ops here. You have your ML ops and then you know the data engineer whatever code and assets they have built is going to be uh used with the CI/CD hook.
(1:01:45) Um they might contribute to creating the feature store and that data is flowing in. But as an ML person like as a data scientist the code that uh that they are producing that also goes into a CI/CD system. But then the models the uh the validation results and everything else is also securely saved. They are put pushed into production and then the tracking and management.
(1:02:09) Basically the same terms that we saw below but now we are just layering different concepts on top of it. Uh you know making the demarcation between who's doing what um how the logging of the scoring is happening. So as as inferencing um uh results are made available that that can influence the recreation of certain features as well. So remember the customer lifetime value.
(1:02:34) So every time the customer purchases something of course the feature will have to be recomputed. Um now comes MLflow. So what is MLflow? MLflow is like um a fantastic open-source um uh framework that uh data bricks has built for quite a while. Um and it is so popular that um even our competitors have standardized on it.
(1:03:02) That just shows um the strength, the power, the the capabilities that are punched into it. Like it's it's just a wonderful tool. It is open source. So which means anybody should be able to use it not just on the data bricks platform and there are certain conventions specs and tools um and like uh any if you are on the data bricks platform then you get certain other uh advantages which is mainly it's um tied to the data gravity which is on the uh platform it is tied to the security um uh bells and whistles around unity catalog so you when for instance you don't have to maintain your own tracking
(1:03:35) server so as a managed service Those are certain advantages of using ML4 directly on data bricks where it was originally designed but if not you can use it uh even on your laptop because it's basically a set of u uh libraries that you would be able to use and people use it in uh EKS AKS environments and containerized course and whatnot there are CLI um it is communitybacked it's a framework and tool agnostic and this is important remember that zoo of different frameworks that we had seen. It's a little agnostic of it. Its purpose is to do life cycle management. It is not in
(1:04:14) the business of creating the model. It's not obsessed about that. But it is obsessed about making sure that the model that you value so much is actually as valuable as as you think it is or it needs some attach. It's API first. So, it's built around uh Okay, there were some questions. Maybe these are from before. All right.
(1:04:40) Um it is built around rest APIs um allowing you to submit the runs um or the models or from any uh library or language. It is modular. So as you can see there are different parts to it. So you can use them all together to make it uh very holistic or if you choose you can use um a few of them.
(1:05:02) Uh it's reasonably easy to get started with and it runs exactly the same way whether you're on a local system or you're on a cloud platform. It's about the same. Uh it's easy for a single developer to use locally but also uh um is very effective in very large uh teams because in these cases a chain is going to be only as strong as its weakest link.
(1:05:22) The ecosystem first um it's got um um the entry point is a shell script. So there's no bias to the language runtime. It makes it easy to run on any of those uh frameworks that we talked about and it's open source and open interface. So it will work with any library algorithm develop uh deployment tool and language.
(1:05:41) But that said there is a list uh so if you are in a very very esoteric setup then maybe it may not work but in most of those common ones it does. And because it's an open uh source project you can of course see the documentation uh the open source version of it just like we saw delta.io. io there is the mlflow.org as well and you'll see all the documentation there.
(1:06:02) Tracking is the most popular one and this is where um data scientists gravitate towards um very quickly because uh the bulk of the time is spent here in experimenting running running running running running different algorithms different uh parameters and then forgetting what they did.
(1:06:22) So here there's no forgetting tracking server is running ex explicitly listening to you and then recording uh what you are doing projects is the packaging part of it so that you can take that reproducible run to any compute platform. So what is the packaging? You might have introduced five other libraries and you forgot about it.
(1:06:39) So another environment you can't do it because you do not know what your requirements txt is supposed to be and what's the environment to create it. models is um you can think of I think we mentioned about it maybe last time as well that the creation of the model should be slightly orthogonal from the use of the model you might have created using Python or somebody could consume it using deep learning or vice versa or from a single model you can have different flavors of the model so your stakeholders who might be very obsessed with using only pyunk version of it can
(1:07:12) use it even though your model itself is like a deep learning you can create a a flavor of the model and registry is where um it's like your u git repository that's the source of truth of your model. It's that centralized and collaborative uh uh place where people can discover models, use it and then when they are they they may have done a thousand experiments but the uh best one of them is going to go into the model registry but that doesn't mean that's set in stone. uh maybe in three months time or six months time it might uh get
(1:07:45) stale and so it might go through a version it might go from v1 to v2 and the other uh transition it's happening is you move from a dev environment to a stage environment to a production environment um and somebody has to keep track of what is happening uh all around this maybe I should have referred to this better this is like all the different frameworks here and you can see that these are all the different um um u you know serving uh uh frameworks and they can be very different. You can build with this and you can use with this. So here you have inline code, you can have dockers,
(1:08:20) containers, batch streaming, um you know tensorflow serving and so on. Um so that's the model. A model can have multiple flavors. The tracking is where you have done all your experimentation. So it's referring to a particular set of parameters, metrics, artifacts, metadata and models. Actually let's spend a little more time here.
(1:08:44) When you are creating a model um you have given it so your parameters are essentially your input right you've given it some parameters and your metrics are basically your evaluation um artifacts. So your RMSSE and your uh accuracy and so on. The artifacts is the model itself and any other um uh data or images or whatever that you have packaged along with it. Maybe there are some graphs that you have, some plots that you may have done.
(1:09:12) Um, and then the metadata could be like additional information as to who did it, how long it took, um, who approved it or what was the data set that was used for it and so on. And the model registry um, you have these versions of that model. So model will be identified by a name as well by a unique path a unique ID as well and it will go through different versions um to make sure that it is uh tested properly and then once it's good it will be promoted to a higher environment.
(1:09:42) So you see typically this is done by data scientists and this might be done by deployment engineers or ML engineers and they're slightly different nomclature in different organizations. Um and this is going to simplify it a little more. Uh this is the tracking uh server and this is the registry.
(1:10:06) So with MLflow you can start a run and start logging it. So you can log parameters, you can log metrics, uh log tags, the artifacts or you could use auto log in which by default all the parameters, all the outputs that you care about is logged. If there is anything additional you wish to log like fu is equal to bar then you can pick one of them.
(1:10:30) Of course, people would gravitate towards autolog because, you know, you might forget to log a parameter, but if there's an additional plot, an additional um artifact, then then you would uh use one of this to lock it. Um what does it look like? So, you have an experiment. So, this is your hypothesis. You're doing your experiment. You're running it multiple times.
(1:10:50) So, each one is a run and that is going to go into your tracking server and you can see it via the UI. the UI you can see each one of your runs with all the uh values but you could refer to it using your CLI and your rest commands as well. So this is an example with MLflow start run. I'm going to log I used these two parameters and with these two values either auto log or directly log these two and then comes the registry which is where your um your most uh uh curated or your best model that you found from your various experiments got logged. So you can say create a registered model not just a
(1:11:29) logged model but a registered model of that model you can create a version of the model. The first time it's uh registered it gets an automatic one but subsequently you'll have to um version it. Sometimes you may have to transition the model version stage uh that now the model has made it into the production stage and you can't remember.
(1:11:53) So of course you want to be able to list all the registered models or see it through the UI. Uh this is the tracking server commands when you are with regards to the uh registry you will again use the MLflow client and um from the client you'll say create the registered model uh and then you give it a a model. So now your trained model has made its way into a registered model from which again it's going I'm sorry about that.
(1:12:31) What happened? It looks like the screen sharing stopped for some reason. Okay. Sorry. Now, now we can see it. Now let's talk a little bit about model and data drift. Um models will get stale over time. That's just the nature of it. It's not because you made a bad model. It's because that is how things deteriorate.
(1:13:01) Uh you know your data also uh if not um properly um monitored uh is not going to give you the same results that you were hoping at the beginning. So our job is to detect this inflection. There will be some variations up and down. But our job is to detect this degradation and retrain it before it falls this far.
(1:13:23) So if you have good uh observability then you would be able to catch this, retrain it, catch this, retrain it and so you you avoid this dip. This is this is bad. You're not getting any value for all that labor that you have put in. Um you have basic summary statistics of the features. that's your data and of the target value.
(1:13:50) So you are predicting something, you are classifying something, whatever is the particular problem that you are trying to solve. Um you will notice data drift even before you can uh see your model drift and that will be an indication that uh maybe the model is getting straight. But that's that said, there are different kinds of drift. For instance, you're going to have this feature drift which is essentially a a data drift and you need to investigate how you can regenerate some features. Uh maybe you could retrain with new data. There'll be label drift.
(1:14:21) So label is basically your um uh the target column and uh maybe you have used some folks somememes to label it but they have not done a good job or maybe over time the characteristics of the data have changed. So you have to investigate the label generation process because that's your ground truth on which some of your data and your modeling techniques are based on.
(1:14:44) And if this is not uh sound then your model will not be you may need to retrain with new data. Then there is prediction um drift. Maybe um the the business impact of the change um has to be understood to see why um those results are not um uh intuitive or maybe it's not um valid anymore. And then there is concept drift. Concept drift is the hardest and it's not as frequent but it does happen.
(1:15:12) So for instance imagine during co um if I were to look at foot traffic in a store to determine how many people are coming and what kind of coupons to give like that has come to a dead still. So the concept of shopping uh shifted completely from the brickandmortar stores to online stores but they were that was for a period of time and that data that was collected was valuable for certain uh again certain um months or certain duration but after that that was thrown away because covid stopped and we got back to some normal patterns but some lingering um lingering patterns remained. So that is concept drift in which we have to investigate additional
(1:15:49) factors, environmental factors especially which is affecting feature engineering that we were not considering before and so this is where alternative approaches have to be done and newer data new data types have to be introduced to understand. So there are some good uh webinars and talks that I have listed here.
(1:16:11) Oh my god, I'm sorry. Um how does ML flow fit into your architecture? So that's your uh batch data, your streaming data. We talked about bronze, silver, gold. So where does MLflow fit in? MLflow typically fits into this section of the uh silver layer. Sometimes gold data could also be used, but it is the silver layer which then uh starts to um be fed into like data scientists will start to play with it and feed it into their uh models.
(1:16:44) Um there are lots of other smaller things that uh we could consider here but that's the key takeaway of this uh slide and uh with that I hope you're still reading your chapters. Uh this time I want you to focus on 7 8 and 9 and um there are links to the labs directly here. Uh we look at an XG boost.
(1:17:06) Um and one of the things uh to do is you'll get it directly from the website but I want you to replace main.default default with your catalog name. So use the CSCIE103 catalog is something you've used for a while. So we know that exists in your environment. Just use that. Um and for the second one, there's a place where the catalog name is explicitly uh written as main.
(1:17:31) So just replace um um that with CSCIE catalog. And uh you it was using spark trials. So um you have to change it into hyper opt local hyperop trials uh to be able to use it in the uh free edition with serverless that we have. Um now this uses optuna I think as the um hyperparameter uh library and here this one uses hyperopt hyperopt used to be very popular earlier but now of course there are other options.
(1:18:04) So with that let's go and see on on did you also want to reduce the number of iterations for the the library or that would also be good there's one place where there are number of iterations we'll look at it and we'll we'll reduce that too um I don't recall it now but uh this is excellent documentation and um this is also another way in which you can import uh data so the link that you see there is essentially this link You can copy it, take it into your environment and you see these three dots here. You can say import and instead of using file use URL and uh plug in that
(1:18:44) URL. So with that you'll get something like this. I'll wait for a second for you guys to do it. Can you share the URL in the chat, please? Sure. Yeah, let's let me do that. Okay. So if you're ready then this is the MLflow tutorial for XG boost and it
(1:19:54) tries to do the full life cycle of experimentation, training, tuning, registration, evaluation, deployment. There's a lot and um first we will generate the data and visualize it uh train and log the models register the models. So this is where the MLflow tracking server comes in and this is where the uh MLflow registry comes in. Then we load and evaluate the uh models.
(1:20:20) So we once the model is available you should be able to place it somewhere and point some data to it to make the predictions. Um there is a pip install here where you are um uh putting mlflow right now I think we are at uh 35 uh so uh it will um it will get you mlflow libraries xg boost as well as optuna optuna is that um hyperparameter um tuning library and this might require you to restart it uh I ran it about 2 days back.
(1:21:00) So these are very standard uh numpy, pandas, mattplot lib, sklearn type of stuff. So do it. What is important here is you are importing mlflow and you are also saying that from mlflow models I want the ability to infer the signature.
(1:21:20) This will be necessary because when you have a model and somebody else is looking at it, they didn't build it. So they need to know how to call the model. So what's the signature? What's the input you need to give to the model to be able to call it? Um you when you you when you are on data bricks you can say set registry URI as datab bricks UC and that configures the model registry to be unity catalog maybe if you're on your local machine or you're on some other environment uh you might um you might point to a different uh registry.
(1:21:52) Now this is how we are creating some synthetic regression data set. um you it has these arcs. So it says this is the number of samples, these are the number of features, a seed, a noise level and nonlinear. This is very very boilerplate code. Don't look too much into it. But it's important to be able to create synthetic data like this to be able to play around. Um you randomize with the seed.
(1:22:17) um you specify um uh what columns um feature columns that you want and then um at the end of this you're going to have um your data frame. Right? So that's that's just a little function that has been created because we'll use it again and again just for some synthetic data generation. Now there is some uh EDA visualizations.
(1:22:42) So you've got uh different types of plots um for your feature. So this one is going to um plot the feature distribution itself. Um and sometimes if the features themselves are highly highly correlated, the model is not going to be very good. Uh and that is the reason why you need to understand the univariate and multivariate um analysis of your features.
(1:23:06) Then you want to plot the correlation heat map. You want to plot the feature target relationships. um and you want to plot some pair-wise relation. There's just so many ways in which you can do um analysis on the data. There's box plots, um outliers, you want to maybe study only the outliers. So there's a bunch of them.
(1:23:27) Again, extremely boilerplate code. You should be able to use this anywhere. Now comes the standard modeling uh workflow. So you have used um XG boost um to configure some kind of a regressor. Um maybe this is where Eric was saying that perhaps you can uh reduce uh no maybe not here.
(1:23:53) Um you have 100 estimators that's a learning rate. Yeah, actually you could uh change it here. And your evaluation metric is root mean square uh error. Um max depth is six. And this is your standard way of train test split uh by your test sizes 20%, you produced a random state and then uh remember we talked about transformations and we talked about estimators. So whenever you see a fit that's where learning happens.
(1:24:19) Now this uh registered model it shouldn't use say I shouldn't say registered model. This model, this exting that this is the evaluation set and with verbus false of course you will missing out on some uh noise here but to keep things simple. Uh now you get um you you have your model. Once you have your model what do you do? You say predict on your test data set to get your actual predictions and then you say okay show me a box plot of what it looks like.
(1:24:58) So it has gone through done all of this and I'm not showing you all the images of all those various uh plots but you can just do display on each one of them to be able to see because it returns a data frame. So an example of looking at a feature um zero by a target uh range to see okay maybe low, medium, high um you might decide that maybe there's too much of skew in the data or okay it's reasonably balanced and so on.
(1:25:25) um lock the model using MLflow. So those are all your data science things and people will do it slightly differently depending on your style and your preference and so on. But now comes the part where we are standardizing that's the ML flow. Remember we saw MLflow start run.
(1:25:47) So here we are saying that um extract some of the metrics your your validation um RMSSE metrics from it. uh extract the parameters for logging and infer the signature from that function that we had um uh used before and then log parameters from all the feature map. So all of these were uh features that were put into the model. log them.
(1:26:07) Uh that's this is like a whole dictionary of features that got logged. And now again you say mlflow with the xg boost style. We are logging the model and regge was that xg boost model that we looked at earlier. We gave it a name. We gave it an input example so that somebody looking at our model knows how to use it. We gave it the signature. And this is the place.
(1:26:34) Remember if you have done a global replace this was earlier main.default. Now it's called CSCI103 catalog. Uh something that you have access to. So that it's not going to bark at you and say oh I can't find the catalog. It's going to log it. Then you're going to say that was my training RMSSE. This is my test RMSSE. And in this case, there are all those figures that you got.
(1:26:59) Um, all those plots that we were looking at, the outlier plot, the residual plot, they are also assets that might be helpful to someone. And you don't log it as params or metrics, you log them as log figures. And these are PNG. So that should give you an idea that anything you want, you can also log alongside it if it will help the consumer of the model.
(1:27:23) uh you might want to plot some uh feature uh importance. So there is yet another diagram called the feature importance PNG that is also logged as a figure. Now you're going to evaluate um and when you're evaluating you you say what your model is, what the evaluation data is, what's the target column. So label was your target column.
(1:27:48) your model type is a regressor and these are again uh standard stuff with that you would log that that's the model URI and these were the RMSC's so the actual values of this is less relevant but the fact that you are getting this URI as a unique um entity uh a unique ID or unique URL is important what would be interesting at this point is if you are running it then you can see this beaker like symbol on your right hand side panel.
(1:28:21) So click on it and you will see that this is your experiment and these are your individual runs. Um you would be able to uh pop this out as well. So if I want to view this particular run, I can just click on this um and go into a run um and in this um I say that um this was this was my best RMSSE. Um this was the um some additional um errors or mean absolute barrels.
(1:28:52) These were some of the parameters. So max depth uh number of estimators some samples um log models uh you get the idea. So when was it run, who ran it, uh did it finish, how long did it take, what was the source um of the code. So it will take me um you are going from writing code slowly into a a slightly ambiguous area where you know traceability is hard.
(1:29:24) But isn't this beautiful that you've left the breadcrumbs so that as you're progressing along when you have created the model itself it knows what was the code that you used to create it and that is what reproducibility is all about. You can have additional tags what was the data set that was used um and so on. So let's go back now. Uh there's a top level experiment. You can go and look at all the experiments and all the runs will be there.
(1:29:47) So you can compare the runs or you can go into the individual runs. And here itself you can see some details but you can see it better on the full view of the screen. Now um people don't run just once one model they will run several iterations of it which is where hyperparameter tuning uh comes into play and you can automate it using tools like or libraries like optuna and you can have nested mlflow runs also.
(1:30:14) So one experiment has got runs but you could have like even each one of those runs to be an experiment and that can be nested as well. So these are all the steps. We'll probably run out of time and I may not be able to cover it all but again you can read it at your uh own time but the important thing is you have installed the library in the beginning when you did the BIP install with MLflow and Optuna and UV and so on. Now you are importing that library.
(1:30:41) um all these are again standard stuff you you need ML flow uh and you are generating training and validation uh data so when you are doing um hyperparameter tuning obviously you're not running a single one as I said you're you're scanning a certain space and saying what happens when this combination works what happens when this combination is given to it and that's um that's what we refer to as the space and you have an objective function which defines that search space uh within which you are going to experiment and the amount of um um uh iterations
(1:31:18) that it's going to do. So in this case here it's suggesting that for max depth I want you to use these parameters 3 to 10 for the number of estimators 50 500 um and so on subsample tree method objective and your evaluation metric is RMSSE. So that's your um set of parameters. Now before you start to run each one of them through the hyperparameter tuning library called optuna you wrapper it around mlflow start run and nested is equal to true.
(1:31:57) You log those params and you again as usual you have your regressor you fit it you predict uh and then you uh find out what those metrics are and you log those metrics you do it for each combination. So these are all the different combinations that it's going to run which is why you see so many of these logged here and then um at the end uh you would have said what is the particular parameter that you care about remember how Daniel said accuracy and we talked about epon score.
(1:32:28) So different use cases will have a importance on a different uh metric that is relevant to you for that use case. So you will say that I care about it and then you say find me the best model of all these runs that you did. Um so in the parent run you save the best iteration from the hyperparameter um uh tuning exercise. Now in Optuna you create this study and your objective is of course to minimize your errors and you're optimizing that objective functions and this is the number of trials is 50.
(1:32:57) If you want to finish sooner you could reduce uh this number of trials to maybe 20 just so that you get a sense because I'm conscious that we are working in a free edition. So you have limited compute but trust me this will run. Um you find in that study uh it might have done uh 50 trials but you tell it based on the metrics that is important to me you give me what is my best trial and from there uh you pick up what is the best model and that's the model that you're going to take into the registry and log it register it. Uh so
(1:33:29) in MLflow again you're logging the metrics logging the parameters inferring the signature same standard boilerplate stuff. uh you log the model uh you evaluate the model you get all your distribution um and correlation and scatter plots everything that you care about and you log all the figures as well uh in this case uh you are going to um get the feature importance as well and you're going to log that um each one of them is here you will get the links to all the versions of the model that it has uh uh created so you can see here created version one of the model uh you
(1:34:05) can click into it and go straight into the registry. I'm not sure how many of you have noticed man how many of you have noticed that this portion is for your common uh parameters like everybody uh starts to use it. This portion was for when we did the BI section, the business intelligence which is all about your SQL persona, your editor, your queries, your dashboards and even genie and so on.
(1:34:41) Then the data engineering one was all about your um runs your um orchestration and your data ingestion. But now we have moved into the Aim ML part. So you have the feature engineering portion here. The tracking is this experimentation phase when you are going through multiple. So if I click on this, let's actually go here and show you.
(1:35:02) So if I just click on this, there are two experiments that lie here and let's click into any one of them here. Maybe one of them was with uh N2N more mlflow. That is what we were seeing when we coming through the notebook as well. These are the two experiments and inside each experiment you will have these log details that we were looking at. This was feature engineering and this is the model registry.
(1:35:25) So um the XG boost regression model at some point was registered here. So I can go in look into the registry and guess what this is registered under UC. So which is why you see that um uh in CSCIE103 catalog under default schema I have this model and in this model you see that there is version one um that's the you know um the latest version of it you can add tags aliases and so on so forth this was the time when it was created I can go into version one and look exactly at the same set of parameters I gave it so now you are approaching the problem from from the right to the left. When you're
(1:36:09) building, you're going from left to right, but now it's out there. And when you have to debug, when you have to retrace, when you have to, you know, show your auditor some proof, then you're going backwards. Uh you can see the lineage uh here. You can see the artifacts, you can see the traces and all the other metadata that was used is also captured uh right here.
(1:36:36) Uh so I hope that gives you a uh idea of how unifying this whole thing is through Unity catalog that you have your data there, you have your feature engineering tables there, you have your models there, you have your dashboards there and they all have the lineage which means you can go back and see how they were created.
(1:36:54) Um it's important to assign a human readable alias. So this is where we are using our API. So we said that MLflow import MLflow client create a client handle and then set the registered model alias um the best model that you had of this um and then once that is set that is what you are seeing uh let's go a little further that's your now once it goes into the registry you refer to it with the models colon so that's the name of your model with an at best.
(1:37:29) So you you can't remember whether it is your version one or version 20. So if you say that it's the at best or the latest or something like that, it becomes easier. Um you can refer to that path get the model URI and you uh use MLflow models predict to say that using this model URI using this new data that has come in um I want you to uh give me my prediction result. So this is just an environment manager.
(1:38:02) Uh that was the UV library also that we got in and so of course this is not human understandable but eventually you'll pick up the prediction column and decide what you have what you want to do with it. Load the register model and make predictions. Now you want to see should I use pyunk or should I use native uh xg boost. So uh in MLflow uh you can if you want Python because it's very easy to work with it.
(1:38:29) You can also load the model by saying uh model URI is equal to this. Now from the registry you have gotten the loaded model which is actually the most common way in which you would access it. The person who's developing the models is responsible for taking it from the tracking server.
(1:38:46) You saw the way we were accessing the tracking server is very different from the way we are accessing the model registry and from here you can say predict um batch inference batch prediction using spark udf and mlflow. This is yet another way. So don't get um too overwhelmed with all the variations that we are showing you. This is just showing the flexibility of um of being able to use this in different uh contexts. In this case I'm going to create a data frame.
(1:39:11) So this is the spark udf way which if you have a lot of data this is like the preferred way of doing it. So you've you've you got um uh xark is basically your data that has been curated you have gotten it um you want to score on it and now from mlflow by funk you use the spark udf and you say that by the way I've just wrapped my registered model through a spark udf so that I can now distribute it to my worker nodes when I get a lot of data I I may not be able to handle it on a single node and then Like before you say with column I'm going to slap in a prediction column and I call this UDF
(1:39:52) with my training columns and what you get here is that additional prediction column and you can display it. So you have all your features. This is um and then you have your prediction column attached to it. And I think that was the last in this particular notebook.
(1:40:13) I'll pause for a minute because there's a lot of stuff we went over. Did you guys uh any of you run it? Yes. Okay. Fantastic. It's it's um uh pretty boilerplate and it looks like a lot of code in the beginning but once you understand what you are looking for it's going to become super easy. I have a question. Mhm. So the graphs so normally when we training models we have some graphs checking evolution of accuracies potentially.
(1:40:46) So where is that stored and um where is that stored? Yeah can I see those ones? Yeah. So here when you go to artifacts um you see I'm in the unity catalog here and we were talking about uh this XG boost uh model so I have um metadata yeah mobile model condu so it should be somewhere here Wait. This is the one. This is the one.
(1:41:51) This is the one. So you see this was the correlation heat map, the box plots, the feature distribution, feature importance, uh target relationships, outlier detection, all those things that you generated as part of it and logged as figures is available as artifacts with your model run. I see. Okay. Okay, now let's go into um a slightly different version.
(1:42:27) So it's some of it will uh look a little repetitive to you but it has some variations and let's go back here and sorry not this here and get the scikitle learn version. So this is the import and let's put that in chat as well so you can follow along. And like before I want you to go into your workspace and use these three dots to say copy or import and in import flip to the URI paste that and import it. That's how you will get this.
(1:43:18) All right. Now instead of using optuna this is using another library hyper opt and you're saving your results and your models in unity catalog as well. Uh so pip install mlflow as before but uh this is like a skinnier version so it's like a little toned down version and then pip install hyperopt restart python um like um like in the other one also we set up the registry URI so you say datab bricks- u so by default the the python client will create the models in the datab bricks workspace and model registry and you to save the models in UC you configure the mlflow client
(1:44:06) um as shown in the following cells. So that's um again while a plate then this is the place where I wanted you to change it from main to CSCIE uh catalog because that's what you have um and your schema name can remain default not a problem. Now these data sets come from datab bricks data sets. There's wine quality.
(1:44:31) Uh there's some white wine and some red wine. Uh and these are CSVs. They are separated by semicolons and they have a header. So by now this look should look very very familiar to you. We are reading these CSVs. Uh and then there's a little bit of um data manipulation that we need to do. Um you know that if the columns um have spaces um then you know you will not be able to work with it. Uh you can't u um load it.
(1:45:00) So look at all the spaces and convert it into underscores. Uh so that's a very simple um column uh name change and define some tables. So again in your catalog and your schema. By the way, you should slowly start getting used to these. Don't hardcode names. Um you know at some point you're going to have configuration files and you will have these parameters.
(1:45:20) You should be able to read it to them. But this is a very handy way of of not having to touch it if some something small happens and you you have all your constants and everything in one place. You have your table names and then um you are going to save the data that you just wrote into two of two tables, a white wine and a red wine. Again, a bunch of um standard uh imports.
(1:45:44) But now this is the interesting part from Hyper Opt you are importing Fmin, TPE, HP, Spark trials, trials, all of this other wonderful stuff. uh load data from uh Unity catalog and uh do some other pre-processing. So uh for white wine obviously is red is equal to zero. For red wine is red is equal to 1.
(1:46:10) You concatenate this as an extra uh table and um uh this becomes your target table. This becomes a label. Um you you have uh some classification label. So you'll say that if the quality is greater than equal to 7 um then whatever like you know some data manipulation that you have to do you split uh data and then um this time you're using auto log uh so all the default parameters and metrics and everything else is defined just with a single line you're able to get it. Now we have started our run.
(1:46:45) So here we are um calling it as gradient uh boost. We have used an sklearn um ensemble gradient boosting classifier. Again this has got nothing to do with what we are this is like standard ML stuff. Don't don't uh get intimidated. Fit remember anytime you see fit you know that's an estimator and there is some learning involved. So with this data your model has learned and now you can do predict on your test.
(1:47:11) Um you can uh do your area under the curve um or because this is a classification example now you're going to log your uh test area under the curve and see how much it is here itself you'll see that mlflow was involved and this is your run this is your experiment experiment is the parent and individual runs are there you can click from here or you can click from this beaker icon or you can go from here right all of these are different ways to get there and that's your test You see now start a new run and assign a run name for a future uh reference. Uh so
(1:47:46) what did it do here? Um uh this is another test run that it has done. Um maybe it's just simulating a new parameter and doing yet another run. So at least we have got a couple of runs to compare against and showing you how to view the runs which we saw. Like if you if you click here, you'll see this popup thing.
(1:48:13) You would be able to click on those uh individual runs through this. Um now loading of the models. Um when you load a model, when you see runs here as opposed to models, you know that you are referencing it from the tracking server, the MLflow tracking server. When you see models here, it has made its way into the registry and you're picking it up from the registry. So you can load a model even from the tracking server by giving it its unique run ID.
(1:48:39) Every time it runs it gets a unique run ID and the model and the name and whatever is this unique ID. So what what from the run you get info and the run ID and that is unique um model loaded you can say predict uh and you have another uh model two from before and you can predict. So you'll get two sets of uh values. So that is to do it manually.
(1:49:06) Now if you were to automate it you have to use a library a tool and in this case we are going to use hyper opt. So hyper opt was very powerful um and we didn't really talk too much about optuna but looks like option is also a good uh library at your disposal.
(1:49:27) You are in a distributed compute environment so you should take full advantage of the fact that multiple um combinations of these parameters can run. Had you been on your desktop, you would have to run each one of them one by one. So serializing means half the day is spent in clicking the button and going and grabbing coffee because it takes some time to go uh finish.
(1:49:45) Now here you said like I've got like 20 nodes at my disposal. Uh parameters you have parameters 1 through 10. Parameter one uh combination this node. Parameter 2 combination this node. So that aspect of distributed compute is helping you save time. Even if it were bad experiment, even if you were to fail, it's better to fail fast than wait the entire day serally going through individual runs just to realize that that whole um intuition you had or the hypothesis you had is probably uh not a good one. So again like before you define your search space um and as you
(1:50:22) are training your uh models you start um with ML flow and um in this case you're you're fitting and you're you have a new concept called trials and this is where you're going to encounter some issue um because um this free edition and uh serverless does not support um certain version of Spark ML.
(1:50:49) So just um just use the assistant to convert it into local hyperopt trials as opposed to spark trials. And then when you run uh you call out to fmin function just like before you had the objective function. Objective function is necessary for it to converge. You you were telling it reduce my rms with all this. How will it compare? How will it know that this one is better than the other? So train the model which is this function that we we had uh uh here. Um now that is the search space.
(1:51:21) Uh this is the algorithm that you're going to use. Maximum evaluations is 32. Again if you want to shorten the time and just look at what the process is. You can bring this down to like say 10 so that it'll go faster. And these are your trials. So in trials don't use spark trials. Just use a local trials like this.
(1:51:40) It still took me a good amount of time and it will take for you as well. But the good thing is it's doing the hard work for you. In next class we'll see that if you didn't use these libraries and if you were to do it using a cross validator, how painful is it and what are all the different uh places where errors can happen.
(1:51:59) Um now you can also search the runs. So imagine you had 100 runs, thousand runs. You're not going to find it. So I'm going to say search the runs order by um my test area under the curve and the start time. So um max results is 10 and give me the best run.
(1:52:23) Print what the AU is what the what was the number of estimators max depth and learning uh rate. So these were my parameters used for my best model. Um and then um with MLflow Pyunk load model we saw that in the other one also. This is one of the ways in which now we are loading it from the registry and that is my best run of run ID because that's my unique ID. So I've I've um I've just loaded it.
(1:52:48) Um now I could also uh create uh using this best model I can predict and get my prediction. So very very repetitive stuff just five different flavors of it. You can save the results and model into the Unity catalog like we saw before. um drop the table, create the data frame, uh save it.
(1:53:08) So this this predictions that you just uh got as a result of this can be saved back. Now that the predictions are saved back, even a BI person can start to look at it and maybe use it in part of his dashboard. Uh somebody else could use it to score it to in their particular use case. Um save the model to Unity catalog. We did this. Uh so that's the model URI and this time we are registering the model in the catalog and uh that's wine model.
(1:53:35) So you see each one of these parameters is logged along with it and that's it. There are just two different ways of um using either hyperopt or um optuna to find out what's the best model. But the way you are accessing it from the registry or the way you are accessing it from the um tracking server uh is very unique because each run is unique.
(1:54:02) I did give you a lot of information today but it's it's pretty pretty easy to digest. So let me know if you have any questions otherwise I'm going to stop sharing and Eigor I think you had a problem with loading data the quiz data into your volume. Uh do you want to go ahead and share your screen? Yes sure yeah thank you.
(1:54:30) Hey Anuta while Lor is doing that I had another question. Yeah. Uh so there was a question in Zoom basically like uh it seems that some groups for the case study uh not all the members are on Slack. So like how does the how is the case study group supposed to essentially assemble themselves if they can't contact all the members. Yeah, that's um that's really bad if they are not on Slack.
(1:54:53) Um it's you know the Harvard has like a a policy of um privacy. No, I think today's my worst day with calls. You're very popular is the problem. No, the first one was my cleaner and she's dying to come and clean my house. The thing is I'm not at home. She can't clean. So, I've been trying to tell her not to.
(1:55:20) Anyway, um what was were you talking about? Yeah, for Slack. Um how many groups is that? Is it like just a one group or two? Uh just one group has reached out so far. So I don't know if it's true of other groups but um you know like I saw like like 50 something something people in Slack and I think there are roughly 50 students. So I think most people are probably on Slack but there may be one or two who aren't.
(1:55:43) Um I think you just let us know because um it's not very easy to track down a person's email. The best way would be to write through uh Canvas. Um like and in the Canvas group you can you can broadcast a message to the rest of the group and they'll they'll get it through their email. There's a broadcast uh functionality. Okay, got it.
(1:56:07) Well, I don't know about that, Eric. Do you want to show them? What I was thinking is just go to email and you choose your class. you can see the uh people like say the person's name is fu you type fufu will come up there you can't see their email address but when you put that they will get notified about it and also let me know um who that person is because obviously um they have not many made any attempts to connect with the rest of the group that's that's a bad sign yeah yeah that um I have the name and they the person who's not on Slack has submitted like both the assignments as
(1:56:44) well. So, they're active. It's just they're not having Slack. I think they just don't know that Slack exists. Yeah. Pro possibly. Yeah. There was also another um person moan who changed uh emails and wanted a Slack. Did that get resolved? Yeah. Yeah. Yeah. I I pinged them and they said they were resolved.
(1:57:02) They only needed access to the drive uh and they were able to figure it out after I pinged them. Yeah. Okay. Okay. Perfect. All right. Oh, yeah. So, we have one more report. So, this is the second report. there's another person who was asked on Zoom. So, another group is missing one person. Two groups are missing one one person. One person.
(1:57:18) Yeah. Sometimes uh one or two students drop after uh a couple of classes. So, we'll have to make sure that the person is still uh actively listed and um uh yeah, I'll follow up on that. I think um you guys also write to the concerned person and CC uh the teaching staff uh so that we are aware of it and if there's no response then we'll have to take some alternate measures.
(1:57:46) Yeah, you're saying right through the canvas email. Email. Yeah. Yes. Yes. Yes. Um that's fine. Um the question was should I share the missing person's name? Yes. Yes. We are trying to help uh the person. It's not uh fingerpointing or anything like that.
(1:58:10) It's pos because this is like many people are remote and if they have not kind of caught up on all the turdus and have not joined slack that's really unfortunate but they will be on canvas because they took this course. So through canvas update them put their name on slack in the in our main CSCI thing so we are aware. Um, and let's give them another day or so to read it and uh respond. Okay. Yeah, I got the second name from Kinichi.
(1:58:37) So, I'll I'll ping you offline on Slack. Okay. Okay. Okay. All right. I Okay. Could you uh are you able to see my screen? Uh yeah. Okay. So this is the uh this is the cell that I I run which creates the volume uh the bike data uh which is right here it's there under quiz one catalog data. So I go here to upload from here like this and then I browse. Mhm.
(1:59:10) And I select the data file trip data and I open it and then upload. Then this is this is what I keep getting every time. Wait, what does it say? It says network error man. Um any chance you're using any VPN or are you in corporate environment like a company is blocking maybe your certain IPs and stuff or do you have a interesting uh firewall? Uh have you tried it on a different computer? not different computers. So, this is my personal computer like no firewalls and anything. It's just a Mac.
(1:59:54) Uh I have used the same thing to upload. You do I see that you do have a ad blocker. I'm just saying like maybe look at all the things that you have that blocks things. I mean I can turn off add blocker. Uh can you try from another place? So click on the new on the top left of the workspace. There's a new. Yeah, new.
(2:00:21) Yeah, click that and go to um there should be add or upload data. The first one. This one? Yeah. Just click that. It's just a different way to do it. So now in the middle panel there's an upload files to volume in the middle. Yeah. One to the right. Yeah. Just click. Yep. Pick that and then search. Uh click on Yeah. Click your catalog name at the bottom here if you know where you want to put it.
(2:00:48) this place. Okay. Awesome. So now try to drag or browse Yeah. to that file. Yeah. Let's try from here. Same error. Yeah, I've never seen this error. It's very interesting. Should that volume be created first somehow? I think he has the volume is there. Yeah. Uploading. Yeah. Then I can upload. And you have done other exercises which have required this. Yes.
(2:01:29) So I have uploaded other files in the past uh and it has worked perfectly fine. And I also get this error sometimes for some reason. I don't know why but if I refresh it'll go away. And uh like if I go to data and like here I have uploaded other files but it just I keep getting this failed to fetch. Yeah you are getting yeah some this is not completely normal.
(2:01:52) There's something going on for sure. Um can you look at your network panel? So if you right click and click inspect and click network here. Yes. and click network on the top. Uh network. Yes. And now click on one of the places to get that error like you're getting the network error. You don't need to upload necessarily.
(2:02:22) Just you know when you click on things you're getting the error. Yeah. Just browse go out of that and come back to the volume like go somewhere else and come back to the catalog. Okay. And see if going to record something. Yeah. So now let's go back. Go back to that some volume that you were trying to get to. Okay. So, you see all these reds? Something's going on. Telemetry. That's fine.
(2:02:46) Uh, can you So, you have you're getting a connection error. And that really makes me think, can you stop all your ad blockers? Yeah, because why are you getting blocked? Like, you should not. Do you have another ad blocker? I see something else too. This is what I paused it right now. Okay. What is the other thing on the two right side? This one? Yeah.
(2:03:16) This is like uh block sites um which I don't use. I block it so I don't go and YouTube or anything, you know, while I'm working. I I understand. I wonder like something is blocking. Try again now. Go go go back and ports to this folder. see if it will continue. But I I would first check everything that's technically block it.
(2:03:55) Um I would personally also for now disable the other blocker too or if you have any other thing I would stop Uh, one one second. Can you go back to that one? Go back to the workspace. Uh, see like sort of like near the top there's a line that says 20158 trip data CSV in the network tab. Like there's lots of reds but one of those lines says 20158. Can you click that? Yeah, this one. Yeah, the one above that.
(2:04:21) The one above that. Okay. And go to like response. Click on the tabs on top. Preview response. Okay. There's nothing here. Yeah, it's there's something going on in your system. I think literally network is blocking you. Is it possible to try it? Um or on um through another uh device or or another laptop or maybe maybe disable that site blocker that you Yeah, all of these we we like it might you might have more things uh Eric. So definitely do that.
(2:05:03) But for a quick test um because this is all uh through URLs access it uh from another machine. Okay. Just resolve the problem first because it does look like your environment is somehow stopping it. Yeah. Interesting. Okay. I did you install something recently also because you were able to do it. Uh maybe not install.
(2:05:32) I mean, these are the exact same settings that I've always had uh in my browser. So, I've not changed anything. I see. So, that's that's why it's interesting. But I can try it on the on a Windows machine that I have. Okay. Tried in that and see if that's let us know because if not uh we will reach out to um the serverless and the free edition teams to take a look. I mean it's a long I'm uh posting a link here on the chat.
(2:06:01) So it's basically uh how you would capture a HAR file. Um so if you capture that HAR file the next time you do this whole thing um just like capture I mean the instructions are there on how you capture it and export as a HAR file and it'll sanitize stuff. It won't send any PII or anything. uh if you send that we can probably uh pass it along to somebody internally who can uh figure out what's going on.
(2:06:26) Okay, sure. Yeah, I appreciate that. And and sometimes just restarting your browser, maybe you've already done that, but restarting the browser actually with different browsers like Edge um and I was getting the exact same error in that one too because and my browser doesn't have these settings.
(2:06:45) So I was like, okay, something else is going on. Not sure what was happening. But I can try it on another machine and and let you guys know if if that's the case. What about incognito mode? Have you tried that one? Uh yeah, I tried on that one, too. Yeah. Okay. Does free accounts expire at some point? Oh, does it? I was just asking. Oh, no, no, no, no. They won't expire.
(2:07:18) Uh, this is for life. It's yours. Okay. So then that's good. But yeah. Uh, sorry I you're not um we are wasting so much of time on this. That's very unfortunate. We'll try to get to the bottom of it, but for now try another machine. Okay. And otherwise uh do write back what what your findings uh on on the channel so we know and uh we'll at least uh escalate the issue.
(2:07:58) Um my my last resort would be try to create uh another free edition with a different u email maybe. Okay. Okay. Hopefully you don't have to because you know you have got certain things here but um these are like know very very messy things. Got it. Um, yeah, I'll give it a try uh right after class on on my Windows machine and see how that goes. Okay.
(2:08:21) If you need if you need another machine, I can also you can use my machine. We can try that way. What? Sorry. If you need another machine, um, I I you I you can use mine. I we can we can connect and we I can try to help you also. Okay. Thanks. Yeah, that's wonderful. Okay, thank you. Yeah. All right. Good night, everyone.