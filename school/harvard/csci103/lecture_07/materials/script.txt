103 day 7 - YouTube
https://www.youtube.com/watch?v=nD6Qc_TdADs

Transcript:
(00:01) All right, welcome everyone. Uh, this is lecture seven of our data engineering course and tonight we'll be talking about operationalizing data pipelines. So um we'll um start with some logistics around the homeworks and assignments and quizzes and we'll um talk about uh introduce uh the concept of operationalizing uh data pipelines and we'll look at requirements and why they're important and then look at go through some guiding principles to help us with our uh designing our pipelines and look at components and validation and um data
(00:49) profiling and governance and scaling and and also talk about um pipeline automation and security and um disaster recovery. And then tonight um Paul will um present the lab and um he'll go over some architecture and um considerations as well as um do a deep f deep dive into delta live tables or DT. Yeah, Eric. So I'm I don't have the lab to go through.
(01:25) What I have is a a present a demo of Lake Flow declarative pipelines. Okay. Yeah. Yeah. Um, okay. So, that'll be that'll be fine. Paul, thank you. And just a note, Anandita is traveling. She's going to be presenting at a um uh uh spatial data conference uh tomorrow in New York City. So, she's on her way there and and will not be um attending uh tonight's lecture.
(02:02) So logistics um so assignment one uh the grades have been uh dispersed and and if you didn't receive a grade for some reason please let us know. And assignment two grading is in progress and should be um completed soon. And then assignment three um is has been made available and you should be uh working on that. And then um the quiz one, there's been a lot of interest in about the quiz. So, we've decided to go ahead and release that this week.
(02:32) And so, you'll be able to um see it. It will be it won't be like a time quiz. You'll have um days or potentially a week to complete it. And um and it'll be a test of your knowledge about what we've been talking about through the course so far. And then also um our first case study will be released this week um and that'll be on data architectures and it'll be a a team project.
(03:04) We'll assign teams to work together on it and um and um we'll we'll also um onto will provide a um a a deck um like as to act work as a template for for that assignment. So, it'll be a team or group group assignment where where you'll work together to um to address the um the case study requirements.
(03:40) Any questions about logistics at all? Yes, I do have one question for the case study. Um do you know how long we will have to comp So, it will be released close to the end of this week and do you know how long you're going to have to complete it? Yeah, at least um two weeks um maybe longer, but the the details will will be um the due date will be included in the assignment and I think um we'll probably release it either um Thursday or Friday this week. Okay.
(04:14) Um also the quiz is looks like quiz is also due on the 20th. Um which is also the deadline for for assignment three. Um in any way that it can be extended a few days. Um you mean the quiz due date? Yeah. Yeah. Yeah. I think so. if you would maybe you can um post a note to um Slack about that and just say is it is it possible to extend quiz one a few days and then um we'll remember to address it.
(04:48) Sure. Appreciate it. Thank you. Uh one question on the case study. We're going to be assigned groups or can we choose groups? How would that work? Um well we'll be assigning groups. It'll basically be um like randomly selected groups using the um group group tool that Canvas provides. And there'll be multiple case studies.
(05:14) So um and and think of this as an opportunity to meet um other students that in the in the program that you may not know yet. So, it's um Harvard is a great um place to network and and um and build friendships that can last a long time. So, use this as an opportunity to get to know other people. Sounds good. Uh about the quiz, how what is the structure of the quiz? How what type of questions or how do we prepare for that quiz? And uh is that going to be open book? How how would that work? Yeah, it basically it's just a set of questions that will ask you to um
(05:54) complete uh most likely a notebook and um and then and then um you'll you'll it'll be open book so you'll be able to um respond to them and over over a course of days versus like an hour or something. Thank you. Oh. Oh, and Morgan or I guess Noah is pointing out that the um the quiz has already been released, so you can you can go and see it now. I didn't realize that.
(06:32) Thank you, Noah. All right. And as always, um the best place to ask questions is on Slack. That way everybody can see that not only the question but also the answer. And generally if you have the question probably other people have it as well. So you're helping not just yourself but the other other students in the class by asking questions and getting answers to them through Slack.
(07:00) If something's personal and you don't want to share it, that's fine too. You can you can reach out directly to Anadita or myself or one of the um TAs. All right. Um, so that's that's um logistics for the assignments and um yeah, I think yeah in terms of the case study um make sure to um you know put put um put effort into it.
(07:41) Um like the group projects are often uh you know um not always like everybody um participates at the same level but to get the most out of it the more the more you put into it yourself the more you'll get out of it. So, um, everybody should work hard on the, um, case studies even though it's a group group project.
(08:03) You know, contribute all that you can and you're likely to get more out of it that way and also the your your team will have a better uh, solution and and um, and probably grade too. All right. So um as mentioned we're we're going to talk about operationalizing data pipelines this evening and and so what is that exactly? So the idea is a data scientist maybe has created a um notebook with some outline and and maybe working a working prototype of of what needs to be done.
(08:49) So to um the idea is okay now um the business has decided yeah that that could be useful let's operationalize that and make it um an an automated data pipeline that we can um use to um produce continuously produce the results and um and provide uh clean um usable data for downstream consumers and where the downstream consumers are generally um business users that um are looking at dashboards, maybe data data scientists um using the the data for um ML models or other other applications.
(09:30) So basically you can think of this idea of operationalizing data pipelines is taking a like a raw notebook something that's been um put together by a data scientist proof of concept and then turning that into a a reliable automated data pipeline that continuously produces data for the business to use. and and the and the reason that the the business would want to use it is um ultimately so that they can use that information to make better dis business decisions.
(10:07) So um before we start though we should have a clear um understanding of the requirements um for for what what what is um desired and so and this I I also teach a software design course and requirements are a critical part of design because before you start designing you need to know what it is that you're um that what what it is that you're building and why.
(10:40) And um and usually the requirements are specified by the business or the c customer that you're um building the solution for. And um and they help get everybody on the same page in terms of expectations about what the system should do as well as how the system performs. And those two questions of of what versus how are um divided up um by in terms of functional requirements that tell you what the system is supposed to do from a business perspective and then um also non-functional requirements about okay um basically how the system should provide those um functional requirements. So, so let's let's look at
(11:26) some of the functional requirements. Um, they might be around business rules like um how the business um uh particular things that the um the application needs to follow certain business rules. Uh uh corrections or adjustments or cancellations to um uh transaction data maybe administrative functions.
(11:58) authentication, authorization levels, audit tracking, external interfaces, like maybe you're supposed to connect to several data sources. Those could be um defined within the requirements and maybe not only um incoming data sources but also where where the data should be provided to and maybe um information about what sort of formats and things certification requirements um reporting and and also uh use of historical data.
(12:32) So these are um these are all examples of functional requirements. They're um basically for a um software application it might talk about like um the the the users and what they expect to do with the system and and and and how the system should respond. Data pipelines are much the same way. And then non-functional requirements as opposed to functional requirements specify how the system is supposed to perform the the functions of the functional requirements.
(13:06) So this gets into um things like performance like the latency or throughput um the scalability, the capacity, the availability, uh reliability, recoverability, um maintainability, uh security, um constraints, um manageability, how easy is the pipeline to manage? environmental um concerns like maybe energy consumption for example uh data integrity and usability and interoperability.
(13:48) So these are all like um not not necessarily or not functional requirements but um how this the system is supposed to perform or or um support those functional requirements on the left side. And these are often um and often forgotten um like the functional requirements are usually more um clear and um to the business and and those are usually better better documented than the non-functional requirements.
(14:21) But the non-functional requirements can be as important as as the functional requirements because if this if the system if if the system doesn't perform according to the expectations of the business then um the the the um the business may feel like the um the the results of the the work that went into producing the data pipeline isn't isn't sufficient.
(14:53) So it's really important for if the the um functional requirements aren't provided to you, you you really need to ask the questions like well what sort of um performance do you expect like what should the is latency of 5 seconds or 10 seconds or or should the latency be in in milliseconds um uh for for the user response time? Those are important.
(15:16) And if you get it wrong and present it to the customer, they may reject it thinking or or you may um like with performance, you may um overdesign it so that it's it's super performant but maybe um uh extremely costly to get that performance and where that performance really isn't needed or required by the business. So um so those those are all scalability u like how much data how many users um availability should it be up um available availability is often um referred to as as the number of nines like five nines is is basically the the system has to be up the entire year but can be down for five minutes with five nines of um 99.999%
(16:08) of availability. So, and that can be really expensive, but it may be important for the business or it may not be important to the business. Um it's important to know whether or not you the system could be down for a day and and that would be all right or or if that would cause a major problem for the business.
(16:28) So um some some things may be less critical and could could support lower availability than um something that's very critical to the business. There's a lot of information about both functional and non-functional requirements, but it's um really important to remember that this is really the starting point before we start designing a um a system.
(16:55) We really need to have a clear understanding of of both the functional and non-functional requirements. So um components of a a a pipeline we have are sources and destinations. So this the sources could be many sources and and many diff various types of sources including uh not not only structured but semi-structured and non-structured data and um and then the destinations where where we expect the the data to flow or be consumed by once uh once the data is is prepared by the pipeline.
(17:40) a graph captures dependencies and and movement of the data. Um, and we'll see um in Paul's presentation later we'll see how um that that graph can be um um specified um in a in a in a graphical interface. The storage is important. Where where is the storage going to be for the data that's managed by the pipeline? Is the um is the is the pipeline using ETL or ELT or or extract transform load or um extract load and transform and often that depends on whether or not that the type of data that's flowing in if it's well
(18:27) structured and the schema is well known then maybe ETL works otherwise ELT is probably better and then uh workflow for controlling the scheduling and orchestration. of the pipeline. And then monitoring is is essential um to make sure that uh the um the the pipeline is working appropriately and that um that we're not suffering um data drift or or other other um problems with the the data that's flowing through the system.
(19:03) Some things that um monitoring can also help with is um providing robustness like making sure that um if something goes wrong maybe it can be retrieded and and keep the keep the pipeline going and also um metrics um for performance data characteristics as well as um key performance indicators.
(19:30) often um you can look at things like uh using business metrics um as an indicator whether or not the um the pipeline is working correctly. For example, if if sales suddenly goes from from $10,000 a day or or $100,000 a day to to 10 $10,000 a day for some reason, then that could be an indicator that maybe the data pipeline is um has some sort of issue where it's not reporting the information correctly.
(20:03) So you can use uh different types of metrics including business metrics to help um ensure the pipeline is working correctly. So um some some um best practices or guiding principles for effective um pipeline curate data and offer trusted data as products. So think think of what you're building as a product.
(20:34) You're offering a a a product or service that um can be consumed by business users or maybe data scientists. So think of um think of like you know basically building and selling of products that that are useful for maybe uh internal users within the business but outside as well. Remove data silos and minimize data movement. We'll look at all of these in more detail.
(21:03) Um, democratize access um to the to the products uh the data products that you're um creating. Uh adopt uh uh organizationwide data governance strategy so that you have a consistent data governance across across the um the the lakehouse in this case. so that it's consistent and that through consistency um may be complete and and and valid.
(21:32) Encourage the use of open interfaces and open formats. We'll talk more about this. And then um build to scale and optimize for performance and costs. you um we'll talk more about this, but it's it's critical that um to maximize the uh return on investment for the work that we're doing in creating these data pipelines is that they're um not only function correctly according to the requirements and non-functional requirements, but also they they function efficiently so that the um they're not super expensive to the
(22:08) business and and and the business feels like they're getting good value for um for their money. Okay, so let's let's drill down into each one of these. So curate data and offer uh trusted data as products. So this this is a a a image that you've seen quite a few times now where we have the source flowing in to the pipeline and then business users consuming.
(22:40) On the other side we have the um multihop architecture or sometimes referred to this as the medallion architecture where you have the raw layer where the the raw data lands from the sources and then a curated layer that can be um that's cleansed, filtered and enriched data that can be used for things like uh uh the data scientist and and building ML models.
(23:10) And then finally the final layer uh which is um business ready data that could be um aggregated um semantic marts uh data marts and cubes. So um which is generally used by the business users and we see that um as we work our way through the pipeline the uh through each hop the uh quality of the data um improves and um with with each hop.
(23:43) So, and the highest quality is the the final layer, the the gold layer. And um with with with the increased quality um the the cost of achieving that also goes up. Any um any questions about uh trusted data as products? Okay. And then the next is remove data silos and minimize data movement. This is um really for efficiency and also um helping avoid uh uh data silos that can become stale and and also um a a source of um maintenance and and concern.
(24:37) So uh moving data around introduces fra fragility um quality concerns latency and also increases cost and um so we want to avoid uh duplicating data um and uh and avoid uh unnecessary operational dependencies on that movement and also uh potential um creation of data silos that may become um issues for having to maintain and keep them in sync later.
(25:11) So um we can to synchronize data we can do um use for example shallow versus deep clones of the data where if you remember a shallow clone only copies the metadata the actual underlying data set remains. Although you can also do um deep clones where it copies both the the uh metadata as well as the actual underlying data.
(25:35) Uh we can uh share share the data sets um directly. We can use federation to access d multiple data sets around the organization. We can also share data through um views as well as materialized views. And remember that materialized views are views that have already um been qu um queried and the data has been cached and available.
(26:06) And so that can that can um help improve uh performance or response time as well as reduce um cost um rather than doing the same query over and over again. And then delta time travel is another way um that we can uh rather than having uh copies of data from like yesterday or the day before, we can use time travel to using the same data set, we can go back in time a day or two or or maybe a week um without having to have um backups of of each day.
(26:43) So, and of course that's helping reduce um the requirement for copies of the data. Okay. Um we want to um democratize access to to the data through um self-service. Uh so um uh making it easy for um business users and and data teams to be able to access the data within the lakehouse. And um so this this allows for datadriven uh decision making by the business um moves away from centralized u um to decentralized decision making.
(27:29) Now people throughout the organization can have access to the the data that they need and they have rights to access and they can um they can look at the data themselves and make um better better decisions rather than having someone from an IT organization or maybe an executive um pushing down looking at the data and pushing down decisions.
(27:55) Now everybody within the organization is empowered with the data the same data to to make um good good business decisions. Um so having having access to the right data along with the right tooling. So um making sure that um users have the the tools the the analytics tools and and um and along with the data to to do what they need to do and um with the right guard rails.
(28:35) So with um guardrails basically mean um governance so that um you know only only the um appropriate people are accessing the data and that the data is um the the security of the data is maintained and also um maybe um the um validity of the data the the like making sure that not not everyone can go in and and change or or delete data. So that leads to that um good data governance strategy uh and organizationwide so that every everyone is um is under um the governance rules that are defined for the um s data system or the lakehouse. So um uh and and governance covers three uh
(29:30) important um parts not not just security or or access control but the data quality um which include constraints to ensure the quality and also um SLAs's on the timeliness of the data um maybe if the data is over a week old then it's it's no longer regarded as valid or are correct and is discarded. Um uh a data catalog um is also a part of governance that helps um identify the available data and um any metadata about it like where where it came from, the quality of the data, the trustworthiness of it and also the lineage of the data.
(30:16) what what transformations has the data gone through um to arrive at where it's at right now? And those are important um facts to know for a business user or maybe even a maybe more so a data data scientist type user that needs to know what they're working with and and how how they can trust the data. Um and then access control.
(30:45) So um governance uh um the third third pillar of the um data governance is around access control and this can come in the form of row and column level access to tables. um masking and anonymization of um data. Um specifically things like um personally identifiable information that may be sensitive to your uh customers and then also um audit audit logs that in that record um who's who's seen who's seeing what what data and and maybe what who's modified what data. So that you can go back and and look and and validate that
(31:34) um you know who's who's using the system and and how and um and how the systems how the data within the system has been modified. Any questions about governance? Eric um professor u I have a questions on the the data lakehouse the layers um would you say that those uh is it this transformation well this movement of data is it a stateful movement uh meaning um if anything happens you know failure happens between any layers is it and we've just restarted it will recover itself or what kind of mech mechanism we
(32:23) should build in to create that stateful. Well, um, we'll look we'll look at some techniques for for doing that um, in in just a few slides. So, I'll just defer to to then, right? Thank you. Sure. Okay. Yeah. And then um the the fifth um principle is is uh use open interfaces and and open um data formats.
(32:55) So this is important for um to provide interoperability with um thirdparty tools as well as um partner tools. So um so the if the data is in a open format you don't have to worry about converting it from some prop proprietary format in a format that they can can use.
(33:24) It's just accessible as it is because the open format is well understood and and accessible to to everyone. It also helps avoid um vendor lock in where you're you you commit to a particular vendor with a particular proprietary format of data and then suddenly um moving away from that um proprietary data format becomes a challenge and um and then and may prevent you from moving to a more open and maybe lower cost um solution in the future because you've got so much invested in in um the data that's in that proprietary format.
(34:02) And then and that can lead to um also using open interfaces and and formats will help reduce your cost of ownership. Um um using lowcost uh cloud storage and um and avoiding expensive um proprietary platforms with with high license cost and and also um maybe um high operating cost as well.
(34:34) Um so having um open interfaces and open formats for example lets you store the data in lowcost um um like a a spark framework where the the data is is writing on lowcost um disk um in the cloud which which are not directly linked to compute uh costs. So um you can you can have lots of data but um do it efficiently and without a lot of expense.
(35:07) So um the cloud storage is is is important and there's different flavors of cloud storage like um on and has mentioned this before where um you can have fast cloud storage that is more expensive and or you can have slower cloud storage that is is um less expensive and and but still the data is accessible just takes a little bit longer.
(35:32) But all of that's possible through um these open open data formats and open interfaces and then um remember to build um to scale and and oper optimize um performance and cost. So um especially with big data systems you have lots of data and maybe you have um lots lots of needs for compute uh you can um you can scale um both vertically and horizontally.
(36:11) So remember that um horizontal scaling means that if you need more compute or more storage, you can you can scale both out by just adding more resources, more compute resources. Vertical scaling refers to compute where um uh you may have a bigger um VM or or um basically computer that can do um do more.
(36:43) But generally um with with big data the um generally the this the scaling is is horizontally where you add add more more compute by adding more more and more lowcost computers versus um increasing the size of your computation but but you you can do both um price performance cost versus performance.
(37:07) So remember um and this goes back to what we were talking about with requirements and non-functional requirements. It's important to know what sort of performance is is expected by your business users. Do they expect that the um do they expect the the the data flowing into the system to be um accessible uh seconds after it's arrived or maybe um maybe hours is okay or maybe even a day is okay.
(37:38) So th those sorts of expectations can be used to um model how you um how you what what sort of performance you want to build into your data pipeline and that that that the higher performance generally indicates or leads to a higher cost. So um knowing knowing the expectations around performance, you you can adjust the um you can adjust the design to support that performance and maybe be able to um improve the performance over time but um at least design it initially to to match the performance expectations of the business. And then
(38:22) storage versus computes. It's very important to separate your storage from your compute and avoid um systems that that bind compute with storage. For example, um I not so long ago used elastic search for a system to do monitoring and um and with large amounts of data.
(38:56) And as the data grew um the with the way that elastics um search works is that um as as the amount of data grows the storage requirements increase and with with the storage requirements you have to also increase the compute um to support that storage. So that's that's an example a counter example where this the compute and storage are bound together where you have more storage you have more compute and it gets very expensive fast.
(39:23) So storage is generally um less much less expensive than compute. So you want to keep the two things separate so that you can expand your storage in an unlimited way and then add compute as you need it to support your um computing resources. And um so that way um that way you you can you can have uh vast um amounts of data in your system and only only use compute against it when you need it and not be paying for it all the time.
(39:56) But again, this gets back to the uh return on investment for the business that they're putting into the data pipeline. Uh they um they want they want um the results according to their requirements. Um but you know they um they're also a business is also concerned about the the bottom line in terms of how much it's costing and and um and of course wanting to reduce that but still still get the results that they need for for making competitive business decisions with the data.
(40:38) So um this this is uh addressing the um question about data validation. So we have um different types of data checks that we can do on the data as it flows through the system. We can check for missing information, incomplete information, um mismatch in in schemas, um where something come something's supposed to come in as a as an integer and it comes in as a float.
(41:10) Um um differing formats or data types and then um user errors when when writing um to data producers. So all of these can can create um issues and um so and things that we can we we can automatically check for um using the tools provided by um by our our platform. So we can um one of the things that we can do is we can um in the in the spark platform we can specify a bad records path that um where if there's any problem with a record that we're processing it can be written to a um the the description of the error
(41:58) can be written to a file within that back bad records path and then then you can go back to it later and look at it and try to understand um what was wrong with it and maybe um fix fix the pipeline so it doesn't um occur in the future. Uh there's different modes of um of um handling um bad data.
(42:28) We can uh drop malformed is basically a way to ignore um corrupted records. it'll just um just won't accept it and we'll we'll exclude it from the incoming data. Um and permissive um is is the other extreme where it it allows uh the data to flow in and be recorded and but it will um like for missing columns or things like that it will um just put a null null value and maybe um with this corrupt record it can also add the add the record to a a column called corrupt so that uh you can easily um analyze the um bad bad records um after after it's been ingested. And this um last last one
(43:15) is just showing how you can set the bad records path to um to some directory where um any any information about um the records that were rejected will be written to with some data about why why they were rejected. and you can use that to go through and and um analyze and and potentially fix um processing problems.
(43:44) So, and this this this can be inserted this um handling of corrupt records can be inserted basically anywhere um along the um the pipeline to help help ensure that um the data as it flows through is actually increasing in quality versus decreasing in quality. Um we can also to improve we can also modify the data to improve the quality.
(44:20) We can um we can drop um for example we can drop um duplicate records. Uh here we're dropping duplicate records of ID or favorite color. Um we can also for for missing values uh we can have different strategies for handling missing values. We can drop records if um if if a column is is missing a value or one or more columns is missing a value.
(44:53) We can just ignore ignore that record. We can um I think Anandita demonstrated this the other day where you can um add a placeholder like a minus one uh to indicate that there was no data in that column and maybe be able to go back and and um look at it and and see see the rows with missing data and maybe fix it later. You can automatically imputee values as the data flows in.
(45:23) If there's no value provided, you can have a best guess for what it should be. Um like in in this example, we're um setting setting if the temperature is missing, we can add um a temperature of 68 degrees. And if the wind is is is missing its value, then we can just set it to say six maybe miles per hour. And um this is a way to to fill in and maybe these are average average these temperatures were derived from averages of of all the data.
(45:59) And so um rather than having a null or zero, we can have something that approximates a real value. And then you can also do more advanced imputing where you use some sort of model, machine model or or maybe some sort of function to um be more um nuanced about um what values it it it um provides for the um to fill in missing missing data.
(46:33) There's also functions to explode, pivot, cube, and roll up um data um that you can use to manipulate the data within the pipeline. So data quality um we should we should have a good understanding of what data quality is. It it's um includes the accuracy of the data. Is the data um correct or valid? Is it is it um or is it correct? Is it consistent? Is the um are relationships of in the data consistent? Um are names and ids consistent? Is the data complete? Are there is there data missing or um or maybe um partial? So completeness is important. The validity of the data um is it is it using the
(47:31) right schema that the timeliness of the data has it is it is it is it basically is the data fresh or is it old and the integrity of the data. Um again getting back to the um the the correctness of the data and then why why is data uh why is quality important and you can think like I always think about like garbage in garbage out.
(48:07) So, um, if if you're if you're making um business decisions based on faulty data, then you're probably going to be making faulty business decisions because your data is incorrect. So reliable decision- making is is critical um and or data quality is critical to um decision making and then analytics and AI same way um some of the early AI models and and probably there still is um some issues around bias around the AI based on the the data that the AI models were trained on. So um you know um with especially with ML models if the if the if the data
(48:58) isn't complete or or um or accurate or um it it could lead to um bad um ML models operational efficiency. Um so if you're if you're having to deal with a lot of issues around bad data then that will slow slow you down and maybe cause your um your um data pipeline to go offline compliance and sec security and then customer trust.
(49:31) So this this goes back to the u reliable decision- making. If if customers use your data, they trust it, they make some decisions based on it, and then discover later that the data was bad and the decisions were costly to the business, they're going to lose trust in in um the the um the the data pipeline or the ability to create um useful data pipelines.
(50:01) So one of the ways and this is just one example that we can improve data quality. We can um for example we may have a table that includes a birth date as a time stamp which is um could be considered um not that useful because really when we're thinking about birth dates we're thinking about the month date and year.
(50:28) So um one thing that we could do is we could take the birth date and add add a um date of birth uh column that um and u basically cast the birth date from a time stamp to a date. So then then you're which would result in the month day and year. So, and that that this is an example of making the data more usable to um to business users. So governance um it's it's um something that we want to have throughout the data pipeline and um and for all for all data not just structured data but semi and unstructured data like videos and and audio and um and for all of our data assets including models, dashboards and
(51:24) services and products. So um think always thinking about who who can access the data and and and what level of access they have and and and to what data. And um governance includes things like access controls, the lineage of the data like what what sorts of transforms or transformations has the data been through, what was the source of the data and and what's happened to the data since it arrived.
(51:57) Uh that would be the lineage, the discoverability of the data. Um like having uh um cataloges that can be used to uh find metadata about the the data and and finding useful data and um data products. Uh monitoring of the data auditing um monitoring is is is um monitoring the the flow the data pipeline. But and then auditing is is analyzing who's accessing the data and and having a record of that and then sharing like making sure that the the data can be um shared uh with the proper access controls.
(52:44) So we can think of data governance as a a system of control over our data pipeline and um including rules and processes and data stewardship and the importance um basically uh in ensuring the the data quality and consistency uh security and compliance operational efficiency um increasing the confidence in the data by the business community and also um and and other data scientists and then also um in terms of streamlining engineering workflows so that we um by having a and and Paul will show examples of this by having a graphical representation of the um of the workflows in in the
(53:42) data pipeline we can um better understand uh what sort what sorts of um what are the sources and destinations and also what's happening in between those two things. So um the concepts of scalability, elasticity, reliability and availability. So um so um scalability and elasticity help um improve availability and performance um with changing demands.
(54:22) So um a system is scalable if it can increase its workload and throughput when additional resources are added. So if if um so um we want to double the capacity of the system in terms of data storage, we should be able to double double the the storage facility uh for the data and and and it should increase um accordingly and that that would indicate that the system is scalable. The same is true for compute.
(54:56) We can um if we need to if um increase the um compute, we should be able to double the compute size and get you know some some uh according um some increase in the um the compute or throughput of the system based on the increased um compute. So elastic elasticity is um uh the ability for the system to adapt to workload changes.
(55:26) So um assuming that it's it's scalable um can the the can the system easily be extended to support um a greater demand and maybe also decreased um for decreased demand as well. But maybe can the system be scaled down to support a lower um throughput of data or processing so that again we're only using the resources that we need.
(56:01) Um we're not um we're not paying for resources that we don't need when we're not using it. But when we do need it, can we scale up easily to use it? And the um the serverless um framework that you guys are using for data bricks is a good example of elasticity where you're you're it may be capped um based on because it's the free addition but um when you're not using it, you're not you're not paying for it.
(56:31) you you you only pay for what you need and and if you do need more resources, it can scale up to a a there's a limit, but it will automatically scale to support your your your needs. So, that's a good example of elasticity through um the serverless framework. Uh reliability is is it is is the behavior correct and um availability is is the service available? Is it up? And um so reliability is is is it is it producing the correct behavior and um correct and in this case correct um data um output and availability is is is a system available for use by by users. and um
(57:20) availability um sometimes it systems are not available all the time. Maybe that's okay. Um that's going back again to our our uh non-functional requirements. Availability is something that we need to understand from the um business requirements like is this does a system need to be available all the time or is it is it okay if it's down maybe at um you know um an hour or so every week for um for maintenance.
(57:57) So u something to to be aware of. And then scaling um if we go back to scalabil scalability we can either um scale up vertical scaling with bigger nodes um and or scale out with horizontally scaling by adding more nodes um and generally smaller smaller nodes um that can be added easily and and generally this can be autoscaled.
(58:24) So as the um needs for more compute increase, you can add more um nodes horizontally and to absorb that demand. And then as the um as the needs reduce then the um the number of nodes being used can be reduced again to to support the lower lower requirements. And then this of course helps with um lowering the cost of the system. Again compute is very expensive uh compared to storage.
(58:59) So we want to we want to um optimize our use use of compute and and the one way to do that is um if you're not using it um you scale it scale the system down to maybe like with serverless you can scale it down to basically zero zero nodes and um you're not paying anything while you're you're not using the system.
(59:30) So just um a few slides leading into Paul's talk. I'll just go through these quickly. Um with LakeFlow um using Lake Flow for a data pipeline um helps you with ingesting um and by allowing you to connect to many different data sources and and and data suppliers. um both open open source as well as um proprietary um data sources.
(1:00:03) uh being able to efficiently transform uh data um clean it, reshape it, join in using declarative pipelines and also um with helping with orchestration um um of um workloads in production using jobs. So the um these platforms including data bricks are continuing to be improved and and um with new new um new capabilities including um new new types of task um new types of um flow control, more um nuanced um ability to have different types of task and and um and how those are done. better scalability here with serverless compute which you guys are using in your um labs and assignments
(1:01:04) with the database um data bricks free edition and then um higher task limits um so maybe um being able to as we were talking about just a minute ago um being able to the elasticity is is better like um being able to increase the amount of compute um and in high demand situations easily. Triggers um for different types of um to trigger your data pipeline through files um table updates and periodic triggers uh cronbased things and then also improved monitoring um of jobs and pipelines and um and better tagging. So the um I guess long story short, the
(1:01:59) the capabilities and and um and functions that are being provided by the data platforms like data bricks are increasing um quickly and um and with with those advancements um it make helps make our jobs as data engineers easier because we can do more with them. And then um this slide is just showing the importance of of having a data intelligence um platform that can easily um share data with our users.
(1:02:36) Again going back to the point about democratizing access to the data to users throughout the organization um through um by by producing high quality uh data products with um correct governance to allow um access by business analysts throughout the organization and quick view of the lakeflow designer.
(1:03:08) which um Paul will will go into more detail on, but you can see here bas basically a graphical interface that allows us to define um different different task um within or jobs within um a data a data pipeline to achieve some sort of goal in terms of the output of the data. Okay, that I'll I'll just stop there and see if there's any questions.
(1:03:49) So I think um automation is the key uh to um to providing uh information to the business users and data scientists. that's um that's timely like being able to um provide it quickly, doing it, taking the human out of the loop so that it's automated and um reliable. And uh and um and also um if things do go wrong that you can catch them and fix them before they uh create a lot of troubles.
(1:04:32) and and you also have uh strategies for dealing with um malformed incoming data from one of your data sources or or or something going wrong with the internal processing, how to how to um how to handle that and address it. So, I think that's a good segue for Paul and his presentation on DT. So, Paul, are you ready? Yeah. Uh yeah, I'm ready. Let me just uh um I'm gonna stop sharing. Yeah. And you need uh just give me permissions to share. You have it.
(1:05:02) All right. So, see here we go. All right. So, everyone should see my screen. Yes, we can. And um so I'm going to start with just a quick little presentation to really um give some context around what declarative pipelines are all about uh in the context of lakeflow and uh why we would build such a thing and then I'll show an actual um simulation.
(1:05:40) It's really a demo an evaluation system that's built off of a real world use case uh for an actual data bricks customer. So the code you'll see in a moment you know reflects in some degree what does really happen in the real world. So let me just go into slideshow mode. So you know when we think about building data pipelines actually the code we write to manipulate the data whether it's to insert it or to do the updates to do the transformations it's actually the simplest part but the reality of actually building a data p
(1:06:17) pipeline that has resiliency that has scalability and that can have all of the attributes in terms of data accuracy and such that Eric just took you through is actually difficult, right? Uh and it can be labor intensive, right? Um pipelines are prone to error, right? Uh you can have bad data.
(1:06:44) So you have to handle these uh non happy path cases all the times, you know? So how do you you could have uh data you're ingesting that all of a sudden violates uh the schema that you're expecting, right? um you you can have situations where you have to not just insert new data but you might be updating new data, you might be deleting new data.
(1:07:03) So over time, you know, data engineers were writing a lot of custom code to handle all of these things. But when it got down to it, um there were a lot of common patterns that emerged for how to handle those. So uh you know with that in mind um data bricks built what was originally called Delta live tables uh so that's Eric as he described it later on it got renamed to lakeflow declarative pipelines and then it actually became part of the spark standard and so you'll hear it often then referred to as spark declarative pipelines because the ability to create these declarative statements that then
(1:07:40) can be fed into a uh ingestion engine uh is part of spark. So uh and the idea is this also helps with the operational complexity right uh you know how do you scale a pipeline right how when you get unexpected periods of more data coming in right you're going to have to handle those kinds of cases and uh how are you then going to write write your code so it can handle both the batch and streaming situation so the like I said the part of the the the code that really does the data manipulation and the transformation and that becomes the
(1:08:18) easiest part. But to create something that's really resilient um took takes a lot of work and so we set out you know data bricks to solve these problems and that's where we created these uh declarative pipelines. So it allows you to simply author with some simple statements. I mean it really can be as simple as one SQL or one Python statement that actually behind it is going to have a lot of power because um the the the declarative pipeline engine will handle all of the optimization that needs to happen. It can recognize is it
(1:08:53) doing batch is it streaming? It can unify them all into one pipeline. So you don't have to worry about writing the what I would call the infrastructure or the operational code. we can just get down to the business of focusing on what does the business need with this data, right? And with becoming it becomes more costefficient in in this mode because um you can have an engine that's just really focused on how to run efficiently, how to determine when it should scale, how to build in the mechanisms to scale up and down, right? how to leverage different types of
(1:09:31) compute from like what we call classic to serverless. So all of that can be handled by the engine and and we can worry about as data engineers uh the part that's important that builds value for the business which is like building out the actual data assets that the business needs to run because that other stuff is going to be commoditized. It's going to be repeated.
(1:09:54) If we decide to do it in one system we're just going to start then copying that across other systems. So let's take these reproducible patterns that are not unique to the business but are just important for operations and let's build them into an engine and that's what declarative pipelines are all about. In the data bricks world it sits in something called lakeflow right and I think Eric had a slide that was similar to this. So you have a set of a way to connect to your source data right and the example that I'm going to show is
(1:10:25) going to be using autoloader which you're you should be familiar with. I think we've had a assignment and we've certainly had a lecture where we're talking about autoloader and structured streaming right so um it's going to be reading data from autoloader and then the declarative pipelines is this declarative set of code that the engine uses to actually run the pipeline and it's going to look if we I'm going to show you some Python code right I'll I'll show you a quick example of what it might look in like in SQL but my actual real example was written in Python it's
(1:10:57) going to look very similar to what you might have written with structured streaming. The difference is uh it it's the same format but it's going to be run um run somewhat differently uh by the engine and and actually from an orchestration perspective what lakeflow is going to do is it's going to look at your whole pipeline right so if we think about a pipeline we're going you know we have some bronze tables we have some silver tables we have some gold tables we might have some manipulation in between those right we might have some
(1:11:26) like data quant uh data uh uh quality checks that we're doing all that stuff, right? Like you know when we're writing just our own code in notebooks maybe in structured streaming we're really just writing it's really just one piece at a time that's running but actually what declarative pipelines can do is look at the whole entire pipeline and really then optimize a plan for writing data into that pipeline.
(1:11:52) Right? So uh you might have parts of the pipeline that can run independently of each other. Right? because you have tables that are just completely isolated, right? So those can actually be run in parallel with other things. It it understands all of the dependencies. So it's looking at the whole entire pipeline.
(1:12:10) It's got observability and data about the pipeline so it can optimize itself and it can know how to scale. So um that's that's really one of the key aspects of it. So programming is simple, right? Uh if we look on the left right here's a quick example of doing a uh a readstream sort of right stream scenario using upsert to delta and in this situation we're doing what we might called uh an upsert right so we might be getting CDC data so we might getting change data that is actually either going to insert a brand new record or it's going to update an existing record or it's might even be deleting a record right and these things that Delta can
(1:12:55) support, right? And the asset transaction capability of Delta can support, right? So, we're writing code that looks it's still pretty simple, but you know, this code in and of itself has got to go into a larger context. It's got to be run inside other code sets that, you know, you it it's not going to necessarily have the logic it needs to know how to scale, right? It's it's um it's just actually the data manipulation pieces in and of itself.
(1:13:27) So when I'm thinking about just taking something that's structured streaming, I have to maybe sit down and think about okay, how how much data am I getting over the course of a day? What are the spikes look like? How do I want my clusters to be sized? You know, do do I want to try and scale dynamically? Do I want to set my cluster just for my worst case scenario, my most volume, and then I've got a lot of slack time, right? what are the downstream uh rights that are happening after this? Right? I I have to think about all that and sort of map that out into a set of workflows that work. Whereas with the declarative pipeline, if you look on the right, it's
(1:14:01) as literally as simple as like, hey, I'm going to be doing some auto CDC data, right? I'm going to get a source file. Um I'm going to have some way of identifying what makes a record unique. Um and I'm going to have a some way of identifying an ordering of the records.
(1:14:18) And I'm going to have some way of telling it, hey, when should I delete, when should I uh and if I if it's as simple as updating all the records or inserting, I don't have to say anything more than that. So with this simple statement uh on the right, declarative pipelines will actually basically be running code that will do all of those operations, all of the upsert operations automatically, knowing all the business logic it needs to break ties.
(1:14:43) If I got two records that are the same primary key, which one I do I want to update if I'm doing like SEDD1 or if I'm doing SEDD 2, am I taking both of them in? It handles all of that complexity that we would normally put into a fully functional productionized pipeline. Uh so it it's simple code uh but it's really just all of the business logic baked in, right? And so when you look at like what a production pipeline might look like and you know you look we've got our source on the left and we've got all our outputs on the right
(1:15:19) all of the stuff in the green dotted line would be something that I would have to code each aspect of right my data quality checks my transformation checks my schema checks what do I want to do if a schema fails like what are my options all of those non-happy path cases right that happen between each layer Right? I have to code all that. But with LakeFlow declarative pipelines, it's going to do all that for me.
(1:15:45) So I'm literally just writing the transformation and data quality checks in a declarative way in a few statements and then all the mechanism needed to actually implement that gets handled by the platform. There's really two major building blocks for a pipeline.
(1:16:05) So there's streaming tables, right? Um there they could just be regular like tables that you're refreshing but you know when we're doing sort of an incremental pipeline we're doing streaming tables uh where you're doing your incremental processing of data you're processing data just once right so data you've already processed it has state so it understands data I've already processed I'm not going to reprocess again and it can do this with low latency and it can also do this scaling so this is really what's handling for the most part uh what's getting me from my bronze into my silver layer, right? My bronze layer, I'm just landing the raw data in my silver layer.
(1:16:40) I'm probably doing the apply changes steps and doing the real like uh CDC. So, I'm getting just the data I need. I would have done all my data cleansing in there, right? Uh and then I've built out uh uh the other the other building blocks is okay, I want to build out a gold layer.
(1:16:58) I want to actually take you know, usually this is where we're doing aggregation and we're building we might be building views, right? We can then materialize those views with a pi with the pipeline as well. So we're getting incremental refreshes of that gold layer table simply by creating a view um that uh can build out that source data and let the engine figure out how to update do all the incremental updating.
(1:17:21) So it can become very efficient to get those updates into like near real time right. So you know streaming tables it you know it's as simple as like you can just write some SQL right. So here's an example where it's just doing a streaming table from autoloader. Here's one where it's reading for Kafka.
(1:17:38) So any of those data sources that you saw in like structured streaming, they're available in like flo in in declarative pipelines, right? All your change data capture, right? Your upserts, right? So ability to automate the process of doing inserts, uh updates and deletes, uh super important, right? um handling of out of order events and I'll show you all this schema evolution talk about in a second right materialized view like I said as simple as creating a creating a materialized view and it will automatically do the refresh so it's a
(1:18:14) statement as simple as that uh in SQL I'll show you an uh this is the SQL version and my demo is just doing um the CDC layer but it's just as simple as uh creating a materialized view like this and and turning it and just uh making it part of your pipeline.
(1:18:37) So it and you don't have to worry about the logic required to incrementalize it, right? It's going to happen automatically. Another important part is how do we want to manage uh schema changes, right? So uh the first thing is we might want to enforce schemas, right? Like we might want to actually stop the pipeline if we don't have the right schema.
(1:18:59) Um we can also have ways of uh evolving the schema. So we don't have to just fail. So we can decide we can tell it to merge schema and we can give it a number of strategies for how we want to deal with that. And this is very uh use case specific. You might decide um when you're evolving when you're when you're writing a pipeline that this is the kind of pipeline where if the schema is wrong from the source data something must be very wrong and I want to stop I don't want any processing it'll maintain its state so I can go back to the source fix the schema or I can go back go to my
(1:19:30) pipeline make changes and then and then restart it but there might be situations where you say I'll I'm okay because I expect my source data to actually give me new information I don't know about but I want to have a way to capture that and then bring it into the source data into the the the target data later.
(1:19:47) So you have these options but it's a huge problem with um pipelines is like what happens when schema changes and you have the kind of pipeline where you really you really want to evolve with it. So that just was the context I want to give and then let me um switch over and show you a little bit of pipeline.
(1:20:16) So just before I do, are there any questions on any of that? All right, I'll keep going. So just to give you an this is a notebook I created to uh just to generate some data. So I wanted to generate a table with like 500 million plus rows so we can show what a real ingestion looks like. And actually we created this for a particular customer so that we could mimic what they were doing in their environment and it would help us troubleshoot and um uh organize you know changes just organize our process of optimizing their pipeline.
(1:20:48) So all this has really done is created some sample data using some basic data generation techniques and it's created this giant set of parquet files out here. uh sitting out in DBFS. So this is my CDC data. This is the data that I want to ingest into my pipeline. And when I'm ingesting, I'm going to be doing both all three types of potential operations.
(1:21:18) So inserts or I could be doing updates or I could be doing deletes. So I have you know 4,200 files sitting out in represents I think uh around 500 uh and some odd million rows. Okay. So that that just to give you an example what that source data looks like. It's very typical. Parquet is a a typical format. JSON is a typical format.
(1:21:43) Uh you could have CSV as a typical format. Uh but it's just reading these files. And so my code and and as Eric was pointing out there there'll be a DAG. So in the editor in data bricks when it knows I'm building a uh lakeflow pipel declarative pipeline it actually can give me this UI where where I can test it out and run it and see what it looks like cuz you actually can't run this notebook as a notebook right like it can't run outside the engine but it can do stuff using uh the engine to like validate or to to actually kick off the job and I can watch it in my notebook but it's pretty simple like um
(1:22:26) uh I'm just creating uh three basic uh objects. So I've got a set of bronze data that's going to be using autoloader. So if you look at my code, right, this is my code for bronze. Okay. Um it's just looks very similar to what I might write if I was doing structured streaming. And but I'm not actually writing a table here.
(1:22:52) This is just defining the operation of the pipeline. The code will be written by the the run by the engine. But you know I'm this is very typical what you might see. So this really two lines of code here. You know you could do this in SQL as well. This could have been a SQL statement. We were writing this specifically in pipeline.
(1:23:15) But what makes it a DT table is this decorator that I've used here. Okay. So that's what actually and I'm importing the DLT library here and because I've given it this decorator right this is what allows the engine to know okay this is what I'm doing to define and I've called the table bronze transactions it can be called whatever you want right so I've named this table this function is I've also named the same but like this is what tells it it's the bronze table it's the decorator in front of that function so this is defining how I want to ingest my um my
(1:23:50) my data. Okay. Uh and pretty simple bronze here. We're just taking the data as is, right? Uh and then I have a view in the middle, right, where we're doing some data cleaning. And really all we're doing here is uh casting some data, right? So I've got a schema spec and I'm I'm really making sure that the data uh is cleaned in this layer before I put it into silver. So we weren't doing too much in this application to get to silver.
(1:24:22) all of the real transformations were going to happen in a set of system views that you know are complex uh you know joins and queries across lots of tables but we needed to make sure the data was cleaned and so um so we're just ensuring that right uh in in this so we get this view that's just getting built right off of uh the bronze transactions and with lakeflow declarative pipelines or spark declarative pipelines as we're now calling it.
(1:24:54) Um, this is just running here in memory. This is actually persisting a table. Okay, that I can go look at and this is in in memory and this is going to be persisted. So, silver then comes in here. So, here I've got my DT view called clean transactions and then this is real where the real like uh apply changes comes in.
(1:25:18) So I've got another streaming table called silver transactions. Uh and it's got a schema. So I wanted to give it a schema that it had to meet. We wanted to be with this application. We wanted to we weren't trying to evolve the schema. If the schema was wrong, we need we want the pipeline to stop. And so the pipeline would fail. We'd understand what the error was.
(1:25:44) We could then fix the source data and then you can restart the pipeline. And that way we're never dealing with cuz the the downstream applications we don't want them to have bad data. Okay. So we create that streaming table and off of that uh we're running the supply changes. So basically we're saying hey I've got a target. Okay that silver transactions table. I've got some source which in this case is the cleaned up version of bronze.
(1:26:11) I've got some key to identify what makes a record unique. And in this case, we had a very complex set like these tables could have like compound keys. So we actually created a hash. So we'd have a single value for each key. We're actually doing that up here, I think, in the bronze. Actually, no, it was put in the source data.
(1:26:29) So, but you could do it there as well. Uh, and then we need to tell it how to sequence the data. So, what if I got two keys that were exactly the same because maybe I'm ingesting data where there were multiple updates. We're saying this is the order. This is how you identify the order in which they exist.
(1:26:51) And then what declarative pipelines engine will do is it will say okay I'm going to take the second one. Uh I mean if I was doing type two it it it um it could apply both but here I just want the latest version the latest value. So and then finally what what what will happen if I actually get an operation that tells it to delete right? So you can have any logic to say hey if this if this expression evaluates to true this means I'm deleting a record and so all of the actual if you think if you're writing this as like SQL or in Python I might be writing three discrete different types of blocks that handle
(1:27:26) each of these different cases plus all sorts of logic to say like well how do I know if I have the right record we don't have to write any of that we just need to give it essentially the right parameters and the declarative pipeline will write that code for us because over time like I said like these patterns were like similar.
(1:27:44) We started coding these things all the same way. So it made sense to make an make a an engine that could actually do all of the redundant tasks if we gave it the right information. So that's all it took to like create this pipeline, right? And to show you what the pipeline looks like when we're running the pipeline, here's an example of a run I did.
(1:28:08) So, and I did this just before um we started. You can see like all the history of all my runs are available. So, I I did a test run. There was no data. I just wanted to see the pipeline run and it ran. So, it ran pretty quickly cuz didn't have anything to do. Okay. Uh but then this was a run that I did after I generated the data and I had 537 million rows to ingest and so um I wanted to run it because one of and and the way I ran this is I did what's called a full refresh.
(1:28:37) So the other thing we have to consider when we're creating a pipeline is I might have to refresh the entire set data set. I might have to go back to the beginning and rerun everything. There are instances where that happens. Or I might for a particular table, we only have one two tables here, but imagine I had a 100 tables in my pipeline and 10 of them needed to be updated. I don't want to rerun the whole entire pipeline. I might want to just say, hey, I want to run those 10 pipelines.
(1:29:02) So also that's another edge case you'd have to code for that lakeflow declarative pipelines automatically handles. So if I was going to do that, I could uh selectively uh update tables, right? Right? I can say, "Hey, select the tables I want to refresh or I can automate this all with like with an API call, right? I don't have to actually run any of this through the UI." And we do that.
(1:29:28) When a customer runs this in production, there's usually like some sort of controller function uh that lets lets kicks off the pipeline into different modes. So, um so I did a full refresh, right? So, I just came in here and said, "Hey, refresh all." And it took, you know, the process it goes through. If we look there's an event log. So uh uh so you can see the first thing it did is okay there was a user action and Pauli said start an update. Okay it records that.
(1:29:55) So the all of this log information is actually getting stored into a delta table and I can see that later on. So when if a pipeline fails you have full observability into this. I can see it all in the UI, but I can also see it later on through these tables, right? And I I also can go back and look at all my historic runs, but I can see, all right, the first thing it's doing is waiting for resources, and then it initializes uh and then it resets everything cuz in this case, it's like doing a full refresh. So, it's basically resetting
(1:30:23) the state, right? I'm going to go back to the beginning. Uh, so I might have ingested the 4,000 files already, but it's going to say that's getting reset and I'm going to reingest everything. And then it starts to create the tables and then it will start to do the run. So I can see everything that this pipeline is doing all along the way.
(1:30:45) Right? I can also look at the query history that it was running. So I can essentially see the two different actual uh queries that were running. And so this you can see it's running this query statement right here. I didn't write this query statement. I didn't write something that looked like this. Um, but this is actually what it's executing and this is the query plan that it has for it.
(1:31:09) So that's what the declarative pipelines is doing automatically. And I can see it ran for 6 minutes and 17 seconds. It uh read two billion rows. Um, and then um it took how many bytes read? How many bytes written? And then it did the silver. Oh, I'm sorry that was the bronze. Okay. So, and then it did the silver. Okay. And then so I can see what each one of these are.
(1:31:40) And if I would click on any one of them, like let me look at the bronze processing, it's going to give me more observable observability into the processing time, the number of bytes read. These are all important um you know, how many records may have gotten filtered out if I was filtering. These are all important things to understand about a pipeline. like when we're building out a pipeline and we're testing a pipeline or monitoring a pipeline, we want to know like how is it when I want to look at performance like optimizing query and pruning files. These are all just performance metrics
(1:32:09) that once you if you understand a lot about how spark uh works and how delta works then then those are very important things to understand. So these are all important information for you to know like when I'm analyzing if my quer is running properly or not.
(1:32:29) Now the good news about declarative pipelines is a lot of that optim optimization isn't automated. So I don't expect to actually have to make too many changes uh over time but I I do do want to see how it's going. So I can look at both queries and how they were running. Uh so this was the bronze and then I could look at the silver. Right? So I I have all of this observability.
(1:32:48) And then if I were to run this again, right? Like let's say I were to run this again uh and just start it up. I'm going to get another instance of this pipeline running. Uh and I don't know how long it'll take to get resources, but actually it should run very quickly because I'm not uh I'm doing a refresh of all the tables, but I'm not doing a full refresh.
(1:33:11) Right? When I did the last one, it did a full refresh. So it's rebuilding the tables from scratch. Right here, it's only going to be looking for new data and then ingesting it. because I haven't actually created any new data. Once it gets through this process, you'll see it'll render the graph and it'll run very quickly because it has nothing new to do.
(1:33:27) And in a production mode, I might often run just in a continuous mode. But there are times if I know I'm just processing data once a day through a declarative pipeline, I can just trigger it. Right? So now you can see it's built the it's built the uh the DAG and I can see it's run had nothing to do. Right?
(1:33:47) So I can see it nothing nothing processed. Right. When it gets to silver, should be the same. Right. So, it should run pretty quickly. And we're done. Right. So, I had no records to process. Right. So, uh if I needed to just reprocess silver, right? I can say, "Hey, I want to select the silver table and I want to refresh that selection and it's going to refresh it.
(1:34:11) " So, there's all different ways to actually um kick off a pipeline. Uh and just to give you an example, when we created our pipeline, um let's see where did I have that? Oh, all files. Go back. Yeah, here it is. So, we actually created a little Python program I can actually use to through the API kick off the pipeline.
(1:34:46) So if I wanted to actually just trigger different ways of this pipeline running, I have a job that can actually do it. So that's it's a typical pattern for running pipelines is to just control how I want to run. So I have all these just different convenient methods for I want to refresh the whole thing. I want to give you a set of tables to refresh.
(1:35:04) So when we're building out pipelines for customers and we want to like it, if I've got a simple pipeline that's got like three or four tables, fine. But like this one has like 400 tables and they're being ingested all throughout the day and if we had to restart the pipeline and just run three tables like we want to be able to do that simply.
(1:35:20) We don't want someone to have to go into the UI and like do it. So or have someone write some code. So we can actually automate all that as well. One other thing I wanted to show that was not in this code the code set I had but I think is pretty important is this whole notion of managing data quality with pipeline.
(1:35:38) So, I'm just going to use the docs for this. But this is also very often something we have to deal with is what do I do when I get bad data? And when we're writing our own pipeline code, you know, we might want to stop a pipeline, we might want to quarantine some data, we might want to take some action to uh change the data if we detect it's wrong and have it continue processing.
(1:36:00) So, there's all different sorts of techniques. Once again, it results in a bunch of code that, you know, we have to custom write for a particular application, but it ends up being the same pattern over and over again just with different business logic. So, you can set expectations, right? Uh, right into the pipeline. So, it looks as simple as this, right? I'm here.
(1:36:19) I'm just creating a a a table and I want to set an expectation that's I'm going to have a a a field in there called valid customer age. And I'm going to be doing a check on it that it's between 0 and 120. God bless the people who are living to 120. And it's that's it. That's all I have to write. And then I have some choices about what I want to do.
(1:36:44) And and you can you can write some complex validation. So you can write these different expectations. It can be pretty complicated logic if you need it to be. It could be a function you call. So there's no real limit, right? It could be as simple as checking something's null or not, right? Uh, but it could be more complicated, right? What do you want to do at that point? Well, there's really like three choices, right? I can say, "Hey, great.
(1:37:08) I know I got bad data, but I want to write it, and I just want to note that the data is bad." So, I I can have a way of tracking bad data. And you can have pipelines where it's like, I'm going to let all the data in, but I know downstream I'm going to filter out the bad data, and that's okay. Like, if I was building an application that was tracking clicks on a website, I'm not I don't need high fidelity, right? I could have records that are wrong or records that have missing data and I don't have a lot of control about the source that's
(1:37:32) providing the click data. So, I'm okay with bad data and I'll I'll I'll I'll deal with it downstream, right? Or I might say, hey, I'm just going to drop it. Like, if it's bad data, I don't even want it in there. It's gone. Like, it gets dropped from that table and then I don't even have to deal with it.
(1:37:50) And the third I might say is, hey, I want to fail, right? So, I want this thing to fail, right? So becau like if I was dealing with highly sensitive financial information and I was missing some key data in there, I might say this needs to fail because I should never not get this field be empty or I should never uh not match this um this condition.
(1:38:10) So you build these expectations right in and then the declutter pipelines will take the appropriate action right. Uh the other thing you can do it's not um I don't think it's shown here. Oh yeah. So here's well what is the notion of quarantining data. Okay. I might say to my main table right I think maybe here it's sort of right let's say I have an expectation and it passes.
(1:38:40) is I say, "Hey, I'm going to keep the record." Right? But if it fails, I have a separate path that's that's got a constraint that says, "Hey, if it passes, keep the record. For everything that fails, I can then decide to put into another table." So my fail flow could be I'm writing those records into a quarantine table.
(1:39:00) So now I can go back, fix those records, and I can actually reinject that into my pipeline as a source into that t that target table. So that's a very common pattern. If I want to keep the pipeline flowing, I want to quarantine the records that are bad because I know I might use them to identify what I need to fix in the source.
(1:39:19) I might fix them in, you know, in another process outside of the pipeline and then reinsert them. But it's a way to keep my pipeline running in situations where I can tolerate a certain amount of failed data um in in my pipeline that I then want to capture later on. So very critical part of pipeline, something that without an engine like this, you're going to write a lot of your own custom code, but it's going to look the same for lots of different applications with these different variations.
(1:39:52) So really, the engine's just essentially allowed you to declare those and it does all the logic that you would automatically have to write yourself. So, so that is what I've got on declarative pipelines, but if there's questions or comments, love to love to open the conversation. Hey Paul, I I had a question about the It looks like it's it's abstracting away a lot of the details, but what if you wanted to um override some of those details that it's generating? Is that possible? When you say detail, like what details? like well some of that underlying code that it's it's generating for you if you
(1:40:39) if you is it possible to go in and and change what it's generated or or No, you're you're just writing the declarations. Okay. Yeah. You can customize by writing functions like you can customize the business logic you know you don't have you can make that as sophisticated as you want but how it decides to go and plan out your pipeline is go is going to be based on the declarative stuff that you write. Okay.
(1:41:12) Are there are there other other questions for Paul about this? Just I mean I'm curious. Does this make sense to folks? Does this seem like a lot to swallow? Like it's you know if you've done pipeline coding before, if you've done structured streaming, uh I think for folks like that it tends to make sense because they've had to write some of these things before.
(1:41:37) But um uh if you're not, it may seem like, you know, something very out of out of the box. All right. With that, I'll I'll turn it back to you, Eric. All right. Well, um, thank you so much, Paul, for that excellent presentation. Uh, are there any um are there any questions for Paul or or or um or anything about the lecture tonight or anything else? We have a question about the homework.
(1:42:29) I mean, I'm not able to really run the the pipeline to make this ingestion on the streams. It's keep keeps getting error. I don't know. We can How can we address this one? Is it something where you can share your screen now and show us or do you have the error it's getting? This is for the third assignment. Yes, for the third assignment. Yes. Yeah, I can share my screen.
(1:42:56) So, let me share my screen right now. That's the one. Okay. Is this one right? So I was running this code as per the assignment right uh but that code is erroring out it's telling me about the checkpoint location. I tried different options for the chairpoint location but I'm still getting an error.
(1:43:26) Um and I was using this this one right here. I also changed it to use whatever was provided here. Hey Daniel, we had answered there was a similar question on Slack. Let me just point out. Okay, thank you. Basically said how you should do this. But basically, you should be using a Unity catalog volume as a checkpoint location and then it'll work like this one. Uh, yep. Yep.
(1:43:54) But it's still getting you just don't pass it. I'm sorry. Is it If it's okay, can I answer? Yeah, sure. Please. So, uh, do you see the dots load? Actually, you put a comma after that and you put the volume there. Um, but how I figured this out is, do you see the diagnose error bottom at the like right after the error? I just click on that and 90% of the time my problem is gone.
(1:44:22) So yeah, I tried that one, but it keeps referring me to the DFS like this and then I get stuck again. Is that volume exist? Like if you go to your Unity catalog, do we see the volume there? I did create it. I did create it. But I think I see I think it was uh No, that was suggesting something.
(1:44:46) I think I see where the uh when I click that one again, I see the the advice that you mentioned about the comments. I will try that again and potentially that will work. Yeah, it is okay. I can share exactly what code works for that section only on the chat. It's okay with me. Yeah, that would be really helpful. Yeah, thank you. Here's the Thank you so much.
(1:45:11) Thank you so much. I appreciate it. I will I will investigate it. This works. Uh just don't forget to I also added this before that. Um, basically you need to delete the checkpoint every single time if you want to read any data otherwise it just doesn't do it. So here is the line that you need to put before this. So I basically delete the checkpoint and here's my checkpoint.
(1:45:41) Yeah, you're you're exactly that's a ringing endorsement for declarative pipelines. It's exactly like if I want to refresh a pipeline and I've done it this way and I've deleted all my data and I'm going to reingest it, I have to make sure I've deleted my state and deleted my checkpoints. That's what like a full refresh all does for me automatically. It will reset that state.
(1:46:00) So absolutely if you're running this multiple times, if you've deleted some data perhaps in the source table and you haven't deleted checkpoints, you'll also run into problems. Um yeah I actually had to delete it before every read action. Uh one more note I share string code it is actually in the reverse order. So we need to first define the checkpoint then delete it then display it. This should work.
(1:46:29) Thank you. Thank you so much. I think I think the big difference there is it's looking for that DBFS colon volume switches to the volumes. It's probably needs um it it can't use the direct you know the Linux style it uses the protocol style right I will try it thank you so much this was very helpful all right okay then Paul thank you for the presentation and unless there's any other questions we can go ahead and wrap up for the evening Uh I did get a question on Slack.
(1:47:12) Um Roan or one of you if you can just contact that that would be for nothing. Uh sorry. Sorry. Was that for me? Yeah. Noah was I think it was Noah was just saying that there's he posted a question on Slack that maybe you can look at. Oh yeah yeah yeah. I I will look at it. Yeah. I'm not at my keyboard right now. All right. Thank you. Yep.
(1:47:39) All right then. All right. Um, thanks everyone for attending and we'll see you tomorrow night for um, section or I'm sorry, Thursday night for section. And um, until then, thank you. Thank you.