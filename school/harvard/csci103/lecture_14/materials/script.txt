(2) 103 day 14 - YouTube
https://www.youtube.com/watch?v=Dx_Ce2VPPLs

Transcript:
(00:03) I'm going to um Daniel Ivor, any questions before we start class? >> Nothing for me. >> Okay. So, next class we would be having a couple of guest lectures from the industry. uh that would help you kind of tie the theoretical or class learnings that you've had with um actual industry um uh you know what's happening in the industry latest and greatest things are changing very fast.
(00:48) So um do make it um priority to attend if you can. Um today we are going to go over some of the newer things that are happening uh within the data bricks uh road map in terms of features which are not fully fully hardened but which are um in public preview or going to be released uh pretty soon. Now this picture should resonate with you.
(01:11) At the bottom we've got uh the cloud storage and we've got protocols like delta and iceberg ensuring that everything is open format and is managed at scale. you have the governance layer and then you've got different categories of um applications. So agent bricks uh which we had seen maybe three classes back is about creating agentic AI using certain frameworks and evaluation u scaffolding that data bricks provides in a very easy manner.
(01:41) So it's almost like AutoML where you point it to the data sets and a lot of the um uh you know the scaffolding and the building happens behind the scenes and you get uh common tasks. So the there are four that are out there and more which would be coming. DBSQL is all around warehousing. Uh lakebase is brand new.
(02:03) We'll take a look at it. Um now it's about a transactional database. If you remember platforms like uh data bricks are more for analytics purposes. Uh so very early on in class we saw the differences between OLTP and OLAP OLAP and OLTP. So one is an analytic platform which is what data bricks is traditionally. Transactional processing is typically the very large scale like the oracles of the worlds or the mainframes and the um you know the SQL servers and so on.
(02:35) um that kind of relational uh databases uh is provided at scale through a Postgress implementation and that is what is referred to as lakebase. We'll look at it a little more closely. AIB we covered when we did DBSQL. Uh it's around business intelligence. Uh lake flow is about um bringing in the data. So the ETL, the streaming and so on.
(03:01) apps is created right on top of all this curated data. Um so any of the services that you have on the platform is securely available. So what that means is if you want access to dashboards, if you want access to models, if you want access to vector search indices or the ability to trigger pipelines, apps is the way to do it very conveniently and very uh in a very um secure way with guardrails.
(03:28) And typically it is for um uh a lot of business users who do not need to know everything that you see on a datab bricks portal. Uh normally and marketplace is to discover other uh data and models and code so that you can start leveraging it you typically from third party and it can be an internal marketplace it can be an external marketplace as well.
(03:54) The key investment areas are around uh data intelligence capabilities um built-in data governance, security and sharing tools to build the agents and then warehousing, data migration and data engineering. They're just like think of them as four main buckets in the data intelligence capabilities. It's all about how the platform can become smarter so that you as the data persona need to do less and less.
(04:23) So anything which can be automated and as part of the basic uh platform uh should just be delegated down. So uh datab bricks one is in public preview. Actually what I'll do is I'll keep a uh window in which I'll show you what I mean by datab bricks one. So this for instance is an example of our um no yeah this is a good example.
(04:54) So when you uh log in you will see a couple of dots at the top of your workspace. Let's just wait for it to come up. So this is where you'll see um uh two or three options like in the free edition that you have. You'll probably not see this lake base because it's in still in beta. It's in early um maturity stages.
(05:19) So you will not see it. Instead what you'll see is lakehouse which is the typical um window or portal that you see with all the workspace detail, the SQL, the data engineering and the AML. Now if you go to data bricks one that is more for your business user and if you switch to that you'll see things like dashboards and genie and um uh lakehouse apps across all the workspaces that are there in the account and the third one is around this lakebased postgress which is going to be the OLTP aspects of it.
(05:53) So behind the scenes it's postris and some of the key features here is that storage and compute are segregated. So you're going to see some things like these. These are existing uh databases u that have been created uh in postris. So let me pick one uh it doesn't matter any one of them like let's say zk demo u by default there is a production branch and there is a development branch.
(06:20) You get to choose what is the version of Postgress. So in this case it happens to be 17. So that's the dashboard. There are branches. There are more settings. You can monitor it. There's a SQL editor. You can create tables. You can backup and restore. All the typical things that you would expect a Postgress database to do is made available here.
(06:41) Now you may need to connect to it. So this is the Postgress SQL link that you have. So you can copy this snippet and that's the connection string and using this uh default role which is basically your your username. You would be able to compute uh connect into this compute. Now in this tables um I don't have permission because this is owned by somebody else.
(07:07) You can create your tables as if you are in Postgress. You can do backup and restore. There are no snapshots that have been created. This is a SQL editor in which you can write your SQL just like you were able to do it as part of the lakehouse. You can do your monitoring as well. So you can see the query performance, the system operations uh and so on.
(07:27) Um but most more importantly it's important to see that um uh by default you've got two branches the production as well as development and these can scale. So as your workload grows they can more CPU can get added to it and when nobody is using it the compute can become zero. Now this is fundamentally very very very different from how we have typically dealt with um um servers of this nature.
(07:56) So when you buy that server that is up and going all the time whether you have one row of data or you have billion rows of data that compute is always running. But in this case the uh the big paradigm shift is that your storage is very different from your compute which is why your storage can still remain as is when you don't need the compute you it can bring down like that was possible when you did your uh lakehouses and your data links but and maybe your data warehouses but not your databases that is never a
(08:31) thing that was not something that we have seen uh you know in any of the other implementations that we have seen so far. So that's big and that is what is going to help it scale as well. Um so we saw what is data bricks one when somebody says AIB deep genie research what does that mean? So let's go back again. Uh no that's not it.
(09:00) Um let's go back. So how would I go? So this is a selector here. I can go to lakehouse. This is data bricks one and this was lakebased postcris. So I can go to lakehouse and back uh back here you would be able to see what is your eibi. So basically it's this dashboard which you would need to do as part of your final project and you could do genie on as part of your dashboard or as a standalone as well.
(09:32) So these two together are referred to as AIB. Uh then comes custom assistant instructions which is in public uh preview. All of you have probably worked with uh the databicks assistant to help you write code to debug code and explain code. Now there are some custom instructions which is going to be give you more uh complete end to end uh um artifacts that you can use and lake base which is the postgress database that we were just uh looking at all of this is considered as the data intelligence capabilities bucket in the data and AI governance we've got
(10:16) Aback so stands for attribute-based access control. Typically what we use is role based access control. Attribute-based access control is slightly different in the sense within those columns you might have some additional governed tags like something could be called PII and then you would make some rules or policies which will say if it has got PII then these are the members who can have access or maybe you need to have some additional logic around it.
(10:53) Um that helps uh with that helps the administrator not have to maintain too many roles like assuming a new data set comes in and it has certain rules and it has got a column with PII information but the administrator never had a chance to see this new data set uh and create some rules on it because there are some attributes which are governed by tags.
(11:16) It automatically follows the same rules. So you see how it simplifies the governance policies and the administrator doesn't even care that a new data set has come in as long as it is properly tagged that same policy the same rule is going to apply to this as well and it's more powerful uh than role-based contextbased ingress control uh data quality monitoring MCP in the marketplace is now in public preview.
(11:44) Now this is important uh because in the uh world of agents and when we talking about marketplace having um notebooks having models having data having MCP servers is also important because you might have some uh some custom uh data in your organization or you might have some code you might have some models and your agents can refer to your MCP servers and your MCP servers are also under the governance of Unity catalog sharing to iceberg clients.
(12:19) Um now we had talked about Delta and iceberg being the two important formats and Delta sharing was possible. Uh but what about iceberg? Like if we have to bridge the gap between all these protocols then iceberg should also be sharable and whether it is uniform or whether it is managed iceberg both of them um produce the delta metadata through which they can be shared outside tools to build AI agents.
(12:48) We've got agent bricks and I think I've shown that to you in a previous uh uh lecture. Let me show it to you again. This is not available in your free edition. But um if you had the full-fledged uh data bricks version then you have got agents and beta when you click on it you have things like information extraction knowledge assistant AIBI genie that's that's different you've got the multi- aent uh supervisor custom LLM or create your own agent these are all examples of um different ways in which you can create it and it's super simple it's
(13:25) almost like autoML so this for example example, you give it a name, you specify the data set location, uh the set of fields that you want to extract out of this data and that's all. You would be able to create uh an agent in a matter of a few minutes. Um the online feature store is in public preview.
(13:47) Again if we go back to the portal uh which is here uh you've got the in the ML category you've got feature and that's the online feature store in which you can um you know earlier it was backed by delta tables now it is going to be backed by that lake base that we just saw AI parse document that's in public preview so using SQL And using this command of AI parse document, you can point it to some PDFs.
(14:23) And if there are structured data, if there are tables, if there is uh information that you want to be able to pull out of that um textual uh data, then it would be able to do so very readily. And I'll show you an example of this. the latest and greatest models from uh the llamas of the world like the metas of the world, the uh Googles and Geminis of the world, the open AI and chat GPGs of the world uh as and when they become available uh sometimes it's hard for you to get into the public previews or get access to GPUs. Whereas on the data bricks uh
(14:59) platform if we click on out here and hit uh models you will see the latest and these are the registered models but let's look at um the default ones which are out there. You can see actually let's just create now where would that be? So is it in experiments? No, it's not in experiments. It should be in models.
(15:29) Oh, maybe it's inserving. Give me one second. Let's wait for this to load. Yeah. So, you see this is the chat GPT uh 5.1. These are the Gemini Gemini models. These are the uh Claude models. These are the um llama models. Everything is available for you to use either on a um in the playground or in batch inference mode as a throttled um provision version or per token basis.
(16:02) Um but many people do not have access to it as quickly as this whereas on optimized GPU serving these are made available for you to uh test out. So that's what it means by our partnership with um all the model creators. Um and uh sometimes for some tasks you might use one over the other um that's for your experimentations and then serverless GPU compute plus the MCP servers is in public preview.
(16:31) You have all been using serverless but CPU compute. So providing GPU gives you additional acceleration for certain use cases and the MCP servers uh that are now available so that you can uh you know you your agents can start to interact with them. The last one is data warehousing, data migration and data engineering.
(16:56) Multi-statement transaction and stored procedures is in public preview. Um again all of us come from relational world. Um we have written store procedures which are not just one statement but several statements and the whole thing should either all go through or never happen like so it should all commit properly or it should roll back.
(17:16) That's what it means by multi-statement transactions and stored procedures. We have written UDS um but stored procedures can give you a little more flexibility in how you write your code. So that's in public preview. low latency improvements, uh, new unified migration UI, LakeFlow connect, um, zero bus ingest.
(17:40) We'll we'll look at it a little later. So, just want you to remember that there are these four categories of data intelligence capabilities, governance, security, and data sharing tools to build AI agents and then data warehousing, data migration. This is all about bringing the data, hydrating your lake and making the data engineering story stronger.
(18:06) Uh you may or may not u really care about this because this is just to let you know uh what stage these are. Uh so there are three main stages. When a new feature is made available, it's in beta stage. It goes to private preview where some customers have access to it. Then it goes to public preview where it gets into our documentation and you have a chance to try it out and then it becomes generally available and a lot of enterprises have a requirement that only when it is generally available can they use it in production because maybe
(18:39) before that they don't feel confident or maybe before that they feel that the rate of adoption of that feature by other customers has not been high enough. So that is why uh when the features are listed it says whether it's in private preview whether it's in public preview whether it is generally available and it's like different companies might use slightly different terms for this but the process the release cycle is the same with regards to data bricks AI uh and agents uh agent bricks uh these features are in uh generally available the online
(19:13) feature store is available AI pars document which is very very powerful ful to throw some documents from volumes and have it just parse out uh the text uh for certain keywords like you might be looking for a company name or you might be looking for a company address. So you specify what you need or maybe you don't even specify that and all of that is going to get uh parsed out and made available from an unstructured into a structured form using a single command which is EI parse document uh and in the context of a SQL statement vector search
(19:48) storage optimized end endpoints and this is for your similarity search uh when you're doing rag models or whether you're doing some semantic search all of that would require a vector storage uh serverless GPU we talked about it managed MCP servers um different uh models all of that is coming now lake base we looked at it um there is branching there's snapshots there's instant point in time recovery time travel uh schedule updates query so this may or may not make as much sense to you so let's uh go and look a little bit
(20:26) about the documentation so uh you can go and look for lakebase. Uh you'll see that it is a beta feature. That means very very early on but I showed you how you use these um uh switchers to go either into a lakehouse or into datab bricks one or into lakebased Postgress. So this is for all your OLAP needs.
(20:51) This is for your business users and this is for your OLTP needs. Uh right in here you can create your first project. you can choose uh what version it is. Um and then by default you'll get two branches, a production branch and a development branch. We showed how you can connect to your database. You get this uh psql connection string that you could use.
(21:15) So you can say psql and that whole thing and you would be able to uh use your oath token right to to connect. uh in the SQL editor you can create a table um give it a like this is exactly how you would uh create a table in any relational database give it a name so this is playing with lake base you run it now once that has been created then you can switch over to your lakehouse default uh create a catalog uh actually this is interesting let's make sure that you know how to do it so If you go to a catalog um and create a brand new one
(22:01) um let's come here to a catalog and here let's say that I want to create a catalog. In this catalog you see this type you've got a standard catalog which is typically what you use for your projects. This is a foreign catalog. If you are looking into um an external uh data store that you want to bring in, this is shared every time you want delta sharing.
(22:34) That's the type of catalog. And then this is the last uh category of using a lakebase Postgress. So if you choose lakebase, you can give this a name. uh here you have autoscaling which is in you know remember how I said the compute and storage is separate and as your load increases your compute can grow when when it is done you can bring it down to zero you can save costs you can select your database instance so it's all the databases that we just created with the default branches you can give the database a name create a database if
(23:09) doesn't exist and hit this so once the catalog has been created and you have connected it to your Postgres database then you would be able to access it by referring to your catalog name and then this is your Postgress database name. So I I this is actually very important. I want you to understand these different types.
(23:35) Standard is what you use typically. Foreign is when the data itself is not in the lakehouse. Your data is somewhere else. It might be in red shift. It might be in snowflake. It might be uh it might be in the terror data and you are just creating a foreign catalog and that's federating the data source.
(23:55) This is shared when you're using delta sharing. Delta sharing is zero copy ETL and the final one is your lakebased Postgress. You can also reverse sync the ETL. So imagine you've got your lakehouse and you've got your lake base. Lakehouse is your lake and your warehouse. Lakebase is your Postgress thing. You would typically sync small portion of your data into your lake base so that it some apps or some other machine learning use cases or some consumption use cases are going to um read from it at very low latency. Now it's possible that some of
(24:33) your business users may make some modifications to Lakebase which can then sync back into the lakehouse. So look at what is happening here. You're creating a table in Unity catalog, right? And now you're going to sync that into um your Postgress. So from the lake, you're pushing a sliver of your data into the lake base and the reverse is also true from the lake base also you can do reverse uh sync into the lakehouse.
(25:09) So select star from default user segment synced where engagement is equal to high. So you're in lake base and you can still see the values. There are lots of concepts here. We would not have time to go over each one of them. So how to connect, how to manage, how to do backup and restore, monitor, all of this is possible as if it was a full-fledged database.
(25:36) And this diagram shows you um how compute is dynamic. So it might get started as your load increases maybe you know it gets even more and then it comes down. You can scale right up to zero. So instant readiness because we have serverless behind the scenes. So subsecond startup time means uh extremely responsive to consumer uh applications um and fast setup for dev and testing.
(26:06) branching and development. So remember how we saw a production branch and how we saw a development branch. So that is necessary because sometimes when you make some changes, you want to know exactly what it looks like before flipping it onto your production. So instant zero copy isolation means that teams can safely build and test without touching production and live workloads.
(26:30) You have git style workflow for data and you can have temporary environments so you can create it very quickly. Recovery and backups. So in any database you always have the concept of um having snapshots in point in time. So schedule snapshots, point in time recovery and safe validation. Um now move the app to where the data and AI leads lives.
(27:01) So you've got your lake in which you've got data, you've got your models. Now the apps are going to move to where it is and it should be very very simple to be able to create this. So let's see where you would go to create your apps. Let's get out of this. Let's go to compute. And in there you've got this apps in your final project.
(27:23) If you guys feel up to it, I would encourage you to try to build an app because it's fairly fairly simple to be able to do so. You can create a new app using either from an existing template or you can have a custom one. You can choose Dash or Flask or Grady or Shiny. These are all Python frameworks. You could use NodeJS.
(27:44) You could even have React if you so choose. And um again in each one of these like for instance I often use streamllet because it's very simple. Uh you could just use have a data app. Um so imagine you have curated your pipeline your medallion pipeline. you have your final stages of gold uh data in u maybe multiple tables and you have got a SQL warehouse which you can use as your compute um to visualize the data um but make it safe and secure and give it a very nice shiny look because nobody's going to come to this portal right they
(28:21) will get intimidated but if it's a nice little app and there are only a few things that they can click or do then you know it's it's easy for your u consumers uh chat bots. Almost every organization will have multiple of these uh chat bots uh using a large language model and model serving. So these are the resources behind the scenes that it typically uses but you can put as many resources as you want.
(28:47) Um we were looking at the Postgress database and that itself could be the base for one of your apps or this is like very very simple hello world just that gives you just the scaffolding after which you decide what to do. So again uh apps are very powerful and uh maybe you should consider uh creating an app uh which can host your um uh chat.
(29:12) It can host your dashboards. You can embed your dashboards in there. And the difference between a dashboard and an app is that in a dashboard it's one way. You're just reading data. Even in Genie, you're just consuming and interrogating the data. But in an app, you actually have the ability to write the data as well. Maybe you look at some column and say, hm, that rate value is not correct.
(29:35) Let me fix it up because I know the data. I know the domain. I'm going to fix it. So you make small changes. So it's not a whole full-fledged ETL pipeline, but small changes to the data. And that birectional communication of being able to read a curated data, but also be able to write small amounts of data is why apps are more powerful than just dashboards.
(30:04) All right. So that's apps and we saw how easy it is and the beauty of this is we have brought these apps to where the data lives. So making it uh very convenient uh to create um and in the upcoming releases you're going to um have um much larger instance sizes which means you can do more powerful things. And there's horizontal scaling.
(30:29) These apps are given to your end customers. Uh maybe they are just your business users or maybe they are end users. So imagine you're a company and you have your own app like uh Facebook has meta. Now Meta is used by millions of people. So imagine how much it needs to scale. So that horizontal scaling um in just in time is very very important and um you know um there's always scope for improvement there like how how little time that you waste how quickly the app becomes available um is is an important consideration app management so what's
(31:06) the health what's the observability the developer experience yes I showed you the UI and there's the APIs but maybe an SDK might be Maybe directly deploying from git might be better. Um, nowadays nobody really codes from scratch. Um, a lot of people use uh vibe coding tools. I'm not sure if uh some of you have experimented but things like claude and cursor and others are there and they are here to stay.
(31:34) So the sooner we get to know how to use them and use them effectively uh to produce better innovations the the better it is for all of us. Uh we showed you data bricks one again that's for the business user. Uh so if you look at the portal we said click on these go to datab bricks one that's that's what it means there.
(32:01) Um you have account level access. We could give a vanity URL. So nobody wants to see a database.com. If you work for company XYZ, then the vanity URL means you'll see XYZ.com. Uh domain integration. Uh so maybe you have a marketing domain, you have a finance domain, you have um like a tech domain, what have you.
(32:24) So you organize your dashboards, your Genie spaces and apps by business domain. So more uh intuitive search and navigation. They imagine if you're a very large organization you've got hundreds of these genie spaces just navigating through the genie spaces would become uh so hard instead if marketing has got five genie spaces and I come from marketing domain I see only those five then it becomes my job much easier but this data bricks one is not by workspace it is at the account level so across all workspaces when you have um multiple
(32:59) genie rooms or apps or dashboards they are all made available in a single place with that vanity URL making it easier for that company to deploy. Uh EIBI we said it has got two parts. One is the dashboards and the other is the genie. Uh so from a dashboard you might want to subscribe to Slack or Team's channel.
(33:23) Why? Because sometimes when there are certain notifications or alerts um maybe a job has failed then you'll get a slack notification and maybe the user is going to go and take a look at the dashboard or take a look at the pipeline. You can subscribe to account level users. Uh there's multi-page subscriptions, CSV attachments, just so many other reporting uh things.
(33:49) And then um bulk filter change per widget, custom ranges, relative range. These are too fine grain. I'm not going to go into it. AIPI uh has got a genie uh re uh has got a research mode. So what do we mean by that? So if we go to genie which is right here. Remember we said that genie is uh to answer questions which are very factual and uh you can give it additional instructions, you can give it SQL expressions and example queries and how join should be done um and all that so that you get very consistent responses.
(34:29) But Genie is going to answer the what questions. Anytime you have why questions, anything which needs to um think a little more and uh uh not guess but uh probably have a hypothesis as to why something is happening. Then you should go into research mode and in in this chat you can just flip it and then it goes into this research agent.
(34:56) And in the research agent you can ask those why questions which typically genie would not um answer. But in again in this research mode you might say uh uh why do you think um um or what what is going to happen if the uh tariffs in this region go up by x amount or price of oil changes or gas changes how is how is that going to affect my portfolio? So these are hypothesis questions which is going to go look at all the ramifications have a chain of thought and it's not going to be a very quick response. It's going to take a
(35:32) couple of minutes, couple of maybe several minutes and then it's going to come and show you the uh the the various uh paths that it took and then from there uh you're going to pick one and say maybe that's the one that I'm going to dig a little deeper. But it's very simple. You either get into a chat mode or you go into a research mode.
(35:53) Uh all right, let's go back here. That's what it means by research mode. enhanced benchmarks, document upload, dynamic knowledge extraction. Even uh in the genie uh that we were talking about here, chat uh you should be able to add uh sample questions or even let me actually create a new one so it'll be easier.
(36:19) Let's go to Genie here. Insurance underwriting. This is chat. For some some reason that um uh clip is not showing up. Ideally that should show up so that I can actually let's see this is just the typical question that is being uh asked but what I was looking for is like a paperclip uh marking which I'm not sure why it's not showing up here.
(36:56) In that paperclip marking, you can upload some CSV file or you can upload some other PDF documents that is then going to get integrated with your structured uh data which is right here. So these are the tables that you have given it. This is all structured but you could bring in some CSV data. Is it >> that little plus button? >> If you if you click the add button on the top right of data, you'll see it.
(37:21) >> Oh, this one. M >> there's a little plus button at the bottom next to the chat button. What does that do? >> This one? >> Yeah. >> No, this one is to get into the research mode. >> Uh so this is like regular genie and this is the research agent genie. I do not know maybe if I create a brand new room it might give me uh I could try it or maybe it's not as important.
(37:49) Let's see. Let's um let me take these two and create. No, it's I I swear I saw it two days back, but I'm not seeing it. It comes as a little clip here. >> I wonder if somebody turned off like a flag in, you know, that's possible. Okay. or maybe I saw it in log food. It's possible that it hasn't come here. Uh and then in data bricks one you've got the account level experience.
(38:26) We were just talking about it as to in a single place across all the workspaces. You might have different assets but they are all bubbling up into this one experience. You've got the vanity URL and then you've got this domain integration. Um yep. So this is the thing but uh uh yeah you can bring in not just your um structured um tabular data but you can bring in some CSV files which is still structured that it's going to be able to extract out of volumes.
(39:01) I could actually go into dog food and see if I can look it up there. So if I go into dog food, this is important because now the distinction between uh structured and unstructured is getting lighter and lighter. So I'm going to look at owner is me. I know I don't have it here. Okay, just go in anywhere here. Yeah, see this one? This is where you can attach a file. It can be a CSV.
(39:35) It can be an XLS or it could be a PDF as well. Um CSV is still structured. XLS is a real Excel file, not just CSV but you know it can have multiple uh sheets. Uh and then PDF is again unstructured. You bring that you have it in volume. Bring that in and combine it with your structure data. I think now I'm I'm beginning to see that it was probably in log food. I may have seen it. Okay.
(40:03) um you ask questions directly of the documents, you can query the structured data inside the PDFs and you can enrich your insights with business context. The next one is drill down for AIBI dashboards. This is also something that customers ask uh quite frequently. You can explore these hierarchies like typically these things were not uh drillable but now you can actually click on it.
(40:30) You can see uh you can go one level down, you can click again another level down. So that's the beginning of the data actually talking to you and you being able to go from say a region into a country into a state into a zone and so on to u get better insights. The poor man's way of doing it would be you have a drop down and then you drop down and and keep dropping down into multiple uh options here to see the same view.
(40:59) But most people don't want to do it. They're used to being able to click on an information and seeing a different granularity. Stay in the context finances faster. But that's that's uh um summarizing the assistant and notebook. So in the assistant you've got the data science agent. This is already in GA. One might ask what's the difference between the regular data bricks assistant and the data science agent.
(41:29) Here you can write a full-fledged uh ML uh workflow and have it executed for you. In uh assistant you kind of know in the context of a single cell and you ask your questions and it gives you some response and then it's up to you to bring it over and then execute it. But a data science agent is almost like an agent bricks. It's an autoML mode.
(41:51) Similarly, you're going to have a data engineering agent right in the heels. This is G. This is beta. You can do uh better asset discovery. You've got MCP support. I'm not sure what next edit uh suggestions are. Um then uh oh th this is again in the context of the um of of the assistant in which it will be able to give you suggestions from multiple portions not just the single cell that you are on on the single item.
(42:25) So it's the next edit suggestion. So it's kind of helping you write it better. uh from the notebooks and SQL editor perspective uh you've got uh DAB and repo scope unit testing many people don't even do unit testing because it's very hard to do it in the data layer uh from a software engineering and logic perspective we can do it but um it being able to do your unit testing for you is pretty nice uh multifile find and replace as opposed to doing it on a single file um just like you have projects and you can drill and
(42:59) say find it in all the places within the project you should be able to do do so uh parameters populated from query results and the SQL editor so that your dashboarding uh efforts are simplified um support for unity catalog tax uh performance and enhancements these are general ones okay this is what I was referring to so if you look at you look carefully at this little snippet then you can See um it is able to autocomplete beyond where the cursor is.
(43:36) Uh surfacing edits anywhere in your active cells. Uh common workflows available in both notebooks and uh SQL editor. That's fine. Just makes our lives easier. Agent MCP support. You can work smarter, stay open and source. So bas basically um your MCP servers where you have a host of other capabilities are now democratized and uh still uh UC is around to govern it and agents can connect to it.
(44:06) So they they are able to get to all the power and all the code that is sitting behind your MCP servers or maybe there is additional data that you are unleashing through it. Lakebridge it's completely free open and AI powered tooling and lake bridge is for data big data migration there are some 20 plus legacy um data stores from which you can um uh you can move from you know older code over to new code and there's got the analyzer part the converter part and the data data validator the fact that this is open means you can take a look at the
(44:46) If you want you can change it as well. Um one end to end UI so makes it easier to see enterprise SQL features. So we talked a little bit about the store procedures uh which will help you streamline migrations. multistatement which was something that we haven't done in the past but uh anybody who has come from database world knows that when you write logic um multiple statement is important to either fully go through or not go through temp tables spatial SQL this is actually big because we didn't support spatial SQL
(45:23) much but now uh we do things like um geometry geography um you know intersection of u points Like when you have like one of those hexagons and you have a point, you want to see whether it falls within the hexagon, how far away is it. Uh so if you have a a calamity for instance, you want to see what are the areas that are affected.
(45:48) All of those things can happen very easily through just plain SQL. Uh that's in public preview. Uh multimodel and document parsing. So we have talked about AI functions. uh and which are very uh specialized and provided out of the box. If you want to write your own, you can use AI query. Uh multimodal multimodal is actually coming not by this particular function but by the uh model that is powering it.
(46:19) So here for instance you're saying select AI query uh filter our catalog for white shoes from blah blah. So you've got a lot of items in the catalog. So first it has to understand what shoes is. Then it has to understand what white shoes is and be able to filter uh just like it works on text it can work on uh other modalities also which means images videos and so on.
(46:47) So let's start with images at least and it's able to do this as as easily as this. uh in document parsing we had talked a lot about AI parse and um you know this is um a function that is very very widely used and I believe there were some benchmarks that were done on this particular function it's extremely performant and uh costoptimized as well uh these are the typical forms that will be sitting in your repository somewhere uh just for regulatory purposes compliance purposes now you can actually extract the value out of them by parsing it, converting it into tabular
(47:26) information and bringing it in along with the rest of your documentation. Uh from an engineering perspective, LakeFlow connect uh basically you that is how you will pull in a lot of data sources. Again, this was an area that data bricks didn't play once the data is already in the cloud. We used to pick it up from there.
(47:49) Now we are supporting a wide range of connectors. So it could be from file like Excel or SFTP um Postgress, MySQL. Uh there might be some uh marketing uh data like uh ads or Salesforce marketing cloud. In fact there is a zerocopy ETL, Tik Tok ads. I have not even I have not taken much look at this. customer 360, financial services, insurance, so on.
(48:18) Uh, autoloader with managed file events, query based connectors, zerobus inest, we'll talk a little more about this. And then the connector templates, um, DT was the old name, then it became u, LTP and now it is called Spark declarative pipelines. It's generally available. So it's completely uh open- sourced and which means that even um the most um optimum uh and innovative ETL enhancements that have been done on the data bricks platform is now made available to the general public.
(48:59) Kafka and Delta Syncs I think this was already there. Um incremental batch APIs lakeflow designer uh I think I can show that to you. So, if you go into uh SQL and there's this little button here uh which if you click is going to put you into this visual mode and from your catalog you can drag and drop uh these um options or you can use an operator.
(49:30) So you can have a source operator, you can have an um a a filter, a join, um an aggregator, an AI functions. All of these are there and if you want if you like, you know, you could drag them or you could just write here what you want and it will be able to do so. Now in this case, that's a join.
(49:50) So you can see it's giving you visually what are the different kinds of joints. So how to use it, uh what is the input on one side, what is the output. So it's extremely intuitive. you can transform the data. It'll tell you exactly what are the filter columns. Um then you might be able to So these are various stages of your pipeline and behind the scenes it's generating uh Spark declarative code that's the designer. Then lakeflow jobs.
(50:20) Um again if you uh remember when we create that end to end um DAG of different nodes and tasks that's the job uh materialized view and streaming tables change owner support is generally available. Maybe that's not as interesting but when you're productionizing stuff it is important. Arbback for exclusive access.
(50:44) um arbback we all know what that is for exclusive access means sometimes I may work in the role of an auditor sometimes I might work in the role of a financial analyst my access needs in both those cases are different and I wear two hats so when I go in as an auditor the information that I see will be very different but I want that entire trace to be uh noted that uh I uh Ananda I'm going in now as the auditor and these are the things I'm suing and these are the things I'm doing.
(51:17) So that that exclusive access is which is more of a role based uh is what arbback is meant for and that comes as slightly different names. I think in some cases it's called pbackback purposebased access control or policy based access control. So that's that's in public preview structured streaming transform with state time travel. Okay.
(51:40) So all of that is good. Now let's look at um the data intelligence platform for collaboration. So you've got these uh data models, notebooks, solutions and delta sharing. So we know all all of that but in addition you've got marketplace and you've got clean rooms. So marketplace bundles it all together makes it available.
(52:04) Could be internal, could be external. And clean rooms is makes it possible for two parties uh which have some shared data to come and uh look at some insights without diverging sensitive data from each party. Uh so you've got aback in public preview, sharing to iceberg clients, metric viewing, um a foreign iceberg sharing, MCP in the marketplace, shared output tables, support for Scala.
(52:35) Nothing too interesting there. This is definitely very interesting. Sharing with attribute-based access control. Now you've got a provider and you've got a recipient. And we know that delta sharing allows you to share assets uh from on this side to this side without copying. But if you had some um if you had some sensitive rows or sensitive columns then um that was not honored.
(53:10) Basically you would not be able to delta share if you had any such attributes. with a back the policies themselves are taken over to the other side. Um and that would help with um the sensitive um fields either rows or columns from being uh um made available like if if they were sensitive on this side, they should be made sensitive on the other side.
(53:34) So both providers and recipients benefit from this attribute-based data sharing. uh they can share tables, views, materialized views, blah blah with these policies without needing to make copies. Recipients can apply that policy um or maybe perhaps either the same policies or slightly different policies on the shared assets as well as the rowle uh and column masking details.
(53:59) This is big because before this we were not able to do so uh sharing to iceberg clients. So we've we've talked about uniform which is basically delta with an additional attribute uh because of which it generates not only delta metadata but also iceberg metadata and then managed iceberg which basically generates iceberg metadata but in addition delta metadata as well because of that delta metadata through the delta sharing protocol uh you know the recipients would also be able to get to the data without copying. So sharing to iceberg
(54:33) clients is possible through these uniform enabled delta tables. Uh and finally we've got MCP in the marketplace and um uh this is big now everybody's talking about MCP. So along with all our tables and models and whatever there is the MCP servers as well and through the servers um there's going to be um um ability to reach uh thousands of these databicks uh users.
(55:04) They can list these MCP uh servers in the databicks marketplace and they they can have um uh agents uh directly tap into the code. There are some examples that we could probably show you next time. That kind of covers this in very broad strokes. Um, do you have any questions? If not, I'm going to stop sharing now.
(55:34) Ram, do you want to take up um maybe assignment three and uh quiz two? >> Yeah. >> Yeah. I'll probably begin with uh quiz two and then uh switch to assignment three. And uh I see Mo is on. So Mo uh if I miss something, please uh you know uh uh complete what I'm thinking of and didn't happen to say. >> So this is uh this was quiz two.
(56:12) This is like that automated quiz two that you guys did. Um so the first question here was uh you know create a table but uh you know you have a uh timestamp which needs to be converted to a date and uh how can you convert this time stamp to a date. So there is a generated always as clause that's provided uh by uh datab bricks uh SQL.
(56:40) One of the advantages of generated always as is also with partitioning what happens is if you filter on this particular column it'll automatically filter on the um uh you know the the so if you partition by this this particular column because it's a date you cannot partition by time stamp right but if you filter in your SQL clause by time stamp it'll automatically uh resolve the partition because it's it knows that these two columns are linked together by this generated always has uh there is another generated uh clause here I think it's called identity uh so
(57:16) if it's an identity then you you can get an identity column with delta tables okay uh this is a pretty easy question so this was the four s's uh you know optimization u uh question with spark so typical um you know problem areas with uh spark optimization are data skew shuffles spills and uh small files right so small files you can uh use optimization predictive optimization to to uh you know resolve uh some of these are resolved automatically by what we call as adaptive query execution or the optimizer within spark but sometimes you
(58:05) know with very complex projects you might have to go in and actually check and figure out why a particular uh you know shuffle is occurring more than it should and uh if there is any way to prevent that from happening. Um so which of the following statements are true about um um uh Delta Lake? So typically delta lake has um you know partition pruning and zordering uh are common ways.
(58:40) There is another way now called liquid clustering that has uh come in. There is also another way called uh bloom filtering which is not used that often not recommended also but it is a optimization mechanism. Um liquid clustering is kind of replacing uh you know these two here. Um because with partition pruning you know you you you need to have like low cardality columns and zordering is typically for high cardality columns but liquid clustering can handle both of them in one shot.
(59:13) So uh that is one of it and then the other one is delta lake obviously open source open format uh um highly scalable uh you know u format storage format. Okay. So these two are the correct answers for Delta Lake. Um you noticed the colleague is manually copying the data to the backup folder. So here when you see the word copying right uh automatically it should point you to kind of the uh time travel feature.
(59:49) So the time travel feature here is the um you know the correct answer. So with time travel any transaction that you do on delta automatically is a is recorded and you have the ability to go back to that transaction uh you know at any time. Um so there is no deep clone etc that is uh involved here. Um so which of the following SQL statements can be used to query a table uh by eliminating duplicate rows and I know most a lot of you are SQL ninjas but uh easy way to do that is distinct but obviously there's you know multiple other ways that you can tackle
(1:00:33) this uh issue uh but uh this is this is uh pretty easy way to uh handle uh duplicate rows. So that's why the star is here. Uh because we are looking at all columns. Uh you're working on a process to query the table based on batch date and batch date uh is an input parameter. Um so so this is to do with parameterization.
(1:01:07) So with parameterization u you know one of the ways ways to do it is essentially with um uh you know with notebooks you can do it with what are known as widgets. Okay. So the widget will automatically then translate to a uh named parameter within SQL that you can uh uh handle. Okay. So if you're doing notebooks then widgets is a good way of parameterizing your query.
(1:01:38) If you're using plain SQL SQL variables is a good way of doing it or there is one more way where you can use SQL named or unnamed parameters to actually control your SQL. Okay. So there's there's multiple ways of um you know again tackling it. Uh but in this case since we're talking uh mainly about notebooks um you know widgets are probably the way to go about doing it.
(1:02:03) Um so this is essentially about creation of a streaming view on top of a delta table. So streaming means definitely it's going to be a read stream. So read is not a stream. Read is kind of like a batch. And then once you do the read stream, you essentially want to make sure that uh here there is no such thing called mode stream.
(1:02:30) Uh there is no such thing known as a trigger stream. Trigger is a processing time kind of a thing. So this is the correct answer here. Uh this is not the correct answer. Sorry. Uh yeah, this one. So here you're essentially doing a um uh No, this is also not the correct answer. Sorry. uh table sales trigger. Yeah, this guy right here.
(1:03:03) So this essentially is doing the read stream with this on this particular table. Remember delta tables are streaming sources. So you can do a stream on a delta table and and um you know it it will it will uh regard it as a stream. uh and then we are doing a create a replace temp view with this particular name.
(1:03:23) So now you can do a select star on this and get uh streaming uh uh output. So remember delta tables are both streaming sources as well as streaming syncs. Okay. So that that should work. Uh so which of the following techniques uh structured streaming uses to create an end toend uh fall tolerance? So essentially it's checkpointing uh is definitely there and then um you know these item potent syncs right so uh the the combination of item potent sync with uh check checkpointing will give you the best fault tolerance that is possible uh with streaming. So you will get exactly
(1:04:03) once kind of processing um uh if if the sync essentially handles it. Now item potency in a sync can be done in with many different ways. For example, you know, for each batch or the sync natively can can also kind of support item potency. So um you know there are many varied types of syncs, right? So the the the strategy there could change.
(1:04:30) Uh which of the following statements about broadcast variables is correct? So u broadcast variables essentially are immutable. So I I see that uh option here. So that is the correct answer here. Uh along with broadcast variable there are also things like accumulator variables etc.
(1:04:52) you know those are interesting variables especially distributed computing that uh you know you need to be aware of. So um so that is uh you know something that you guys might want to take a look at. So which of the following describes a shuffle? I think this was a pretty easy one. Uh I'm sure all of you got this correct. So um you know essentially with spark and distribution sometimes there is a um um you know a process that involves distributing data across the cluster um so that it can be processed parallelly.
(1:05:25) So that is the correct answer here. Can I >> sorry I see a lot of hands. Uh so >> yeah can you go back to question nine you had put broadcast variables are immutable. Um that's what I answered but it it was wrong. So is there another option that we missed? Um so I think also broadcast variables are local to all worker nodes and not shared across cluster.
(1:05:58) Yeah, I think there are two here. Correct. This is the other one. >> So they're not shared across clusters. >> That's correct. So they are local to each worker node also because you're broadcasting it, right? That's the concept of a broadcast. Thank you. >> Hey Amir, I think >> yeah, I had the same question. >> Okay. Sorry. Okay. And then uh yeah, question 11 is um uh so data frame with the larger data frame on column item ID.
(1:06:43) Uh yeah, so this looks like this this definitely is suspect, right? So uh this broadcast is not a valid joint type here. Um that is the answer and then which of the following code blocks efficiently converts uh data frame from 12 into 24 partitions. So here it's increasing right? So if it's increasing you know you definitely cannot use coalis. So this guy is out.
(1:07:14) Coalis is good for reducing partitions with you know much lesser overhead than repartition. So the answer here has to be repartitioned. But this repartition doesn't take any other arguments. So you know this is the correct answer here. There's no such thing as a chunk uh command either. And then here there is an error.
(1:07:42) Um I believe here uh it needs to essentially you know create two columns. um item ID and attribute. So item ID and attribute but the attribute needs to be listed in every single row. Right? So essentially we need to explode this. So we see explode here but explode is part of your select statement not part of your u data frame. Okay.
(1:08:12) So uh when you're doing a select on this particular uh u exploded column, it'll then explode the uh each of the elements and then associated with that particular uh row. Okay. So if you select item ID and attribute explode attribute, then it'll be one in blue, one and winter and one and cozy as the three rows there.
(1:08:35) So that that is the answer there. So explode is not a me method of data frame. Explode should be used inside the select method. Okay. Uh it should convert up to five rows. Um and have the value 25. So that that is a filter we can make out and then it should be brought into a Python list, right? And not a data frame list but a Python list.
(1:09:07) So um collect can't be because collect doesn't take uh any arguments. Collect is going to try and collect everything. So take is you know what we want because take will take arguments. Right? So the other ones are we're going to filter it. Uh filter the column store ID equals 25 so that we only see columns with store ID that are 25 and then take five rows up to five rows.
(1:09:37) Right? So um and then take will always return the result back to the driver uh where it'll be a Python list. Collect also does the same thing but collect is going to return all the responses back to you. Uh which is kind of inefficient especially if the data frame is going to be very very large. So this is a very common issue with uh people trying to do collects uh on large data frames and then running out of memory. Okay.
(1:10:06) So uh thing to watch out for collect is it's dangerous. It's good for debugging but uh pretty dangerous. Uh it's also good for sometimes you know some kind of uh multi-pro process where you're trying to collect a response of a small smallalish data frame and then use the results of that data frame in in another spark command. Right.
(1:10:28) So, uh there is some logic that you can do with collect but most of the time you know I've seen people use it for debugging and then forget to remove it and then they run uh get into out of memory errors. Um here uh outer join on transaction data frame and items data frame but the columns are different right. Uh so uh we're doing the join here um with outer um and then um it looks like um yeah so you cannot have if it is of varying types right you cannot have an you cannot have a list like this you need to have this equal to command so
(1:11:23) that you know you're comparing the correct column with the appropriate column from the other data frame. So item ID here is being compared to product ID and and the equal to operator is that you just cannot put a list there. So that that is the that is the uh error here. Um so which of the following is a correct statement? Uh yeah, I think uh you know I think Anandita and I talked a little bit about this and it looks like maybe the answer that we marked was wrong and I think some of you flagged that. Um so there is a underscore delta
(1:12:06) log folder for every delta table and under that underscore delta log folder each JSON file will represent a single transaction right so when you have three transactions you'll have three JSON files and then every 10 JSON files you will have what is known as a checkpoint file that is created which is a par file um so that you know you get checkpointed so that you don't have to replay all of your transactions right otherwise you got to replay one JSON after another so if you have 100 transactions you know rather than having 100 you can go to the
(1:12:41) last checkpoint at 90 and then uh you know replay from 90 onwards so this is the correct answer but I think we flagged it as a single JSON as being the correct answer it's a single folder uh delta log folder but uh the correct answer is one or more JSONs uh and obviously you're going to have park files right so I I I think if you see maybe underscore delta log here uh I think there is a image here right so underscore delta log so each of these JSON files is essentially a transaction right and this is your data here so this
(1:13:21) is partitioned by date and you have one or more park files under it so uh this is this is the correct answer here that every transaction will be represented by a independent JSON. >> Um this one >> sorry before you move on. So I happened to select that as the correct answer and it was marked as incorrect as you just mentioned.
(1:13:49) So should be reach out like should I reach out to you guys or will you be >> correcting that? >> Yes, please reach out. Um it's 0.1 it I ideally should not uh be very very uh margin but why why should you miss out on it? Absolutely. Um >> okay >> if you just um uh put a note to me I'll go and uh edit it. >> Okay thank you. >> Mhm. And then uh this one is um uh cloud object storage.
(1:14:25) And then we want to ingest the data, right? So from uh storage and we want to ingest it incrementally. So this is a file. So obviously you know if a file if you have files that are coming into a location and you want to read it, it's got to be a cloud files which is autoloader, right? This format cloud files is always referring to to to to autoloader and then the rest of the blanks are essentially autoloader kind of syntact syntax.
(1:14:54) So in this case, I believe the correct answer is this guy right here. Okay. So you know the cloud files format is is CSV. Uh we have a checkpoint location. We have the cloud files. Where the schema for uh cloud files so that there is schema evolution that can happen. So there is a schema location also that is mentioned and then mode append uh col true which which which will append the data to that uh particular table.
(1:15:29) Uh what is the purpose of bronze layer? I think you guys uh are experts in this. So um so essentially it is uh yeah so efficient storage location quering of full unprocessed history of data because it's the first layer in your multihop architecture and then again what is true of a lakehouse um so lakehouse essentially uses standard data formats like parquet Okay.
(1:16:05) And then um lake houses essentially decouple storage as well as compute. Right. So I believe those are the two answers there. Sorry on this one I also marked the APIs. Oh yeah yeah correct. API. Yeah that is that is true too. Yeah that's correct. So it's not limited to Python or SQL. That's correct.
(1:16:35) So those three are the correct answers. >> So what about the the parket formas? The one below. >> Uh yes, even this is correct actually. >> Uh we had three three correct. I think it's the last three there. >> So yeah. So correct. So this guy three, right? Yeah. So various APIs park and yeah storage and compute these three are the correct answers.
(1:17:10) >> So not the parking one. >> Yes these three. >> Okay. Okay. >> Yeah. And then um what is AQA in Spark? It's the uh you know query execution query optimizer in Spark. So this guy right here um AQE can be turned off if necessary but uh you know typically not recommended unless you know what you're doing. So yeah uh how does Lakehouse replace a dependency on data links and data warehouse? Right.
(1:17:52) So um I believe the answer here is all of the above. What uh this canvas test does is it it shuffles the answers. So sometimes all of the above, you know, goes uh above. Uh but this should have actually realistically been at the bottom here. And then uh which of the following code blocks returns uh unique rows for columns product ID? So um there is no such thing as unique.
(1:18:31) So uh so drop duplicates right is the correct answer. So this is the correct syntax. And then um the rows the sorry the columns that you're interested in has to be put in a list. So this will be the correct answer right here. Okay. Um so you can put more than one column here and it'll drop duplicates for those where those columns are having equal values or the same values.
(1:19:08) uh data scientist wants to parallelize the training of trees in a gradient boosted tree. Um so he wants to parallelize this booster tree algorithm uh which can be difficult. Which of the following is true? Right? So here the correct answer is um that it requires data from the previous iteration. So uh so this this gradient boosting requires this uh information from the previous iteration to perform the next step. Okay.
(1:19:40) So that is the correct answer. So it cannot be parallelized because it's got to wait for uh you know responses from the previous step so that it can boost the next step. Uh which of the following evaluation metric is not suitable? Uh so regression problems right so regression problems I believe it's f1 um is the correct answer so f1 is used for classifications um so that uh yeah so you know that is the correct uh answer there you guys probably know the theory much better than I do for that so This one is for hyperparameter optimization using spark. Right? So
(1:20:35) there used to be a library called hyper opt here. Um that was very very popular but hyper opt uh they stopped development and there's a new library called optuna now that has taken over. So the correct answer there is optuna. Um so you can do hyperparameter optimization on uh on uh you know your uh libraries uh using optuna and I believe this is used for you know the yeah the single node machine learning models.
(1:21:09) Okay so things like skarn and things like that. So um yeah so series of binary indicator feature variables you know should be obvious. So if you take a categorical features and then you know split it out into binary indicator feature variables you know that's one hot encoding. Uh that's the correct answer there. Uh so two models for single machine learning pro uh uh problems.
(1:21:44) one model performs well less than five and the other model greater than equal five. uh okay so decides to combine the two models right so when you want to combine the two models the uh typical way is through ensemble uh learning so you take the output of one model feed into another model so um and and create an ensemble model so here I believe this is using hyper opt itself if I'm not mistaken uh but The answer here hyper opta so answer here is uh you know it's using fmin and it's using r squar in the objective function right so
(1:22:31) uh we know higher the r square value the better the model is but here we're trying to do f min and hence you probably have to mark the r squ with a minus r squ so that the objective function you know works uh with r square now it's going to give you the lowest r square value as the best model so That's that that's the error there.
(1:22:57) Uh for imbalanced data sets, I think uh assignment four you guys, you know, uh I I noticed most of you had, you know, it right. So over sampling, under sampling, smokeote are all good um uh ways to tackle imbalanced data sets like the credit card fraud sample. Um so which of the following is a technique used to reduce dimensionality? Um uh I think it's a very well-known technique is the principal component analysis of PCA right? So that's the correct answer there.
(1:23:36) So imputation right? So imputation of missing features. Um oh okay this is for uh mode over mean or median right? So mode I believe is you know the most occurring which means that it would be the categorical type. Um you know uh because to to to impute missing categorical features you probably want to put the one that has the most uh number of uh values of that type.
(1:24:09) There are also other ways to impute here for for example for numeric values or dates or times etc. uh spark.ml provides a imputer function. Uh you can also use windowing functions for example to do it um using what what is called as first and last. Uh so there's there's couple of ways to do imputations on non-catategorical types but categorical types yeah you can use more to to figure it out.
(1:24:38) Um MLflow model register again assignment four you guys you know uh probably all figured it out. So to store, govern, manage machine learning models. So it's a governance um uh you know store. So you can promote, you can provide aliases uh you know which model to then use to promote uh a champion challenger to a champion.
(1:25:04) You need to have the proper governance again to do it. So you guys walked through all of that in assignment four. Uh yeah, model registry used to be it own registry. Now with the Unity catalog uh model registry now is rolled into Unity catalog. So you know you can store your models within Unity catalog. Um so which of the following is the method used to handle missing data? Yeah.
(1:25:34) So you know we just answered that imputation. Um data scientists are using MLflow to track their machine learning experiments. Um yeah they want to identify the run ID with the highest accuracy. Right. So there is a MLflow.arch runs function. Um this search runs takes an experiment ID and then uh you know it orders by and then gives you the uh the one with the highest accuracy that you can choose.
(1:26:06) So here you know we are doing it by zero. Okay. Or is it descending? Let me check. Maybe it's descending, right? Because it's zero. Um yeah, it's descending because it's zero, right? So we want the highest one. So yeah. Uh what is the purpose of the MLflow auto log? Uh MLflow auto log essentially is a is a convenience method.
(1:26:44) Uh rather than you having to log parameters and artifacts and metrics manually, you just say auto log and let MLFlow handle it for that particular flavor uh of your model. Uh so uh you can provide automatic um uh auto logging you know for uh supported integrations right so this is pretty simple so one of the ways that you can create a pispark data frame is also using a pandas data frame so you can do a create data frame and pass a pandas data frame into it so that is the Um uh answer here again pretty simple one. Uh what is the
(1:27:33) best way to describe a data lakehouse compared to a data warehouse? Right. Um so I >> Yeah. >> So going back to 36. >> Yeah. Was there another answer? Because that's what I had marked as the create data frame pandas. >> Oh, is there another way to do it? Let me check. Uh >> I think is a spark.create data frame that is showing up as an valid.
(1:28:10) >> This is correct, right? This one is that correct? >> Uh oh yeah using pispark. Yeah. >> Okay. Yeah. Thanks. Yeah. Yeah. Yeah. Thanks. Thanks for bringing Yeah. Alerting me. Yeah. This is another valid way of doing it. Yeah. You can use from pandas and uh this will take the pandas data frame and give you back a spark data frame.
(1:28:36) I believe there is also if I'm not mistaken there's also a twoore pandas if I'm not mistaken. Okay. This this library used to be known as koalas before. Um so they changed it now to pispark.pandas. So if you if you see something about koalas in documentation is nothing but this guy. And then what is the best way to describe a data lakehouse compared to a data? This is okay.
(1:29:18) So there's only one answer, right? So this Yeah. So data lakehouse enables both batch and streaming analytics. Which of the following SQL command can be used to insert or update or delete rows? Um oh yeah. So insert update delete rows with u you know with conditions and with one shot is always merge into right. So this is the one of the most powerful uh uh SQL constructs that there is.
(1:29:54) Which of the following code blocks creates a new data frame with three columns uh that shows the biggest and smallest values of column value. Right? So we're essentially taking uh product ID. So aggregating by product ID and then doing a aggregation of min and max on this right. So by this group.
(1:30:18) So group by product ID here and then aggregation of max and min. This this particular thing is missing the aggregation. Okay. So yeah. So AG is required uh after the group by which is the most effective form of uh model creation. I think this is a little bit of a silly question. So I think obviously the answer is depends on the use case, right? So yeah, any any other questions on this? I think most of you did pretty well on the on on this quiz.
(1:31:08) Okay. So, let's go to the um assignment three. So I I picked this assignment from uh one of you guys's uh responses itself. Um so the first one is you know Kinesis and Kafka. Some of the differences between Kinesis and Kafka. General idea is Kinesis totally managed and provided by AWS whereas Kafka is cloud agnostic created by a company called Confluent.
(1:31:55) And then uh open source they have their own version. Um uh uh paid version commercial version that also does you know uh you know in place streaming. There is also a language called uh Kafka SQL I believe that that K or KSQL that does you know uh you know operations on the stream as it's happening. So there are some and and they have a whole variety of connectors Kafka connectors that that will connect to various data sources.
(1:32:25) So uh that's additional in the paid version or the commercial version over the their open source version. Uh with uh Kinesis also you have the concept of sharding. Um so the way you scale it up or down is depending on the shard. So a particular topic you can set up to have as many shards as you want but each shard costs you money right.
(1:32:51) So uh it's it's based on that. So you essentially create the number of shards that you want depending on your scalability and your economics. Um Kafka essentially deals with uh something known as partitioning. So you know when you create your topic you create the number of partitions that you want and then your Kafka producer will actually produce messages that are roundroined across those partitions or um you know you can there is a what is known as a concept of a key value pair also with Kafka. So depending on
(1:33:23) the key it get it gets pushed into the correct topic uh partition. So that's kind of the whole gist of uh this kind kinesis and Kafka. Um and Azure by the way has something known as event hub that's used uh that's kind of uh you know a similar concept to Guinnesses on the AWS side. I'm not sure what it is on GCP.
(1:33:55) I don't know man or Anandita if you know or Eric if you know I >> think it's pub right >> oh pub sub okay yeah correct yeah Google pubsub so this is all essentially boiler plate code uh so you know to get your volumes uh etc correct and then please create a table pretty simple stuff um the interesting thing here is right uh you know when you're reading things like CSV files etc from a location um and this happens with autoloader also when you're reading uh something from a location is you get this hidden field
(1:34:37) called as underscore metadata and metadata essentially you'll have a whole bunch of uh additional metadata you know fields there like path name size etc so um I think the question here was to try and take the file name and parse out the uh date from it. Okay? Because there's no date here in in any of the columns.
(1:35:02) So use the file name to get the date and and then put it as a column there. So there's again many different ways of doing it. Uh this is this is the SQL way of doing it here. Uh but essentially most of you guys also did the you know the regular expression method but you know people also did the replace and then to date.
(1:35:24) Replace obviously is going to be a little more complex than uh using regular expressions. Um but here you have a new column known as update date which will take that uh metadata file name and convert to a date. And then I I think uh here it is also partitioned by file name. So you have your three partitions that you get here based on the data. Okay.
(1:35:51) Um and then um um I thought there was also a way it was done via okay maybe he he did it only via SQL. Uh so you can also use pispark to do it. Um use regular expressions and pispark to do it. Uh I think some of you did it using SQL, some of you did it using pispark. Um some of you did both. So this was a question on slowly changing dimensions.
(1:36:31) Um type one and type two. I think most of you got uh this question correct. um you know pretty straightforward. Type one is a complete replace of that row uh which is updated whereas type two or or a slowly on a dimension table is is usually referred to as a slowly changing dimension because it it changes pretty uh slowly and it keeps the version it tracks the version of of the dimension that has changed.
(1:37:00) So for example, if a user moves from city A to city B, you know, it'll track what the city A is and then what the city B is. And in the fact table, you'll know which records are associated with city A and which records are associated with city B because it points to that particular key there. Um I think here um yeah so this this question was a little uh you know tricky.
(1:37:35) So the the key concept here is only the city column will be changing. Um so you know that was one of the key concepts here. So essentially we wanted you to create three tables based on the date and then update a target table one by one. But when you're using the merge command, right, we we want you to make sure that only the city column will change and not any of the other columns.
(1:38:01) Um so you know so that was kind of the u uh trick there. So this this is essentially all boilerplate code to generate the three. But I I think the answer here was so when matched and target city not equal to source city that's when we want the update to occur. Okay. And if it does not match then you insert it. Right. So uh so if it is matched and the city column is changing only then update you know the the the row.
(1:38:35) So yeah, the next one was to do with streaming. Um, so we had set up a Kafka server uh on a on a on a AWS instance. So this IP address was pointing to that and um here this this had like whole bunch of sensor data. So yeah, so it had all of these schema elements in it. So user ID, device ID, number of steps, etc.
(1:39:08) So initially you're going to get it as uh you know a JSON when you read it. So this Kafka stream is going to have this value. Remember I said there's a key, value kind of a concept with Kafka. So you're going to get this value cast it as a string which gives you the complete JSON and then from JSON you're going to get your data out.
(1:39:28) So this will give you you know columns uh from the JSON and then we're doing a select data star there because this alias is data. So you're doing a data star and then this was an interesting thing. So this particular user also did a drop duplicates on ID time stamp to make sure he's processing all records exactly once. uh and then um um you know essentially outputting uh appending to a target table uh with uh the this is the target table right here and uh the trigger here is available now right so before available one available now was
(1:40:13) available uh there used to be something known as once um and what once would do is suppose you had like let's say 20,000 records in Kafka, it'll try to read all 20,000 records at once, which is inefficient. Uh so they switched it to ivable now, which is kind of similar, which which also tries to read the 20,000, but it does it in micro batches.
(1:40:36) Okay, so it doesn't overwhelm the spark streaming uh worker nodes. Uh so it'll read the 20,000 based on things like max bytes per trigger or max files per trigger that you you you can set up uh to control the size of your microbatch and then you know it will it will um essentially do it in micro batches. So the whole thing about spark streaming is it's extremely similar to batch-based processing.
(1:41:02) Uh one of the big differences is it does does things in a micro batch format rather than a single large batch that you're used to with the uh you know spark.right or spark read. And then the second big thing is the checkpoints right so it maintains checkpoint so that if the stream stops and restarts the combination of the sync the combination of the source and this checkpoint location you can exactly start off where you left off.
(1:41:34) So now you have your streaming data flowing into this table. So you know you can see the streaming data. Uh theory question what is the initial position? What is check uh point location trigger and processing time? initial position you can control from where you want to read because usually the streaming source um like Kafka for example has what are what is known as a retention date so till the retention date is uh you know is uh you know happens all the records are maintained within Kafka okay so when when the
(1:42:11) retention date occurs that's when the oldest records get deleted or or truncated So if you blindly read from CFKA without a checkpoint then you know you're going to read all of the records. So this initial position can essentially control you want to read whether you want to read from the earliest record whether you want to read from the latest record uh etc.
(1:42:37) So checkpoint location you guys know it's essentially a a location in a volume right uh where uh your checkpoints are stored so that spark streaming knows what has been processed and what has not been processed. Trigger and processing time have to do with when you want the stream to run. So if you provide processing time as a trigger like every 10 seconds for example then the stream will run every 10 seconds right it'll run process the records stop and then after 10 seconds it triggers again process the records and stops um and uh and the command that uh
(1:43:13) controls that is trigger. So trigger can be processing time trigger can be available now trigger can be once equals true uh so on and so forth. Uh so this this particular uh question I think some of you had some doubts also because this trigger once equals true which the question is asking you to do is is deprecated now but it should I believe still work uh with uh serverless um but uh yeah this this is depre like I said earlier this is deprecated for available now equals true.
(1:43:56) But um you know it's just testing your um the ability to say hey what is the difference between a continuous process versus a trigger once kind of a process and then here we are actually going to read some image data. Unfortunately, there was a little bit of a, you know, some hoops that had to be jumped through because image is a well-known type for spark um within delta.
(1:44:32) So if you read an image content right image content within uh delta if you stored the image within delta and you read it then if you say display if you use the display command then you know the image will actually show up immediately but that doesn't happen with serverless. So with serverless you need to have you know some function or some library to actually show that image to you otherwise it's built into this display.
(1:45:02) If it detects that one of the columns is of type image then it's actually going to show you the image when you do the display which would have been very convenient but um you know there's a few extra steps that uh you had to go through but the good thing is that this code was provided uh so you know you didn't have to figure that out.
(1:45:23) So here we're using copy into which is a SQL type of uh command that can copy incrementally from a location into a table right. So remember there is autoloader where you have the cloud files but that is pispark. Uh if you want the same construct using SQL you can use like copy into and and and get it.
(1:45:50) Okay, here again I believe the question asked for profile image to uh you know to pass profile image out to get you know the user ID or whatever to and store that as a column. So that's why the uh regular expression replace is happening here and then storing it as an integer here uh integer plus one right. So yeah so you have to add + one to that uh image to get the user ID.
(1:46:17) Uh so here you can see that the image data is shown. Uh so you know the image file was read. We have the image data. Unfortunately if you do a display on this data frame this won't show up because in serverless mode it doesn't actually render the image data to an image. Um and then here it is you know uh you know total number of miles walked per day.
(1:46:48) So there is some group group by or group group by going on uh group by logic going on to sum up the miles walked uh per user ID. And we are actually joining two tables here. The streaming table as well as the uh thumbnail table using user ID. Right? So uh join these two sum up the uh miles walked and then um uh you know get your response out.
(1:47:15) Now the last part was to invert images. So using uh pispark udfs. So here um we use a library called pillow and then uh essentially there is a function here that you guys have to fill out uh to to actually do the inversion and then there is a for each command here. So the for each command is going to go through each row and apply that function there.
(1:47:44) Right? Again there's multiple ways that you can do this. You could have created a UDF spark udf here and actually done a width column and then you know applied the uh spark udf there as another mechanism. You could have used map as a function here. Uh this map is not a python map but a pispark map. So you can use a pispark map and directly provide this function like how you do for for each.
(1:48:09) But if you use width column you cannot use this function as is. this function needs to be converted to a UDF before it can be used. So there there's many ways of uh doing it but this is perfectly fine to do it this way. Uh again the content doesn't show up here with display. So you know there's a subsequent step where you need to actually open the image and show it but essentially now the images in the source table were taken and inverted.
(1:48:40) Right? So you see the negative image there. And then uh finally some uh you know uh theory based questions uh advantages of incremental data loading uh spark streaming uh you know so uh I I I won't get into the details here but uh I think most of you were able to answer this uh pretty accurately. Um so incremental data loading right is is is a very important concept with uh change data capture for example um you know processing incrementally so that you don't have to do things over and over again which is why delta being natively a a streaming source and a
(1:49:33) streaming target is very very helpful. So that's why it can do both batch as well as uh streaming and delta also right has another benefit is it can emit what are known as change data feed records. So if a delta table changes for example and you subscribe to that delta table's changes then that delta table can output what has changed right so whether it was what is the operation and what has changed so whether it was an insert and what got inserted whether it was a delete what got deleted whether it's an update and what got updated right so it
(1:50:14) can it can emit all of those records to you so as a target you can read those changes and then do something that you need to do with it. So it becomes a very efficient way to do change data capture. But with change data capture the source table has to emit those records and delta does emit those records.
(1:50:33) So that's uh another big advantage of delta because it does change data feed uh kind of uh operations. And then advantages of streaming versus batch. I think I I kind of covered it in the beginning. Um obviously incremental um you know streaming requires a little more infrastructure but streaming has also the ability to do fall tolerance.
(1:50:59) Uh streaming is much more efficient, much more easy on memory. Um so you know you have uh you know those advantages but uh on the on the flip side it's it's it you know you you need to worry about exactly one's processing things things of that nature. So that that logic needs to be put in there. There are some schools of thought which says you know you can do everything with streaming that you can do with batch.
(1:51:30) So uh it's not subscribed to very often out in the practical world but you know uh it is a possibility though and then um uh job scheduling uh essentially an orchestrator and uh you know here it's asking different operations can you t tie them together and things like you know workflows that we went through data bricks workflows are datab bricks pipelines uh with delta life tables or spark uh declarative pipelines uh you can tie different operations together within the same orchestrator so that the output of one you know gets
(1:52:12) into the input of the other you can create medallion architectures so uh essentially that this question is asking you guys to explain all of that um you know and and here you can see that this particular person has explained bronze, silver, how a bronze layer would look, how the silver layer would look, how the gold layer would look uh in in such a scenario.
(1:52:37) Okay, so these would all be notebooks. These notebooks could be tied within that orchestrator, within that workflow, right? So u and then always a good idea to stop your streams. So that was kind of assignment three kind of a mixture of uh some amount of batch processing some amount of streaming some theory you know you you tested your skills against a actual Kafka server that helped you do streaming.
(1:53:06) Uh so hopefully that was helpful to you guys. Any any questions on this more than anything I missed? >> Sorry. Did you ask me something? >> No. Anything I missed? >> No, no, no, no, no. I think you got it. Y Okay, cool. That was again a very very thorough uh review of both quiz 2 as well as um assignment uh three.
(1:53:42) We tend to do it just a little late because um there are folks who sometimes make late submissions. So we don't want to dulge uh all the responses too early but hopefully you had a chance to re-review that. Um next class we will go over the final assignment um which is four. Um any last minute questions for today? Otherwise we'll let you go and um we will have some guest uh speakers next time from the industry.
(1:54:16) So if if you guys are watching this um and if you are able to make it do come it would be good to have it a little more interactive and for you to be able to ask questions directly of them. >> I just want to say this was really um useful. Thank you so much. >> Oh good. >> You very much. >> All right. Good night everyone.
(1:54:44) Thanks Ram. Bye.