103 day6 - YouTube
https://www.youtube.com/watch?v=h1RspSShQYA

Transcript:
(00:02) Okay. Um, welcome everyone. Tonight we'll be discussing BI analytics and data visualizations. So we'll um look at the history of warehouses and why they're still useful. uh look at um business intelligence or BI and compare it with um and also um be um business analytics as well. Uh look at JDBC connectors.
(00:39) Um think about uh key performance indicators or K KPIs in terms of um current concurrency and latency. review um data visualization and also um take a look at the lakehouse architecture and then the second part of the lecture will be lab again on BI reporting. So before we start with tonight's lecture let's review some concepts from last week.
(01:09) So can anyone um say the differences between um data lakes and data warehouse? Warehouse is for structured data. Lakes can take unstructured and structured data. Good. Anything else? So where how in a in a data lake? data lake uh the data is dumped uh and uh the schema of data is decided while reading the data and in data warehouse the schema it's a structured data so schema is already decided before it put into the into the warehouse okay yeah very good so schema on write versus schema on read um anything else the size of the data data warehouses for
(02:04) moderate size. Data lakes are large amounts of data store large amounts of data. Good. Yeah, data links can handle lots more data than traditional warehouses. And um anything else? The retrieval time I think is uh also a difference. retrieval time in data warehouses is a lot shorter. Yeah. Where Well, yeah, traditionally warehouses are um designed for speed.
(02:40) Um so they're they're generally faster at retrieval. And um one other thing about the um ability for how lakes can um receive data versus warehouses. Well, um data links can you can stream data into data links where warehouses are usually a more of a batch operation. All right, good. So, um, what are some of the ways of hydrating a data lake rest? Yeah.
(03:34) Um, data from your history. Yeah. We last week we looked at like um three or four techniques that Say it again. Streaming data. Yeah. What about uh do you remember the autoloader? Yes. So the autoloader um automatically detects new files and and and and reads them in to the from the cloud and reads them in another way.
(04:19) Uh copy into is another and um merge upsert. Insert. Mhm. Yeah. Autoloader. Um, copy into, merge, inserts, all of these are great suggestions. Mhm. Okay. And then, u differences between a what's a data silo versus a data swamp? Not necessarily the differences, but what are they in? Data silo is when data sits in different teams, databases, and they're not necessarily connected.
(05:11) And uh data swamp is when uh data is just dumped into data lake uh relatively irresponsibly without much planning. So it just turns into a mess to deal with. Yeah. It turns into a it's a data swamp is where a data leak has uh missing governance and and turns into a swamp and becomes unusable. Okay.
(05:38) three phases of the medallion architecture. Uh bronze, silver, and gold. The bronze being the the raw ingested data, the silver being um data that's been cleansed to the dup duplication joins and the gold is kind of refined analytics ready data. Yeah. Good. Yeah, you can think of the Olympics to remember that.
(06:06) And then but why why is it important to have have that separation? So if you have to uh for errors go back uh for um you always have the raw data set and you can always go back a layer. Good. If we directly go from unstructured data to gold there's just like too many steps in between we can do it but uh if we first pull the data in a structured way which is our uh bronze um that is now you can actually hydrate multiple tables from it.
(06:52) So it just creates this initial uh clean step going from unstructured data in your history to a structured known format. Now you can build upon and uh silver is you do your cleaning. Now now that you have the data, you do your cleaning, you group things, uh you if you want to combine a few data from different places, you do that.
(07:16) And gold is uh the data like this is this. So silver silver is the data is good now and it is usable but gold is when it is produced in a way that is going to serve to production and that can happen in multiple formats. You can structure the same data in several different ways to serve different uh needs or different optimizations.
(07:43) Yeah, I think that last point that you mentioned is is especially um important that it can support different types the different um different um locations can support different types of users and different sorts of activities. So that's important too where the gold is for business users and and maybe the silver can be used by data scientists and others and and then the um and then the the raw or the bronze level is always there to go back to and maybe restart. So good.
(08:16) And then um data as a product by decentralized domain centered teams is an example of of what what kind of architecture Is it data mesh? Data mesh. Yeah. Data mesh. The concept there was um different teams pro support different um sets of data and and then expose it as like a a product and then data wherever it resides.
(08:51) Cloud onrem edge is an example of So this is similar to data mesh but more focused on the technology data fabric. Data fabric. Excellent. Okay. Good. So you guys are remembering well. So good to see. Okay. So tonight we're going to look at um BI and and talk about what what is BI um look at some data warehouses and and some BI tools and also think about the um primary consumer or persona that uses the BI tools and um and the the skill sets for a BI person and and how is BI different from BA and AI.
(09:48) And you can think um we added this picture of um Dorothy and Professor Marvel from The Wizard of Oz and they're looking into a crystal ball. And that's really what the business um wants out of um out of all this data and data engineering that we're doing.
(10:15) Ultimately, the business is really interested in being able to see insights and and um especially if they can see into the future and and what the future will bring and and how they can position the business in a way to succeed in that future. And um so um very important um competitive advantage for businesses to be able to process data and not and and um and then be able to use it and analyze it and um for um competitive advantages.
(10:48) So um let's um compare um data lakes and data warehouses um again. So uh with the um the data lake um and they both have pros and cons and we'll see that um the lakehouse is is an architecture that brings the both best of both worlds from the data lake and the data warehouse and and um and and and helps helps um with with um using large amounts of data.
(11:25) So the um for the data lake the um uses open um file formats of all types. Um it separates the compute from the storage and uses economical storage. So so the storage is is is cheap and then then the compute is um used efficiently and not wasted. So that's that's better.
(11:52) And then um there's a huge um ecosystem of tools and frameworks that can support the the data lakeink. And then the the data warehouse um provides um better fine grained access for um controlling access to the data. It's um easy to use and and supports high concurrency and low latency.
(12:17) And it's um and it it also supports um SQL which um many of the business users are familiar with. So they like that. So um so and then the um the cons for for each are are are are removed within the lakehouse. So the lakehouse is really combining the the best of the data lake and the warehouse together and um and so um you you you get all the benefits of the data lake plus the data warehouse but um where the data warehouse is could be limited to um to the amount of data that you can um process because it's fundamentally a lake underneath.
(12:58) uh there's um you can have huge huge amounts you can work with huge amounts of data and still get um good performance. So review um the architecture of a a warehouse where you have your structured data is um fed into the system through an extract, transform and load process where um we we know that the schema on on right goes into a data warehouse which might be some sort of um like a dimensional um type of um structure and then uh can be exposed through uh reports and and BI tools.
(13:46) And then uh with the data lake we have basically um the way that we supported data warehouses there was we we had the lake and then we would um from the lake we would use an ETL process to uh um populate the data warehouse which could then could then be used for reports and and BI as well as data science.
(14:13) And then u machine learning was also supported um through through the um functionality of the lake. So this this was better um because we could support large amounts of data and also structured and unstructured data but um somewhat cumbersome since the data warehouse was still there and um and had to be supported externally from the data lake. With the lakehouse um the warehouse functionality is included um as as part of the data lake.
(14:45) So the um the data lake is there underneath holding the data both structured semistructured and unstructured data and then um directly from the lakehouse you can um support um BI reports data science and machine learning. So it's really um all combined together and now uh with the lakehouse you no longer need to invest in a separate um data warehouse to support your BI and reports. It can be done directly from the lakehouse.
(15:20) So that's um that's pretty good. So simpler, easier to manage and um more cost effective. Any any questions about the differences between the the warehouse data lake and lakehouse. Now option number two uh sorry option number three is improved version of two right like where we have data warehouse so instead of that we cache the met meta data uh and uh indexing layer that's the difference right well the the lakehouse includes the functionality that's provided by the data warehouse so now you can do um high or fast queries uh SQL based queries
(16:13) directly um against the data We um I think Anandita last week talked about the um the the support for the SQL that's now um included in the lakehouse. So um you can do SQL queries directly against the data that's that's stored in the the um delta tables within the the lakehouse. Okay. Okay. Got it. Thanks.
(16:41) Sure. Okay. So, business intelligence um includes a lot of a lot of different things including data analysis, governance, analytics um and um and strategy around like uh document documentation about what is the business mission and what's its strategy in terms of succeeding in the market.
(17:14) And then um that can feed into advanced analytics that can help again help the business see into the future and and um succeed. It it um uses different types of data, marketing data, customer data, supply chain data, uh resource resource data, all feeds into uh the BI data that then is is run through this process. So we can think about data is what you need to do analytics and then information is what you need to do business. So the the data is not enough on its own.
(17:57) You need information extracted from the data to be able to um succeed in business. So, and then BI is is basically the the process of of um the using the different technologies and applications to collect, integrate, analyze and and present um business information to again better support um business decision making.
(18:31) helping helping the business um see see not only what's happening today but maybe um predicting what will happen tomorrow and um and helping improve um the the business outcomes. Some examples include um uh customer contact and interaction analysis uh uh closed deal analysis for sales and then um also Google Analytics is an a good example of analyzing um website traffic.
(19:13) So the the the storage of or or the data storage has evolved over time. We started early on with um spreadsheets as a way to to anal collect and analyze data and then uh that evolved into uh data warehouses and we had Bill in Enman um use it who started with a ER model to create from there produce um dimensional models from the ER data and then um Ralph Kimble um kind of skipped the ER part and just went to the the fact and the dimensional tables either in a star or snowflake um configuration and then there was a point where um basically the data got too big and
(20:11) wouldn't fit into the data warehouses. So then uh companies like uh Greenplum and Terod data came up with this concept of masses mass massively parallel programming um or MPP databases that use proprietary um systems to basically spread out the not only the data but also the compute across um multiple machines.
(20:43) So those those um systems like the the name terod data suggests we're able to now process um terabytes of of data and provide um analysis or analytics on that data. But um the problem was is with those systems is they were very expensive and and not very um cost effective. And then um uh Google Google first came out with um big table and um which was a a big basically a columner database that could handle um terabytes if not pabytes of data.
(21:21) And that was that was kind of the um the start of no nosql solutions or not only SQL solutions where um large very um basically we had tables and we could do queries on those tables and and on very large amounts of data but it was again it was um all tabular information and then Hadoop came along thanks to Mike Carett Caparella and Doug Cutting, they um they created Hadoop and Hadoop allowed us to um suddenly process um extremely large amounts of data um and um not only store it but also uh do um do processing on that data across uh more um coste effective um hardware platforms that um
(22:17) were able to separate the the storage uh using lowcost storage from the from the compute and using a distributed architecture that um they scaled horizontally. Um so this this this led to the um creation of data lakes and then um but with the data lake um as we know without proper governance it can easily um turn into a data swamp and become unusable.
(22:52) And then um over time the um this idea of a a data mesh or and fabric emerged where the data becomes a service and um or a product and uh and then finally that was um followed by the the concept of a lakehouse that uh helps um keep all the data together um supports good governance and also um combines finds the ability to support u data warehouse functionality directly from the the underlying um data lakeink platform.
(23:34) So it's been quite an evolution and one thing that we should remember is that it's not over. Things are still um progressing and evolving. So, um maybe in a in a year or two there'll be some some new technology that um that replaces the lakehouse. It's hard to say, but definitely it's a it's a um we're on a journey and and um in terms of improved ability.
(24:07) Now, right now AI is really um having a big impact in on on the world really and LLMs and things. So that that is likely to um change things even more and maybe even accelerate um change in the future. So another um view but more focused on the BI world. So uh it's uh over the years since um around 2000 uh the journey began we went um data being controlled by IT departments to um introducing 64-bit computers.
(25:00) Um before that were 32-bit computers which were limited in their ability abilities. Um and then self-service BI and and cloud computing with uh Amazon introduced um their AWS which um changed kind of changed the world dramatically with cloud-based computing and then um mobile BI and um um exposing BI to to uh ordinary businesses that maybe um small to mediumsiz businesses that couldn't couldn't do it before uh without the IT departments.
(25:42) um data visualization um um PowerBI from from Microsoft um and um machine learning started having an impact um AI machine learning and then um low code and no code and mobile and now in this graphic stops at 2020 but now of course we have um chat GPT and and other tools that we can ask questions of and and get answers and this will um this will continue to evolve the the field of business intelligence.
(26:29) So it's more I think probably mostly in terms of making it more helping democratize access to um business intelligence. Now now anyone can um will have access to BI through just a natural um conversational interface. But still still to to get us there, we still need um the the data engineering that goes into preparing the data um for those tools to use.
(27:00) Any any questions about any of this so far? A lot of people think that um or at least some people believe that the um the economy has really been on a tear since um like the through throughout this um this journey and um a lot of people attribute it to um a AI basically uh helping helping businesses um be more effective.
(27:38) and and efficient in their businesses and uh and and and helping improve profits and and the value of these companies. So that that's um and and not just recently with LLMs but um every every um well since since the um beginning of the century really because um it's we have been using machine learning and and and um AI to to help us all the way along this journey and now with LLMs and their capabilities it's it's accelerating even more.
(28:19) But um a lot again a lot of a lot of people think that um the the economy is is is is benefiting directly from from uh business intelligence. Okay. So um so BI traditionally is more for descriptive analysis. Um and then but the trend is towards prescriptive and and what we need mean by prescriptive is being able to not only understand what's happening today but um make suggestions for what what the business should do um today or tomorrow.
(29:02) Um so actionable insights to business stakeholders for decision- making. So smart um support, smart um decision making by the business with um self-service um monitoring of of usage which and monitoring of usage is things like well which which um tools or or services are the the business users using? That's important to know.
(29:32) um performance optimization. Um always um looking to see how to make the tools better. And then also security controls are important to make sure that we're not exposing data to um people that shouldn't shouldn't have access to it like for example competitors or maybe um individuals in the organization that that don't need access to that.
(29:59) Um it's always important to have security and and governance um over the data. And then story storytelling is a way of of sharing um information and in a way of stories that makes it easy for business users to understand and and digest the information that we provide. So easy easy to grasp um information sharing. So, so they um the different steps involve like collecting the data um into some sort of warehouse and then um organizing the data um into um data models like maybe dimensional models and then um and then analyzing the data um and then uh
(30:53) using SQL queries or other types of queries to query the data and build visualizations and dashboards. And then finally uh those results are shared by to with the business and then the business executives and others within the business use the data to to do their job and make decisions in terms of um maybe what products to develop or or which products to cancel or which markets to go after.
(31:26) all sorts of um interesting questions that the the business leaders need to answer these um the business intelligence or BI tools can help. So different different um parts of the um process um mining and u predictive analysis, statistical analysis, big data visualizations, KPIs, KPIs are uh are key performance indicators helping helping the business know uh where where they are.
(32:05) Is the um are things are sales going according to plan or or are they behind uh performance benchmarking and being able to query and and ask uh questions uh in a more ad hoc basis. So um BI versus um BA. BI uses past and current data to address the what and how and BA uses past data to explain present and predict the future. Addressing the why and and what's next.
(32:40) So um so things um like what happened, when, who, how many and and then um why did it happen? Will it happen again? Um what happens if we change X? And then um are some of the questions that can be answered by um BA and BA, BI and BA. And then um different different ways of of sharing the data with business users using dashboards or go cards, scorecards or u through ad hoc queries.
(33:28) So we talk a lot about different personas in data engineering. So the BI analyst is is different from the data engineer data scientist. Their primary primary skill set is using SQL. Um the BI data that they're using is curated probably in the gold layer and or silver or gold layer.
(33:55) um BI data is typically stored in a warehouse and um uses special um data marts and then now as we talked before um data lakes have the um have the ability to support um warehouses um as a feed for the for the data warehouse and then the lakehouse is able to do the job of both the um data lake as well well as the warehouse and then um all of this helps uh democratize um access to the data through um self-service BI um being able to um have access to individuals having access to the data where they can do their own uh data discovery and um and also um supports data mining
(34:54) for what if um analysis. Some terminology um for warehouses that we should be familiar with. A catalog as you know um stores metadata. Uh the database or schema provides a namespace and and provides logical and physical organization of the data. uh tables um can be either managed or unmanaged u where the managed um tables are managed by the um the the data lake and um unmanaged tables are stored outside of the lake and and um accessed through metadata.
(35:44) Um there's different types of um keys port um primary keys and foreign keys uh to help us um uh with understanding relationships in the data. Identity columns are important um for um identifying uh entities in the data. There can be surrogate keys uh or um natural keys and uh also there's some constraints that we may have on the data uh to help ensure data quality.
(36:15) Uh views are virtual tables that are hydrated on demand as when you access the view the the underlying query is run to produce the view. Um uh there's federated queries that uh can can cross um data store boundaries and and and request um data from multiple um data sources through um push down predicates.
(36:49) Materialized views are similar to views except they're precomputed and uh support faster access to the data. stored procedures or UDFs um encapsulate uh logic and um the semantic data model um provides um captures the the relationships of uh business entities and using business terminology that the that that the business understands. For example, customers and products and transactions So um some some best practices or um or or components of good data include data collection uh where data is gathered from multiple sources across the organization. The data is prepared um through um
(37:46) integration, modeling, cleansing, enrichment and then analytics um what like looking at questions like what has happened and why and how did it happen and uh what might happen in the future. data visualization, providing charts and and graphs uh to help us um visualize the data. Uh sharing sharing and collaboration and then uh governance um making sure that we know who has access to the data and and who doesn't.
(38:30) And then um strategy documentation, uh centralized data catalogs for where to find data throughout the um the business. And then uh ease of use uh and and um to make make it easy for users, the business users to um get access to the data. basically democratizing access to the data so that more more business users have access to the um tools that they need to make good decisions versus um where we started where where maybe the data was managed by the IT team and and only they had access to the data and and if you needed something you you might have to ask them. Now um the um the tools are such that uh with the
(39:18) right access you can um you can you can analyze the the the the charts and graphs that are already provided for you by the um BI team or maybe um create your own or do your own queries. Okay. And with that, I I'm going to hand it off to um to Anandita to talk about more about the um specifics of the data bricks platform and its support for BI on are you there? Thanks. Yes.
(40:06) Now we'll talk about um you as a data engineer or a data architect or a data persona when you are talking about holistic systems stitching uh various sources of data, various use cases, various data journeys. uh what is a common way to represent that and this is one way in which you can bring the various parties onto the same page yes I don't think you're sharing it I am are you not able to see it I can't see your screen that intelligence platforms okay got it sorry okay no problem um so you've got uh uh sources um that this could be your regular ETL
(40:58) and we've talked about how you can have structured, semistructured and unstructured data coming in from multiple clouds and you having to actually physically ingest it. Federation is a new term. Can anyone tell me what uh why is federation different from ETL? Okay, no guesses.
(41:32) Um, with ETL the lake becomes the owner of that data that is coming in and there is significant amount of compute that is spent in harnessing this data as you go through your bronze, silver, gold uh transformations, refinements and so on. With federation, you are bringing in readonly data of course from a different store where you are not the owner and you're not bringing it into the lake.
(42:01) It is still in a separate body like it could be like an on-prem Oracle system or it could be an Amazon red shift uh uh data set but it is not uh together like how we talk about breaking the data silos. This is actually not quite there but it is consolidating the data. You're bringing in say a lookup data a reference dictionary not huge huge amounts of data but modest data is being brought in and um gathered along with your ETL data.
(42:34) So if somebody were to ask you uh what is data virtualization this is an example. So data federation is an example of virtualization where you have not physically ETL the data but you are still using it on the lake to join against some critical tables. Okay.
(42:59) Then in the in section we talked about batch and streaming and autoloader and DT and LDP and copy into all of that stuff happens here in the transformation. Yes air. Yeah, sorry to interrupt. I'm um I'm not sure if I quite understood the the difference between federation and ETL. Uh if data is stored on different sources and you're reading from those sources, then how is this different from like let's say the extraction part of ETL? Yeah, because you get it and you actually in ETL you have a landing zone then you get it into bronze and silver and the gold. So you kind of own the data here. You are pulling the data
(43:40) joining it against your maybe your transactional data here. Um but that data is not part of a lake. Um there is ownership there. There are boundaries and large enterprises do not uh rely on just a single platform to do their business and many groups will have uh political agendas as to why they wouldn't want to come on uh a single platform.
(44:05) So in an ideal world you would be able to break these silos very easily but in a real world due to legacy reasons due to political reasons due to cost reasons due to licensing reasons a whole set of um criteria as to why you will still have like SAP systems and on like SQL servers and Oracle systems and Amazon red shift. So your data is still scattered.
(44:30) You're trying your level best to combine it and break down those silos and it things don't happen overnight. So at some point you might be might need to be able to bring in critical data sets from these disparate systems just so that you can do your holistic analysis on the on a single platform. So that is where federation so can we say ownership is the distinguishing factor. Yes that is absolutely correct.
(44:54) So you're here the ownership like the catalog is somewhere else and you're just bringing it in through a federated mechanism because without it your analysis may not be complete. I see you're still using some amount of compute to bring it in but you are not treating it as like the source of truth for future uh analysis.
(45:17) It's like oneoff that you come maybe join, get an additional column, write it back, persist it back into a gold table or a silver table. Appreciate it. Thank you. Yeah, there was one more question. Yes, ma'am. Um my question is uh then are we talking about what we had a concept that we had discussed earlier? Are these mostly external tables that uh we'd be referencing and then joining th those to other physical tables that we have in bronze, silver or gold or exactly federation is actually very interesting because let's take an example of a terod data system right terod data where was very popular some time back but it's very expensive and
(45:57) not too many people know how to use it. The native script that they use is something called as BTE. Um now you do not know BTE scripting but you know that there is one table in terod data which is very important for you to do your uh analysis and your join so that your reports that is going to be read by the execs needs to be complete.
(46:17) So you are going to federate that data in and the beauty is if federation is supported fully like that particular data source you can't do it against all sources. It should be only those sources that the federation engine supports. Then the the query that you're writing is going to do a push down with your wear clause.
(46:39) Bring in exactly the amount of data that you really need for the join. Um finish it and you don't need to know BTE. You're still writing in NCSQL and federation is translating it into the native query doing the push down bringing in the data and making it available for you. Of course it's slow and of course it cannot be done on very large data your that is what your lake is for and your uh ownership of your catalog and your schemas and your data sets is for but still it becomes imperative to be able to get to such data sets that is why it goes handinhand with ETL but I want you
(47:15) to understand the difference in concept between the Okay. And when we go to Unity catalog, I will show you how we can create a foreign connection to limited data stores that are available in a drop-own list. And once you make the connection, usually an admin persona will make that connection. You are now admins of your own workspaces.
(47:45) Then you would be able to use that connection to be able to pull certain data sets from uh from those locations. And again once it appears it will appear as a foreign catalog in Unity and you would still like you know from a usability perspective it is actually pretty seamless. You just do your select the normal way.
(48:05) Nobody would even realize that there is a foreign connection and there's federation happening. But our job is to break down these silos. And so these are different techniques by which you bring your data together. Okay. So you got through ingest. Now transformation is the heavy lifting. This is where you know you you cleanse, you join, you drop, you you do a lots of uh things to normalize and then u your federation your aggregations may also happen here. Query and processing.
(48:37) Um there this is like uh whatever data you've just brought in has going is going to be used and typically there is a bifurcation we are talking about BI uh and BA today. So the data warehousing portion of data bricks is called datab bricks SQL and you can see it in in one of the uh tabs on the left navigation side.
(49:03) uh so it once the data is in you can immediately start to use regular and cql to be able to query it to do some transformations even use llms in SQL which is very powerful. Uh the other part is you might have some ML which has run on the data and the insights have been persisted and those same insights can now be viewed through your DBSQL dashboards.
(49:29) So that makes it truly democratized because now you don't need a data scientist or a PhD or whatever to be able to view um those those insights and those details from SQL. You can call LLMs, you can view those insights and so everybody has a chance of looking at uh what the predictive value of the data is. Um your storage is always in cloud. So data bricks is multiloud.
(49:57) uh you might in fact using serverless you don't even know whether you're hitting an S3 underneath or or Azure or GCP that has been abstracted away but say for some practical purposes let's say this is like a Amazon S3 that's where your data truly resides there are certain metadata elements which are in unity catalog like your schema some description some lineage information and because data bricks is a managed service those very very critical pieces of information is managed by data bricks and that is important when you go to see um some of the GNI capabilities or the true intelligence capabilities of the platforms you'll see how the metadata is important um to understand your table to
(50:39) understand how you're using the table to do some optimizations on your behalf um and so on. Uh so that's the bronze, silver, gold. When you are using compute, you are taking the data from storage and putting it back into storage. So your delta, your transaction logs, everything is in storage. Um and this is purely a conceptual thing.
(51:07) Um later classes we will see how delta sharing and marketplace are additional collaboration. So remember this is about breaking silos and being able to data to reach the right hands. So when you are um sharing to folks maybe who are not on a data bricks platform um delta sharing could be a a way out. Uh unity catalog is um um tied to a meta store by region.
(51:35) So when you are going to go in a into a different region and you have to share data, data sharing becomes another way. Marketplace is a way for you to get to third party data like maybe weather data or credit bureau data or um some financial transaction data, SMP um data um done in Brad Street data, Bloomberg data all of that should be made like third party data should be made available in marketplace.
(52:02) Um we talked about Unity catalog being the uh governance layer. It takes care of not only cataloging but data lineage and access control and it's for not only for files and um tables but also for models and features. Um and then data uh data bricks IQ is the the intelligence engine which is sitting in between and watching and observing and making your life a little easy.
(52:28) Uh so for instance um a lot of you use assistant in your notebooks you can use assistant in the SQL editor just in the portal um and then we'll talk about genie rooms where using structured data you can ask any question of the data that's pretty pretty cool it's using LLM's behind the scenes to uh in fact an ensemble of agents uh to take natural language and generate SQL out of it uh execute it show you visualization explain to you how things are doing.
(52:59) Um, and in the middle here is of course Spark and the next generation of Spark was a rewrite in C++. Spark originally was in Scala. Photon is in C++. So it's just vectorized and uh um a little smarter uh a little more performant um and um newer, right? So it's it's as I said it's a rewrite of uh Spark.
(53:23) Um when you use SQL you automatically use uh photon when you're using serverless also you're using photon you just don't know it um we will have a session on DT DT is delta life tables um it's um contributed to the open source and it's referred to as spark um declarative pipelines the um the name on data bricks is lakeflow um the reason why this is different is because want you to understand that the core piece of it is open source.
(53:54) Anybody and everybody should be able to do whatever you built on data bricks, you can take it back to open source. Whatever you built in open source, you can bring it back. But because data bricks is a managed platform and if you're on the platform and you're paying a premium for it, you get additional features like you'll get um uh you you'll get um an integrated IT environment.
(54:17) uh you you have be now going to introduce a designer where you can have dra drag and drop ability a lot of things. Uh on top of this um uh area is where you have data science and uh uh we we first finished the data engineering along with the workflows and pipelines and everything. Today we are doing the data warehousing and then starting from um not next class but the class after that we'll get into the ML side of the house.
(54:41) uh that doesn't mean we'll touch on what is a model and like you know all the different uh nitty-gritty details about how you will create models but how do you manage the models how do you do um ML on large data data that cannot fit into a single machine uh how you going to uh once you create this model how you going to protect it or how do you know when there is drift and so that you have to retrain the model what is feature serving how how can you reuse it what is vector search in um unstructured data.
(55:11) So all of this comes into the category of data science and geni and workflows which Rahm had shown you last time is about your orchestration. Um at the boundary of this is analysis. So we can have um dashboards which we are going to look at today. We can have leakhouse apps. So you can actually build an app very quickly on the data bricks platform.
(55:40) You can have external BI tools tap into uh the data through the uh data brick SQL endpoint and it'll be very performant. Uh you can have third party apps as well not just the lakehouse apps uh to um you know you can have like other serving layers like an RDS or a Dynamo DB or what have you consume from it because once the data is like ready primed analytic ready as you call it um it's it can be used in a variety of different ways.
(56:08) Um again outside it there could also be additional integrations. There could be an identity provider. They could be enterprise cataloges like Alians or Calibbras. They could be you know um the cloud native services like maybe if you're on AWS there could be like some models you could be using from bedrock uh you could be tapping into hugging face.
(56:33) So all in all the thing to emphasize here is that when you start thinking of your flow and how things are related and uh you're not going to probably use all of this. You start fading it out. But remember this source in J transform query and process serve analysis and integration on this side. Write out all the sources and start filling in the boxes.
(56:53) So somebody who looks at it will understand how your use case is uh being laid out, what you plan, how you planning on architecting it. Okay. Um so data brick SQL delivers some analytics on the freshest data because your pipeline has just hydrated it. When you have a workflows one of the nodes, one of the tasks that you can create is actually a dashboard.
(57:18) So it's going to rehydrate or refresh the dashboard as newer data comes in. Um uh data engineers uh like people who who could write code and all uh it's not just for them. Data bricks is for the data science folks and for the BI folks. So um DBSQL is the home for these analysts. Uh they want to use the other portions of the platform. They'll probably gravitate more towards this NCSQL. So your code is very portable.
(57:46) Uh open foundations again. Um and uh not only can you do dashboarding straight on the data bricks platform but you could also have a broad integration of other BI tools uh like the Tableau and the PowerBI and the lookers of the world that can also hook into the lake data.
(58:06) So they can continue to do their sophisticated visualization of the freshest data without like earlier what was happen. So let's just contrast it earlier all these BI tools you had to first uh take a subset of the data put it into a warehouse and then have the BI tool talk to the warehouse because the BI tools typically could not talk to the lake.
(58:27) that with data bricks with delta with with you know all that uh innovation that data bricks has done you can tap into the lake um all the data in the lake instead of just a subset of it or a time snapshot of it or so on and that's huge like there's a whole lot of problems that has been uh eradicated by providing that um direct hookup it's a better price performance than any other cloud data warehouse you can simplify discovery and sharing of new insights so we showed unity catalog how we show the unified search interface, simplified administration and governance, setting up the catalog, discovering data, viewing your lineage. Uh we look at lake view dashboards today
(59:02) and we look at genie spaces today. Uh so again these are mental images of how you should think of the various layers. You've got your raw data all kinds of data structured semuctured unstructured that's your lake. Now why it becomes governed or why does it become um uh you know trustworthy because delta is a protocol that uh ensures your asset compliance that's the first thing your optimizations your reliability your um comp your asset compliance is given and then governance as to who can do what uh
(59:36) where is my lineage where is my audit that comes through unity catalog the intelligence engine is sitting and watching and helping uh making your job everyday job easier. Then you have the data science personas, you have the data engineering personas, you have the devops um and workflow persona and now we this is the BIBA business analyst persona.
(1:00:00) So like before from the landing zone to the injection zone you could buy forkit like maybe there is one stream which is needed immediately for presentation. So you'll put it into your uh warehouse where um maybe revenue recognition report does not need any transformation. You can use it as is.
(1:00:24) But perhaps to get a sales forecast like any predictive uh value out of the data um a revenue recognition forecast uh instead of a report. Report means something which has already happened right there's nothing much to do but predictive means what will happen what can happen are these kind of forecast reports where you might have to do some curation might have to do some prediction on the data.
(1:00:49) Uh and this is just a recap because um we have um um we have looked at modeling in maybe our second or third class where we said that in object- oriented programming you have um constructs like you have to say is a has a so something is a means it's a parent child relationship. Something has a means it's got an aggregate relation an association relationship.
(1:01:17) Typically you look at the nouns and the verbs and you uh and you know what are the relations and what are the entities and then we also looked at how you start with a semantic model uh view of the world uh where you just understand what needs to be done then you bring it down to a logical view and then depending on the text stack that you have you translate it into a physical view.
(1:01:36) The patterns here we talked about uh the third normal form the bill inmon uh and the kimble strategies. Um so Bill and Mon was all about third normal form tight referential integrity and uh Kimble was about the dimensional modeling. We talked about the star schema where you have the facts followed by the dimensions and the snowflake schema where you have the facts with the dimensions and perhaps even more dimensions.
(1:02:02) Um so the query are optimized to support these BI use cases. We also talked a little bit about data vault, why it was uh created because it's supposed to be more flexible, allows for rights. Um it has constructs like hubs where your core businesses uh concepts are stored like say your products and your uh customers.
(1:02:22) And then your links are the uh relationships, the primary key and the foreign key between the various hubs and your satellite are your business semantics, your descriptive attributes. Um when you have a vault, remember we said that your silver layer could have a vault, but um again in your gold layer, the reporting layer is typically back to a dimensional model.
(1:02:49) So u this is just uh to remind you uh how we were thinking about the various hubs, the links and the satellites um in the context of bronze, silver, gold um data engineering or analytics engineering. So when a hardcore data engineer works, it's data engineering. When a business analyst or a SQL persona tries to do all the same task and by the way they absolutely can, then that's typically referred to as analytics engineering.
(1:03:19) Um they will curate and provide access to the gold level tables for the rest of the organization. Uh and usually follows the best practices with proper modeling that we talked about earlier. Uh works well for less technical users. um as well as serving external users. So people will sell their data. There are companies which uh like the whole they exist because of just that uh one business model. Uh end users typically do less service.
(1:03:42) The last mile ETL um the heavy lift is not typically done by the business analyst. They do slightly simpler uh version of it. But regardless, it's either the silver layer, usually the gold layer, but sometimes the silver layer, which becomes um the target uh for your um reports and your dashboards.
(1:04:08) Now let's look at the different people who are involved. So you've got these data integrators who are bringing in the data. So there could be data engineers who do ETL and curate. Then the data scientists will add their ML insights. So they are constantly working on refining and adding insights.
(1:04:29) The data steward um is responsible for setup and administration like um onboarding new users, onboarding new workspaces, making sure access privileges are there, um central query logs, usage attribution, who's using, who are the power users, how much compute has been used, who's going to pay the bill, debugging, troubleshooting. That's the data steward.
(1:04:48) Then the business analyst or the BI uh persona is going to come and write queries. They are reasonably comfortable with the SQL. They'll write queries. They'll understand the data. Um and they'll create maybe dashboards and reports. Right? So this is the SQL endpoint which provides the compute to be able to do so.
(1:05:09) Um uh and then comes the um executive persona who is going to maybe not write reports per se but consume the reports that uh one of these guys has created. Most probably it's the BI analyst report that is being consumed by a business exec. Uh when we were talking about streaming and batch we talked about you know different things like volume, variety, velocity.
(1:05:38) In streaming we talked about TPS which is the transactions per second um a peak query and so on. When you talk about a BI um uh use case the two parameters or the two KPIs that are very important to understand whether this platform can handle your use case or not is concurrency and scaling. Concurrency refers to how many uh simultaneous queries can be thrown at the system without the system getting absolutely slow and unresponsive.
(1:06:12) Um and scaling is uh as the data grows uh can uh this still keep up with it. Um why is concurrency important? Because when you're looking at a dashboard there are so many different widgets. they are all being hydrated by different queries and those queries are cached. It's not that it's unusual that every time a person comes in it will um refire if it is static data that that is typically cached and they are able to view it.
(1:06:36) But as more data comes in that warehouse should be able to tolerate five people and then 10 people and then 20 people and five uh gigs and 10 gigs and 20 gigs. So the scale should happen and the concurrency should also happen. Usually concurrency in case of BI is a very hard problem and um you should be careful make sure that u that um uh aligns with your use cases.
(1:07:04) So you will know that maybe uh 200 analysts are going to work on it. they're going to hit the SQL uh endpoint and the um the compute should be available uh for them when they come in and when they write their query their query should execute they don't have to wait so long for the uh initial cluster to come up or you know have bottleneck issues and today when we look at the uh query um performance the query profile you'll see when are the queries being executed they'll be green and when are the queries being cued they'll be
(1:07:37) yellow. An ideal situation or an ideal sizing is when most of it is green and there is very little yellow. So that means uh most of the queries are getting the compute they need as and when they're arriving and then some of them are being cued so that your compute is not wasted.
(1:07:55) On the data brick SQL you've got the editor with autocomplete. So it's not just a notebook. It's a full-fledged editor. You can have parameterized queries which means um I am region X, somebody else's region Y. I don't even care what is in region Y. So when I come in, I'll give only region X and I care about my data. There are built-in visualizations um that you would be able to build very easily.
(1:08:19) But you can also use natural language to be able to build these dashboards. Uh they can refresh uh you have alerting mechanism. So you can say that if this query produces uh less than this many rows then maybe something is wrong with my pipeline and I should my CEO should not see this dashboard.
(1:08:38) Let me get an alert and take a look at it and fix it before somebody notices it. And there are built-in connectors for BI tools. So that makes it rounded holistic for not only usage on folks who are onboarded on data bricks but also folks who are around the ecosystem who can also tap into it. So we talked about federated data warehouse where you're joining multiple sources.
(1:09:07) Um excuse me anendita I think sema has a question. Yes. Yes. Sema. Yeah on a slide 20. Um if you go back yeah so um the SQL endpoint I I believe the meta store is nothing but the uh databases right? different databases. No, metas store in our case is purely logical and we said that um uh meta store is um needed for unity catalog that's the hierarchy.
(1:09:43) Uh it is by region and um from a meta store you will have your cataloges and then underneath your catalog you have your schema. underneath your schema you've got your uh tables and your views and your dashboards and your files and so on. Mhm. Okay. Okay. And how the SQL endpoint connect to the meta store then like will uh will that be a drivers or something connected? So just like um you know you don't fully see the driver and worker nodes.
(1:10:10) Similarly here the metas store is in charge of you can almost think of it as a driver node for every um catalog um level operations that are going on but it's purely logical at that point it's for a demarcation it's for name spacing um and it is to hold as a container um what did we have have earlier you had a database name and then you had uh your tables right now it's a three name space so it's your catalog your uh schema or database that they are kind of used interchangeably and then your table. So metas store is like the whole container just like you have a
(1:10:50) workspace in which you are working but you don't really touch anything of the workspace right everything is there similarly think of metas store as the topmost level construct which holds the catalog and your schemas and your tables got it okay that's thanks now you can have this is probably wrong.
(1:11:19) There's no virtual data warehouse but uh virtual um uh views and uh materialized views on the data with ales and caching. Um so we had talked about the difference between views and uh materialized views a while back. Materialized view have their own storage.
(1:11:41) So it's premputed queries which have been run with complex joins and so it becomes easy for uh your consumers to directly hit it. Um so if you have it on an auto refresh uh then you know their performance is they will not even notice that whereas in a view view does not have storage it is off a table and uh you may create it because you don't want some sensitive fields to be shown and you might create multiple views of a table but every time a person hits it uh the underlying query is being uh run because there's no place to store it.
(1:12:11) uh caching streaming union of hot data some cache data um ETL despite support for federation and uh virtualization so people say why doesn't everything get virtualized then why isn't everything federated because you can right uh well if you do that it is going out pulling small amount of data so remember how I said it's for modest data for large volumes of data you cannot uh um you cannot support it through federation so whenever you need uh low latency, high throughput um um from source then you know ETL is your best choice. Uh sometimes you may have time series databases, data warehouses um you know
(1:12:48) you you will need a window functions and stuff like that. Uh so your data lake basically captures as much of your data your meta store is for your definitions like everything is kind of being uh stored right there. uh support schema evolution and um intelligent data warehousing is where uh access is for everyone to ask questions to of their data in natural languages and we'll see that uh tonight.
(1:13:14) um automated management and tuning. Again, because of your use of serverless, you can be assured that behind the scenes things are being optimized for you. If not, you would have had to take care of uh a lot of things and that will also result in optimum total cost of ownership so that there are no inadvertent human errors. Um what does EDA stand for? Explor exploratory data analysis. Absolutely.
(1:13:48) So whenever you have data uh you have to have some component of EDA. So you should be able to use the SQL functions to do exploratory data analysis through the editor with intelligent autocomplete and NCSQL. um you would be able to see the query history um and be able to profile um use the data explorer and we won't talk much about data sharing now towards the later half we will uh connectivity um and again these are best shown but let's just go over that the bullet points and then we'll come back to it performance is photon engine with predictive um IO uh there is um uh there's query
(1:14:29) federation there's materialized view, there's workflows integration, uh you can have Python UDFs uh in uh SQL as well. You have notebook integration, you have geospatial capabilities. Um serverless data warehouse.
(1:14:49) So it's what is serverless? Serverless is essentially compute is running in uh the VMs are already up and running uh in the data bricks environment. The minute you ask for it, it's given to you. uh in non-serverless environment the longest amount of time is to get these VMs from the cloud ecosystem especially if you have say 20 nodes to acquire those 20 nodes and then put the spark uh software on top of it and patch it and like you know get it ready and give it to you takes solid 3 to 6 minutes and um people get impatient with that. So serless definitely is a better experience from that perspective and high concurrency BI
(1:15:22) to handle your intelligent workloads as well as your um results caching. There are in fact multi multi- layers of uh caching. You have caching on the disks. Uh that's the SSD caching. You have caching um of your uh results. Uh and then um there is one more layer I'm I'm forgetting. governed and secured by Unity catalog.
(1:15:49) So at this point all of you must have played with your own cataloges. Uh you would have created cataloges and schema. So you kind of know it. Uh and discovery means to be able to go into the unified search and look for what you want and it'll give you results by some kind of a usage priority and by categorization.
(1:16:12) So maybe you have some notebooks and you you have some dashboards. They all have the same name but they will come in different sections. Uh federation lineage compliance enhanced security and auditing. So what is auditing? Anybody? It's like how the data flow through um a source to destination like what are different that is more like lineage. No that is more like lineage.
(1:16:39) So how the data is flowing what are the transformations that is data lineage auditing is like validating the data. Sorry I what did you say? Is it like validating and verifying the data? No auditing is in this case is who is doing what with these tables because some of these uh tables might be sensitive tables. um and uh if one particular table is being uh in high demand and a lot of people are using it, maybe there's some valuable data in it.
(1:17:08) So audit means understanding uh your data assets that you have and who is trying to access what. For instance, there might be an internal um threat uh issue in which there is a sensitive table. A person does not have access but they are trying different ways of doing it.
(1:17:32) So every time I try to do it, there will be a line written that I tried to access this table and it was successful or not successful at this time and I used it for this amount. All of that is like an internal audit that is happening. So in if you're working in a regulated industry, uh your auditors may want to see this kind of information to see how you are safeguarding your end customers data.
(1:17:54) uh so you have to prove to them that tables with PII information is only accessible by HR or or whoever and not everybody. So you they would want to see that um these people were trying to access it or could not access it or what not. All right. Now um if you combine the query federation and remember we said query federation is when the data is not uh in data bricks but it is being uh managed by some external data store like maybe postgress or synapse or snowflake or my SQL what have you red shift that is federation and materialized view is you can join these
(1:18:39) and have the query pre-agregated and ready for your end user. users to access. So putting these two together uh gives you a lot of power. You can accelerate these across joints across uh multiple um very very disparit data sources along with your uh data in your lake as well.
(1:19:01) uh we haven't talked much about uh streaming tables because we haven't talked about uh lakeflow declarative pipelines or DTS but you can create a streaming tables um as select star from a stream and you can read um this files so from cloud storage you are bringing in any new data that is coming in is landing into this uh streaming table and this is SQL so you can do this in Python you can do this in SQL you can get it from message cues as well.
(1:19:33) So what are message cues? Remember the cafkas, the kinesis and any pubsub type of system. So here you will say create streaming table as select. Now these things usually are in JSON format. So you will first have to explore it from JSON and then respond to it. Now the benefits of this is of course the real time use case.
(1:19:55) um you are a BI persona who was like at the end of the tunnel where everything has happened and then you are notified that such and such has happened. No, you are now much more empowered to be able to get the insights and the notifications and the data as soon as it arrives. So the ability to support real time for BI scenarios apart from you know everything else that was happening on the data engineering side more scalability so efficiently handle high volumes of data and these could be incremental data so uh this incremental processing is important because uh it's very easy to do kill and fill large bad
(1:20:31) jobs but that incremental stuff that CDCD that we were talking about understanding change data feed and what has changed and what needs to be updated is pretty hard stuff. Uh you can enable more practitioners because almost everybody is very very well familiar with the SQL and it does not appear as intimidating as maybe some of the data engineering side of the things.
(1:20:57) Uh like streaming tables we have uh we can create materialized views where we'll select something from this and we'll join against this and this table has its own storage and is available. So again the benefits of this is your BI dashboards are going to run much faster. Um when the data is not available and the computer is still going on they'll see the older data.
(1:21:17) The minute this is available they'll see the newer data and it is cached and it is made available for the next series of people who are hitting the same dashboard. It will reduce the data processing costs as we talked about and it's going to improve data access control. So you can govern um instead of giving the entire tables um over to somebody um you you will control exactly which set of columns that they are going to be able to see for off the base tables orchestration from your SQL.
(1:21:52) So when we were talking about workflows uh and we had um this type here you can specify SQL and your SQL task can be a dashboard. So this data is going to hydrate your dashboard directly. How powerful is this? So you you have a data engineer, you have an ML persona and you have a BI persona all stitched together through a workflow. Uh and this is the alerts.
(1:22:12) So if your volume of data has dropped significantly, you you would know that something is wrong and you want to be the first person to be made aware of it. So there are observability um dashboards. uh the every day these jobs are running. You can see how long it's taking, how much data it's bringing, uh who's uh run it and you know whether it has uh run successfully or failed and you can have alerts to a slack channel or pager duty or an email or what have you.
(1:22:42) Now this is where uh it gets even more interesting. So LLMs are not just for chat GPT um interfaces or uh the data science interfaces but even in simple SQL which is this so select star right you're selecting something but in between you have embedded something called as an AI query you are calling out to a model so you can write the model name here and you can give your prompt and maybe um there are some parameters uh like the product name or the skew that you can do here.
(1:23:18) So you are you are embedding um a model an LLM model with a task uh as part of this AI query in your select statement from your transactional data and that is pretty enriching. So that's exactly what it says here. Integrate any LLMs in SQL to enrich data and empower analysts. This is AI query. Um there is another thing called as AI function in which you don't even have to provide uh a model name.
(1:23:48) There are some inbuilt tasks like sentiment analysis or um um uh what is it extract information or um summarize or you know those there are about 15 or 20 of them that you will just say AI function and that and it will just pro provide an additional column with those uh uh details for you. Was there a question? Maybe not.
(1:24:19) Uh, empower analysts uh to extract these um actionable insights uh using LLM. So, here are some uh that we were just talking about. Um it's called AI analyze sentiment and you give a text. Obviously, this text is is like hardcoded, but this text can be part of your data that has come in. Maybe it's a customer representative that you're talking to and they have the transcript is can be fed and said analyze uh the sentiment uh or you can classify um uh to say whether this is an urgent request or not so urgent request or very easily you can classify in triage things you can extract you have text you want
(1:25:02) to tell it now show me where is my social security number or maybe pick up the name and It's going to give you those additional fields uh and you can specify that I want this to be um yes Sema. So I think this is great. Uh my question is mainly like uh this AI queries like in in embedded in the select statement or in SQL statement uh how how long it takes to uh resolve that first because I believe uh when in order to execute a whole statement first AI statement or prompt should get resolved and followed by the query.
(1:25:40) Yeah. So these are like little nuggets which have the prompt inside it and is made available to you as a function. Mhm. So the there is no additional prompt. It knows what prompt it is. So its task is to just um take the text and say positive or negative. So it's like a quick band-aid given to you to use.
(1:26:07) And there are lots of cases where we want sentiment analysis. So instead of having to build one or whatever, you you just uh use it right within your SQL query. Now with regards to the additional time if you didn't run it versus like if you just selected this text and then you used one with doing obviously this is going to take a little more time but it's well worth it because you are getting an additional insight out of it. Got it? You can fix grammar.
(1:26:35) You can ask it to mask uh some sensitive data. You can ask for similarity. Again as I mentioned there are about 15 of these. Um then the last one in fact I will be in uh New York on uh next Wednesday at the special uh conference and it's uh an exciting time because we have got H3 support in the platform and we have some geospatial support right in SQL as well.
(1:27:09) So you can do there are uh parameters like geometry and geography um that is going to in fact Noah was is it was you right you are doing a project on covering the entire world with hexagons or maybe squares or some structure and then finding out uh pathways that's a very interesting project your idea to create indexes and network yes did you try that yes Yeah. Mhm. Very nice. Okay.
(1:27:36) So, efficient storage for spatial data in both large and small bins, faster joins because we are now talking about lots and lots and lots of data. So, my particular um topic for Wednesday would be uh there are geospatial info uh information on floods and fires, natural calamities.
(1:28:02) So, you kind of know what's the sensitivity of your particular location, right? And then when a catastrophe really happens, you want to know which area is affected. And in there, if I'm an insurance company and I have insured certain property there, then how much payout I have to give like what is it to my business? Those are interesting because they're all layering uh businesses and then bounding and then being able to react very quickly uh to so much of chaos and so much of data that is going back and forth. the third party data is very important as well and visualization is is super important.
(1:28:35) Um all in all the takeaway here is that data brick SQL is a complete data warehouse. You can do um you know data engineering and ETL type stuff. You can handle it for scale and performance and you can use it as a native uh uh business intelligence and data warehousing uh ecosystem.
(1:28:59) So things like autoloader, materialized views, streaming tables, lineage, um primary key, foreign keys, erds, you can actually if you have the primary key and foreign key and this is important. It does not enforce it like traditional databases because big data systems cannot do it but it helps you understand the relationship in the data.
(1:29:20) um SQL and CSQL by default. Um tabbed uh SQL editor. Uh you notebooks on SQL warehouses. Uh when you write your notebook, you can you can specify your compute to be a SQL warehouse. Um the execution API so you can write rest APIs. So you can hit that endpoint. Python UDFs can be written in your SQL editor. Um you have rowle security, column masking.
(1:29:46) So not just at the table level giving grants but even at a finer level to say that these rows are sensitive and should be viewable only by this. We won't get into so much of details but you should know that it is possible and these columns are sensitive. That is what is it means by row level filtering and column level masking and you have the schema browser which you're already using. You are using serverless there is autoscaling happening for you behind the scenes.
(1:30:10) uh adaptive uh routing, predictive optimizations. These are all to improve the performance. Um uh results caching, liquid clustering that is the positioning of your data. Um query scheduleuler, so it's very very integrated to your workflows. You can see your history and there is system tables where all your usage data is being recorded.
(1:30:35) So at some point you would be able to say uh who did what and how should u uh your lines of businesses have a chargeback uh policy or maybe some rogue users would have to be reprimanded. Um there's data bricks assistant uh for lake view um delta sharing we talked about um sharing out from outside two data bricks from data bricks to outside between two data bricks system across regions there lots of options marketplace data rooms clean rooms are built on top of it we'll come to it later integrations lots of partners and Tableau are very popular um BI tools that uh work off the data in the Lake uh
(1:31:18) DBT again is another tool that is very integrated strongly integrated. These are um partners who bring in data from uh onrem systems or um you know mainframes and whatnot. So you know once they land the data directly in delta then the autoloader can kick in and uh the whole process can start. There's oath cloud fetch there's just so much.
(1:31:45) All right. So, with that, uh hopefully you can go to your um you can go to your uh catalog and you will see that there is a delta share that all of you would have received by default. In there there would be something called as New York City taxi and there is a table here called trips.
(1:32:25) Uh it has got um a pickup uh time, a drop off time, the trip distance, the fair amount, the zip code for pickup and for drop off. You can look at the sample uh data. It has to be attached to a cluster for you to be able to see it. You can see the details, the permissions maybe right now uh without uh compute being attached you can't see uh there's history there's quality and so on.
(1:32:50) So that's the data that we are going to use. Now I want you to go to dashboards and you'll see um two um dashboards out of the box. One will be the New York City taxi trip analysis. The other would be your workspace usage. So go ahead, click on it and you would see something like this. Come. I'll give you a minute so you can all get settled.
(1:33:24) And at the top here you have two tabs. One is the data tab and then there is a summary tab. In the data tab there will be uh the trips file and then there is a route uh revenue um and this is the data that is going to be used to hydrate this um dashboard. Now, at the bottom here, you see this little blue box.
(1:33:58) Um, you have you can add a visualization widget. You can add a text box, you can add a filter. So, this is an example of a text box. So, you can edit it, you can uh drag it around and so on, right? And so, if I edit it, you can see you can choose what is the size, what's the color, bold, etc. just simple stuff to kind of tell you what this is. So that's a text. These are filters.
(1:34:27) So when I click on this, it's giving me some information on this side and it says that okay uh this is a widget which is using these values. Now this is a pickup time. It's a drop down. So let me move this around a little. So here um you can this this is a drop-own but it can have multiple values or this is a a drop off zip and that also can have multiple values.
(1:34:58) So I can click on it and see all the zip codes that are there from the table. Right? You can add more filters if you want and just by dragging it and by you know clicking on this adding it here and then playing with it. Now what about this? So this is like a count. It says total trips that have been made.
(1:35:18) And do you see this symbol here? So that is the assistant which is there in your notebooks. It's there in your SQL editor and now it's here on your dashboard as well. So you can refer to it and ask it some questions in natural language and it's going to do it for you. Um these are different um uh different types of visualizations. So let's see if we can create one. Let's go ahead uh add a data visualization.
(1:35:41) So it's come up right here and this is my my uh assistant. It's in fact suggesting uh some uh total revenue by route revenue distribution uh trips by drop off zone route revenue promotion so many things. Um let's just look at number of trips by zip uh code right. So it's um it's produced uh a code like you know some visualization for me.
(1:36:14) On the x-axis I see the number of trips. On the y-axis these are the zip codes. Obviously not every number here is going to be um in full view. So there is a clustering that is happening here. If you go to the left hand side you can see the more details like this widget has a title. Yes. And you see the title is number of trips by pickup zip. If you want, you can change it.
(1:36:38) Do you want to add a description? You can do so. What's the data set it is using? Remember the two data sets that we saw? So, it's using the route revenue. What kind of a visualization? It's using a bar chart. Uh maybe we want to have this as a scatter or uh let's see is there a scatter here? Something like this.
(1:37:01) And then you can uh change it uh to say that instead of a bar I want a a scatter. and it's going to change this. So it's by default it thought that a bar is the most likely thing. So that is how it did it. You can also specify here what kind of visualization you want. But the main thing that I'm trying to highlight is it maybe there might be times where it doesn't get you. It maybe it's wrong. Maybe you have to work with it to fix it.
(1:37:25) But the interesting thing is what it has done here. You can see here you can give it additional stuff and you can either accept or reject this. And that's it. Your dashboard is going to be created in a matter of minutes. You can choose your colors, your size, your tool tip, your annotation and so on so forth. So there are three main things.
(1:37:42) Your visualization which is off the data, your uh text boxes just for uh you know documentation and your filters to kind of make your uh dashboards a little more um uh dynamic and uh intelligent. Okay. So, let's get rid of this. Um, any questions? So, behind the scene, it's uh data uh datab bricks SQL or it's a normal SQL queries.
(1:38:17) See, this is where I think we have to um be careful as to what SQL we are talking about. So, datab bricks SQL uh is referring to this entire section. So, this is the data engineering persona. This is the ML persona and today we are exploring the SQL persona. So everything here is referred to as DBSQL but the query language is NCSQL which is a universal format. Um okay.
(1:38:41) Yeah. So you can take this query and run it in any NCSQL um editor or engine and it should work. And uh will it be also supports like AI related functions which we saw previously? Yes. Yes, absolutely. Um, so here in the SQL editor, you can uh refer to some of your tables that you had earlier and start uh uh running um and it's going to attach to the compute.
(1:39:16) You can even use um generate code with the assistant. Um and what are some of the other things that you can do? So this is basically a a typical SQL editor and it's multitab so you can have multiple tabs that are on and it will attach to compute. So the most important thing here is your warehouse which is your SQL endpoint and in my case it's called serverless starter warehouse or what have you.
(1:39:40) You can see that there is a a size here and because it's serverless it's not allowing you only when you attach to it it will it will come up right. Um but uh behind the scenes actually let's go inside it. Uh you can see that currently it is uh stopped um after 10 minutes of inactivity. So if I've not been using it, it'll automatically stop. It was created by me.
(1:40:07) These are the connection details. So remember how we said Tableau can connect to it, PowerBI can connect to it, dbt, Python, Java, NodeJS, Go and there are more tools. I I it's not all listed here but you have the full list here perhaps right um there if it is like a JDBC yes Daniel yeah professor just um on the dashboard how is what is the advantage of this dashboard I saw that there was a an option to publish but what is the advantage of using the dashboard through data bricks versus for example powerbi or tableau very good question yes so these You do not have to pay any licensing fee for
(1:40:48) these. It you can build it up in a matter of minutes. Um so then the next question would be why would you use this versus um a PowerBI? Now your CEO may want a very very sophisticated he might care about the blue color here and the yellow color here and the 3D effect here.
(1:41:09) So for those kind of things like you know PowerBI should be the right choice. But you have tons and tons of operational dashboards. for that we would recommend that this is the easier thing to do. So actually let's go back to what Daniel was just uh saying and go back to this and the publish because I kind of uh did not mention it.
(1:41:27) Uh this is the developer view of it. Um you can use it you can share this internally uh with other people right to manage or to view or whatever but you can also share it outside um to a person who is not on data bricks and that is where you will publish it with embedded credentials.
(1:41:52) So that means when somebody is using it and it has not been run and you know if it has been run the results are cached for 24 hours but if it has not been run any compute they use is going to come out of your um account right but it's still pretty good because you you are giving it to somebody to be able to see it and um if they are using it and they are able to uh make better decisions based out of it you know cost as to who's who's giving it is a different thing.
(1:42:17) So when you embed credentials and usually when you give it to somebody outside the organization you will embed the credentials and publish it. We can do that right now. And at this point you can also enable Genie on it. So Genie is this uh no code tool that will help answer uh questions about your dashboards.
(1:42:39) Um and let's paint the picture of when you would use Genie and when you would use these AIBI dashboards. um somebody will tell you that I need these kind of widgets and these dashboard because I look at it and I look at it the whole day and make some decisions. So you'll create it for you to give it to them.
(1:42:57) But they are like data experts, domain experts. They will say, "Oh, I wish I had told you to do this." Uh but now just to go back and forth between the business user and the IT user takes weeks. You don't want that. You'll say, "Okay, I've given you the foundational think of what you had asked for.
(1:43:15) any small tweaks here and there, you can just ask it in natural language and manage it on your own. That is when Genie comes in handy and they can play with it and ask these questions not in SQL but in natural language and it will answer and it'll generate the SQL for you. So let's go ahead and publish this. So dashboard is uh published and you can view the publish dashboard here.
(1:43:36) So if I were to give this uh dashboard to uh some any one of you, you would still be able to use it and if you need to and if it has not been refreshed and 24 hours has gone by or whatever the time is for however long the results was cached then I will incur the cost and so it's a very very powerful feature.
(1:43:55) It's not that all the people have to be onboarded on data bricks. You publish it out like this. External people can also view your dashboards, right? You can put it up on a schedule here. So, uh you can um specify what kind of a refresh. Right now, this was 1 hour back. I might say that I want this dashboard to be refreshed every 15 minutes or might put it in my workflow as one of the tasks and in which case as my new data comes then the work my dashboard will automatically get refreshed.
(1:44:27) Um, so is that clear as to when you would use like all your data is on the lake. You're you're uh uh you're creating these dashboards in a matter of minutes by talking to it or making small tweaks. So it's super easy. You have um control over who can access it.
(1:44:47) So you as the owner of course has full privileges but you will determine who are the other people who can access it who are within the databicks ecosystem and externally you will publish it. So those are some of the key points. Thank you, professor. Mhm. Now that uh I published it and this is the published view of it. Uh you can you see the icon has changed.
(1:45:15) Earlier it was just that rhombus like thing to get the assistant to write questions. Now I'm this is saying ask genie some questions and genie is going to pop up and it's going to um you can ask a question here. So we can ask the same questions like um um how how what is an average trip duration? What is an average trip duration? I'm going to make this a little bigger so we can all see it.
(1:46:05) Uh so let me basically the same thing. I made it a little bigger so we can all see it. Um I asked it uh what is an average uh trim duration. So it's asking me you want to find out the average uh duration in minutes. Uh and it says it's 13.7. How do I know this is correct or not? So I will look at what is the code it generated to compute it.
(1:46:28) So it is going to use the average function uh and takes the drop off time and the pickup time takes the difference uh computes the average from all these trips where the pickup time is not null and the drop off time is not null. That looks pretty good to me. I think I can believe that that number of 13 whatever is indeed correct.
(1:46:54) But sometimes you it may not uh it may be hallucinating a little because it didn't understand your question properly. So you can go into the instruction sets and give it some examples like you will say by trip duration I want the duration to be in seconds not minutes.
(1:47:13) So it will instead of it telling you that it gave it to you in minutes maybe it's going to give it to you based on you know your additional instructions that you have configured for it. And these are um SQL queries that you can give as well and say that these are very tricky things and I want I don't want you to get it wrong. So if they ask for this then use the SQL function.
(1:47:33) So it's very very very powerful and it it won't hallucinate most of the times. It might misunderstand your question and your intent because you are not clear but it will not hallucinate because it's not going to the internet and picking up some data. It is looking only at the metadata of your tables that you have given to it. So it's not looking at all the tables.
(1:47:51) It's looking at only the tables that you gave to it in the data canvas and uh trying to address your questions to the most optimum way that it can. Yes, Jeremy. Yeah. So the uh the AI to its non-deterministic, right? It's like an LLM. So it's like it's it's so I mean I think you might answer the question.
(1:48:16) So if if if precision is necessary you just you would use it to help you get the queries and run the queries yourself. Uh absolutely but you see there is no LLM here. This is pure SQL. Yeah. Okay. Talking about AI query and AI function. We had LLMs there and you could embed that in your SQL. But here it's not making up anything. So if you ask a question like uh what is the sentiment of the people or something like that it'll say no I'm not going to answer that because you know that's got nothing to do with the metadata or the data set asking me. Yeah.
(1:48:47) Okay. Thank you. Because this is for the business user. So when you compare this with chat GBD chd whatever question you ask it tries its level best to please you which means it'll stretch it'll bend it will like give you an answer.
(1:49:06) Here it'll tell you now sorry I don't even know what the hell you're talking about or can you clarify or can you give me something here for instance it told me in advance that okay I'm honoring your request but remember this is in minutes because you didn't specify that I asked it what's the average trip duration but I didn't say in minutes that is what a precise ask would have been thank you has a question yeah something about the cost like so if If I publish the query uh you mentioned like whenever uh the the query or the dash dashboard is used by the external user I'm going to um the one who is u publishing the the dashboard is going to
(1:49:47) bear the cost. So how is there is any view where we can see like um it's like to track what is the cost and uh who that cost goes to like computes like um um cloud computings or uh data bricks platform um it will be both but in serverless like there is a cost that is coming because of the underlying uh compute and uh infrastructure your storage your VMs and then there is a data bricks uh skew associated with um um the use of the platform and the particular feature and the compute that you're using. Uh so both of them are rolled together in
(1:50:25) serverless. So you don't get two bills, you get a single bill like if you were to use data bricks, not the free edition, you would get a single bill for the serverless um um you know endpoint that was up and for how long you used it. Did you use it for 1 minute or 1 hour or 10 hours or what have you? Mhm.
(1:50:45) Okay. Now in the same place that we were looking at the dashboards there was one more dashboard which is around workspace usage dashboard. So if you bring that up um you are going to uh I do not know whether it's populated or not. Uh this is there's a lot of data sets here and uh maybe this is somebody was trying to create it but it's not really full but there are dashboards that we offer based off um information that goes into the system tables on your usage and you would be able to track that's exactly how the admins do it. Now in this particular case you have only the
(1:51:31) workspace uh because you are a workspace admin it's your workspace but you don't see the account console because of the free edition. If you could see the account console then for each one of your workspace for all your usage you would be able to see uh you know on today I use two this much of uh SQL warehouse cost and job compute this much and vector search this much you would be able to see all that. Okay. Okay. Thank thank you so much.
(1:51:55) Mhm. Uh we we talked about I think I think Noah Noah has a question too. Oh yes, Noah. Maybe not. So you've got the I'm sorry I did not click on the uh mic. So, uh the genie will it come to um all like are we also going to be able to share the genie externally? Um more specifically maybe behind I know there's the genie API but uh it's genie widgets to bring it to your application.
(1:52:42) um the API is what you would be able to um share and then um in compute there's a concept of um apps and uh you can create an app and embed Genie there too. Okay. Mhm. Um queries is nothing but like you know some of your most sophisticated uh carefully curated ones. You can give it a name and refer to it, reuse it and so on. dashboards.
(1:53:10) We saw Genie we saw you can create a new genie room alerts is um where you can create an alert of a query. you'll say that for this query for this condition um I want it to be alerted maybe the first time or every time it occurs and I want the notification to go to my pager or my slack notification or my email or what have you and advanced stuff right so this is just so that you are ahead of the game query history is every query that you have run is logged right here and in fact that is one of the ways of um it's more than just the compute part.
(1:53:47) It actually shows you who's doing what on the system like right now um uh if I look at in the last uh 7 days um for my serverless compute or my pipeline compute um or for duration which is greater than 1 hour or for the status which is failed. Let's see if there is anything which has failed.
(1:54:16) You can see three of these failed. You can those are the more interesting ones and you can see why they failed, right? Um maybe here it will tell you um it's a permission denied issue and this was the query time uh how much time was spent in executing and how much time was spent in uh uh waiting uh so on.
(1:54:40) So let's actually remove this and go to the standard one. Um these are all describe history. Let's go into a select. Um, so it'll show you more details around the query profile. Um, like right up to the amount of time that was spent, the memory peak uh you'll have to look at it more and the number of rows it was returning.
(1:55:09) So earlier we could uh see the spark UI, the ganglia metrics and so on so forth. And now in serverless and with using the SQL these are some of the details that you would be able to see the duration the sources the um like you know two rows when you did this only two rows were when you generated one row was returned and then this this had a full scan. So if you want to optimize your query in some manner, this is a way to do so.
(1:55:36) And the administrator is also concerned about seeing which are the more heavy users, the longer duration, the failed queries that somebody would go and be able to help fix. And we were talking about um the warehouse itself. So you can click on it and we were seeing the connection details.
(1:55:56) So lots of external tools can connect to it and these are the details like so if a JDBC connection you take this and you can connect to your endpoint if your ser SQL warehouse is running it could uh connect uh and then uh for monitoring remember how I said you can see how many queries completed what were running what were peak capacities and so on.
(1:56:17) So uh these are the running clusters. Um actually we don't have that much of activity so it's not uh as interesting but in this case this is blue and orange. So blue is the peak running and the orange are the ones where they were cued. Uh so all of this was running and this was so obviously more of blue and a little bit of orange is a good sign.
(1:56:45) uh the completed query count, running clusters and of course all your other queries. So you have the monitoring right at your fingertips. You have the connections uh all the tooling how to connect and these are these are the uh warehouse endpoints and that's the compute running only when it is running can all your you know rest of your stuff uh um connect to it and get you the data.
(1:57:09) So let's see is this uh currently running. Yes, it is running. And remember at the beginning when I went into the catalog I was not able to see some details because there was no compute running. So now let's go back and look at exactly the same thing. Um I could see the overview that was not a problem.
(1:57:29) Um but I could not see the metadata or maybe sample data because you need some computer to be able to pull that. Now it can and it's showing me um the details was coming through but um permission cannot be viewed or edited on sample data because this is uh a shared data and there was no history or there was no quality because this is not really ETL.
(1:57:52) So when we talked about federation and delta sharing and ETL so those constructs and concepts should be very clear in your mind. Um that is essentially all that I had in mind to kind of cover from a DBSQL perspective, warehousing. Um there are tons and tons of other stuff but I think we covered most of the major ones.
(1:58:34) Any last minute questions? Okay. All right. Thanks everyone. I have a question. Sorry. Um um do do maybe we have already said about this. Um do do we already have a date for the quiz one, two, three? Because I have a big trip coming up. I want to make sure I have a plan enough time for Yeah. Yeah.
(1:59:07) Um I think um I will release it um and give you like maybe two weeks to complete it. Um let's see. We released uh assignment three recently, right? Was it this week or the last? When did we release? This week. Yeah, this week. Uh so when is your trip? we can uh help you make sure that you uh well as long as I'm not traveling um I have to go visit my husband's family which is really really far from the US uh so it will be really like as long as I'm not on a plane I have enough time to basically oh okay first let me make it clear that for a quiz it's not like I'll close it in a day time or so I'll still give you a week
(1:59:48) okay so you can take it anytime time and it should not take you very long. You guys have already done quite a bit of labs and assignments and sections and office hours and everything else. So, it should not be bad at all and it's not a day. So, you'll have enough time. Okay, sounds good.
(2:00:06) Yeah, I I will be traveling in about 3 weeks, but I think I should be fine. I have enough time to finish. Okay. Is the quiz is what's marked as case study in the syllabus? No, the quiz will be separate and then um you know we will be starting our case studies right after that after quiz one. Uh we have two case studies.
(2:00:33) Uh the first one is more around um a design on data engineering and ML and the second one would be more around integration and understanding the data ecosystem around you in terms of like uh do you understand warehouses? Do you understand MLOps? Do you understand you know quality frameworks? Those kind of things.
(2:00:54) So very often in your company you would be asked to um make a choice like work with the vendor, work with the partner, select something and once those decisions are made it's very hard to revert to those. So when you have to do a lot of due diligence in choosing the right tools and frameworks and so this is just an exercise to force you to think holistically uh what kind of a requirements is important for you, how do you go about comparing tools and so on.
(2:01:20) And then along the same lines as what Landy was saying, I also have a trip upcoming on October 30th. So um let's get a quiz. I don't know when you are planning to release a quiz, but if it can be released before then is if releasing early is a good idea, we'll release it this week itself. Yeah, I mean very helpful to me. Okay, sure. Thank you. Okay, good night everyone. Good night. Thank you. Good night.