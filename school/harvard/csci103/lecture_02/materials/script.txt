103 day2 - YouTube
httpswww.youtube.comwatchv=4049-Mk3pX0

Transcript
(0001) Hi everyone. I think before we start with the class, it might be useful uh to touch on a few things around the assignment. Any burning questions Otherwise, u Paul will just remind you of some um some things to keep in mind around your assignment. Uh hi, I I was uh curious if you can use the AI assistant inside the data bricks.
(0034) Um so that is that allowed is that needs to be disclosed Yes, you can use it definitely because um you know the purpose of this is to ensure that you learn concepts and we are in an age where you cannot escape um all these assistants and co-pilots around you and so to use it um to your benefit um by asking it the right questions and getting the job done is definitely one thing.
(0103) I would also say that there is something about muscle memory being able to write uh the simplest of things. Um so if we didn't know ABC we wouldn't be able to read right. So what we are trying to teach you in this course is the ABCs. So even if you do get the answer through your assistants and your co-pilots and and whatnot um do make an attempt at first that way you'll retain that information longer.
(0132) Thank you. Um maybe if if there's time I can ask one more question. There's this um I think it was 2.9 uh about creating a database. Um what is creating a D like I see that there's catalog scheme table but um what there's a there's a question about creating a database and that I didn't understand fully. Okay.
(0200) Um Paul do you want to go into it I think the database was created right at the top. Uh so perhaps we don't need to create it. Uh but we are creating a table inside the database. Yeah, sure. Sorry, I I I was on mute. Um no, in 2.9 we have this code here that will create a catalog for you.
(0226) If it doesn't exist, you probably already created it and then it'll make sure you're using that catalog. So that's the active catalog. So any any SQL commands you uh you uh you run after that will be doing it against objects in that catalog and then create a schema. That's creating a database. So schema and database are interchangeable terms. Um okay that I got stuck there.
(0251) I was thinking okay how do I create a database I okay so schema is the database. Yeah, you're basically creating tables under the schema. Correct. So, you just run this code and it'll create it for you if it doesn't already exist and make sure that you've selected those.
(0314) So, if I go to my catalog, right, and I go to see this catalog under um I never created it here. So, let me go back to that. Uh, we just go back so I can run this code. It's going to be there in a second. Charlie's starting the cluster. So now if I refresh this, I'll see I have that assignment 01. I don't have any objects in that schema yet, but I have it. Right So you'll end up creating objects.
(0421) Does that make sense Yes, thank you. Okay, I have a follow-up question, please. So, on the question, it says uh the database has to be called underscore assignment one instead of the name that is there. So, how follow along with the name Uh yeah, that that's um that's that's I'll fix that. But follow along with the name here.
(0448) It should be um really it's just like th this is instructions from last year when we were all sharing an environment. we had to do this like username_assignment one but now because you're in your own environment you can we just so we just need to uh you know it's really you know run the the cell below to create the catalog also also I'm looking at it right now. And I feel like my task is like 2.
(0529) 9 is different on my site than what you have here. My task says pers all data in CSV format into a table. Um, so we no longer are doing that. I thought I did that. Maybe if I look at the version that I have online, we don't need to do the CSV table. So I hadn't updated the instruction. I did later and re-uploaded it.
(0552) Maybe that's not what you're downloading. Is that the only part you changed or do I need Yeah, you're still Yeah, that's the only thing is how you're creating the database and um or schema. And you don't need to do the CSV, just the delta table. You might want to download it again just to eyeball it, not to have to redo your work, but just eyeball it.
(0617) Uh but Paul, can you go to the volumes part Maybe if I look here for a second, Uh, yeah. Oh, I still have it there. We don't need to do that. That should get the Yeah, you don't need to do the CSV. So, that that that's a change I think I made. Maybe I never uploaded it, so I apologize. Okay, so you can skip the CSV. Okay, I'll take a screenshot of like I will watch this again and I will copy paste this stuff here.
(0658) Yeah, I I are are are they downloading these from Google Drive or they're downloading them from um right now Google Drive. So once we update the Google Drive so if they've started the assignment it's going to be a little uh difficult. So just eyeball it one one time. I don't think there are too many changes but just to be on the safe side sometimes we do find uh small uh you know uh writing stuff which can confuse the user so we update it but all good.
(0730) Now uh Paul do you want to go to the the volume spot because that was confusing. Yes. Okay. So the so the other thing that I saw there was a lot of back and forth questioning on the um on Slack about was how to get the data right like when the data assets are all available for you like to download from Google Drive or from Harvard Canvas and I mean it seemed to me like maybe some folks were trying to find data sets online or it's okay if you don't do do it exactly this way.
(0807) It just for consistencyy's sake, you know, when we're grading these things, it's if it's everyone sort of follow this, then it's not it's not going to be a problem. But on Google, right, in the assignment 01 folder, there's the assignment notebook and then there's a data directory. And in the data directory, there's the J the the blog JSON, which is used in section four.
(0834) There's the people 10 million data which is used which has one parquet file in it which is used in section two and then in section three we introduce this names data set. So um you need to create a volume. So, you know, I mean, I could sort of do this again, right Like I would create a volume, you know, it's just going to be a manage volume. And I'm going to say create that. Now, um I'm going to create uh a directory in there called people nm.
(0929) And I'm going to create another directory in there called names. Oops. Create it here. I'm going to create that. Okay. And then I'm going to upload some a couple things. So, I'm going to say upload to this volume. And I've already downloaded this and I have it locally.
(1016) So, I'm going to take this blog JSON and I'm going to upload that just to the to the assignment 01 directory. And then I'm going to come into people uh 10 million. And I'm going to upload to this volume by going in here and grabbing this parket file. Yeah. So, you can call the directory whatever you want. drop the file in there because the schema of each of these files are very different and when you're reading them then you know refer to either the directory directly or the file path otherwise you're going to be like possibly mixing up the various data files. Yeah. So, you know, with parquet, you can have multiple files and they can be
(1052) organized in subdirectories. And so, very often when you're loading parquet data, you're not referring to an individual file unless you just need to inspect a file. You're more likely um referencing the top level directory and that's sort of think of that as your table name, right Uh so so that's really the best way to organize this data in how it will be easiest to access it because you can just I can now when I want to load this uh file here right if I come back here right if I want to actually load this
(1130) and I want to know what to call it I can just copy that path and now if I was in a notebook Uh just it's going to give me that's all I need to now to know how to get to that table. So everything else will be how do you load a parquet file using spa spark but if I want to know how to get to the file that's where it is.
(1206) Also the way you read those different formats is uh going to require a different uh syntax like you can't say.json and read a parket file or vice versa. So you have to use the right file format when you know what the data type is. Um last time moan had also shown you how to do an uh fs ls fs head so you can inspect the files manually as well. Yeah. So hopefully that should um help you get uh going with your assignment.
(1233) If you have any other questions, uh reach out on Slack and um on through office hours or um any other mean. Now let's get back into lecture mode. All right. Thanks a lot, Paul. Turn it over to you. Eric, actually. Oh, Derek. Sorry. Take it away, Eric. All right. Just a minute. I'll share my screen. Okay, you should be able to hear me and see my screen. Yes.
(1330) Okay, so um welcome everyone. This is lecture two. Tonight we'll talk about uh data modeling and metadata. So for the agenda, we'll quickly review the previous lecture. Uh talk about data modeling approaches met uh discuss metadata, data formats, data compression, data profiling and then um review some best practices and then second half of the lecture will be um our second lab or lab 01.
(1401) So first uh let's u review some uh some terms and and concepts from last class. So acid um you can already see what acid stands for atomicity consistency isolation and durability of um usually associated with relational um databases. Uh in contrast space stands for and anyone can shout out the answer basic available sufficate. That's right.
(1442) Um so um basically um u eventual eventual consistency is a key thing here and and this relates to the cap theorem that Anadita talked about last week where um you can have um consistency, availability and partition tolerance, but you can only have two of the three. And um and so relational databases um provide avail um consistency over availability and and most NoSQL systems support availability over consistency.
(1523) So, I AAS and PAS and and there's a a third one that we talked about last week. Software as a service. That's right. Uh software as a service and um Okay. Lakehouse combines the best characteristics of two things, data warehouse and data lake. Yeah, that's right. And DAG stands for directed as graph. Very good.
(1606) And um so the biggest challenges with data are let's see the data quality and staleness of data. So and that that'll be um somewhat the focus of tonight's lecture. Uh the three main uh data types are structured, semistructured, and unstructured. Yeah. And what's an example of unstructured data Um text um text files, word documents.
(1648) Yeah. And also images and video and things like that that wouldn't wouldn't have a a schema. Uh, ETL stands for extraction. Yeah. And um, Spark Spark architecture consists of a driver and nodes. Oh yeah, one or more worker nodes. So that's how the um, the map reduce works. it it um maps out the work to the worker nodes and then reduces back the results.
(1734) Um, spark um is considered polygalat because why can be used with different languages like Python Yeah. So it's it's it's it supports many different languages. Why why is Spark um 100 times faster than Hadoop It uses memory instead of hard drive. Yeah.
(1809) So um in instead of instead of writing the results of each map reduce operation to disk it it keeps it in memory and um so that greatly enhances the performance. And then the five big five Vs of big data are I reviewed this last week. um volume, velocity, uh veracity, variety, and value. Yeah. Yeah. Very good. So, um and veracity, what does veracity mean I think it's like how accurate basically the the correctness of the data or validity of the data.
(1904) Okay, good. All right. Um, are any questions about any of this Okay. Okay. So, um leading into the um discussion on on um data modeling and metadata tonight, uh let's think about the amount of data that um is being collected every day. It's um 2.5 quintilion uh bytes of data which is 10 to the 18th um bytes of data which is huge huge amount and it's it's only increasing and there's there's many many different sources of data now that um as as uh data scientists and and data engineers that we use to um do um data analytics
(2004) and other other things with data. but including uh documents uh social media uh cloud-based data um websites, internet of things that are collecting information from like smartwatches and and manufacturing uh doorbells, all sorts of things, databases, uh uh profiles, user profiles from social networks, um uh producers of data, um like social influencers uh activity uh generated data uh warehouses and um data warehouses and also um monitoring um systems.
(2055) So many many different types of uh data sources and all producing um different um types of data and at the same time this data may relate with each other sources of data but it's um it's quite a challenge to um to manage um these this data because of the size of the data and the and the variety of the data that we're collecting.
(2130) So um let's let's look at um modeling as a way to help um deal with this complexity. So um basically when we're modeling data we want we're organizing the data um so that it um in in terms of business processes. Uh the um the modeling includes uh logical relationships and how the data relates um to um other data to support the business and also we can think of um a conceptual model that um business users can use to help understand the data and um schema as well as how to um consume it. So uh we we start with the uh conceptual or semantic data model that really
(2218) defines the the business model of the data. And you can think of um like in the the banking example um accounts and transactions and customers and and um other other types of entities uh that might be at the semantic level that the business would understand. And then um a further refinement is the logical data model that um defines provides more more information about the structure of the data, the relationships of the data, the types of um properties that are associated with the data.
(2255) And then finally um the physical data model which is the data model that we use to actually store it in some sort of um a a um data data storage solution like a for example a relational database. So how how is the data model to fit into that um technology whatever it might be. So uh we we we care about data modeling because it helps us um get the most value out of the um data by understanding the properties and relationships um in the data.
(2341) And then we can the model also um provides a visualization to help us understand the um data and how it um relates to itself and um and it also um relates to the operational processes and and also the structure of the generated data. So things to think about as we're as we're considering data modeling, we want to design or um come up with a model that helps us um persist the data and and allow retrieving the data in the most efficient way.
(2424) uh the as I mentioned the visual representation of the data helps us um understand the data and it also allows us to check for inconsistencies or um errors in the in the model. So being able to visualize and see the see the model makes it easier to um to find and correct um errors in the model and um it provides a common vocabulary or documentation.
(2457) So um every everyone in the organization can refer to the data the same way and that helps um prevent confusion and um and then also by modeling and this is um consistent with uh design. Uh we can by having a model we can identify and and and um correct um errors in the model early on in the process versus later on. And the earlier we discover issues, the the less expensive they are to correct.
(2532) So again this this shows the flow of um the modeling. We start with a um logical data model and then convert that into a physical data model. And once we have the physical data model we can actually load data into that model and um then derive um business value from it. So the um the process models and data requirements flow into the logical model.
(2600) The technical requirements and performance requirements often um help define the physical data model. And then the business data is what's fed in to the model to um allow access to the data. Okay, let's look at an example. So um starting with conceptual model, we have um a customer and a and a product.
(2630) And we can see that the customer um basically purchases the product and we also have some information about the customer and their address and uh example of of what that might look like. So uh kind of a rough um sketch of of the model from a um business perspective. And then uh we can move on to the logical data model which is a more refined uh view of the data uh where we now we have we still have the customer and the product and um but now we've added the types to the um properties and we've given the properties name names here customer name and customer number
(2711) and also um assigned um types to them and then also here's the customer and this is an example example um of the customer and the address as a um as a representation of that logical data model. And then finally we can move to the physical data model where um and this is the model that we would actually use to store the data into in this case a relational database.
(2745) So the customer um is still associated with the product and we we have um now we have some additional information about the primary keys used and the unique key for the product and we see an example of a a customer um and the customer's address. But in this case, we've actually added an additional join table to support this many to many um relationship between uh customers and and addresses.
(2816) And here we also have a usage code that we've added um that's of two characters to um define the the type of address that the customer is associated with. So um a multi-step process to go from the conceptual model that the the the business model um to the actual storage model that we use to store the data. So um with um NoSQL data modeling so and in in contrast to uh more traditional relational modeling we have um more choices for how we um store the data and because more choices of how we store the data there's um more nuances in how we model the data. So we have um key value
(2915) stores um big tables document document databases um graph databases and also SQL databases. So um with um no SQL we can have things like key value stores that are just um a key and a value and s Amazon's S3 is an example of that and then um big tables are um basically the like Google's big table was the first um example of a a large a database that could store instead of millions or billions of rows like uh like huge huge numbers of um rows and and and very large amounts of data to to scale to um the needs to support the needs of like a company like Google is
(3008) managing huge amounts of data. But uh Dynamo DB, HB and Cassandra are examples of um big tables as well as Google's big table. And then um there's document uh stores like MongoDB and couch DB that um basically store uh data in in the form of documents like some sort of key and document and then full text search um lucine elastic solar and then graph databases like Neo4j.
(3041) So these are all examples of um NoSQL uh data stores and um and and each one has different um characteristics in terms of how you would model the physical um data model. So there some relationship between um model complexity and scalability where key value stores are the the most uh scalable but also um maybe the least complex and then graph databases are the most complex but maybe um not as scalable because of that complexity.
(3123) When when we're um thinking about data modeling, we want to think about like how how will users use the data How will um how can we identify patterns and and partition or bin the data And uh how often is the data added or updated And these are all considerations to um determine maybe the best way to um to to model the data or and also maybe the selection of the physical data store as well.
(3158) Amir, do you have a question Yeah, the the picture of the or the photo of the apes at the top of the slide, I guess somehow suggests that SQL is more advanced than than the NoSQL variant. Is that true or intended Is it referring to the complexity or is well yeah I mean um roughly true I think um you know the what you can do with a SQL with a SQL database is often um you know uh you can often do more in terms of with a SQL database because it supports things like joins and um more um fine grained access to the data where um key value stores you can think of
(3246) like compare it to a key value store where you just have a key like the customer ID and some value maybe um maybe like the customer's name or something maybe another way to look at it would be that um SQL is so popular that even these other systems will offer some kind of a SQL like interface base. So for instance, if you look at data bricks, data bricks is in the cloud.
(3313) It's supposed to be spark. It was an offshoot from Hadoop. Um and so it should ideally be no SQL and it is but at the same time NCSQL is a first class citizen on the data bricks platform just shows how utterly popular SQL is and um you know SQL engines or use of SQL kind of triumphs all these various u data stores. Cassandra provides a very complex um SQL like experience.
(3344) Um and again if you look at each one of them they will have an interface which kind of feels like SQL. So this just says that well we started with SQL and we have made a full circle and then we are back at SQL but handling big data perhaps. I don't know this this picture was just for the fun of it. Okay. Yeah.
(3408) I mean also one thing to consider is that SQL's been around since literally around since the 80s or maybe this even yeah the 80s and um so it's it's more maybe more mature and and has been developed further than a lot of these um other technologies that are more recent. Makes sense. Thank you. Sure. Um so even though we say um no SQL is schemafree, it doesn't mean that you don't need to think about the the model for the for the data. In fact, it's just really just as important as before.
(3450) It's just that with no SQL we have a lot more flexibility in in um in the type of data that we handle and like where in in um in contrast with with relational modeling before you can populate a relational database you have to define the schema including the tables and their uh you know primary keys and relationships to like foreign key relationships to other tables.
(3518) So um and and so with with NoSQL systems often you can load data without know like knowing beforehand exactly what the um the schema will be like maybe there's additional columns in some rows that weren't in other rows and and with no SQL that's okay with with SQL or relational uh databases that's um that could be an issue.
(3545) So um and and kind of just a a difference in thinking about these systems is with relational um databases. We think about what answers do I have based on the the the data and the structure of the data that I have like what what what can I answer And um with NoSQL systems, we we think more like well what questions does the business have what what questions do do I want to answer and and um position it that way.
(3619) And so it's less about the data and and the structure of the the data that we're given and more about what we want to do with the data. So um yeah, and with NoSQL um or or with with SQL we think about normalizing the data. With no SQL, it's the opposite. We want we wanted um we're okay with uh duplicate data and and denormalized data. In fact, it's it's um it's it's it's often required or or preferred.
(3652) So, and the reason for that is um joins are um not well supported in most NoSQL systems. It's it's you can't easily do a join between two tables. ers and that's because of the size of the data that we're dealing with. It's it just exceeds the memory. So usually um we we avoid joins um and and because of that um NoSQL systems support wide tables um where lots of data can be put into a single table and and when we query that table we can we can get everything out of it that we need and um and and they often support like millions or sometimes billions of columns versus relational uh
(3739) databases. systems are often limited to maybe thousands of columns and um and then also uh thinking about how the data is is linked. We can embed um data and or we can reference data. So here's an example of embedding data where uh we have an order um ID and then we instead of referring to a product in an in another table we actually include the product information in the order information.
(3820) So um so when we fetch this uh the order 00001 we not only get the information about the order but also the the products that were included in the order. and from a single query. And so and that helps make it fast where uh referenced uh we could still do this in a in a in a NoSQL system. And this would be more more like what we would um be familiar with with a relational system.
(3846) But in this case, the the order ID um includes a product ID as well. And then the product information is kept in a separate table. So if you were using a NoSQL system and storing it this way, how many how many queries would you have to do to pull out the information that you um that would take one query in the embedded system Can does anyone know So in the embedded system, we would just do one query to pull out the um order information with the with the associated products. How many queries would it take to do it using the reference system
(3933) Is it O I'm sorry. Is it O N Well, or in in this example like um since there's only one one product referenced would either be multiple requests or a join. Yeah. Well, um, if if it didn't support a join, how many how many queries would it take Two, two. Yeah. So, so anyhow, so so that's so you can you can store the data together and and um and and pull the data in one query or you can do it in two queries.
(4019) So, which do you think would be faster one query would be faster than two queries that are sequential. Yeah. Yeah. So, so yeah. So the um having it this way even though it it seems weird to us coming from a relational background um it's actually more efficient and and and often done this way just because of the speed even though it's it's maybe duplicating data and um and and and denormalized rather than normalized.
(4057) some rules of thumb for modeling with um SQL data stores. Uh as I was saying, normalizations are are um used to reduce data redundancy and improve uh integrity of the data. But with NoSQL data stores, we normally want to denormalize the data to prov provide improve the read performance.
(4124) and um and even though it's it's maybe making the data larger through redundancy and um and and again no SQL data stores have either no or minimal support for joins with data marts um and we'll we'll talk more about this um we want to use snowflake schema or star schemas and we'll we'll talk about what those are tonight and then with graph data stores.
(4153) Um we use edges and vertices um with each with properties to capture data about a system. And uh graph data stores are um popular for things like um social social platforms like Facebook was the first to to really use um graph data stores uh to um manage their data about their users and and um their relationships with each other.
(4226) And then um in memory data stores uh key value pairs and and provide quick access. So if you need really fast access for for something um you can use in-memory data stores which act as very fast key value stores for for quick lookups for um information. But um again the um flexibility here it's limited because it's basically you have a key and you get get some value back and um not a lot of structure there.
(4257) So um now let's let's um think about metadata and um the management of metadata. So this this picture shows the data um at visible to the to the um to the eye but um underneath it there's often lots of metadata that helps describe the data and we'll we'll see what this is.
(4328) So uh first metadata is um data about data and it helps us um understand and describe the data and uh it's used by uh organizations to uh define uh informationational assets and um and and helps that understanding that we get through the metadata um helps improve um the business value that we get from the data.
(4358) So if we understand the data, where it came from, how it can be used, then we can get um better better use out of it. So um some things that um metadata can help address is like okay things around like who created the data um who's responsible for managing the data who's using the data who owns the data um who's regulating it and um and then also questions around like uh what's the business definition of the data what are are there any rules around using the data security levels, um abbreviations or acronyms for the data and u maybe naming standards. And then where is the
(4442) data stored Uh where does the data come from Where's the data used Uh where's the backup for the data And and are there any um regional restrictions around the data And then why um why are we storing the data what's its usage and purpose and what are the business drivers for using this data and and again all of these things can are can be captured and and um provided by um metadata.
(4516) When when was the data created Um when was the data last updated How long should it be stored And when when should it be purged or deleted And then um and then some questions around how the data is formatted and also um how many databases or data sources store this value data like um so if I need to delete it um clean up the data for some reason where do I need to go to do that So a lot of a lot of valuable information um provided by metadata that can be used by the the business to or organization to help um manage the data
(4559) and use it and and understand it and get get more um business value from the data. Um so drives business value and innovation and collaboration and mitigates risk. And so I think we things that drive this are data governance, data quality and um trust. uh um the growing complexity of the data um requires like um being able to have metadata that helps describe the data helps manage the complexity of of the data.
(4642) We saw earlier all the different data sources and types of data that we work with. Um having the metadata that helps define the data um helps us um get get value out of it. Um also many more um business users are actually accessing the data directly um through data marts and other other means. Uh so it's important for for the consumers of that data to be able to you know see see the answer to you know all of these questions here so that they can decide whether or not the data is is can be useful to them.
(4726) and um and then the metadata helps accelerate um use of the data within the organization. There's different types of metadata including business metadata um and then technical metadata and then uh operational data made metadata. So um the um business metadata defines um things like ontology, business rules and KPIs and then um technical metadata gets more into the details of the the models and and lineage and um and how the data is stored and then operational um metadata provides um information about SLAs's and freshness of the data
(4814) and also usage patterns. So um to one way to um help manage metadata or store metadata is through cataloges. Cataloges are basically a a technical resource for storing metadata and um so and they through through cataloges we can support discovery uh like uh discovering the data governance of the data u privacy and risk related to the data and also the data value like um through through knowing the key assets and and um in economic importance for the organization and then there's um different catalog providers. In fact, there's a a catalog
(4915) within um data bricks itself. Um but um they they support different um features including glossery lineage of the data uh collaboration support um profiling of the data, data quality, uh workflows associated with the data and and also how the data was prepped.
(4946) And uh you can see that not these different solutions provide different um levels of support for um these different um uh features or capabilities. But uh but cataloges are again a way to um capture and and share and and store um metadata. Okay. So um now let's talk about OOLTP systems versus OLAP systems. So, LLTP systems are things like um entity relation type data stores that are good at um uh processing uh transactional type data like and with um with random reads or writes of the data to to quickly store and and access data. And then um OLAP systems or online analytical processing
(5048) systems uh support analytics and uh usually mostly um support reads or queries of of large um u data stores to provide um information for business users and u uses um either a star or snowflake schema and um and and data warehouse as an example. example of of of a um OLAP type system. often OLTP systems feed into OLAP or um that like billing systems or other other types of databases will use some sort of ETL process to extract, transform and load the data into an OLAP system that can then be used by business users to
(5141) access the data efficiently. So um some differences between OLTP systems and OLAP systems is um you know um day-to-day operations sources decision support um more um applicationoriented versus subject-oriented um uh OLAP system support ad hoc queries um OLTP readr versus um scans and and complex queries.
(5221) And um OLAP systems can support terabytes of data where um OLTP TP systems like at least traditional ones like relational databases can support gigabytes. And um and the measurement for O the performance of OLTP systems is um throughput transaction throughput versus OLAP which is more based on the number of queries that the system can support.
(5256) Again uh OLTP systems are used to collect data and store data and then OLAP systems are used to share the data with business users for analytics. Here's an example of a ER diagram that we would use for to maybe define a OOLTP system using a relational database. So um you can see the ER model captures the entities um the attributes of those entities and the relationships between the entities and um and there's very very um very um technical and specific in terms of defining the actual types of um fields that will be used to or columns that will be used to store the data and also the relationship ships and foreign
(5348) keys between the tables. So, um I think at this point I'll hand it over to Anandita to talk about um star versus snowflake databases. Okay, sounds good. Let me share screen. So Eric just talked to you about um uh entity erd diagrams. They were very popular and uh your second assignment is going to talk about it.
(5434) So just take a look as to how you model um the objects around you as entity and u the relationship the cardality all of those are important things so that if you give that diagram to somebody else they kind of understand what the data is all about and how you have modeled it. Uh but from an analytic perspective you have dimensional models and there the two ones that are common are star and uh snowflake.
(5500) uh it's all centered around having fact tables and then having dimensions around it. Uh to give an idea uh when you're making a transaction that's a fact table but the customer is involved so that's um a dimension table. Uh maybe the product catalog is involved that could be a dimension table. The address to which it is shipped could be a different table.
(5517) The price points are different in different countries that could be its own table. So that's a very simple star schema. In case of um snowflake schema um you a dimension can also refer to another dimension. So there is a hierarchy that is built in here. Um that's the primary difference.
(5538) So as you can see snowflake can become very complex very quickly. So people uh stick to star mostly. One fact table surrounded by dimension tables. One fact table surrounded by a dimension table which in turn can again be surrounded by other dimension tables. So you can uh create these relationships between the fact and uh uh dimensions and here you may require many joins to fetch the data because this implies another join. So it becomes very complex but it is more normalized.
(5604) This may be slightly denormalized um in an effort to uh run faster. Uh high level of data redundancy as you can see uh this gets it into a more normalized form. Uh and single dimension table can contain aggregator data. cube processing is faster. So um I do not know how many of you are still in the um in in uh areas where you refer to OLAP cubes but that was the thing in the past.
(5634) These uh cubes would be constructed uh once using several dimensions and be made available to analytical users. Um the flip side is that um uh you are your data is going to get stale because it takes a significant amount of time to create these uh cubes. In the modern age um people still use cubes but they are going out of fashion because uh access to compute and uh you know latencies have improved so much.
(5702) Um but in a quick takeaway is that star schema offers higher performance and tables may be connected with multi-dimensions and here it represented by the central fact table but which is unlikely connected with multiple u dimensions. There are ways in which you can contrast your ER modeling which is typically done on um your transaction processing systems to your dimensional data modeling which is typically done on your analytical processing. Several dimensions you can look at how data is processed.
(5733) Uh one is transaction based, one is just um area based, subject oriented based. I want to look at my sales versus I want to look at my um marketing channels or I want to look at my customer churn. So those are analytical questions and subject-oriented. Here it's purely transactions.
(5751) How people are interfacing with your business and how the data is flowing in. We had talked about CRUD in our last class stands for create, read, update, delete. So you can do usually all of these activities there mostly um updates though in this case it's mostly reads. Um many users are transacting with the system. Here a few users are building the analytics behind the scenes.
(5814) There are continuous updates, batch updates. That's not quite true these days because you can have real-time analytics as well. Uh supports business operations whereas this supports your strategic needs of the company. This is an operational database and this is an analytical or an information database.
(5833) From a modeling perspective, you want to um these systems are highly normalized. you want to eliminate the redundancy but as Eric mentioned just a little bit earlier u usually these systems are denormalized there's a lot of redundancy uh joints in fact sometime some years back were not even allowed or encouraged in some systems but now you can have a small number of joints but it comes with a price um natural keys um usually are used here um very often you use surrogate keys and in a future class we'll talk more about uh how surrogate keys are constructed. You validate against your business functional uh
(5911) analysis and here you validate against your reporting requirements. So when your CEO or your CFO needs their um reports and to be updated in the morning, it is in in these analytical systems that is that these uh functions are um u carried out. Um then if you look from an application uh perspective it's high transaction volume here you typically don't do too much of transaction although in some ways the analytical systems you saw how through an ETL process starts to mimic the um operational system but you know they there's some amount of buffering
(5949) and uh it's a batch or maybe it is a semi-real time um typically it's not as high from an incoming perspective you balance the needs of online versus schedule batch processing. Here this is for reporting. Um and then of course over time uh we opened it up to more ML use cases. There was classical ML then deep learning and now geni. Um highly volatile data lot of data redundancy.
(10016) This is OLTP applications and this is OLAP applications. So we kind of get the idea now. Oh do you want to share your camera I just noticed. Uh oh I see. Sorry. Um then let's look at some of the modeling techniques that have been popular uh over time. Uh it started with um uh Bill Inmon in early 1990s who um popularized a top-down approach.
(10045) So uh third normal form is where how the data warehouse was set up. So you have your data, it's come to your staging area, you've created your warehouse of your staging data and then you create different data marks for different functions and your OLAP cubes and everything else is happening here and different business users are consuming it.
(10109) So the the key takeaway here is that this is in third normal form highly normalized and that is the core of your data warehouse and these data marks um they are they can be denormalized but they are working off this main data warehouse. Then came Ralph uh Kimble and he used a bottomup approach and these two gentlemen are considered like our fathers of modern data data warehouses.
(10130) Uh and dimensional modeling was popularized by him which means there's some amount of denormalization that is going on. The star and snowflake schema that we talked about fact table surrounded by dimension tables. So you can see the difference right here. This is the data warehouse. It's not in third normal form.
(10147) It's already a dimensional table. And this uh you know your data marks instead of being derived from here it's the data marks which actually created and then your cubes and everything else goes in. Small minor differences but big like if you go out and talk to a fanatic on on these modeling techniques they will say they support inman or they support Kimble and so on.
(10212) Um then uh over time like in uh 2010 or so Dan came up with the data vault uh and this was uh to compensate for some of the deficiencies that existed in the earlier uh Inman and Kimble uh models and there were a new just like you had facts dimension star snowflake here you had a few new concepts introduced hubs represented the core business concepts links was the relationship between these business concepts And satellites stored the attributes.
(10239) Uh remember when in the entity relation diagrams how you had the entity and the entity had those relationships. So satellite stored attributes of these relationships. Hubs were the core business concepts and links.
(10258) Why do you think this happened Because they found it difficult to handle change in both of these u modeling techniques. And so this is what it looks like. As you can see this part remains exactly the same. This part remains exactly the same. It's the middle layer which changes a little bit. And the data warehouse is considered as the data vault. You have the raw vault to kind of capture data coming from source as is.
(10316) And then you have a business vault. And this is important because you some of your business terminology and everything else is uh kind of being applied here. Uh from which you have your different information uh marks. Um another look at taking um uh a scenario with um like what we were talking about one one table surrounded by you know your product your customer and your order.
(10343) Uh it would look like this in um a star schema and it would look like this in a vault. So your customer, your product and your order are the entities. Um and your hubs are going to be your unique uh business uh keys. Um your links are these relationships. So you can see that you know customer is an entity right it's a hub and then your links um are going to be the link connecting your customer to the product to the order nothing very different from what you wouldn't have uh logically or conceptually uh thought of and then all the history of the transactions is how it's captured later you're going to learn about sedd type
(10422) one and type two and you'll see why this small change in um um tweaking the modeling technique helps with managing change in the uh future. So it addresses specific use cases around auditing uh loading speed because in the beginning remember the raw vault and the business vault. So inserts can go in very quickly. They don't care about any updates.
(10446) It's slightly more resilient to change at the cost of complexity. Traceability of the data is better and of course all data is captured. Um we don't want to get you too wrapped up around these modeling techniques as you work with your specific organizations. You follow what is best practices there. Um we just want you to be aware that when you are given a data set, when you're given a problem, when you're given a data engineering u situation to solve, don't just rush to create your pipeline. Understand the data. And to understand the data, you have to have
(10516) some method to madness. And some of these are techniques of uh grappling with that complexity. um in any um form or in any technology platform that you may have uh encountered there will always be um consumers and producers. So your producers are on this side, your consumers are here.
(10540) Um what ultimately business users want is uh reports and ML insights and um um maybe dashboards and um uh PDF documents that needs to be sent out to their board. Right in the middle, your raw data has to be first filtered, cleansed and augmented. Uh so you might do some joints here. Um you might be able to normalize some of your data.
(10604) Um and then you have to roll them up. Uh maybe somebody wants reports by u like all the transactions, all the revenue for the day, for the hour, for the last 5 minutes. Uh they might want it for 3 months. All those aggregations happens in the gold layer of which reporting happens because reporting is um you know zooming uh out a little bit and here these are raw level. When needed somebody can still come here and be able to work off this data.
(10630) You will begin to appreciate the importance of conformed data. Remember this Lego diagram that we were seeing on last class. So when data comes in it's really unclean like this. when you sort it out, when you you know you apply the raw vault, your business vault, uh like all the yellows have to be stacked like this, then you come to a conformed vault that maybe you should not go beyond uh six levels or maybe all the yellows and the reds need to be together.
(10702) Some some form of uh business rules that are being uh uh put into place. Another example could be which I always give is um uh people ask you for your name, your address, um your uh zip code. Uh and somebody may give like uh a longish zip code. Somebody may give a short zip code. But if you are going to do some analysis, you want everything to come back and say no five digit is what I'm looking for.
(10727) Or if it is a gender field m and f or male and female or zero and one they all have to come to a way in which the final analytics needs to be need need not care how it came from the source data. So that is data conformance. You sort it, you arrange it, you group it, you bring in that consistency.
(10746) So anybody can start to use these Legos to build these uh wonderful uh reports. So these are essentially your reports and these are your ML um doing something beyond adding some more insights to it. From the source system you get the confirmed raw vault, uh the confirmed business vault and then finally your information vaults. Um again terminology will vary.
(10812) Uh we call it as bronze, silver, gold or somebody may call it as raw, refined and um uh aggregated or curated but it's all the same thing. If you look at how the data vault fits into the lakehouse uh then it's something like this. your enterprise applications is sending data. Your OLTP databases are sending data. You have a landing zone. Um we talked about CSV files, JSON files, parket files, delta files.
(10831) All of these uh different kinds of data. File formats are coming in. Um bronze is where you make a very few checks like maybe you add a datetime uh stamp or you add the file name or something like that. And in the silver you have the raw the business. This is where you join against different data sources and you normalize it.
(10851) Uh so data becomes really cleansed in the gold. You create your um uh star schema, your marks, your um uh your specialized views so that uh your different kinds of consumers can um uh read off that data. This slide I'm going to skip because uh you guys can take a look at it later. But in short, Inman is normalized structure. Um it is a single source of truth well suited for large enterprise data.
(10919) It is it can sometimes become complex to implement and maintain and slower query performance. So remember more number of uh tables because it is all third normal form. You have to join and join and join. Kimble is optimized for reporting and analytics is easy to understand. Uh but the cons is um because it's already a little denormalized.
(10938) You do not have a single source of truth and the data fault is primarily around operational flexibility easier to add new sources. The cons is the modeling complexity grows a little. So populating um these um in between um business boards and then eventually again creating the marks out of it is additional work that needs to be done and additional complexity that is introduced.
(11007) So key things to keep in mind is that the physical data modeling technique always depends on your underlying data store. So you can take a logical design but if you implement on this data stack um the physical implementation is going to differ. If you are going to implement on a different data stack you have to tweak it.
(11024) The conceptual remains the same, the logical remains the same but the physical implementation will vary. Design a system not a schema. The schema is important of course but at the end of the day you're going to get new uh data sets and you're going to get uh um your schema over time is going to evolve. We'll talk about schema uh modification, schema evolution and schema conformance later. So don't get too fixated on the schema.
(11051) Um think of the overall design that you are uh system design that you're building and you're going to have so many data sources you it's going to make your head real. So you can't possibly if you have never modeled before you're not going to be able to do it all overnight.
(11108) So you should look at what is your core business data that is critical to your business operations and start modeling that data first. Uh so that you don't get overwhelmed with all that that is around you. Uh we talked a little bit about metadata and you should think about improving the quality of the metadata. Sometimes people will write a table name is not very intuitive. The column fields are not very intuitive.
(11128) the there are no comments. So somebody else who's going to look at it from some time even you for that matter cannot remember why you built it, what was the intent, what was the purpose of in having uh certain cardality or certain primary keys uh and so on. Identify key consumption patterns.
(11147) uh you will start to see that um uh you have to bring in the data of course and you are at the mercy of the source system but after a certain point you have to also cater to your consu consumer needs and so are they quering u by location by date by product by customer so that you will uh have to create the right type of indexing uh clustering on the um on on the gold data sets.
(11218) Now we talked about the Inman, the Kimble uh and the vault style but then what about the uh specific uh industry verticals So there you nobody builds these from scratch over and over again. So if you're an uh manufacturing company, you go and look up the base standards of what that entity relationship should look like and then you tweak it to match your your particular um organizational needs. So that is called as DDD.
(11244) It's the domain driven design where there are services at the top and then there are these entities uh which are tied with these value objects and there are repositories in which um like you know there are so many uh teams within an organization typically called the lines of businesses. Within lines of businesses again there are sub teams.
(11304) So all of them have to come together and the whole organization has to understand the data model that they all create. So, open management group the link is here is um going to give you an idea of how these templates are there and how they can be tweaked. And so I've also linked the property and casualty data model uh for insurance just as an example.
(11328) So if you want to look up some other industry um model, domain model, object model, you can look it up from this. This is a very popular one. Um let's now quickly switch to choosing a data format. So that is the L in your ETL which is the storage format. Why is that important Because not all tools will support all data formats and sometimes you may get constrained by storage.
(11351) Sometimes you may get constrained by memory. You have to understand what the needs are and then look at how your raw data is structured. Um if you are dealing with large files and the files are not splitable that's a big problem having 20 nodes is not going to help you right because that fi that data is not going to if it is not splitable then you can't break it up and send it to these worker nodes. So understanding the those characteristics of your incoming data is important.
(11418) So if it's not splitable you lose all that parallelism that is possible in distributed computing systems. how many columns are being stored and how many columns are being used for analysis. Sometimes in big data systems there is no limit to the number of columns.
(11438) It's all constrained by the amount of memory that you have and so not all of them are needed for analysis. And if you want fast queries and if you don't care about uh 500 other columns that are there, why bother bringing it up So when you do a select star, think twice. Does your data change over time If it does, how often does it change and how does it change So, we'll talk about um uh change data uh feeds and uh change data capture later. This is CDC and Sedd. They are like uh terms we'll uh soon come across.
(11507) So, um in terms of data formats, the two main categories are text format, which is human readable like your CSVs and your JSONs and binary formats like the parkets, the AVOS, the OC formats. And then you have row based formats and column based formats. So your CSVs, JSONs, AVORO, they are row based.
(11526) Even if a row is a binary, it is still a row based. Don't assume that you know text is one category and binary is another category. Your OC, park etc are column based. And then the next question arises when to use what So role stores have the ability to write data quickly. So for transaction processing.
(11550) So that means OLTPS typically use row based uh systems and columns are good for aggregating large volumes of data. So one of the benefits is faster query speeds. So big data systems for analytics of course will use uh column based systems. Uh normalizations versus denormalizations. So column databases are prefer a denormalized data structure.
(11610) Not surprising because your OLAP systems are all denormalized and they typically used a column based approach whereas normalization of data make makes updates to some information much more efficient in a row store. So imagine if data is duplicated in 50 different places when you do one update you have to update all 50 places.
(11629) If your data is already normalized updating the data is better. So that's the OLTP part of the system. And this is uh again just contrasting a few formats using properties like is it column store, is it uh row oriented, is it compressible, is it splitable, is it readable, is it binary, is it complex, does it support schema evolution. These are all considerations while building your pipeline. That's why I said don't just rush to create some code.
(11655) Um think of what's the best system that is going to stand the test of time. These are some more um differences between row and u uh column. We kind of talked about it. So I'm going to uh skip um rules of thumb. Why is it not a good idea to use a text format You'll say that's JSON. I can I can read it. I can uh I can verify it.
(11720) So is it not easier for me to go through the process so I can check every step of the way Well, the utilization space utilization is not very good. Um numbers as strings waste space. um because strings automatically have um like a certain um size associated with it. So wherever possible values are good. Putting numerical values as the initial columns of your tables is also good.
(11745) No type or structural checking uh cars could wind up in numeric fields if you don't have it. Um CSVs um no metaphor metadata. So when you have a CSV file you have to remember that there is a metadata that is there in the beginning. In JSON um that metadata is repeated over and over again. So it becomes very voluminous.
(11805) So it takes up more storage, takes more time to process it as well and a lot of them do not have any compression associated with them. So even if you have repeating values, you cannot take any advantage of compression. Compression goes through it and knows that you these are the places where the same value is repeated. I just put a marker and you know they do so many things behind the scenes and no native indexing or search ability.
(11824) So all of these are reasons why you should u not look at text formats for all your big data use cases. Now both avo they are binary formats. In fact par is columner whereas avo is row based and one would again say okay which one is better. So park is better for query response and you know storage or capacity type of considerations where averro is better for schema evolution.
(11850) Uh we will touch on delta today but we'll talk about it more later. Uh delta is basically park plus some additional metadata um which will help with um additional properties like asset transaction schema evolution and so on. So what is delta Delta is an open-source data format.
(11911) It's almost it's a actually a protocol not even a format. The data format is park and delta is a protocol on top of it and it has these additional transaction logs which uh maintain what is happening. So if you look at um uh the data like you suppose your your um delta table is my table underneath it you will always find a directory called delta log where you'll see a your data is in um your park files but there will be a lot of other metadata in JSON files.
(11943) There might be some checkpoint files and so on. If you have partitioning then you will see the partition. So if this is partitioned by date then in that partition you will start to see your um your data and so on. Um why was u delta as a protocol created Um data bricks actually uh did this um almost in 2018 and um open sourced it in 2019. It brings reliability to big data. So you are in a distributed compute.
(12011) If one node fails, does not clean up, you have a bigger mess in your hands of how do I recover from this mess. Uh you can't control schema. All that wonderful functionality around begin transaction, end transaction, roll back transaction is suddenly lost in the Hadoop big data era. It was very hard uh to have reliable data because asset transaction or asset compliance on that data was lost. So Delta as a protocol attempts to do so.
(12040) Um rather than performing an expensive list operation on the blob storage um which is what the regular park reader does, the delta transaction log serves as the manifest. Uh so it tells you exactly what happens. Later we look at the history on the file. So you'll see exactly who created it, when was it modified, when did you do an upsert, when did you do a merge on it and so on. Um it simplifies upsert operation.
(12106) So upsert is a merge operation where with a single operation you're doing an update on those rows which exists and insert for those rows which are not there and then perhaps a delete for those that you know need to be eliminated. So doing it in a single operation is a big deal. Plus delta supports batch and streaming in a single uh manner.
(12127) So you you do not have to build a pipeline and think of it differently when you were doing batch and suddenly you decided to do streaming. It's just a configuration change. Now let's um look at data compression. These are concepts which you as a data engineer should be aware of. So the process of encoding information using less bits of data than the original representation is what compression is all about.
(12153) And uh why is compression important Of course, it saves disk uh space. Also, it reduces the IO bandwidth. So, every time you send data, you go to disk, you pull the query. If it's already compressed, there is fewer uh data points that is coming in over the internet, right So, you'll get your data much faster.
(12213) There are compression algorithms which fall in two categories, either lossy or lossless. So, there are some use cases where a lossy algorithm is perfectly fine. like an image which has got a very high resolution, you want to bring it down to a low resolution but you can still recognize what is there in the image.
(12231) So in that case lossy compression is okay because you lose a little bit of fidelity um some fine grain details but the human eye is not is not uh you know able to make that difference. But in a database where you are talking about uh um financial transactions and uh you know things which have to be reconciled, you can't afford a lossy compression. It has to be a very high fidelity compression otherwise it's useless.
(12258) Um so a codec is basically a compressor and a decompressor and all these compression algorithms they have to make a trade-off between the degree of compression and the speed of compression or decompression. Imagine if it takes a very long time to do the compression then the whole reason why you're doing the compression is kind of lost because when you have to read the data back it's going to take u too much time. So these are some popular codecs. These are their file extensions.
(12322) It says whether it's splitable, what is the degree of compression and what's the compression speed. Now let's look at um data uh profiling. So remember how we started out in our first lecture by saying that Gartner calls out that data staleness and data quality are the biggest problems that plague all data personas not just data engineers and it costs us businesses more than three del yeah that is the amount of time people spend with the queueing the code refixing it and so on and so it's important to uh review understand the quality uh so that not
(12358) just me as the owner of the pipeline but me as the consumer of some data is well aware of the quality of data before I build my precious use case on top of that data. So in spark using a data frame describe or summary uh you can see the uh details uh of your data set very very quickly.
(12419) You uh the advantages of course is uh you know you you have credibility in what you are offering. You make better decision making and uh you can um you can be more organized. The techniques is you can do a structural discovery. So whether the data is consistent and formatted correctly that is first important and these are basic statistics.
(12438) So your min, your median, your standard deviation, how many missing errors. Then content. So this is the data quality apart from the structure like the schema. You now come to the quality. Um how many of them have nulls How many of them um are not falling within the range like somebody gave their birth date as 00-0000 and it was accepted because the length match but the field itself is completely unusable.
(12504) Relationship discovery identifies uh connections between different data sets. So I gave in my address as uh uh 48- something and in other place I gave four uh space 8 and if I don't merge the two together I might be identified as a different individual in the system. So understanding and scraping and using fuzzy logic and uh entity resolution is also an important part of uh data profiling data correlation.
(12537) So you do univariate and uh multivariate analysis on your data. You're you're all from the data science uh um stream. So this must be second nature to you. Uh you have to use uh several types of analysis to do that. Like some of them could be factor analysis, some could be cluster analysis, variance analysis.
(12556) There are lots of libraries that are now available but you have to know which one to use and when to use. But the key point is that if two data points are highly highly related and you use them in your machine learning algorithm and you think that uh um a person working at this place often means that the person is going to make um a purchase here that's like very very bad right that those those uh they are completely independent uh variables and you just uh needed to get rid of some parameters before you were doing your analysis uh to ensure that they are all independent uh variables when you're feeding it into your model. So what is the impact of each one of your independent variables
(12636) on the dependent variable That is the analysis which leads us to data correlation. Uh of course data visualization is important. All your reports, all your dashboards, they are heavy on visualization. But that's for the consumer side.
(12654) Even as you are going through your engineering practices, you need monitoring to see who the outliers are, when the outliers happen because our human eye is trained to find it. Uh and of course you define your threshold so that you're able to um detect it sooner. Um you'll see the DBSQL dashboards and there are a couple of um um visualizations that are inbuilt there.
(12720) maybe not as uh rich as the Tableau and the PowerBI but enough for you to for most of your um um lab use cases. So to summarize uh you have to understand your requirements and model the data for consumption patterns. Uh we'll show you how you can profile the data in more detail. Um you need to establish quality guardrails. Um if the data is bad to begin with, there's nothing much you can do about it.
(12740) We'll talk about how to understand sensitive data and to classify it appropriately, how to um keep uh up with your metadata. Um design patterns we didn't really talk about, but we'll get to in future classes. Um the importance of choosing the right file format, the impression, the importance of compressing data, matching the compression type to data.
(12806) Not all compression types are possible with uh all kinds of data. Um and then data dduplication will also become important at some point. It's less so in analytical systems um as compared to um transactional system but still there might be places where you do reporting and counting on uh transactions and summing up revenues and things like that. So data duplication is still important in analytical systems.
(12832) Sometimes you may have to use a specialized GPU, specialized processes and usually your use cases are of your curated zone with that. Um um if you have not read your uh previous chapters do get to it uh this week read up chapter 3 and four assignment one is posted and as per request we had given you the lab.
(12858) If I go right to the beginning um in the agenda, lab one was already linked. So if you have not downloaded it, please go. So do so. Um where is the agenda Moan, are you there So in the agenda, we have the link to the lab one. You should be able to I put it on the Zoom chat and I put it on Slack also. Okay. Should we get started Yes. Yes, please. Okay.
(12936) So, um I don't know if folks have been able to uh download I mean you can click that link. It'll take you to Google Drive and you can download the zip file and if you have the zip file or if anybody Okay, awesome. So, once you have the zip file, you can go to your workspace and import it. So, let me just share my screen. Uh, which one am I shifting Okay, this one.
(13001) Let me know if you can see my database workspace. Okay. Uh so if you uh were able to download the zip file uh I mean there's multiple ways in which you can import it. What I typically do is I click on the top workspace uh button here. So I just go to workace and I like to click on home. So workspace home.
(13038) It's sort of habit for me to click on workspace and home just to make sure I know where I am. Uh and when you click on home it should be workspace users your username right So you're in your workspace. It's your home. So your username and then literally you can import it anywhere. Uh what I'm doing is importing it into my own labs folder.
(13058) So if you wanted to structure it a certain way, you would create a folder, call it labs, and import it there. It doesn't really matter how you want to do it. Uh I have created a folder called labs. I'm clicking into it. Um actually I I put it in a different place. So I should correct what I'm saying. I put it in this subfolder.
(13117) uh and um I have other things here so don't worry about it but if you import it in you should see this lab01 uh and within that lab01 you'll see two notebooks uh one that ends with pispark.mml and one that ends with scikitlearn and I'll explain why we have two versions uh the thing that we will be running today and I'll show you how to run is the scikitlearn version um the reason for it is is varied and complex uh but just for to simplify it uh we are in a free edition workspace and in free edition workspace data bricks only allows serverless compute so you don't
(13151) control the cluster types sizes security modes anything like that uh and that serverless environment and this is true for customers as well serverless compute comes with a certain setup limitations u which will eventually go away but for now spark ML which is a distributed training library um does not work on serverless so hopefully that'll resolve soon at its some uh testing in one of our internal staging environments and it seems to be working. So hopefully uh at some point in a few weeks we'll also be able to show you the scikit I mean the
(13223) the spark ml version of the same notebook. Um so if you have got to this point uh you should see these two notebooks. Um and I'm going to open the scikitlearn one. I'll actually open both cuz I want to show you um both. So let me open the scikitlearn version first and I'll also at the same time open the pispark ml. And you don't have to do this. It's just for your reference.
(13246) Uh but uh you can sort of look at it side by side. You can see I have the scikitlearn version here and the um pispark version here. Right And before I run through anything or before you start running anything, I just want to scroll all the way down because I ran this earlier, but I'll rerun it in front of you all.
(13306) Uh if I scroll all the way down, what you want to end up with is a cell like this. uh that shows you the RMSSE uh the root uh mean square error that we ended up with uh and a visualization which you will create in this lab and it should look something like this like 80% and 14% like 8140 if I switch to the scikitlearn version uh and scroll all the way down again you won't see this because you're seeing a version that's not run I just want to show you what it should look like once you run it if I scroll all the way down in the pispark uh sorry the scikitlearn version. I will have a similar outcome,
(13343) slightly different percentages like 80 and 15. The other one was 81 and 14. Um, but very similar, right So, this is just to tell you that both notebooks are doing the exact same thing except using different sets of libraries. Uh, and we're doing this essentially to get around the fact that Spark ML doesn't work in serless.
(13404) So, we essentially created a pandas plus scikitlearn version of the same notebook. Um and you know in the half an hour that we have we probably cannot explain all the differences uh between these libraries. Um but you know at a high level right uh scikitlearn is basically a single node machine learning library and pandas uses distributed data frames in spark right so there's a there's a very big difference typically if you're running on your local and working on you know small data sets or a single node machine somewhere uh you would probably use single node uh algorithms like uh
(13441) scikitlearn um and otherwise you would lose something like spark And obviously data bricks being a spark company will say oh you should use spark ml for all distributed things except we don't quite support it on serless.
(13500) Uh there's a little bit of confusing messaging there which we'll resolve shortly, right Um but let's start with the scikitlearn u notebook and it's it's quite step by step. Hopefully I don't have to read through every cell here. There are a few steps uh listed right up top here, right Business understanding, load your data. These are the normal steps that you would do uh if somebody gave you a basic data science um sort of assignment or at work or something says here's some data how do I train it how do I test it uh how do I make some predictions uh based on you know whatever I'm supposed to do uh this is just a normal set of generic steps that you would go through uh and then you
(13535) would plug and play the libraries and the techniques in between the steps but the overall workflow if you want to think about it as a set of sort of connected um units is always the same, right You're always iterating through these similar set of steps. Um in this case, we are trying to predict housing prices given a set of prices.
(13555) So you have some prices in your base data and we want to do some predictions and we want to see how well we predicted right and RMSSE is one of the ways in which you can see like how close your price and your predicted price are close or close so far, right Um and if if the all of that is like alien for some reason to you u you know it's just some basic uh reading material that you can uh do after the after the class right um but anyway the the principle here is we have a data set we want to go through a set of steps uh and we want to predict
(13626) housing prices given a set of prices um and so we'll start uh we'll we'll start this this first cell is just a dummy cell it's all commented out uh it's basically showing you a bunch of you know data bricks uh magic commands which is how you switch context within data brick cells. Uh we did some of these things earlier as well you know percent fs percent sh uh and we'll do some of that now.
(13651) Now this cell should look very familiar if you did lab 00. It's essentially identical. The only difference is we are creating a new schema called lab01. Uh so in the first lab you would have created a schema called lab 00. Uh there's no no other string difference here. It's an exact copy paste of that cell except for the fact that we're creating lab01.
(13713) So if you run this cell um within your existing catalog, it should create a new schema called lab01 and uh create similar types of volumes of input and output. So I'm basically trying to give you the same starting material in each uh lab so that it looks familiar.
(13731) Uh there's a catalog, there's a schema and a bunch of volumes, right And maybe as we go along we'll create other assets as needed. Uh but this is pretty uh good enough to start the basic thing. Now if you ran this cell uh you may not have everything else that I have but if you switch over to the catalog which you can essentially right click and say open in new tab or anything like that.
(13755) If you switch over to that you should see this uh lab01 right and you should see a schema like that. You will not see these tables because we have not yet created them. You have not run them yet but you should see the schema. Uh, and so that's what we were trying to do. We just wanted to create the schema so we can do the rest of this assignment.
(13812) Okay, the first step we're going to do is run a shell command which is going to get a CSV file from some public place. In this case, it's a public CSV file from a GitHub uh repo uh from the Spark Spark examples master. You'll find these uh repos listed in learning spark if you read that book. Similar repos and same repo actually is listed there. So we're just using a publicly available data set and you can just run it and what it'll do is it'll do a get and basically put it in your uh input directory and the input directory is something we created here create volume if not exist input.
(13845) So we're just putting it there. The next cell is just purely illustrative. Uh it is showing you that uh you can copy files using db utils from one directory to the other. Now you don't really need need to do this in real life.
(13906) Maybe you need to uh but I am just showing you that hey we had an input we had an output and I'm just showing you that hey you can copy from here to there. Now you do need to run this because everywhere else in the subsequent cells I'm referring to output housing and if you don't run cell five well you won't have anything in output housing because we only uh you know put it in input right so it's again just an illustrative thing to show some file movement uh in the middle of you know typical things you may write something out you want to move it somewhere else for temporary thing or you may delete it later um things like that right and then
(13936) we're going to set a variable uh to make it easy to essentially refer to this path later on because I will be referring to it like this um uh you know going forward. So you can just keep running these cells um and I don't want to spend too much time on these. I want to get to the actual core. So the first step obviously is we want to load your data. So we have that output housing.
(13959) I'm going to read it into a Spark data frame and I'm going to display it. So just so you know, you may do this uh as you inspect your data the first time you see it. It's a good way to just figure out what's happening. What does my data look like Um and as I touched on like dfsumarize, you can write that command here if you wanted to and it'll give you some statistics about all of the columns in here. Right now we haven't done that. Uh but you're free to add that command.
(14025) And you can look at essentially this data set and it says okay there's essentially a date a price bedrooms bathrooms square foot living uh we'll see if there's some correlation here right uh like for example if you have a larger house is is it always costing more maybe it does right but it also may depend on the zip code um we are now essentially augmenting our data frame with more columns so we're generating a new column called date and we're converting it from the Unix time stamp that was part of the source data and we're generating another column called zip code and we're going to cast
(14058) it as string in this case. Um so that's again uh just extending the data frame. Now you can display the data frame again and you should see the two new columns uh somewhere here. So we have date and we have zip code somewhere uh zip code is here right So we pulled the zip code. Okay awesome.
(14122) So now this is all of this was reads uh you you read a data frame um and you you know read another data frame you augmented the data frame so all of this pretty much you're operating in memory uh now you are going to write out to the file system so you're going to create a delta table like we mentioned earlier delta is the default format in datab bricks uh so uh here we are explicitly specifying the format so we're saying write format delta mode override which means if the table existed they just overwrite it uh
(14151) I don't really care about, you know, incremental data or anything at this point. Uh so it's going to create if this table doesn't exist, it's going to create it. If it already existed, it's going to overwrite it with a new version. Uh and we can you look at the different versions of delta tables if you do like describe history.
(14210) Uh and then we're going to select from that table. So now this is a delta table. It's not a data frame anymore. We're actually selecting from object storage. uh it's looking at that transaction log that Anand spoke about and and essentially reconstructing the state of the table based on the transaction log. Uh but it's exactly the same thing as a data frame, right We took the data frame and we wrote it out.
(14227) Now we have a delta table and this delta table is table housing. So if you switch back to your catalog explorer, you should see the table housing and it should look like this. And if you click on details, you'll see that it's a managed table which just means in data bricks that uh you this is a managed table in unity catalog means you did not specify exactly the path to write it out to it just was written out somewhere in your screen right um but you can see the format is delta right okay awesome so if you keep going uh the next step you
(14259) would do is you do some exploration of your data right uh you can run commands like describe housing uh describe the table rather uh You can also run a slightly variant command called describe extended. Um, and there's another one called describe formatted. So there's all these convenience functions on how you can study your data uh your table and where it where it sits etc.
(14323) So here it's telling you hey there's a bunch of columns that it can see uh you know what catalog does it exist in um and uh don't worry about predictive optimization and stuff. We'll get to that later, right But anyway, you can see the provider is delta, which means it's a delta format and we've written it out. Okay, so now we'll just write some basic queries.
(14345) Let's get some pricing trends by zip code and keep going. Uh let's visualize data, right So now we we did a basic thing like we did a select, we did basic group by, we got some understanding of our data. Um and now we're going to look for correlations right so remember I said earlier like when you look at a as a machine learning engineer or data scientist you are looking for patterns you are looking for correlations uh as you try to understand your data right and this this is housing data. So let's see if we can find a correlation. Uh we are looking for correlation here between price and
(14423) living area. And you'll get a a data set output like this. Now, this is the first thing that you will have to physically do other than just running a cell is to click this plus sign next to your table if you don't have it and create a visualization. So you would click and say visualization and that will take you uh into uh this visualization editor and we uh want to choose in this case scatter plot and we want to choose price and we want to choose square foot and you'll see there is a likely correlation right so this is
(14503) essentially you can slice and dice by any any of these fields that you you think are related uh but this is the visualization you'll end up with Again, just to show you how you might try to understand your data apart from just selecting and group by because visuals convey uh you know um some some things better than some things better than other things. So I have this visualization already. So I'm going to say um yeah I don't want to save it.
(14528) And I have that same visualization here. And you can hover over it and look at these data points. Uh right this lowest price here is 89,000 square foot is 570 and you go up. Okay. Okay, so now I'm way up here. There's like a 10,000t house for like $5 million, right So anyway, just showing you a distribution of the data and maybe some correlation between price and square foot.
(14558) Um, again, just showing you some other queries that you would typically try to understand. Hey, what is the condition of the house Maybe this is a factor if you understand the data set uh compared to, you know, relation between that and bedroom and bathroom. Again you could create another correlated sort of uh scatter graph here. Okay, let's keep going. This is the most important part.
(14616) So we want to do data preparation which means you want to train data, you want to test data, you want to you know create a prediction etc. Um in serverless data bricks defaulted an SQL which is the standard SQL uh standard that everybody abides by to true. um it will give you a little bit of a conflict with pandas because pandas is not by default uh ancesql true right so we are going to run this cell to set an false for the rest of our session here uh and then we're going to run this and let's try to understand what this cell 24 does right so we are obviously
(14654) importing pispark pandas we are importing pandas again this is just to show you that there are different libraries available to you pispark pandas is basically a distributed variant of pandas uh that is part of the pispark codebase.
(14715) Uh this started off as a project called koalas many years ago and you know koalas was cousin to pandas. Pandas is pandas. Koalas was started as a as a separate project and then koalas was merged into uh oss spark codebase and it's now called pispark.pandas. Uh the good thing is you can switch back and forth between these you know when you go from single node to distributed uh things like that.
(14734) So it allows you to jump back and forth uh very nicely. Uh and then we are going to import from scikitlearn because like I said we're going to use all single node libraries here because of the uh current limitation in serverless. Uh and we're going to import this label encoder and we're going to encode the zip code column using this label encoder.
(14753) Right Uh so what this means is it's going to assign a unique numerical value uh to each distinct value in the zip code column. And then we're going to assemble some features uh because as you you know probably know it's very important to have a set of features uh that you will you know further use for modeling analysis um in in you know in subsequent cells right so just typical activities that you would do uh the one thing that I will call out is I'm doing some uh data frame to data frame translation here. So here we created a pandas dataf frame. Um I mean we took the data frame
(14828) from above which is a spark data frame. We converted it to a pandas data frame and then I called a bunch of essentially single node libraries that work well with pandas dataf frames. Uh and then we're converting back uh calling the pispark pandas library and converting back to a spark data frame.
(14848) uh and you don't really have to do this but I just wanted you to show that you can convert back to you know this function called to spark to get back to spark right so you started with spark this is a data frame and you ended up with spark but you did everything in the middle with uh pandas um and you just I just wanted to show you this pispark pandas jump as well uh just to show you like a few funky things that you can do um and if you run this um you'll essentially get again a result don't worry about this uh warning for now. Uh and you know, you pretty much
(14921) ended up with the same data set as what you started with except now you have a list of features, right So you have these features um that that are ready for other steps that you would do uh downstream, right Cool. So now now that you have that, you have to do some data modeling.
(14941) The first thing you would do for that is you would split your data into a training uh data frame. you know you need a training data set and a test data set. Uh so in in Spark you would do this using the random split function and you can choose whatever values you want here. The seed 1 23 is important because it uh basically makes sure that the next time you do this split you get the same split like meaning it's reproducible.
(15005) If you don't specify this, then every time you do the training and test split, you may end up with a different training and test split uh on that same data set uh which you don't want. You want it to be reproducible. So you want to set a seed uh and that seed can be whatever you choose, but just make sure you set a seed and reuse the same seed.
(15024) So that means the split is going to be the same way. Um there is a comment here about uh you know spark ML pipeline which we're not doing uh that we are doing in the spice spark notebook. Um but we not doing that here because we are sticking to the single node algorithms. Um now this is one of the important cells. So let's look at this.
(15046) We have cell 29 and we are saying we're going to what are we trying to do right We are going to call a bunch of things but the main thing you want to understand is we are going to do some linear regression right. So we are doing the linear regression model using scikitlearn on this training data and we want to evaluate its performance.
(15104) Right Um so these are probably uh standard hopefully some standard commands that you are used to. Uh but we're importing libraries we're converting to the pandas data frames. Uh we're extracting some features. Uh we're going to train the model. We want to print some of the model parameters and we want to evaluate the model eventually.
(15122) Right So if you run this uh the thing that we really care about is really uh the RMSSC and the R2 and any other type of evaluation metrics that uh you know you might be interested in. In this case we are just printing out these two. Um and there's a definition here of what uh RMSSE means right Um so it's a standard deviation and we're going to use this to see how close we are in our predicted price to the price in the data set.
(15148) Okay, cool. Let's keep going. Um, now I made a note here. Uh, was it here or downstream Anyway, I made a note here that says, hey, we're using, you know, scikitlearn and pispark pandas, which is single node ML. Um, in if you go to the other notebook and the same sort of cells, you'll see the pi spark, sorry, the sparkl version of the code, right Um, like I said, it won't quite work in your environment, so don't try to run it.
(15219) Uh but let's run uh this cell uh and let's talk about what cell 32 does. Uh okay. So here we are um you know doing um essentially we we extracted it. We want to make some predictions. Uh so we want to make the predictions. So we're calling the linear regression.predict method. Uh and we're giving it the test data set.
(15245) Um and we're going to add those predictions to the data frame and we want to convert back uh to pispark bandas ex is basically to keep within the same ecosystem and we want to display those predictions. Uh we display everything from that the data frame that has the predictions added to it. Um and I want to convert back to spark because I want again want to do some spark operations after this.
(15307) Right And you can essentially look at this uh data set uh that that this is produced. Okay. Um, now now that you have that, I want to write it this to a delta table. Uh, because I want to persist it. All of this again was in memory. So I am going to persist it to this delta table. If you get any error, some weird error that you cannot resolve.
(15327) Uh, this is your like escape hatch. You can just drop the table entirely. Run the cell and then run the cell off that again. But typically you don't have to. I just put that there in case you know something funky happened. Uh so now you should have another table called table predictions in your catalog explorer.
(15344) If you go back in there, you should have this table predictions, right In addition to the table housing that we had earlier. Uh and we're going to start selecting some things and understand uh like what's what's in here, right There's a prediction here. So now there's a price column and there's a prediction column. And these are the two that we want to check to see like what's our RMSSE, right Um so if you scroll down, this is when you're going to start analyzing the model. So how do you analyze the model Uh you basically want uh we are choosing
(15416) to use RMSSE as a technique to do this. So we're going to take the you know the square root of the mean square error to get the RMSSE uh using essentially scikitlearn metrics. um creating a test data frame I mean a data frame of this um again this is just to show you that you can convert from go from Python to SQL very easily even within the same cell because here's a data frame you are operating in Python till this point I created a temp view that only exists within the life of this session but I can refer to that temp view inside uh
(15453) essentially a SQL cell right so now this is I'm going back to SQL I'm saying okay select price prediction whatever um and running that as a spark SQL query. Uh again a way to jump between you know any language in data bricks uh that you can do this you can register a temp data frame and refer to it in a SQL data frame as a as if it was a SQL object uh and then you run that.
(15522) So now we have uh an output of that as an RMSSE evaluation. So let's select from that and look at the actual residual errors and whether it was within RMS. So if you take a visual look at this, you have like this 82,000 98,000 84 17. So I don't know, does it look right Look bad. Uh we'll find out. Uh let's scroll down. Let's see how many are within RMSSE. Um and we want to look at this as a visualization because it helps us more.
(15549) So this is your second visualization that you'll create. uh and you will click the plus button and you'll click on visualization and once you end up there what you want to do is pick histogram this time and say hey what is within RMS and you can adjust the number of bins so I'll give you an example let me do 10 and you can see the visualization changed uh if I do 100 you know it's all sort of like you know piling up right in the middle right uh but let's do 10 cuz it's sort of easy to look Now if if you do 10 bins of that um sort of result set that you got you'll
(15626) see that it's mostly concentrated uh essentially between minus2 and two but mostly in 0 and one right so it's it's right here and uh it's meaning like it bend uh right there in the sort of the mean and if you run the next command we want to say how many of there was within let's say one to minus one and you're essentially trying to create a range to see how it was and then you'll get this result set on which you'll create another visualization. So you click the plus, you go to visualization and in this uh case you will create a pie
(15704) chart. So we want to show you all of these different types of visualizations you can do to understand the data and when you create the pie chart uh just pick pi and then pick the x column as the RMSC multiple which is the output of that and the y column as the count uh and then that's it.
(15721) you need to do and you'll end up with this essentially percentage and you can see that uh sort of 80% of our test data is within RMSSE 1 and 96% is within RMSSE 2. Now how do you interpret whether this is a good thing or a bad thing Uh it really depends right how do you know RMSSE 1 is a good result for this data set uh it really depends on the scale and range.
(15748) So in this case our target uh variable that we're looking at is price and RMSSE is basically a measure right it's a measure of the average magnitude of errors between the predicted and actual values. So because the range of price is so large uh we saw a large range of price all the way from like I don't know 50,000 to 5 million uh and your RMSSE is you know within this small range of like one and not distributed all over the place.
(15816) Um I think this would be considered like a good uh result and that your model was pretty decent and did not go haywire and predicted pretty close to the price. Um that's that's essentially like the thought process of how you would do this kind of exercise. Um again this was not to tell you all of about the ins and outs of all of these data science techniques but to understand essentially a flow of how you would uh do this.
(15846) Um, and like I said before, if you flip over to Pispark ML, you will not have any results unfortunately. But if you have access to a classic cluster and anywhere else, you'll be able to run this. Uh, and you'll end up with a similar result over there as well, right Which means like the technique that we used, whether it was scikitlearn or whether it was uh, pispark didn't really matter.
(15905) um like our predictions were pretty accurate in both cases and fell within a decent uh you know error radius right uh that's how we should look at at this exercise um I'll I'll I'll stop here I think we're at time but yeah it sounds good anything for Moan any questions there is there is a question in the chat about Why um pandas is slower than spark data frames Yeah. So pandas basically is is a single node um uh technique right.
(15945) So spark data frame a pandas uh data frame basically works assumes that you have only this one uh machine that you're running on right so it doesn't work in a distributed fashion like how you might expect spark to work. So Spark as you know is a distributed engine right there's a driver there's a bunch of workers it splits work to all of these workers they all have their own set of tasks and they are working on their individual set of tasks and then aggregating up and back to the driver but in pandas it doesn't do that right it's not working in that same fashion it's basically doing everything within
(20019) the memory of that one machine so the the the memory you get on a distributed engine where you have multiple clusters uh is way different than you would do on on on a single machine now if you had a single machine that was I don't know 128GB uh memory and you had a small data set that was you know 10 GB panas will fly through that no problem the problem happens when your data sets become larger but your corresponding memory on that same machine doesn't become larger at the same sort of linear scale right so that's why the distributed algorithms like spark ML uh outperform
(20105) Uh yeah, there was a question also about converting spark df to pandas and it's slow. That's because what's happening is you you you have a spark data frame that is distributed because spark is going to read from all of these distributed uh data sets essentially on the fly.
(20121) When you query the data frame, it's collecting it all of this information on the fly, right So it it uh uh it's it's split up. But once you bring it to pandas, you have to essentially read all this and the driver, the spark driver has to now essentially collect all the data and then give it to the pandas data frame. So you're collecting all of this data back to one data set and then transferring into the pandas data frame.
(20142) So that it's not the transfer part. It's basically the collect action that's happening to to make it a single object so that you can transfer to pandas is the choke point. uh how we should see these uh labs like should we just go over in what fashion I guess we should study these um are they going to be in the exams should we memorize the things um or should we just know that this is a capability in data bricks yes we can do it but we don't necessarily need to learn every step I'll let you take that Canada and I don't know
(20224) um it would be hard to memorize everything here. Uh but as I said we all have a little bit of muscle memory like if you never knew ABCD you cannot write a cat bad sad right so I would say um as Moan went over um the explanation of the cells it's important to understand what's the intent what's the purpose there was a scaffolding which says there are these seven or eight steps that no matter what project you do if it is ML related you would encounter this.
(20259) So, do you know uh what is the command to um summarize uh the statistical uh quality of your data frame Do you know how to split your data set into a train and a um test or a train validate and test Um depending on the language you're using, depending on the platform you are in, the syntax will vary.
(20324) So, one part is conceptual, the other part is simple dataf frame API operations. And by the way, we are using the datab bricks platform, but what we are doing is 100% open source. So spark is fully fully open source and everything that you're doing here, you should be able to take it and put it on a different uh compute environment.
(20346) It's just very convenient to spin up the compute and um you don't have to pay a penny to run these workloads and that is the reason why we have chosen the data bricks platform as the ideal uh platform to learn uh these big data technologies spark concepts. Uh in fact what we are doing with scikit uh you could probably do it on your laptop as well. Also um I think the the labs have a high correlation with what you'll be asked for in the assignments. So if if you're familiar with the labs it'll help you with the assignments.
(20421) Thank you. Does that help or did we just use too many words to confuse you further No no I I understand. Thank you. Mhm. Question to moan. Just want to verify. So you said like it's a server the on the serverless we can only use the single mode.
(20451) So I'm a little confused which notebook shall we should we be able to execute on the free edition Uh you should use the one that ends with scikitlearn because that is a single node algorithm. The other one that says Spark ML, if you try to run it, it will run about halfway through and then when you get to step five, data preparation, when you try to run the Spark ML cell, it'll tell you some error about uh either Spark ML not supported or something like that, it'll give you an error.
(20518) And that error is because data bricks specifically doesn't support distributed ML libraries only in serverless. Now, the good thing is it's coming. So it's it's possible that a couple of weeks from now if this release goes well we'll be able to you know you'll be able to run that as well and I I will I will mention it in one of the few next let's say two or 3 weeks from now if the release goes well I'll tell you that you can try that as well but right now just note that if you try to run it about halfway through you'll fail. Uh step five you'll fail.
(20548) Okay just a follow-up question. So does it worth the experience if I buy like a cluster so I can try it out and learn it There is another data bricks in well I don't like this one specific one will not work as is and the reason for that is uh data brick because we are using what is called a data bricks runtime and a data bricks runtime is a prepackaged set of lots of libraries that's what's underlying the compute so there may be dependencies somewhere that you need to do additional imports that are not there in this notebook right so let's say you take
(20625) this notebook book and I try to run it on I don't know a typical EC2 instance or EMR cluster or something uh that doesn't have the data bricks runtime. So if there's a dependency that that doesn't exist you'll have to do other imports which we don't know because we don't know where you're running it.
(20644) Uh but that's the kind of thing but otherwise the code is fine. There's nothing super data brick specific in this code. It's just uh it's just there's an assumption that many other libraries may be present uh on the on the cluster which may not be true somewhere else. Thanks. And so that um brings me to another point. We have data bricks community edition which is also reduced compute.
(20709) um you could try there. Um uh and al also September mid or so this particular restriction on running spark ML um on the serverless will also be lifted. So just a little a few weeks out uh we can try that again and see if that support has also been uh added. Once it's once it's official, we'll basically communicate with you guys that hey now you can you can try to run it right and and and if there is some code change needed or anything like that depending on how our testing goes once it's supported we'll just upload a new
(20747) version of that notebook um and then you should be able to but at some point I would expect you know I'm keeping my fingers crossed that sometime in September you'll be able to run the Spark ML version of the notebook also in the free edition itself. Um I have two questions. One is um the labs you're sharing it in Zoom for like Google Drive links.
(20812) Uh is there any way we can put them in the uh files in canvas Uh it's supposed to be linked in the slide deck. I think uh yes it is linked in the slide deck. Um but I think he's talking about uh the files option in Canvas. We have limited storage there.
(20841) So, one of the things I was concerned about is maybe we can get through it uh for couple of assignments and then after that the storage will fill up and I will not be able to share. I think it's capped at uh 5 MB or something like that. Oh, it seems like it's 5GB and these files are pretty small. Uh I can uh I'll put up I'll post a note on Slack if I upload lab 00 and lab 01 zip here. Uh and then for the next session if that works for everybody and if assuming we don't run out of space we can do that.
(20906) Yeah, I think that should be fine. Okay. Anything else I think that's a wrap. Okay. All right. Thanks everyone. Good night. Thank you. Thank you everyone. Good night.