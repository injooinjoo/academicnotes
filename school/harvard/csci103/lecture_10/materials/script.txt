103 day 10 - YouTube
https://www.youtube.com/watch?v=iSPa4_uT7sQ

Transcript:
(00:01) Hi everyone, welcome to lecture 10. Uh today we are going to talk a little more about data concerns which we know will eventually lead to model concerns. So the agenda would be more around model bias and how data imbalance or class imbalance could lead to um some incorrect assumptions, anonymity uh concerns. So sometimes these models can leak sensitive information.
(00:34) So just like the data needed to be protected, the model also needs to be protected. And um uh feature computation for fraud detection at a scale. So when you are at your um point of sale counter and you are scanning your credit card and you're getting your response back quickly, you're signing, making the purchase and taking your goods.
(00:59) behind the scenes a lot is happening to ensure that that is not a fraudulent transaction and it is like very very very fast um among thousands and thousands of features. Um it has to catch that needle in a haststack situation um and um things like anti-money laundering um fraud uh they don't happen on every transaction or every other transaction. So even identifying the patterns and um you know uh creating models around it is extremely hard.
(01:27) So Eric has worked on a very large uh payment system where they built this at scale and he's going to talk about it and in the lab we'll deal with um what are some of the libraries and what are some of the ways you can handle data imbalance as well as we look at data classification. So imagine you had a note field and there was some um sensitive information there and without you realizing that was fed into a model that's quite um a bad um mistake and so how can uh your data pipelines as you're bringing in the data how can they help with the data classification of sensitivity levels so that you do not
(02:05) use such information or if you do then you are well aware of what you're doing. Another thing that we have not uh looked at um too closely is this whole um ecosystem around AutoML. Uh so AutoML as the name suggests you are not going to know all the various statistical characteristics the algorithms and everything behind the scenes.
(02:31) You just point the data click and you get a model. Uh in fact data robot uh was like a forerunner here in this space and they had captured quite a bit of uh market space because at that time uh there wasn't that much of big data and so you know their model uh could scale and they had a very very nice offering and they were successful for a very long time till things started to fall apart because these blackbox uh auto models or autoML goes only so far like any large enterprise would at some point need to look behind the hood under the hood to see what changes to do and there you
(03:09) have nothing to change. So data bricks also offers um autoML but the difference is there are not as many models um as you would expect say out of a data IQ or a data robot or some other uh provider but it is glassbox ML. So what does glassbox um autoML mean? It means that everything that is happening on your behalf is made visible to you to inspect later on.
(03:43) So um all the uh notebooks, all the hyperparameter tuning, the EDA, the exploratory data analysis that you need to do on the data and then going through the multiple iterations and then finally producing the best model and justifying why it is there. Model interpretability to say like this is the reason why this model is good.
(04:03) So short of actually registering it in model registry all those notebooks are made available to you and it is uh in conjunction with uh MLflow like deep integration with MLflow. So everything that we learned about tracking and uh uh registry and everything else is also there. Now many people of use this as uh a way to even validate the worthiness of the data.
(04:30) So if the data does not have the right signals then you will not be able to use it for a uh for a prediction scenario. And so this is like a very quick way of deciding whether to use a data set or not. And secondly, because the notebooks are there now, they can use it and tweak it um and sometimes even use it for just the first phase which is the profiling and the EDA portion of it.
(04:50) And then the second phase of checking uh which experiments were run and how they are doing and then the third phase of looking at the model interpretability and finally deciding whether to use it or take it directly into production. All of that is done. The code is provided to you. It is registered. the um uh trace of it is maintained. Now I mentioned that it has limited models.
(05:10) So unlike the other ones that you might see in the market, there are not tons and tons of models. So it's just for very um very light use cases that you can go with it especially if you have citizen data scientists. You may have heard the word that there is a data scientist and then there is citizen data scientist.
(05:29) Citizen data scientists do not have to know do not have a PhD. They do not need to know everything around ML but they can still play with it in that space. So classification models means uh checking cancer not cancer will it rain will it not rain that that type of classification it could be binary it could be multi as well.
(05:53) Uh so underneath this you've got the decision trees, random forest, logistic regression, XG boost, light GBM. All these models with various combinations of hyperparameters are going to run in parallel which means it's highly efficient, right? And it's going to produce um those results. You get to choose how many iterations you are waiting for or what is the maximum time you're going to wait. So you have an hour.
(06:18) So you'll say within an hour you tell me what's the uh best scenario or you have um the default is 120 minutes actually but you can choose to extend it or reduce it. Uh but the minimum is 5 minutes and in 5 minutes you you can't even get past the exploratory data analysis phase if the data set is large. So be mindful of that and that's the other thing.
(06:42) So you can work with AutoML even with very large data sets because of the distributed uh engine behind the scenes whereas in the other uh AutoML scenarios uh sometimes you are restricted by single node models. In the regression model scenarios similarly you have decision random linear XG boost and light GBM. So kind of like a little similar but regression remember is different from classification.
(07:05) classification is just binning and putting in this bucket or this bucket like is it zero or is it one is it churn is it not churn but regression is predicting a continuous value. So what is the price of uh gold tomorrow or what is the real estate going to look like in a uh month's time then comes forecasting models. So here profit is very popular is also popular. Now here you'll see things like data bricks runtime.
(07:32) So we in class are running through a serverless free edition which is very very new. So when I tried to use AutoML in the free edition, I ran into a few issues. So I want you to be aware of it that there's limited compute and uh limited models that are available to you.
(07:53) But I'll show it to you on a full-fledged um edition of um data bricks with multicomputee so you get an idea of what is the art of the possible. And then um on the serverless side there is only forecasting which means you get a profit you get a auto arma and DPR and that is what I was trying but even that I I gave up perhaps too quickly it requires very small data set and the the signal in the data needs to be very good because it would uh time out and say um that it it couldn't uh find enough signals to produce a model right um we are limited by a certain amount uh every
(08:28) day and so you need a good amount of training time to be able to create a good model. So keep that in mind. Um now let's perhaps go here and see uh if the last one that I had thrown at it succeeded or not. Give me one second. Let's go to experiments. In the experiment section, you will see um these are probably too many uh that I'm seeing right here. In the free edition, you will see one of this.
(09:09) You'll see just the forecasting model using AutoML and you will see one more for custom model. You'll just see three options. Here we get a few more. And I decided to use um AutoML for classifications. And I think I set up something called as churn uh am. That's right here. I kicked it off a few minutes back. I told it run for about um uh 15 minutes and um I just walked away. And here we are.
(09:39) It did run out. So the timeout was 15 minutes. Um I gave it the same churn data set that we were playing with one or two weeks back and um told it that the evaluation metric that I care about is the F1 score. So different problems will require the evaluation metric to be different.
(10:05) So it completed and it has given me all these um runs um and I can look through this and see which one is the best one. So I can view the notebook for the best model. I can view the data exploration notebook. So let's go ahead and look at it a little more closely. So that's the data exploration and automatically it is putting some profiling pandas and so on.
(10:29) Um it is um uh going to um find out if there are any semantic type uh detection alerts. It's going to do the full profiling on my behalf to say how many variables, what are how many missing cells, what's the total size. Um, so you can select any one of these and there are quite a few of them.
(10:53) And for each one, it's going to specify what's the distinct value, any missing values, what's the uh most common um um text like, you know, from a word cloud perspective. Of course, in gender, you're going to see male, female, yes, no. So you would have had to write some code to do it. It has done all this on your uh behalf. Now when you have real numbers then it's going to show you some percentiles and some um you know distributions and this just goes on.
(11:22) So that's your exploratory data analysis. So it it put the libraries um it uh ensured that there is no um big semantic uh gaps in the data and it profiled it and showed it to you. So now let's go back. That was the exploratory. Now of the various models that you see below, like there are several of them. Let me see if I can pull this up a little.
(11:50) Yeah, I'm having a hard time pulling it, but you can see there are there are lots of these. There's one of these bars that needs to be pulled up. Um but if we were to look at the best model um as determined by the F1 score then it's going to be the light GPM. Uh so since this was a classification model uh remember we were seeing uh that in the other screen that these are the different models that it's going to run on our behalf.
(12:22) So for classification it's going to run decision trees random forest logical XG boost and light GBM and of all these models and in that short period of time it decided that light GBM was the better one it shows you full code like this is the target column imports ML uh flow and it is the autoML that it is uh leveraging um loads the data um displays you know the values, selects the supported columns, does some pre-processing. Remember last time we had to do this by hand.
(12:55) Boolean columns, numerical columns, categorical columns. So it has to then break it up into train, validate and test. Uh and then um run the light GBM uh you have to have a objective function and your job is to go through the search space and find the best combination of the hyperparameters.
(13:16) So it has done all of this. All of this code is like pretty solid and robust. It's given to you so that you can use it as is or decide to um you know iterate over it. Now that's the space um and these are the different trials. Uh so it's using hyperopt behind the scenes and like the previous time where we looked at it this is how the pipeline looks like.
(13:47) Um you have the boolean, the numerical, the one hot, some passroughs and then the now um you can log each one of these models of course in uh um in the tracking server. Uh but sometimes somebody may ask you uh how you so sure that this is the best model. So the you have to use some libraries like uh shab lime and so on to show them the feature importance all those columns that we fed in uh which column contributed to what percentage of the uh predictability so that if as a human you have to explain why something went through and something didn't or why the score you'll say it's because this value is higher than this value and so on. I'm I'm oversimplifying it but you
(14:23) know the drill. So here SHAP is enabled and um you are using again uh templatized code using the SHAP explainer um and you can do your own inference. It gives you code on how to register the model the best model that you have created in the registry should you choose to do so and it gives you sample code on how you can load it back from the registry because you are now in the inference mode. Maybe it's somebody else doing it.
(14:54) Um either directly from the registry or directly from the tracking server. So uh you know that this is the tracking server because it starts with run. You know this is the model registry because it starts with model and then other things like confusion matrix um area under the call precision calls um everything is built on your behalf. All I had to do is click one button.
(15:18) In fact, let's go back and show you how we started this. I'm just uh amazed as to why I can't pull this up. There must be one of these things. I I I saw it. I saw it show up on Indita. Just scan slowly. I see. You'll find it. No, I'm not. Go up a little bit more. Up a little bit. No, I I'm not sure what's okay.
(15:53) What's the case? Um, and it's three stages. First is the configure, the second is the train, and then this is the evaluate. But it would have been really, really good. Expand rows. Maybe this is what I have to do. No, it's expanding each one of them. Okay, let me resize to see if that will help. There's just a bar that I'm not able to understand which one to pull. Oh yeah, that's pretty frustrating.
(16:18) Maybe it's a scroll bar on the right. Uh it is scrolling here. I can see it. And there are all of these experiments. So you can see that each run is given a name about an hour back. This was the data set. It took 1.3 minutes. That was the source. This wasn't a best model, but I can go back to all the models that it trained.
(16:43) And this particular one was an skarn, not the light GBM that was considered the best. And these were the metrics. So actually send back any one of them. And you by now you must be very familiar with what it would look like. It will give you the um metrics. Uh it'll give you the parameters.
(17:07) Um actually I'm going to pause here and ask you what's the difference between what we referring here as the model metrics versus the model parameters. The meters is what you use to train the model like the hyperparameters and the metrics is the results after you see the data. Fantastic. Yeah. So these are all the hyperparameters and this is um the scores the the results. Absolutely. And you can see that just this particular run has its own notebook and you can click on this to go to the notebook.
(17:34) Just like we were able to go to the best model, we can go to the second best, the third best because sometimes the best one may not be right for you and you may want to do it's completely tagged. Uh there's a unique it um and then the uh you can see some system metrics, traces and the actual artifacts which is uh where you know some of our um um graphs and assets are there but the model itself is pickled.
(18:04) The Python environments requirements tells you all the libraries that were needed. Um, and the YAML file is going to specify what are the build dependencies. There's a cond model and all of this should now begin to look very very familiar to you. So with that, let's go back and show you how did I do this. So I went into experiments.
(18:28) I chose a classification autoML and here um I chose a cluster. So this is my datab bricks um ML runtime cluster and um uh it's asking me to browse to my training data set. Remember this is a threespace name um three-part name space. So first I have to choose my catalog. So that's my catalog.
(18:55) Inside it I have to choose my schema and inside that I choose my uh table. So this was the churn data set that we were talking about last time that I reused. Now it's asking me what is the prediction column. So I want to know whether it's churn is yes or no. And you have to give the experiment a name. So I had given it as churn- am but you can call it whatever.
(19:17) On the other side, you see all the um uh elements or the columns of the schema and it gives you an option to include everything by default. But should you if you know there are some identifiers and certain date fields that you don't really care about, you can remove it. And there is also um an option to imputee with. So here you're saying I don't care. You just use your best guess uh to imputee with.
(19:43) But if you if you decide uh that you want to use a constant value or the most frequent value or something like that uh then you could choose that al as well. Uh and that's it. I would just click start autoML and in the advanced configuration I had uh chosen my uh primary valuation metric to be F1 score. Um these were the training frameworks.
(20:09) Um and um this this is where I said I don't have the time just do whatever you can in 15 minutes and let me know but you have other ways of uh controlling it not too many levers but just enough for the citizen data scientists to be productive onita I have a question does does autoML assume that the features are already computed and defined in the in the input table? No, this is just raw. You throw the data and it's going to do it because it starts with EDA, then it will featurize.
(20:41) Then it will train multiple models within each model. It'll train multiple hyperparameters and then give you the best model. What it is constrained by of course is the amount of time and whether it was able to do justice uh going through all the different uh models and hyperparameters to present it to you.
(21:02) So given enough time, it will converge and give you the best one. Okay. All right. So, that was a little bit about AutoML. Um, any questions there before we go back to our slide deck here. All right. So, last class we made an interesting assertion. We said, "Yes, big data is important, but we need to have a mind shift from big data to what? Yes, you know the answer. High quality data, good data.
(21:50) So, good data means it's an ambiguous, it is uh well curated, you can rely on the labels. Uh so, having a lot of data points definitely improves the model. But if all of them are garbage then the model is all confused because it can't distinguish signal from noise. Um we also talked about a datacentric approach to ML is better than what is the other way of approaching and improving a model code approach concentric yes basically algorithms which is what AutoML is doing. It's not able to have much control over the uh data.
(22:25) It has minimal. So what it does is it goes over multiple uh models and tries to see which one is better but it's not really changing too much of the code code. These are like highly highly um uh boilerplate uh algorithms that it tries to fit and see is the data better fit for this model or not.
(22:49) No data science or ML project can start without a what. So of course there is good data there's a model but what is very important business needs absolutely so somebody from business needs to say that this is a problem we are not doing it because we are so fascinated by data and um ML but we doing it to solve a real business issue unless it's a research in which case even research has a goal has a particular problem that you're trying to do so the business problem at hand is important important to understand how the model is going to converge uh what your metric is going to be and so on.
(23:27) There are three types of model inferencing and they include so you've created a model you put it into registry you can use it three different ways primarily and I know that's a little vague but what comes to mind real time inferencing yes real time inferencing is one so there is a batch inferencing which means you get a bunch of data and um once a night or once in twice a day or five times a day or something as a batch you're creating the inferencing and making the results available.
(24:02) Streaming is as the data is coming in you know it is getting scored and then persisting into disk and then being delivered downstream and rest uh or um uh real time API based is something which can be highly highly elastic and low latency. Um typically web applications will rely on these kind of endpoints to uh score the models whereas batch and streaming is done directly on maybe the data bricks uh platform.
(24:36) So model serving endpoint um is the rest endpoint and batch and streaming are typically done through UDF because it's cheaper that way. Uh we just learned about SHAP. So what is SHAP used for? Feature importance. Exactly. Feature importance, model explanability, interpretability, there are just different words.
(25:00) Um, now we haven't talked much about ensemble, but it's a combination of uh models. Uh, so you might have multiple models and ensemble as the word suggest is a combination of models. So I don't even know what the question is. So why do you do that? Uh, to improve model robustness and accuracy. uh because any deficiencies of one model any shortcomings are kind of um adjusted by another one and there are different ways in which uh this is done.
(25:31) You might have like exactly how AutoML was doing it. It had multiple models all scoring. But uh once you get the results either you pick the best one or you to take an average of the others or some uh some strategy um to um paralyze the models or streamline the models or or sequence uh them to use the uh power of multiple models and their algorithmic uh capabilities. Uh every model will have errors.
(26:00) Why? because it it deals with floating points. So you never get 100 out of 100. You get like 99.99999 whatever. So that is why model reproducibility is also so hard. You should allow for that little gap. Um we briefly touched on grid search and I didn't um like in earlier classes we would talk a lot about it. I didn't because it's falling out of favor.
(26:25) But you have to understand what grid search is conceptually to appreciate what hyperparameter tuning is. So um but uh the question is uh what is the other form which has been extremely popular and optuna and hyperopt and all of these uh tools and libraries have adopted over the good old nfold cross validation or grid search.
(26:51) um probab probabilistic searches. Absolutely. Yes. And they refer to typically as Beijian models or Beijian searches. Uh and they are probability. Uh except that they use machine learning behind the scenes. So the first time it's going to be completely random. You you'll get somewhere and then you'll predict what the next set of parameters to use.
(27:14) Then you'll see how good your own prediction is and that is how you converge much much faster. uh an ML pipeline consists of stages of two things. Can anyone remember what they are? So one takes a data frame and gives a data frame. One takes a data frame and produces a learning a model out of it. So one starts with a t the other starts with a okay I've given too much transformers and estimators.
(27:48) So in your pipeline uh you're transforming it and finally there's a stage in which the learning happens. So that's the estimator. Transformers as I said takes in a data frame produces a data frame. Estimators take in a data frame and produces a a model. It can be very simple but uh the learning happens there.
(28:05) Um transformers call transform and estimators call um fit or predict. Yeah. Yes. Absolutely. Fantastic. Yes. How can you do data versioning a little back? We're rewinding a little with a catalog. Um yes, you can do it. But remember when we talked about delta, uh it automatically creates these versions where you have a explicit version number and a time stamp.
(28:35) So you as the developer didn't do anything. Every time you do an operation, it automatically uh applies that so that later you can um you can uh retrieve uh a specific u data like the table name is exactly the same. So you'll say select star from table X which is what we would do but you'll say version as of 23. you'll get something very different uh in when when you say version as of 24 or maybe 25 because each iteration there are some updates some delete some some transactional changes right so that is the time travel or the versioning feature which is there built into delta
(29:15) now errors in machine learning can be of different types so there is bias uh which is defined as the difference between the predicted and the expected uh values So you have um you're training and you overtrain or you underfoot right there are two different ways and ML is unable to capture the true relationship between the features and the target. So a good example of this is underfitting.
(29:42) So if you would have trained for a little longer perhaps you would have converged beautifully. So there is a little bit of bias because you haven't fully fully learned. Variance is the result of the model making too many complex assumptions. So if that is overfitting which means uh you've you've spent way too much of time making the model learn stuff and now the model has lost its true predictable uh capabilities.
(30:13) Um and then there is irreducible errors just the randomness in your data and in floating point and in the circumstances and everything else which is not directly controlled by the model but it comes as a side effect of your prediction. So this these are examples of uh you know you have your training data and your test data.
(30:34) So the dark blue is the train and the uh light green is the test. So this is falling fairly closely. You want it to be this perfect line, but this is close. And this is an example of how complicated it is. And so, uh, you know, your train data and your test data, you know, has to be, uh, very carefully fitted for it to be able to understand that, oh, the line needs to go up and down sinosoidal like this.
(30:57) Um, uh, so bias I want you to remember, underfit, variance is overfit, and irreducible. It's not really the model's problem. like lot of other things and so your prediction error is accumulation of these three types of error. Um also remember that when you try to increase um or reduce bias then your variance increases and vice versa.
(31:22) So it's it's not that um uh these two uh go hand in hand. They in fact kind of tug uh in opposite uh directions. So this is an example of high bias and uh low variance uh where everything is just cluttered here. So there's high bias, low variance. This is um it's it's fairly spread out. High variance, high high bias, high variance. This is all in one uh spot. So there is low bias.
(31:49) You're hitting um and low variance. That's uh good. And then this is low bias and high variance. So, it's around the center but still a little scattered. Don't worry too much about it. Just just remember these three things. Remember this and remember that these kind of tug at each other in opposite directions. All right.
(32:14) Now, let's look at model bias. Why does it happen? Of course, we all are uh very familiar with it as we do it um uh consciously and and subconsciously as well. There is the human factor. the machine just mimics the human cognitive bias. Um, so if you look at this uh face recognition technology, this was in the news several years back. So this is like old news.
(32:41) But the red is recognition of darker females and the light orange is recognition of lighter males. So across the board, across all these uh uh companies that spent millions of dollars in research, it is just too much of a coincidence that lighter male face recognition was always better than darker female face recognition. So you might have um seen the uh reasons for it.
(33:12) Uh so that's the accuracy of face recognition technologies and it's not just uh bias is not contained to one organization it's across the board that is a very very strong pattern uh that people are seeing. In fact this is where the the difference is probably less. Uh can anyone um explain why this bias is there? there there isn't sufficient dark female uh pictures that are trained.
(33:46) There are it's probably easier for the researchers u to get lighter male and lighter female pictures and so they train on it. This is a case of underrepresented data and so uh the model hasn't learned enough. So from a darker female perspective, it's constantly doing badly because there aren't enough images that it has been trained on.
(34:10) And um do we do we for a fact know that it was under represented in data? Because I feel like that is something we can measure and know. Um because I can also see that the amount of light might also cause that's very similar biases. Yes indeed. Indeed.
(34:36) um but um it is uniformly um the picture is uniform and these are NIST study findings published by Harvard University. These make very interesting uh conversations for sure. I like the approach that you're taking of how could it be like the pixel coloring that is uh influencing the model uh perhaps who knows but this is the generally accepted uh reason um for lack of um data points um which is why this happens but perhaps some of it is attributed to uh pixel color too.
(35:10) then there is a poor quality of training data. Um and so if if you look at these two pictures uh one of them is a dog and the other is an arctic fox. Can you make that out? Can you blame the model for not being able to do it because the quality of these pictures are low? So poor quality training data and um model performance mismatch. Um uh let's look at actually another one.
(35:39) This is the famous uh tray um like ML study by this was by Carnegie but I think this was the Microsoft uh uh scenario where you know when um when a bunch of people come and start giving information and you're immediately training on that information you are you are uh skewing the model's thought process. So these people came on and said the holocaust didn't really happen and you know this is just a figment of some somebody's uh imagination.
(36:09) So the model started learning that maybe the holocaust never happened and so it started to make up stories. So that's hallucination from the data that it has been fed in a very short context that it has remembered. Um now all of you must be using chat at some point. uh and it's actually gotten so much better with hallucination, but occasionally you would be able to trick it uh to kind of give out some information or uh say something which is inaccur in uh accurate or incorrect even though uh you were it was trained on a certain prompt like you you gave it very strong instructions but then towards the end of the prompt you again started contradicting it and it got confused.
(36:45) anything else uh that people have seen and want to uh bring to the surface um as to what else could lead to model bias. I know that I'm sorry I know that there was a an instance of um kind of uh the association even before predictive models but like uh when Google was doing um uh uh work on their search engines for certain images when you searched images that were associated with certain terms they were all they also followed certain patterns like if you would search the word um good then it would be you know very light pictures or pictures
(37:29) of, you know, white skies or white children, white babies. And then if you searched evil, then it would be minority pictures or something like that. Um, and it was meant to uh the the the idea there as well was same the same as the human factor that it just followed natural human bias in some cases because that's just unfortunately the type of society that we live in.
(37:56) Yeah, I think some of it is just um honest um um mistakes or honest assumptions that kind of being extra scrutinized and some of it I completely agree with you like as a kid I used to read a lot of Annette Blightton and um uh several years later u my husband said my god that's like so racist did you not know that the the doll the gollywalk is always like uh curly black hair and you know was the wicked doll.
(38:27) So it goes right down to the doll also and they are always depicted with the darker colors and so on. So who knows maybe there are these biases which influence these authors and they wrote that stuff without even realizing that they are actually um biased like I feel like sometimes I say things because of my social upbringing or my cultural upbringing but it might be misconstrued as um inherent biases.
(38:54) So it's not really me thinking but it's certain things that have been brainwashed into me. So very very interesting area and sometimes um very tricky also. Um but the bottom line is if we had very good representation and we had high quality data then perhaps a lot of these will um will will be a little more uniform um and it will not lead to um to to such characteristics.
(39:18) you would expect that this would at least tilt in one case but it never tilts. So there is a very strong pattern there. Now I cannot refute Noah when he says uh it could be because the models were uh probably tuned to like you know um that that could be countered though because the the difference between darker female and darker male is pretty significant.
(39:43) Uh yes it is. Uh so no but what he was saying is maybe the color of the pixel could make the model quality bad like you know uh it's harder to see or harder to fathom or harder to grain but very difficult to say. All right, enough fun here. Let's Yeah. So, I I just wanted to ask as as well um do you think that any of it could be due to classification? I think that one of this came up at least the example that I gave before, there was a lot of discourse about um even like the influence from Hollywood where they
(40:15) literally will call certain characters evil queen or evil this, evil that. Um but those particular characters were, you know, usually shrouded in darkness more so. And so it has an impact on when people search similar words. Do you think that maybe classification could have had maybe some similar effect? Absolutely.
(40:36) So when we say poor quality training data, a lot of mislabeling could have happened and once the model has learned, you don't know. Some sometimes these models take very long to train. Nobody really bothers. then they will try to fit a few examples at the tail end and hope that it um compensates for the bad data that it had been fed from. So you cannot really you'll have to remove those data points or correct it for sure.
(41:02) Uh and these neural nets they don't know any better. They are trying to reduce the loss from say a um a classification ground truth that has been given to them. And so if as you are saying like evil or dark or whatever then the model is automatically learning and why because we trained it with bad data um or bad labels and that is poor quality training data.
(41:26) Why did that happen? Well, it's human bias again. And so human bias translates to model bias. All right. So I think Anendita I think Jeremy had a question too. Okay. Oh, I that's okay. I was just going to ask just as another like I was going to ask about overfitting because the lack of diversity of of data like having too many representations of a a single type might overfit particular image or something.
(42:01) Overfitting is over usually not on a per image basis but of like the entire model but there could be some factor there for sure. Um but if you were uh if you're building a model just on that limited data that could overfitit but here you're building the entire model with all these data points and this is just an underrepresented uh part.
(42:21) So um I'm not I'm not so sure about that assertion. I I think I think that whole that previous slide though kind of underscores the importance of the focus of the last lecture though on on the the the importance of good data uh even more than the the model and to to make sure that the resulting inferencing is is is accurate.
(42:48) Absolutely. Also I think data and quality data. Yes. Yeah. This is a really interesting area be as we as we train ML models and and AI in general being aware of um being careful not to um in inject human biases into AI if if if that's not if that's not desired because the because if we're we're training the models based on data from humans then it's going to naturally absorb absorb those biases whether they're good or bad. Mhm.
(43:27) Uh I I think the first uh few models uh that OpenAI trained was from the entire internet. Then they got uh smarter and they had some uh vetting process um of which uh data sets so uh that they should use as part of their model or not. That's a very very long poll.
(43:51) Um and even at data bricks um they brick created an LLM um it was called TVRX uh a while back. It was on the leaderboard for 2 weeks before it got kicked out by Lama 2 I think. Um they had also uh the work of a company called Lilac in ensuring and vetting the quality of the data.
(44:15) Can you imagine how how tedious that process is to scan the entire thing to score it for bias before feeding it into the model. But it was necessary to do it and that whole process of getting the data ready uh was the long pole of creation of a LLM from scratch. So we will look at LLMs perhaps next class or the class later.
(44:40) there you guys already know there's prompt engineering there's rag there is um uh fine-tuning and then there is pre-training so DBRx was a pre-trained model like a a from scratch LLM and those are very expensive very very expensive and it has to be trained on billions of uh um data points so okay now ways to reduce uh bias is of course you can change the model uh so you can use different uh models to vet the outcome tree algorithms are better at handling a bias. Um, use appropriate models.
(45:12) So do not use a linear model if the features and target of your map of your data do not have a linear relationship. So you know having a a completely biased model a wrong model for the problem at hand could be problematic. Ensure the data is truly representative. Um it's diverse, represents all possible groups and outcomes.
(45:33) And if the data is imbalanced, then use waiting or penalized models. Um like in the credit card transaction scenario that we are going to look at uh wherever there is fraud, they are very very small percentages uh 01 or.1% of transactions are probably fraudulent much much less.
(45:59) Right? So with that amount of data all of these models the way they work uh when when you're is that they work on balanced data like half of it is good half of it is bad you give the model then it'll give you a good thing that's the way it has been trained. Now if 99 data points or almost 100 data points are good and only one bad data point, it's not going to learn much from it.
(46:19) Which was our assertion on the previous slide that that is probably the reason why across the board they were not learning enough about the contours of um uh non-coians. Extensive hyperparameter tuning uh would also be good. And um along with changing the model we also talked about how ensembles um of models will also help with uh reducing errors and bias.
(46:46) Um no actually that is not so much for uh bias. It is more for variance. Uh ways to reduce variance is ensemble. Um you know you train with multiple models. You leverage both weak and strong learners in order to improve the model prediction and you train with larger data set because remember we said one was underfitting the other was overfitting.
(47:05) So more larger data sets will increase the data to noise ratio will which will reduce the variance of the model and the model will be better so it can generalize the rule which will also apply to newer data that it hasn't seen. Now in general model errors means you have to choose the correct learning model.
(47:25) So if it is supervised then controlled entirely by stakeholders who prepared the data set uh to um uh Kenny's uh point on uh um if they were labeled wrong. There are stakeholders who are actually putting the labels right. So in supervised the correctness of the label is very very important and in unsupervised it depends on the neural net itself the quality how it was built and so on. Um, do not reuse data sets.
(47:52) For example, data from an area with an ethnically diverse population cannot be applied to a region with predominantly a single race. It just won't work. Perform data processing mindfully. So, um, you know, you might have to have some additional weights or maybe the post-processing may need some more interpretation. Monitor real world performance.
(48:16) uh so test it against uh a real world data wherever you can and frequently uh change the training set because the data characteristics may have changed and address infrastructure issues uh like when I was trying to train the autoML on our serverless um free edition uh it was choking it I might have thought that after two three runs it finished and I would pick the last one and assume that that's good no there were some infrastructure issues um that may uh lead to a lot of errors uh and that may not be uh caught in time and you're just looking at a result but you you you don't realize that there are gaping holes there. The data collection
(48:53) process which is basically the data cleansing, the transformation, the validation, the normalization, all of that requires good scrutiny. Then we'll come to imbalanced data which of course occurs when one or more classes have low proportions in the training data as compared to other classes.
(49:18) So cancer nor cancer um fraud non-fraud these are perfect examples of it and most of these machine learning algorithms for classification or whatever they're designed and demonstrated on problems that assume equal distribution of the classes. Uh they're designed to maximize accuracy and reduce error. Right? So we all know that the consequences is is if the data set is imbalanced then the model will be biased.
(49:36) And so how you how do you detect imbalance? You have to check for the count of the dependent categorical values. And imbalance can introduce majority and minority classes. Some of them are modest and some of them are very very severe. Um and it's just not uh in a zero and one scenario. It might be in multi-class situations as well.
(49:57) And classification problems can have severe imbalance uh in certain um specific industries like fraud, claim, anomaly. This is like in manufacturing, intrusion detection and so on. So how you going to deal with it? You have to augment the performance metrics. So accuracy is not just the only one that you should overfixate on. um uh confusion metrics, precision, recall, F1 score, all the things that we have been talking about and they have um um they they check for completeness and uh sensitivity as well. Um different algorithms. So we talked a little bit
(50:36) about tree and then resampling uh techniques. So we look at one called smokeote. There might be others available. Uh so the two strategies is sometimes oversampling the minority class and under sampling of the majority class. Uh if you overdo it then it's just repetitive and there's no additional thing to be learned. But uh that's the best you can do.
(50:58) Um you can generate synthetic samples but this is actually much more difficult than it appears than meets the eye. You have to do it very carefully and you have to split um into train uh test and train before trying the oversampling techniques. So SMOT stands for synthetic minority oversampling technique. Yes, there's a question.
(51:26) Okay, maybe not. um uh let's say that these big boxes with those lines they are the majority class samples and the um the black circles are the minority class samples. So you will create clusters around it. So here this is what the uh package imbalance learn that we're going to look at in a few minutes does.
(51:53) It finds n nearest neighbors in the minority classes. So the black circles are the minority. it'll find some nearest neighbors and it's going to um um have these uh clusters. Then it's going to draw a line with the nearest neighbors. So between this cluster and this cluster is going to draw a line.
(52:14) Um and then it's going to generate random points on this line as the synthetic data points that it is generating. So it's not taking this space and clicking here. Oops, sorry. I don't know what I'm doing. It's not just clicking randomly. There's a certain method to madness here. creating these circles of influence or um um you know boundaries joining between the nearest neighbors and creating points here is statistically uh representative of perhaps when you take another sample there's a good chance that you might get something here as
(52:55) opposed to just clicking here here here right so that's what we're going to look at and maybe there's one other thing yeah the other thing we talked about these biases and and um anonymity concerns. So PII information can sometimes uh trickle through and um the model can bleed that sensitive information. So imagine you have trained a model with your social security number.
(53:20) Somebody asks it so what is blah blah's social security number it's going to give that right. So removing identifier fields before feeding it to the model. Uh but first you have to detect it. If it is very cleanly noted with tagged and everything else, great. But if it is a hidden in some other information then you need sophisticated data classification mechanisms to be able to um to detect it and avoid it.
(53:44) So uh let's talk a little bit about data classification. And of course this is not your uh regular expressions to scan each field and determine because that's going to be extremely um uh difficult for you. It has to be done at scale and it has to be done very very carefully.
(54:05) So classifying a data optimizing cost through intelligent scanning and reviewing and protecting sensitive uh data. So at the uh workspace level you can turn it on and at the catalog level there is this button where you can say I want this to be enabled. Once that is enabled it takes about 15 minutes or so the entire catalog.
(54:28) You can choose some schemas but by default all the schemas and tables under that catalog are going to be automatically scanned for certain sensitive information. Today there are about uh 15 or 20 fields which are very common and bound to the United States. Like a phone number here is very different from a phone number in some other country.
(54:48) A social security number here is very different from a biometric in some other country. So it is very regional specific and um at a later point you would be able to define your own but let's take the simple things like name, phone number, email, social security number and so on. There are all of these are taken care of.
(55:10) So you enable it and then you would be able to see the results and let me show you how it looks like. Let's go. Yeah. So maybe I go here. So this this is a um catalog in which uh it is enabled. And so when I go to see the results, it tells me that 68 tables have been scanned. And uh this was the count of a table scanned in the last 30 days because nothing changed. So there's no reason to change it.
(55:49) And what is the classification tag? Um so location has been detected in 98 columns and I can actually go and review each one of them. Similarly, name, phone number, email address, US bank number, IP address, user driver license. These are all considered it as um um sensitive information and they have automatically been tagged for you.
(56:15) It would have taken you really really long time and a lot of effort and pain to do it. And this is the catalog. So let me just go back. In fact uh let me go back to this catalog. So it is MCW Hotel, right? So let's go there. MCW auto. So that's the catalog. And if you go to the catalog level details, you see this data classification is automatically turned on.
(56:49) Most of the other cataloges will be off because it's a very expensive operation. So you know, you have to be careful that you don't run too much of a bill. And then this is the results uh which will take us to what we were looking at uh earlier as to 68 tables uh the scanned the audit logs and so on any questions all right so now let's go and look at uh the handling of imbalance so this uh notebook I have uploaded in um uh in um a lab 10 but uh for whatever reason I was not again able to run it completely on serverless but of course here it is fine. So let let's just look at the intent behind it. So uh you pip
(57:37) install imbarn and then uh you bring in some data set. So I got in some credit card data from uh Kaggle and you can see that these are all the different uh features on the data but uh at the end you have like an amount and a class. So zero means it's non fraud and there will be some uh ones somewhere which will say fraud and obviously the number of ones is going to be really really really low.
(58:06) Uh you can use um the uh a very dummy classifier which means that's not really an algorithm but you use the classifier and you can see that automatically the test score is pretty high. So that is why it says that accuracy by itself does not mean much. Remember all the people who came for the test and they had either false positives or false negatives.
(58:28) Those are also really really bad situations that needs to be rectified and changing that is harder. Now we are going to just do a logistic uh regression you know run the predictions and we see how many are zero and how many are one. So there are only 94 whereas there is 71,000 or 72,000 close to that which are good data.
(58:53) And we said that all these models they expect this to be about the same. So you have two choices. Either you have to oversample the minority class or you have to under sample the majority class. And um you want um uh even with this existing data if you were to look at the accuracy F1 and recall you get certain values. Accuracy is very very high but recall is only 64%.
(59:12) So that's that's not very good. Um and F1 score of course. So we change the algorithm, we get a little better. Uh maybe um our F1 score improves uh significantly a little better and our recall is also gotten better. So you can continue to do this and and get better. But let's look at how oversampling can be done.
(59:37) So here you have the reample um function from sklearn and um uh here you will reample uh by stating that this is the um you want to match on the majority class. That means you're oversampling the minority class. Uh and then whatever you get you count. So this time your zero this remains the same. Your um your um uh fraud cases have have increased.
(1:00:04) This is too high. Uh and then you go ahead and uh uh do your u modeling again. This time you got slightly better recall. Don't go too much by it because the the data set itself is very small. But ideally you should get a little better data when the when the data set is better balanced.
(1:00:23) And under sampling is when you take the length of the fraud case and you down sample the uh not found scenarios and now you have you you have got 360 data points in each case and you go through it and you'll get a certain value for your um F1 score and recall and synthetic data generation would be you use uh give it a random state and that's basically a seed and uh you reample and this this ignore these errors.
(1:00:56) Then you do your logistic regression and your accuracy and F1 score and recall will be at a certain level. So um creating these data sets synthetically is hard as I said harder than it looks because you just can't take any random values. uh in one of your assignments remember you used um the faker library from Python where it just looked at the certain range and it was generating some data but in real world maybe the number of um uh males or the number of females is not going to match exactly what it generated because it's completely random whereas here the
(1:01:31) this approach of doing it has got a better chance now in datab bricks um if you look at datab bricks lab project uh for data generation. Uh this has taken a very very long time to mature. So you can imagine how complex uh this particular issue is. It's called data bricks lab dbl data gen.
(1:02:00) Um and companies like JP Morgan, Chase and uh others uh use it to generate millions and billions of data points they can use in lower environments to test something. Uh so very robust uh pretty good and very widely used and uh this takes care uh of all the statistical considerations and a lot of care has been uh gone into the creation of this asset. So if you're interested in this space do take a look.
(1:02:28) All right. So, enough on that. Eric, with that, I'm going to pass it down to you, please. Okay. All right. Yeah. So, um, here I'll I'm going to talk about a project that I worked on a few years ago, um, for monitoring, um, credit card transactions and detecting fraud and, um, uses many of the techniques that we've talked about, but in a production environment with the um, you know, real business requirements to drive it. So, let's let's take a look.
(1:03:26) So we um we had a um product manager and and um she she came up with a set of requirements for us um to to hit. And we were replacing an existing system um that had some deficiencies and and it was um this system was intended to improve the um performance but also um make it more maintainable um and easier to manage and um also improve the accuracy.
(1:04:00) So um so determine if a transaction is fraudulent with a high confidence. I you um you don't want to um uh like reject a transaction that's actually a legitimate transaction because um it might like it might result in the um the business person at the at the merchant um taking away the credit card and maybe cutting it in half in front of the customer or something like that.
(1:04:38) Um and and of course you don't want real fraud to be missed because it could result in loss of thousands of dollars or more. Um latency was important that the system be extremely fast in in terms of producing a result. Um during credit card transactions there's a lot of things going on besides fraud detection and checking account balances and and other things.
(1:05:02) But we were giving um given 15 milliseconds to respond either yes this is a good transaction or no it's fraudulent um you should reject it and so um 15 milliseconds is extremely small amount of time it might like with a normal disc drive to just to read um a little data off of a disc drive would take 15 milliseconds.
(1:05:28) So this was a this was a tough um tough um uh nonfunctional requirement if you will um for um producing a result and um and then the throughput um needed to be um support over 50,000 transactions per second. Um and so that's um that's that's quite a lot and um we designed it to um to support that and we were able to to meet that requirement as well as the 15 millisecond um response time uh configurable and manageable rule and feature management.
(1:06:18) So um a lot of work we had a team of data scientists and they spent a lot of time coming up with um different features and also different um rules. It was rule and model based but the rules were manually configured by um by um the fraud detection team as well as uh the data scientists. So um the previous system that was very difficult and cumbersome to um make a change to a feature.
(1:06:50) So they they wanted they wanted it to be easier to manage those features and the features were extremely important um to the systems accuracy and and um there was a lot of very strong belief by the data science team of the importance of the features and and um also the not only the the managing the features but also um protecting the um the the the fe the information about the features that we're using um and then um at any point be able to um come up with and deploy a new feature and try it out.
(1:07:31) So that was um so if you have a system um kind of running and and suddenly you want to you want to add a new feature that hasn't ever been used before um you know not only configuring that feature but also going through and and computing the feature values for historical data and then and then making all that accessible um for real-time inferencing was um a bit of a challenge.
(1:08:00) But I think these these challenges are pretty pretty uh normal for um any any fraud detection team. And um and so but but the um knowing knowing these we had a very good idea of um like what the business was expecting and what we needed to do to um to basically um get get like a um a pass from the um the business team once we were done saying we were we were successful or not.
(1:08:45) So this is a view um we did some analysis graph analysis of um transactions and and how they're um and the different entities that that comprise transactions like the merchant and the website and the transactions are the blue dots and the the accounts. These are customer accounts. These are um the purchasing accounts um from from customers that are um conducting transactions. And the first time I saw this, I was really amazed at how um organic it looked.
(1:09:19) Um it look it looks much like looking at mold in a a petri dish. Um but but basically it's it's real data. Um and the and you can see that there's a lot of activity um focused around the merchants and and the websites. And then um and then when there was fraud, we could um encode the transactions that were fraudulent as as we usually used black to color those.
(1:09:51) And and you would see um that often there were um clustering of of the um black transactions or or fraudulent transactions around individual um merchants. And often a merchant would one one way that that could occur is a merchant might um have some sort of uh like either the merchant was involved in the fraud themselves which which happened or sometimes the merchants's um uh um computer system would get compromised and um a bunch of transaction data or uh credit card um information would get stolen from that merchant and then used um to um to conduct fraud um at other merchants. So
(1:10:44) um but but interesting view of the data. So whenever whenever you can I think um looking looking graphically at the data or visually at the data is is useful can provide insights. Mhm. So features um as I mentioned um computing features was a extremely important piece of the process.
(1:11:18) Um and the features were um drive based on transaction and dimension attributes and other and and sometimes other features. And the dimensions that we were dealing with um we saw them in the in the previous um image but customers, accounts, merchants, locations, time of day, product, price, etc.
(1:11:44) So these are all things that we would um include as um components of the fraud detection. And then um and then uh so we could compute features for um individual customers and accounts like maybe the um the average um oh well let me let me get there. So, uh, we would compute features on transactions like the distance from the home where where, um, maybe where the merchant was located in comparison to where the the um customer lived.
(1:12:19) Um like if if the transaction was happening in say Istanbul and the customer lived in in Cambridge, Massachusetts, well um maybe that that would be a indication that they're, you know, thousands of miles away from home. So maybe there's a that could be an indication that the a model could use um difference from average, max, min, and mean.
(1:12:45) So, if the customer usually um per does transactions of $20 and then suddenly um they're they're making a purchase for $20,000 or something like that, that would be um a big difference from the average and maybe the max um transaction amount and the mean. And so that could be used as a indicator um for fraud by the model and count of transactions per day.
(1:13:21) If if the if the customer for example if the customer does say um maybe one or two transactions per day and then suddenly they're doing a 100 transactions um in a short amount of time, then of course that could be um an indicator that something's a miss. And then we also computed features for dimensions. So um the average um transaction amount for a merchant was useful as well as consumer and um and other things as well and frequency of transactions for the consumer as well as um merchants was was interesting.
(1:13:59) But there were literally hundreds of features that the um data science team had come up with and we um we had to support um computing all those features and not just computing them but but managing them in a in a way that was more um didn't didn't drive everyone crazy. But um please stop me if you have any questions and and um we can we can discuss more.
(1:14:29) So, one of the things that we did, we we came up with um we studied the the the the data and and came up with a strategy for um handling the metadata around not only the um not only the um u basically we came up with a dimensional model for the data where we had um the facts which were the transactions or We refer to them as events because there could be other other types of events like uh opening a new account for example or closing an account something like that could be an event.
(1:15:11) Um but most of the events were um just the the raw credit card transactions themselves. And then um we also had dimensions like uh the customer and the the merchant and location and other other types of dimensions that we could model here. And this was very extensible. So if we came up with a new dimension um in the future like say the phase of the moon for example, we could um easily easily add new dimensions and and and start computing features and and using that as part of the the modeling process.
(1:15:52) And then um we so properties were um either features or attributes. And features um were basically computed values where attributes were static static values that came came maybe with the transaction or or some other information source about the customer merchant. But the features were were um computed based on either other features or other attributes.
(1:16:22) And um so the features could be built on top of other features and that was built in into our our model. And then the system uh something I didn't mention in the requirements but it was in intended to be multi-tenant so support many um many different customers um or customer or direct customers were payment processors but the system was designed to support multiple um payment processors as well as merchants and customers.
(1:16:55) And so it was important um that we build visibility into the um into our data model so that um like as an often mentioned that uh you know governance is important um aspect of of um metadata. So we we had built this and and this was this was you know close to um 8 8 years ago now maybe 10 years ago and um the the tools um for for managing the metadata are much better better now.
(1:17:32) you guys in in data bricks, we have the um the catalog there that can do can do much of this for us. But um but but then um it was much more um kind of um roll your own sort of situation. So we we we created this. Now, um you guys may may remember that we studied two types of um um data pipelines for um processing data. Um and this one used real time as well as um batch um computation.
(1:18:13) So, does anyone remember um what what this is referred to and compared to and what's the other model that we might have used? streaming. I'm sorry. Say again. Was it streaming? Well, the the real time was streaming and and but um but a data pipeline that includes both streaming as well as batch.
(1:18:47) What what architecture do we refer to that as? Kappa architecture. Well, um, Kappa Kappa is, um, void of the the batch. It's just real time maybe using microbatches. But, um, but what's what's what's the architecture that uses both real real time or streaming and batch? Oh, the lambda architecture, I guess, right? Yeah, lamb lambda architecture.
(1:19:19) So this is an example where um our our real-time processing wasn't sophisticated enough to be able to support microbatching. So we we used both um a streaming service um as well as um batch batch computation to support um uh the um the the feature um feature computation. So the other thing that we did was um that was fairly innovative I think is we and something I I I'm not sure if it exists um in the wild yet but um based on our we based on our metadata around our our features we we could take that metadata and then um generate code that would actually compute the features. So um so rather than and then creating
(1:20:18) specific code for each of the features, we would say okay well this is an average um average of this value and then we would um automatically um use our code generator to generate the code that would compute the average um for the um for the transaction amount for example. And we did that um and that was helpful because we had to support um uh uh real time or streaming computation of the features as well as batch computation and they they use very different um methods for doing the computation. So um by having a code generator to generate the code to
(1:21:02) compute those features, it made um adding new features and feature maintenance a lot easier because we could we could define the feature in the in the metadata and then use the either the streaming code generator or the batch code generator to um to compute those the features in those two different environments.
(1:21:31) So that helped um reduce the human error aspect of of things where um somebody would incorrectly code a feature and then um and then that would basically um produce some errors in the in the inferencing because the the features weren't getting computed correctly. So once we um once we had the code generators, we could define new features and and have a fairly high confidence that things were working correctly and with a lot of um uh testing um and and mostly to protect against regressions. If if we change the code generator, we'd want to make sure that um we weren't breaking a bunch of
(1:22:11) features. Sorry, professor. Can I ask you a question? I I don't understand exactly you were generating code that you will generate a code to create features. Is that what was that? Yeah. To compute the features. So you can you can like to compute the average of of a um you know the average transaction amount for the customer over the last month.
(1:22:40) Um yeah there's there's code either um like some SQL code and we were using Spark and Hadoop to do it there. So that would generate like a little SQL statement to to do that. But in the EPLL or in the complex event processor that we created um we used a EPL language to um to compute the feature.
(1:23:13) So it's basically the the the the actual code that's used um to to produce the um the feature value. So rather than um handcrafting that code, we could do it based on the the metadata about the feature. So that that we could say okay well this this feature is something simple would be the average of the transaction amount and we can say okay and there might be a thousand different features but we could run our code generator to to um produce the code that um calculated not just the average transaction amount but all 1000 features and and do it for either the streaming environment or or or run
(1:23:56) um in the batch for the batch environment as well. So big that that definitely um helps simplify um testing out um new features and and deploying them um quickly. And we had uh one important part of the batch was if somebody came up with a a new feature and we wanted to um use it in real time generally we would have to go back and compute it on the historical data maybe for the last year of data year of transaction data. So we would use batch in the Hadoop environment that could handle all of that data um and and
(1:24:41) compute compute the values fairly quickly. Um we could do that in batch mode and then and then calculate those historical feature values and then and then um import them so that they could be referenced in real time. Like for example like um maybe um the um like the average uh transaction amount for a customer uh might be something that we would um maybe add and then use the batch framework to compute that for all the customers over the last year and then um and then we can then use that reference those new feature
(1:25:22) values in the real time environment. Any questions about that? And this was this was pretty tricky to get right. And um and you know after Kappa was becoming a thing but um at the time but uh it wasn't wasn't possible for us to to do it with our um streaming service that we basically handcrafted to support the um the performance needs.
(1:26:00) So another like um so I think um software is all all about innovating and coming up with ways to um to handle things. So one of one of the challenges was what we had you know um I didn't mention how many uh like customers and that we needed to support but it was like millions and um tens of millions of customers or hundreds of millions of customers.
(1:26:35) So um having having that information available um was a challenge because um the the um data stores that were available at the time. We we ended up using Cassandra to store our data and act as a fast retrieval which was able to provide us the performance we needed to retrieve things. But but we also had to figure out how to like Cassandra is is fairly uh you know it it supports large amounts of data but it's also um can be expensive because as you as you expand the cluster um the amount of data that the cluster can can um support um you're expanding the cost of that cluster. And also another thing that we ran into was uh you know you we had to have
(1:27:24) redundancy. So if we had like um you a terabyte of of storage actually we needed um redundancy on that data. So we'd only get like um two or um a third a third of that data a third of that terabyte was actually available for um for holding data because we we to support um replication and prevent data loss.
(1:27:58) It was actually replicated three times across the cluster. Kenichi you have a question? Uh yes, professor. I just wanted to ask about uh any features or parameters or dimensions you had to build in for um like when you move when you maybe say someone's going on vacation and spending using their credit card or something in another country.
(1:28:20) I remember a long time ago, I used to have to try and give like the credit card company a heads up if I was planning to go to another state or another country to visit because almost immediately once I got there, my card would start declining and trigger the fraud detection. But these days, I don't have to do that at all.
(1:28:40) Um, did you all have to build in dimensions for anything like that at all? I'm just curious to know. Yeah. um like e even then you could call and say okay I'm I'm going to be traveling to to to France um you know please please don't um consider that fraud. So that was a type of event that we might get from the customer that would could be incorporated into the into the rules or potentially into the model as well to to indicate, okay, this we know this customer is traveling to to France. If they're doing a bunch of transactions and Paris, that's probably okay.
(1:29:17) now. Now, if it's no longer necessary, I'm not I haven't I've been out of the space for a while, so I'm not quite sure how they're doing it now, but um probably the I I don't know how they're doing it, but but um there's I think there are lots lots of challenges and um and we had to be pretty innovative about how we approached it to to be able to get it to work.
(1:29:43) And one of the innovations was around the circular buffer. We came up um was worked up with some students from WPI and we came up with a scheme uh for um efficient storage of information. So it works out that like if you have the count and the sum um you can you can um and you do it for different intervals of time.
(1:30:08) This this circular buffer is for one week of data and with the the bucket size is one day. So there's a bucket for Sunday, Monday, Tuesday, Wednesday, Thursday, Friday and Saturday. And um basically we would we would um maintain for the current day which is Sunday according to this we would maintain the count the transaction count that or not necessarily transactions but the count as well as the sum and from that that that was enough to compute um things like the sum the average the mean the max and min and and standard deviation and and other
(1:30:49) things. It was pretty incredible um with this little amount of information what we could compute and then was very efficient also as we moved to the next day we would just so if if Sunday um went to Monday then we we'd leave Sunday as it was and then um zero out Monday so the count would be zero and the sum would be zero and then we'd start incrementing these values and at any point in time.
(1:31:26) If we needed to compute say the the minimum um uh um oh well maybe sometimes we needed to to keep the the minimum value as well. we uh for the average you could you could go back and um and basically um for each each day compute the average and then and then define then get the average from from that. So um using this and this could be not just 7 days a week. It could be 30 days of the month.
(1:31:58) It could be 24 hours of the day. It could be um you know months of the year. It was fairly flexible and and allowed us to and remember that we had like millions of customers and we needed to store this information for each customer. So rather than keeping track of individual transactions for each customer and trying to compute on the fly these values, we could um we could use we could use this um ring buffer or circular buffer to to um store the pertinent information and then be able to very quickly and efficiently retrieve this information and and then
(1:32:40) um compute the running average or or or whatever it what was that was that was required by the feature. And we could also do it we could do it um we could do this per transaction but um um but generally we applied them to um dimensions so merchants, consumers, regions etc.
(1:33:06) we could we could have um these circular buffers defined um that would allow us to um compute these um features. So we we actually patented um this technique as something novel, but it it worked and and it was um provided the speed that we needed and without like huge amounts of storage, which was um expensive at the time.
(1:33:42) And and this was um this is also before um you know at at the time everything was on premise. So we had our own little data center where we were managing the the the um computers and and drives and stuff to um to support this. So and and you know the because we had our own machines and and um facility um you know cost was definitely a factor helping reduce the cost was important to to make um make it um viable for the business.
(1:34:22) So th this is a um a view of our fraud detection. So the fraud analyst was um important um participant in the system. We had a a guey as well as a command line interface where they could define um the the metadata that drove everything. And the metadata included different types of events like transactions or maybe an alert that the customer is is traveling.
(1:34:54) dimensions um in our case from customers and and um merchants and then and location and time of day rules. Um so we supported both both rule based and machine or machine learning. Uh so the rules were basically if then else statements that could be fairly complex and the rules were definitely difficult to manage and there were a lot of rules and and um and the understanding what all the rules were and what they did and and also whe whether they actually function correctly or not was was a big challenge um managing those rules. So now things are moving
(1:35:41) more towards um large language models um and with less less dependence on rules. But when we were working on this um LLMs didn't um yeah neural networks existed and the data science team was using neural networks um but the large language models that we are familiar with today like chat GPT did not exist.
(1:36:07) attributes. Um, features. So, attri attributes were like values that we'd see on transactions like the transaction amount or the time of day. Um, features were things that we computed based on um attributes and then um different types of machine learning models both classical machine learning and then also some neural network um technology was involved there.
(1:36:40) So the metadata service um basically all it did was manage the information about these um different um entities and then uh the and and it could be easily changed like usually the thing that changed the most were um features because the events and dimensions and attributes were fairly constant but the features rules and and sometimes the ML models were updated here in the metadata service and then as I mentioned we use code generation to generate the code that actually um was used to compute the features in this in the streaming service. Um so um so the the
(1:37:21) feature computation if if a feature got updated here then we'd regenerate the code for the feature computation here and this was the real time or streaming um processing and which in included both the ML models the the rules processing and the feature computation. So all of that happened um within 15 milliseconds and the executive would basically say okay well based on the results of the ML models and the rules processing and the features I computed to to path to those two things it would say okay this this is this is fraud or it's not fraud with
(1:38:01) some probability um in um like some like if it's 80% probability that it's fraud we we would um declare it as fraud and that could be adjusted and and then over here on the right um it's a batch processing. So we we had a Hadoop cluster where we we stored lots of data uh historical data transaction data and that we could use that to um um basically again generate code for the future computation and then um generate those codes and and store the features in the Cassandra database. So this was an important um technology that we used
(1:38:47) that um provided the correct speed. So it was um not we didn't use um spinning disc we used uh um basically um memory um um memory on on a computer chip to um to store things and um it was very fast um in terms of retrieving uh reading data and Cassandra is interesting because it's actually faster reading ing data than it is writing data. It takes takes it a little longer to or I'm I'm sorry.
(1:39:27) It's slower reading data than writing data. It writes right away. It doesn't actually write it, but it kind of cues it up. And then um so reading data is a little bit slower than than writing data, but it was fast enough. Like 2 and a half um 2.5 milliseconds is what what what we were able to achieve. And that that gave us enough time to um to to do the feature computation and and pro run through the models and the role processing and um return a result within 15 milliseconds which is um you know sup super super fast. I'm not sure if um
(1:40:10) like even with um something like data bricks and spark um if you could achieve that that speed with inferencing. But it was important because the fraud detection service it was was the payment service was upstream from from from the fraud detection service.
(1:40:37) So it was doing other things besides fraud detection and all of it took time. So you um it was very important to be able to for the customer to to swipe his card and then um you know and then have the transaction go through without I think the entire like time was like a second or two seconds or something was the response time that the payment service wanted to provide for the customer.
(1:40:59) So that um that the customer experience was um for both the merchant and the consumer were as um good as possible and at the same time with like very high level of accuracy was important. So very challenging um project. So this one one thing that we we're able to do an onad talked about columner databases and um and their ability to um ex accept basically like Cassandra I think there basically limitless number of columns that you could add to a row like um I think billions of columns and so we use that to our advantage. we um
(1:41:49) for each each um so the event table events again are things like the transactions. So as the transactions flowed in, we would um basically collect the attributes and also in this purple area over here, we would um compute all all the feature computation that we did for the transaction, we would store that in into the table.
(1:42:16) So we'd have a record of it and the ultimate feature that we were looking for was whether it was fraud or not. So so um that would go into the event table. we'd have um different types of events and different types of event types that we could record and and um basically all all the events went into here. So thinking about like a data mart um or the star or snowflake schema that we've talked about, this would be our our fact table that sits sits inside the different dimensions. And then rather than having a table for each dimension, this is our
(1:42:55) dimension table down here, we had just one dimension table. And we had um basically a dimension type here that we could use to determine what type of dimension. And you may notice that the um the data here is sparse.
(1:43:16) And that's that's fine with column or data stores are actually quite good at at managing sparse data and tables. So um some dimensions would have some columns that other dimensions wouldn't and that was okay. So for the for like when um this is a dimension for a terminal the the terminal that actually takes the card swipe or where you're doing the transaction.
(1:43:42) So that was an important dimension because sometimes um like a gas bump or something like that, you can put card readers in them and and um people would steal uh steal u credit card numbers, information from from a gas pump. So sometimes that the terminal would be a source of fraud and then um and the accounts and and other types of dimensions like the consumer and the zip code or or country or or city or other all sorts of things.
(1:44:16) But with the um the I think the um key takeaway here is with these two tables um which with simply two tables we were able to um manage all of our events all all of our transactions as well as our as well as the dimensions and the number of transactions was like millions of transactions per day. and the um the the dimensions were um especially the customer dimension was uh high cardality. So um Cassandra just just to review very fast read write 2.
(1:44:51) 3 milliseconds um horizontally scalable um to pabytes but um this was with the caveat that yes you can scale to pabytes if you can afford it. So it was it wasn't wasn't wasn't free. So um but it but it was capable of doing it and our system was multi-tenant because we had considered governance and our metadata which was important and then um we had a flexible schema and and columns.
(1:45:23) So at any at any point we could add new new event types, new new types of features as well as new new dimension types as well and um and new new new um attributes or properties or or features as well. So um that was important so that we didn't have to go to a lot of trouble um if we wanted to add like a a new feature which the data science guys were doing all the time.
(1:45:59) So just in in a real world this was a real world application. So for it to fail for any reason like a few weeks ago a Amazon AWS had a problem with their um Dynamo DB um service and US East1 uh like any any sort of problem like that it wouldn't have been acceptable for the system to go down. So we had um reg uh like um u data um center redundancy.
(1:46:34) We had a data center in in the US as well as one in in in Europe. And so only two but that was sufficient for the business. But even that was pretty um complicated because um if the idea was is that if one of the data centers failed um it would automatically switch over to to moving all the traffic from the US over to the like say the US data center failed all the traffic would be um shunted over to to Europe and and that system would handle the load until until the um the US data center came back online and that might slightly increase the latency maybe we wouldn't be able to
(1:47:17) support the latency requirement that we had before but um it would continue to function so that and that that added a lot of complexity to the um the system and um and just figuring out how to um how to like make sure that both systems were doing keeping the features up to date and and stuff.
(1:47:45) And now um with GDPR that would that would be even more difficult because of the um data privacy um that that exists now for um for Europe. But anyhow, real real world um problems or or challenges um there's a lots lots to be considered and many of the things that um we've been talking about in the class um are are relevant here.
(1:48:16) But the governance of the data was um important and then um disaster recovery was critical for um before the business would sign off on it. Um they you know they wanted all all of this. So um complex event processing framework compute features real time apply models and rules return result uh Cassandra data for fast retrieval and we came up with that proprietary ring ring buffer for storing and computing features.
(1:48:46) the combination without Cassandra or that that ring buffer that we came up with um we wouldn't have been able to do this but these two things together kind of came together to to make it possible and then um and then we also used Hadoop um we managed our own Hadoop cluster for um doing batch basically the batch side of the Lambda architecture adding new features There's testing new features like including computing them and testing them out to see how they worked and then um adding or updating data uh correct for
(1:49:26) outages and build and test models as well. So so that's um that's that's that. So just a like a real world example of of applying data engineering and and um data science together to produce a a functional system that was able to detect fraud and and do it in a very scalable way that could um support our business.
(1:50:07) Any questions? All right, Anandita, you want to take it from here? Um, no, I think that is really really fascinating. It was uh built definitely a little while back, but the architecture and the constructs will still remain the same. Uh maybe, you know, some tweaks might come in.
(1:50:30) instead of using um Lambda as Eric had shown Kappa may come in but um Hadoop may be replaced by Spark but um still the uh the general premise of how you can do fraud detection at scale remains the same. I always find it fascinating especially the ring buffer part and I I think you may want to center yourself in the screen. I don't know if it is possible but Oh, I see. I'm sorry. Okay.
(1:50:55) any better. I think I was leaning so the camera was way off. Um but we are coming uh like this is the end of our 10th lecture and um um you know how are you guys feeling? Do you feel comfortable about handling uh data engineering and some data warehousing and some ML use cases? Are the uh case studies helping? Are your assignments geared to your learning? uh anybody I mean this is we are not academicians as I always say we are practitioners we want to improve so do you want to give us some feedback you Noah since you are on the screen you go I I personally uh found that now using
(1:51:44) data bricks I can do some experimentations and there is this overture maps database apparently available through S3 buckets are exposed. So you can directly scan it through uh data bricks directly uh and it's actually quite fast. I tried to do it with DB before but um doing it with data bricks is much faster. Nice. Yeah.
(1:52:15) I mean we have not uh spoken enough about governance and um you know system tables and um observability and tracing and all of this stuff which is um uh people think it's synthetic uh syntactic sugar but it's very very important being uh having all of that build platform is super easy um and um doing stuff um tinkering with it is the best way to learn it.
(1:52:43) So um where we are right now we've kind of given you a little sliver of what to expect and what you're doing like in terms of experimenting on your own trying to find out features breaking things is uh learning for sure. Um any uh since we have fewer classes remaining, you guys already know the agenda, but any last uh wishes uh which we can try to accommodate.
(1:53:09) Anything that you feel like oh I wish you guys would have covered that or uh or something to that effect. Any anything that you feel like you wanted out of the course but we didn't get a chance because there are only so many lectures and so many hours. But uh we have a few remaining ones and we'll try to see if we can squeeze that in. Um personally I'm happy with the content.
(1:53:31) Uh but I think we can publish the if we can publish the case study too earlier and um final projects that would be helpful to maybe start building something and uh with the group projects things goes very slowly. I see. So, are you talking about the second case study? Yes. Okay. Sure. All right. Um, if you are you have made significant progress with your uh assignment four, you're like almost ready, then sure.
(1:54:02) Let's let's post that. Uh, the final project though, I think we need to wait just a little more. Um, so let's say um by November uh like right after Thanksgiving I was planning on uh opening it up but if you want it earlier and it's it's completely up to you we can do it. Um I I personally submitted the last assignment already. Um.
(1:54:30) Mhm. And just based on my experience with the last case study, uh, it was very low like I I like to be ahead of things and, um, we were like we we were able to send it almost the last day. Exp work expense to fill time available and having more people means that um, you can't do it the way you are used to.
(1:54:57) So yeah, I can understand that. Uh but also these group projects are meant to kind of remind you that people come with different strengths and sometimes uh certain perceptions that you have get refined just by listening to something which is diametrically opposite.
(1:55:17) So I hope you feel the especially since this is a remote class you feel the benefit of doing a few projects uh with others uh because in real life that is exactly how things go. um you know, difference of opinions, coming to the same page, being able to convince. So, it's not just the technical chops that win. Uh so, yes, I'm hearing you. You want the second case study to be released. Uh we'll do so.
(1:55:42) Thank you so much. Mhm. Anything? Anybody else? We are almost at time. Uh is there another new data bricks feature that is coming? Uh maybe we can if we can have a class on what does the future look like? What is your road map? You you you are telling us that like data engineering is changing every 5 10 years like the tools are changing the environment is changing.
(1:56:12) What does the future look like and how is data bricks working on this? Okay. So, we can have a road map uh section so to kind of give you um quick peak into some of the other innovative things that are being planned. Um and I've taken a note down on your case study too as well. Yep, we can do that. Any other wishes? I think that's all from me. Thank you. Yeah. Okay.
(1:56:56) All right. Good night everyone. Bye. Good night. Bye-bye.