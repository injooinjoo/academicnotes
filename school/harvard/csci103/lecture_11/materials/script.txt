103 day 11 - YouTube
https://www.youtube.com/watch?v=g2yuiKLFAqs

Transcript:
(00:04) right now. I see. That's very good. Uh yes, the key there is start early. Um divide up the work and nobody slacks. Everybody puts in their fair share and before you realize it's done and it's uh it's a very nice job. I am uh grading the first use case. Um, so some of them are really cool and I think I'll post it so that everybody can see all the other projects that folks uh contributed to.
(00:33) There's cross learning there. Um, and then um, uh, Shya, you had requested that we release the final project a little early because you have some tentative vacation. Um, we'll try our level best to release it by uh, next week. So with that I think from all the various components um of your um you know other than the uh you know the finals for which you'll have to prepare most of the components will be um given to you and it's just a matter of knocking it off and submitting.
(01:16) Um the last day of class would be let me check uh the calendar December 16th is going to be your final presentations. So there we expect all the groups to kind of come together. I know you're all remote but that's our request. Um that unless it's a medical thing or something really really um difficult just try to come together as a team and present.
(01:45) It'll be really fun and you can see what others are doing. Um and uh also try to make it to December 9th. We are going we have lined up a lot of industry um practitioners to come in. So that that will help you uh join the dots between what you're learning in from an academic perspective uh as in in a school environment and then what is happening with the industry and how you should be thinking about it from a career perspective from uh just you know um what's what's the link between what you're doing and how relevant it is going to be for your work field and immediate application.
(02:24) So with that Eric shall we get started? I've started the record. Okay. recording started. So, um, hello everyone. Tonight we'll be talking about developing and and deploying LLMs and agents. So, first um, let's look at so a few announcements. Um, the grades for quiz one and assignment three have been posted.
(02:54) Use case one and assignment four will be um, graded soon. Quiz two is is posted as of last night and um the final project will be released on November 18th and then um before then we'll create some new um groups for you to use and then um you can create your Slack groups for coordination on that and then um no section on on oh Thursday of I think not this week but next week and then um so we'll we'll be talking about um data and AI and um genai and um what about the older models and um use case benefits from LLM, the maturity curve for LLMs,
(04:03) uh how to um select the right um use case for your first um LLM project. We'll talk about rag models and um LLM operations and additional considerations for um Gen AI. And then there'll also be a lab. We'll also do um a short demo of um some applications of LLMs. So the evolution of AI um it's um we started with rulebased systems and then classical um machine learning regression models and things like that and then um deep learning and then gen AI was developed fairly recently with the the creation of large language models for processing text and then um
(05:03) and GAN models for um creating images. And then um and on the left we have a um vin diagram that kind of shows the the different um how these things are related to each other. Artificial intelligence encompasses machine learning which encompasses ne neural nets which encompasses um LLMs as well as GANs.
(05:35) And so Ji um goes beyond by um being able to create um new data based on existing data. So it's a key difference from traditional AI methods. So um their LLMs get their power from large amounts of um data and um they're able to learn based on that much like our um our mind is based on neural connections and um neural nets or large language models are use that same structure.
(06:16) Um, and the the resources that have been put to training the LLMs are huge. Companies like Meta and Amazon, Google and and others, OpenAI, um, basically vacuum up all the data that they can from the internet and use that information to train the models. So the models are very rich in what they know and and um they can answer questions um summarize text uh uh write computer code uh generate content um including legal documents and then also do um translation from one language to another. There's thousands of um different LLMs available
(07:05) um and um and they all have different capabilities. But one thing that's um the um the diagram on the right points out from Vellum is the um advancement of the models and how they're getting smarter in terms of their um ability to answer um qu any any type of question. So why why should we care about LLMs? Um they're they're about to reach a tipping point in terms of their um power and capability.
(07:49) Um so and um and it's accelerating. So, um it's very uh or economical enough that um anyone can really use them now in including things like the um Amazon Echo device is um now using LLMs to help power it. So it's accessible to anyone with a Echo device and also things like Google now include uh output from LM models and then uh the AGI or artificial general intelligence and and then super intelligence.
(08:40) Um people are predicting this could occur as soon as um 2026 next year. which is pretty amazing. And this is basically this is the point where models will um the LLMs will be a as capable as as humans. And then once they've reached that point um they'll be able to um train themselves and and break free of um or go even further. So become um like super intelligent where um instead of being as as smart as human beings, they'll be like thousands of times um or maybe hundreds of times more intelligent than humans.
(09:29) So, um and and one good thing about um the way that these technologies have developed is that they're most of them are open source or um publicly accessible. So, um they're available for people to use and not only use but also customize. Um it does require a lot of compute and especially compute using GPUs or graphical processing units.
(10:00) Um but that compute is available in the cloud and often subsidized by the large companies um pushing the technology. And on the right there's um a cover of an interesting book that you might like to read called Super Intelligence that talks about um what what will happen or what may happen when we reach um a AGI and super intelligence.
(10:30) And so um interesting to think about and and also not just think about but as practitioners ourselves we should be aware of um the consequences and and um and how to how to help um promote um positive outcomes. So um we so the in terms of machine learning we had um deep learning models and classical ML now we have um LLMs that are proprietary as well as open source and um and also um technologies allow allowing us to um uh create um applications using the LLMs things like uh lang Lang lang chain and lang graph and also technologies to create uh
(11:22) agents that um combine LLMs with um other other sources of information on in detail. We'll talk about that more um later tonight. So let's go through some terminology. So LLM stands for large language model. Um, GAN is a generative adversarial network um, mostly for images where LLMs are traditionally for for um, text but now um are becoming multimodal.
(12:00) diffusion um uh is able to produce things like um uh videos and um lip synced um characters and also deep fakes are um example of diffusion models. Foundational LLMs are the the base models. These these models are the ones that companies like OpenAI and and um Anthropic and others are producing. um very expensive to produce.
(12:31) Use basically all the all the information available on the internet to create them and um and then they can be used um for um uh further um what's referred to as fine-tuning to um create more specific models as well as use for um rag and other applications.
(12:57) Hallucination is something that sometimes occurs when the model um responds to a question with very um confidently but with a wrong answer and um and that can that can create lots of problems for users of the data and and can also be controlled by something called the temperature. So setting the temperature to zero will make it as as factual as possible but even then it can still hallucinate.
(13:24) Grounding is um the process of um connecting words or terms to real world objects trying to um connect LLMs to to the world. So which would help with their understanding of the world. Prompt engineering is a process of creating prompts to um instruct the LLMs to behave a particular way.
(13:55) For example, um you might prompt an LLM to be like a um a a a like a um a server at a at a restaurant or something. Uh zeroot learning is um basically asking the model um uh you know to pro provide um with some sort of prompt with a with with the text for the prompt. And then um um fshot learning is the the same as zero shot learning but um adding some examples um to help it um understand what what it's supposed to produce and that can increase the accuracy of the LLM response.
(14:40) chain of thought um uh basically breaks up the um the logic that the model uses into steps so that um and this is a technique that's been used to help improve the accuracy of the models and also helps with the interpretability of the model uh when it when it um when it um comes comes up with an answer.
(15:05) It can also um provide the steps that it used to to um get to that answer. Modality is the um data type um can be either text, images, audio or video. And then if it's um multi multiodal model then that means that it can handle different um different um modalities of data like GPT40 can handle text, image and um video content transformers um ar neural net architecture for um NLP that includes um um three different phases including encoder, decoder and embedding.
(16:01) Um each the encoder basically ingests data into the into the neural net decoder transforms from one one layer of the neural net to another and then the embedding will um basically reduce the outputs down to um like a series of tokens or words. Um tuning uh there's two ways to tune models.
(16:29) you can do it through instruction um and basically prompting the model to um perform a particular way and then fine-tuning is um additional training um with um some domain data to um help um help the model perform in that domain better. For example, like an aircraft mechanic, you might um you might fine-tune the model on on everything about a particular aircraft and how to manage it and um to have basically an expert that could help with aircraft maintenance.
(17:13) Um uh reinforcement learning with human feedback is a um technique used to help create a lot of the initial models and I'm I'm sure it's still used where you have humans providing feedback on whether the answer provided by the model was good or not. And then um retrieval augmented generation or rag is a popular technique for um extending models the um and and providing it's a lowerc cost way of basically similar to fine-tuning but rather than um training the model you provide the um basically search a repository of of information about a particular subject and and find out um or extract or return the um information
(17:59) that's relevant to the current question and then feed that um those results of the search with the question to the model um and then the model's able to answer the question generally um more accurately than not having that data. So use cases for LLMs um one is um to um democratize access to knowledge.
(18:35) Um the um you know now um basically you you can think about these large language models that companies like OpenAI have produced. um they basically have all all of the available human knowledge encoded into them and um and within that single model you can ask it almost anything and get a um fairly good response. So um that's um helping democratize access to knowledge um to not only um people in in the US or in you know basically all around the world and also these models are multi- have multilingual capabilities.
(19:19) So uh you can no matter what language you speak you can have access to the um to the knowledge. So very empowering for for the world as a whole. And examples um include enable um call center uh staff to ask questions, maybe maybe a act as a call center representative. Um and then also um we can uh quickly um get uh insights about unstructured data in a structured way.
(19:58) So you can um like last night I was uh transcribing some video content using a LLM to um basically uh transcribe it into text um that I could then feed into another model. So um so it's very good at at summarizing meeting minutes um uh and um and documents or patents or legal documents um can quickly sift through the document and understand it and and um provide the um relevant parts of it and also answer questions about it.
(20:42) um improve efficiency of knowledge workers. Um like as um software engineers um we're finding out how useful um large language models can be in in developing code um and basically saving lots of time in in um development. So that's a good example. um other um like legal assistants and and others are are finding a lot of useful and also for marketing understanding trends and um going through having a model go through and look at uh feedback from customers.
(21:27) you can quickly understand what are the current trends of um customer issues or um other other types of data and then also um the the models themselves can be used to improve themselves. um basically um tuning tuning product recommendations based on customer written feedback for example and just to I mentioned that um there's different ways to use models you can use um prompt engineering where you simply provide a um prompt to the model to instruct it how it should behave And with that there's there's no um additional data provided but all the all
(22:18) the data is assumed to be in the model um already and um so the quality is is not not high but the cost is um very low. Next uh level is rag where you um augment the um uh the model with information that's retrieved from some sort of database and feed that into the model along with a prompt and um that can include hundreds of thousands of um words um in that's fed to the prompt.
(22:57) Quality is better and the cost is um still low. um but maybe a little higher because now you're you're having to um support um some sort of database to to hold the rag and also the number of tokens that you're processing are higher which can also increase the cost. Fine-tuning is where you take a existing large language model and um and generally open source one that you have access to and um and do additional training with that model given a a certain domain uh context.
(23:35) So the the model can become an expert within that domain. And this this might be something that you might do for um medicine um to help with um some sort of um like help doctors or um caregivers for example. And then pre-train basically. Oh, and and the um the quality is is better. The cost is also increasing and this can handle millions or billions of of words or tokens.
(24:08) And then pre-training is basically creating a model from scratch. Um this is um this can result in a very high quality model but um also um can be very expensive millions millions of dollars in um cost in terms of training the data or training the model. Latency and privacy can also be concerns.
(24:37) Um, pre-training your own model is also the most secure because it's completely your your model. Uh, you don't have to worry about losing uh information or exposing information when you're um asking the model questions, for example. So with LLMs, it's important to um move quickly. Um lots of people have figured out that um these large language models can be um very useful in helping increase um business productivity and competitiveness.
(25:21) So um it's important for for businesses to to um to take advantage of them, learn about them and start using them for real world use cases. Um you need to um customize and secure um and control your LLMs. Um and then also uh um another challenge is to um how to connect your LLMs to existing data. So um for example uh for fine-tuning a model you may need to prepare the data to be able to feed into that fine-tuned model.
(26:11) um or rag, you may need to um index your your um your um data into like a vector database that could be used for um for the um retrieval um for the for the ra models. Some limitations of LLMs include um hallucinating which we mentioned before basically where the model um says something in response to a question that's um not correct and sometimes it's hard to know that the um that it's it it it made it up basically.
(26:54) and and that can be um somewhat controlled by um the temperature setting to zero and also um careful prompting. One thing that I've discovered is that prompting is a very um precise um endeavor like the the more the better your prompt in terms of telling the model exactly uh what it needs to do. um the the more clear and and concise you can be uh the better results you'll get with fewer hallucinations.
(27:30) Bias is something that happens um primarily due to the training data where um where if the if the data is um like biased in some way in terms of um ideas or um or just the the um some social aspects of the data that can bleed through into the um resulting model and that the the model can have those same biases and which is not always desired. They're usually not desired.
(28:09) And um adversarial tokens uh these are uh basically sequences of tokens that be can be fed into a model as part of the prompt to confuse the model and sometimes trick it into doing something it's not supposed to do. Um, Eric, there is a question. Can you uh just pause for a minute? Oh, sure. I didn't see that. Okay, good.
(28:33) Yeah, thank you for you know, thank you for um in the post. So my question is actually like this is an you know the LLM the it's interesting how you know some kind of attacks for instance and prompt injections um you know multilingual jailbreaks and everything these are huge issues in LLMs because you know vulnerability wise they're kind of like um you know it's a it's a big problems you know so what I'm trying to find out is my that's my first question how is so what you know as data engineers what how are we what are we doing to solve this problem you know and and the next yeah next question is just
(29:17) a comment it's not really a question it's a comment because this bias issue you know I think is it depends on where the model is trained for instance if a model is trained um in the US you know probably like some disease patterns you know it gives you answers based on how it's trained and then probably like um for inance Instance maybe if you people in other parts of the world um um gives it um questioning with prompts and maybe a different disease pattern is is more predominant there it might give it might be like a bias but basically like um you
(29:53) know it was based on how it was trained. So I think it depends on where the model was trained. So my my first question is actually like what what are we doing as data engineers to kind of solve these problems like prompt injection jailbreaks um that affects LLMs? Yeah. Well, yeah, the AI researchers um refer to that sometimes the alignment problem making sure that the models are um aligned with the needs of the humans that are using them and um including being secure and and safe and and and not um not trying to deceive or or or or um or even being tricked into deceiving or and
(30:43) um the humans that use them. So there's a lot of work being done um by the um companies producing and re research institutions producing these models to to help um put basically guard rails around the models so that they're they um they function correctly and without um without without issue.
(31:16) But it it's it's a challenge the um that you know and I think anyone involved in creating these models because the models are proving to be um so powerful and and have been like I think I or I did post a note on Slack about Albania using LLM to act as a um as a a government representative there to help with um uh provisioning and um contracts, government contracts to help of um remove um basically corruption and in the um uh and and and contracts and things like that that the government issues.
(31:59) So these um these models are being put into very um and will continue to be put into powerful positions. for example, um um Palunteer is um you know a US company that the the US government is beginning to depend on um for AI capabilities. So um I think we as um practitioners in the field need to be aware of the um the potential um misuse and and also the just the um all the things that all the risk involved here in in terms of creating these very powerful models that will soon be um smarter than we are in terms
(32:49) of intelligence. And um you know it's it's going it's going to be a existential um crisis or challenge for humanity to um to keep keep a lid on it so it doesn't um doesn't cause um cause a lot of problems. It has the potential for um being very um positive in terms of like what we were talking about with democratizing access to knowledge.
(33:21) But um but it's up to us as um you know um practition practitioners in the field to help um make sure that the outcome is good. And I I think that's why like the book um super intelligence that I mentioned before that would be a great book for all of us to read um because it kind of highlights um some of the the challenges and and and what could go wrong.
(33:53) So yeah, it's not not an easy not an easy um thing in terms of the bias like how to how to prevent bias and and model training. I think uh making sure that uh probably the the most important thing is trying to make sure that your data is is unbiased that your data is um encompasses all all available sources so it doesn't um only or if it is only from data from the US then it should be advertised that way so that um when people try to use it in other parts of the they really understand that maybe it doesn't know about um diseases or things that occur in um in in the South Pacific
(34:39) versus what what what happens in in the US. So I think um you know as one of our previous lectures pointed out that the data quality is is really critical to the um to the um accuracy of the models and and um and that that's um that that also goes for um helping prevent bias in the models. Eric Jeremy has a question too. Oh Jeremy, go ahead. Yeah, real quick.
(35:18) Um, are hallucinations intrinsic to LLMs like as just in in a way that it I mean will there one day be eliminated? Uh, is it is it a problem or is it just so intrinsic to the way that they're structured? It's just we it would always be a reality that at some point, you know, it would manufacture information or a response that that doesn't exist.
(35:39) Well, I'm sure they would like to um remove hallucinations. I'm not sure if it's possible. It's much like the Yeah, it's a generative nature. So earlier when we built models and we said no two models will exactly be the same because we were dealing with floatingoint numbers and there was like uh some amount of math behind the scenes and uh you know probability that would make the uh predictions just a little different from the previous but it was very very close to almost be insignificant but now geni is the whole
(36:13) business of generating content. So the motivation those the these models are highly highly motivated to get into that generation mode but then they cannot spew things uh which do not comply to a certain uh criteria and it all depends on how tightly are they wired or programmed to comply to those additional instructions.
(36:39) Are they completely clear? Is it hallucinating because it's ignoring your additional instructions or is it hallucinating because you haven't provided it and it wasn't clear enough to it. So at some point I think we can control it. Uh but because of the generative nature um I think it's going to be extremely hard.
(37:02) Um I when we get to the data bricks um side of the house and you know we have the you've already seen it when we were doing data warehousing. We have genie. Genie will hallucinate less because it's not trying very hard to please you. It's looking at the metadata and trying to create SQL out of it. It's not looking at the data.
(37:26) It's it's it's trying to create a um a SQL query uh which will then run on your data to produce you uh really accurate information. So that's a slightly different use case of um GI uh where we are not looking at the data. We're looking only at the metadata and inferring. Now does that mean that sometimes it would not hallucinate and make a mistake? It could, but the chances are less because the schemas are all very very um like you know sently defined and um a little more deterministic than reams and reams of uh instructions where the interpretation can vary from person to person. Perhaps
(38:02) I'm saying it in too many words but nobody really knows. I think it's getting better over time but it may not be possible to eliminate it completely. Just like we said that biases and variances in models they kind of attack each other from opposite ends and uh there is just an irreducible errors in models which are not attributed to the model at all but they are there um because there is just so many other environmental conditions uh coming in.
(38:31) So long and short, I think hallucination will remain in some form, but as Eric mentioned, it would probably reduce more just like we have guardrails today and we have temperature controls and other knobs, there will be more that will be created in the future. Yeah, go ahead. Thank you. Yes, thank you for those insights.
(38:55) you know you know like you know those um it's actually a very very important issue but I feel like you know I'm just talking on based on my experience using these um these models you know the key is actually giving them um you know the prompt the key is the prompt that you fit into these models you know I've had some experiences I've had sometimes I ask this this hallucination is real sometimes I ask certain questions and the information that is peeled out I know that is is partly correct and you know mixed with some incorrect information and then when I try to correct you know I based on I give the
(39:28) facts you know I put in the facts like this is where I researched and everything and surprisingly the model apologizes and says oh I got this wrong and everything so you be wondering like you know I mean that hallucination of a thing that's why I ask that question what are we doing about like you know this incorrect information because it sometimes it can be misleading you know and I feel by the way when that model apologizes.
(39:51) It's not apologizing because it has accepted that it has made a mistake. You have just confirmed that it the answer that it provided is not what you were expected. So that overrides its own interpretation and theory and philosophizing of how it is to provide the answer. So there are different ways of gaming the system.
(40:16) And uh for instance sometimes when I ask you to generate images it'll give me very confidently urls and you click on those urls they will give you nothing. So it just made up the text. So the text generation which is very good at but there is no image behind it unless you use like a deli or equivalent which which is actually in the business of creating models. Um but it is a very interesting space and it solves a lot of problems but hopefully it should create uh fewer problems.
(40:49) It should be able to solve more problems and uh uh create uh fewer but um only time will say how how good or you know whether we can really go into a state in which the the elements will be perfect. Yeah. I think that the training, the way that the models have been trained kind of leads to hallucinations because they're rewarded for producing answers and generally good answers, but they they want they try hard to provide an answer and sometimes if they don't know the answer, they'll they'll provide one anyhow, which leads to the the hallucinations.
(41:21) But this is a well-known problem and and people are working on it and the the models are getting better and better and and and um more capable all the time. So it'll it'll likely um be be less of an issue in the future. I think one thing that we have to remember is how quickly these these models have like um basically the been created.
(41:52) they haven't been around that long and um and how quickly they're advancing. So there's some hope for the future in terms of um like the the prompting and the prompt engineering that that goes into telling the model how it should behave. This is very important problem. There's an interesting movie I saw recently called Megan where the E is spelled like a a number three, but it's um interesting um perspective on what happens when the the the um prompting isn't quite right.
(42:26) And um so but again um it's it's really up to us as um as um contributors to these um the development and advancement of these these models is to um make sure that like try to try to reduce the risk, try to reduce the bias, try to make increase the security and and also um you know make make sure that they're aligned with um with people.
(43:01) Okay. Um last slide for me. Um basically just showing the iteration um pipeline for for creating these LLMs and and and and using them or basically using LLMs. So, so um the first step is to select a model that you want to use and there's many different models and um Vellum is a source where you can look to maybe select help select a model and the models are changing from month to month in terms of their capabilities.
(43:36) So, and new models are being created, old ones are being uh replaced. So um select the best model for your use case. Do some prompt engineering um to um to indicate to the model what you want it to do. Evaluate the re results. Is there room for improvement? Um maybe maybe you do um some sort of fine-tuned model if you need even better performance or it's not an option here but you uh doing some sort of rag uh implementation where you have a data source that you can search search on and feed into the model to um augment the prompt with additional information.
(44:20) And then um once once you've um created the model then or the deployment you can um deploy it for um for some sort of testing AB testing canary testing or or blue green test um environments where uh that way if the you can release the model and if for some reason you can test it to see if it's better than AB use AB testing to see if it's better than the previous model.
(44:49) uh canary you can deploy it along with the um the existing model to to see how it does but maybe only 5% of the traffic goes there and then blue green um deployments where if um you have a the blue model and the green model if if um you deploy to the to the green model if something goes wrong you can go back to the blue model.
(45:18) Okay. All right. So, now I'm going to um do a quick demo that shows um some applications of of LLMs. Okay. Can you see? Yes. Okay. So, um so this is a um AI assistant. It's it's multilingual and it's designed to um and it uses rag um retrieval augmented uh generative AI to um to produce um responses. So if we
(46:28) start it we can interact with it and hopefully you'll be able to hear it. Maybe not. No, we can't, Eric. I think when you share your screen, there's a check box that to include sound on the on the sharing dial. Share sounds. Yeah. Okay. Yeah. Let's let's try again. Hello, I'm a multilingual AI assistant.
(47:35) You can ask me questions related to our business such as what services I provide. How am I better than others? Uh um can someone confirm that you can hear it now? Yes. Okay, good. So I can I can ask it. I can using voice I can ask a question. So I can say um what is your mission? Please explain in Spanish. It sounds like you're testing out voice interactions.
(48:18) Would you like to ask a specific question or explore how DIY can help with voice and text bots? What would you like to know next? What is your mission? Our mission is to make this transformative technology accessible to small and medium-sized businesses, enabling them to enhance their customer engagement and take their operations to the next level through DIYVA. What would you like to know next? Okay.
(49:04) And I'm going to say um um u how can I schedule a demo? Um please respond in Russian. Yeah. So um so and that so the um this image is actually um using diffusion model to to generate this lip synced image. And then um it's using um natural language uh translation to um convert from English to Russian and then um and then voice using um another model to produce the the Russian voice. So, um, and then underneath it all is a LLM
(50:12) that's been augmented with, um, information about this business that it can use to, um, provide intelligent answers about things about the the business. And it's interesting though, but because it's built on an, um, LLM, it it still knows a lot of information about other things. So you can try to protect that with the prompt, but it it will um it will still um still be able to answer a lot of things that are maybe outside the scope of div.a for example.
(50:50) Okay. So any any questions about this? There are several things to highlight here. Do you see how the barriers to communication and interaction have reduced there? This is truly multimodal. You can use text, you can use audio, you can h you have video, you have auto generation.
(51:15) So the lip sync is dynamic and it's happening in real time. It understands your intent. It can speak many languages. So all of a sudden a lot of things are at your fingertips. Uh like so if I didn't know English, I can still get information from this website which would have been very difficult earlier. Okay. So, um yeah.
(51:47) So, now we'll look at another application where it's using um the LLM for search. So, um so this is a this is a fashion site using LLM. So here we can um we can say uh do you have or just search for blue sars and it's it it's again this is using rag um to with with a product catalog to be able to search and find things but um it also responds in text.
(52:26) So, here are some elegant blue sars and outfits that match your search. And I misspelled sarries, but it was able to understand that my intent was really to search for blue sarries with it um spelled correctly and and then if I wanted to click through, I could click through and see the the actual product there.
(52:51) So, using AI for a more more intelligent search is the idea there. And then um and then here um this is a another example of using LLMs to um for a try on service. So here um this this could be um a product page where you want to see what this looks like on you or maybe um your daughter or someone. So if I I click here, I can load in an image.
(53:24) Um, this might be the person that wants to try on the the the dress to see what it looks like. You can generate and this is also using a diffusion model um underneath to to basically uh take take the dress that's shown here in this picture and um apply it to the woman that's shown here. So, it's a it's pretty amazing almost like magic, but um it works.
(53:50) And so a lot a lot of interesting applications um in uh for e-commerce around using LLMs. Any questions? All right. Well, I think I'll hand it over to you Anandita to continue. All right. Sounds good. Um this is kind of putting the ML and um uh components a little more to the forefront.
(54:40) So you've got uh um the data platform and remember we talked about um structured data going into delta tables and unstructured data remaining in volumes but both of them are being governed by unity catalog with the same um grants and revokes like the same governance strategy. Uh we haven't talked much about delta sharing uh and marketplace but delta sharing is the zero ETL where data can come from third party uh you can share it securely within the organization or you can even bring it outside the organization without copying the data. So that zero copy ETL is big which means you don't
(55:20) spend a lot of money and it happens pretty quickly and it is secure and it is open source very powerful marketplace is uh when you're able to package not just the data but the models the notebooks everything that is needed to be able to use it effectively. Um and then you have feature store that we talked about maybe two lectures back.
(55:45) Um and then we talked about the model registry as part of MLflow. So these are all governance aspects where UC has extended itself. Once you have this then you are able to do effective vector search. Today we are going to look at an example using vector search. Um feature serving. Now this was the older way of doing things but um there is still a little bit of latency even with feature serving.
(56:09) even when you have online uh feature serving. So now it is being replaced by something which we refer to as lake base behind the scenes it's postcress and what happens is everything that is in the lake uh which remains in open format is zero into postgress and as part of the postgress ecosystem now your query latency is going to be a whole lot faster.
(56:34) So lake typically is OLAP analytical processing and Postgress is OLTP. These are our initial lectures. So we have kind of brided the gap there. Uh here this is all about uh preparing the data. So you've been working with notebooks throughout the semester. You've worked with the SQL editor.
(56:57) You've worked with workflows and in your final project you're going to stitch it all together with the workflows. uh we had a a section uh Paul came and talked about DT uh which is like uh simplifying your ETL having quality uh so all of this is data preparation in the middle section is where you spend a lot of iteration so the agent framework and evaluation is important um autoML we saw last time is important models in the marketplace we talked about this marketplace is not just for sharing data but for sharing models and notebooks and code and everything else today we are going to see more about the AI
(57:29) playground how you can compare models uh we'll talk about uh training and these are the different kinds of models so irrespective of whether you want to work with a closed source proprietary model an open-source model or a model that you have created yourself you should be able to do it all in one place um on the serving side uh we've talked about the EIBI dashboard we've talked about Genie uh a little bit more maybe uh on monitoring you'll cover uh next class um more and Paul are going to talk to you more on the governance aspects. Uh there's model serving which is very very
(58:04) low latency and high throughput. Um there's AI functions which we'll look at again today. There's the AI gateway um which is um the uh which is like a front behind which you have all the models. So just like you build a pipeline so that you can put different data sources but the code and the logging and the monitoring and all that scaffolding remains the same. save with the AI gateway.
(58:29) So it gives you some protection around rate throttling, around uh credentiing, around payload logging, um around AB testing. So you don't want to build this again and again. So this is just once. You can think of this as like say Amazon API gateway. It offers you certain functionality which is why all your rest APIs go through it.
(58:48) Similarly, AI gateway is your entry into all the different kinds of models that you're going to experiment with. um like you know every day there's going to be like a model from Anthropic, a model from Google, the Geminis, the Open AI. So there's no limit. There's no no stopping now.
(59:08) The the horse is like going to go full throttle and lakehouse apps uh is going to be a layer on top. Um uh remember um a long time back when we had these uh phones, their whole purpose was to be able to make a call or to text or something like that.
(59:28) Later people started spending a lot of time on their phones because there are a lot of interesting apps there like there is uh um you know picture apps and there's the Snapchats and there's the Instagrams and the WhatsApp. So the amount of screen time people have through their phones is actually very very significant and it's not because of the main capability of the phone which is to make the call but no it's the apps which are addictive.
(59:52) Similarly, lakehouse apps is the layer that is going to be on top of all this data and model that is sitting. So that your business functions are going to be provided through these apps and they are going to explode. uh there are going to be a lot of them and data is boring like you know just looking at rows and rows of data is boring like but once you bring it up into a story once you bring it up into an insight and you know somebody has access to things that they care about then they can start contributing more meaningfully and dashboards is like one way interface like whatever is there you're consuming
(1:00:23) but a lakehouse app is going to be truly interactive like you know I I see a I'm a a business user or I'm a tech user or I see a problem, I want to fix it and I have the privileges to fix it. I can do it and that can go back into your pipeline and on top of this uh we have the CI/CD. So Ram is going to talk about uh CI/CD and DABS um not next class next class is going to be devoted to governance but the the following class is going to be around um uh CI/CD.
(1:00:55) So we'll talk about asset bundles and Terraform. So Terraform is for your infrastructure and data bricks asset bundles is like all your code assets, your pipeline assets, your cluster configuration that need to go from one environment to another from dev to stage to broad and you don't want to spend like 8 months taking something to prod when it took you just 2 months to develop it. Right? So that process needs to be very efficient.
(1:01:19) Um and an extension of it is your MLOps. So your devops, MLOps and LLM ops, we've talked about it and MLflow is a very very important component there. So this kind of gives you the different uh categories and what are the blocks or what are the capabilities from each one of them to give like you know just to ground you.
(1:01:44) Um and this AI maturity curve has got impact on this side and complexity on this side. So obviously you go from left to right the complexity increases but the impact also increases. This you start with BI and then BI blends into AI. So BI is something that is needed to run the business.
(1:02:03) So your basic dashboarding and questions and your SQL warehousing like every business has it. Um but AI is what can help you change your business, remain more competitive, go into new um markets, new use cases, new scenarios and and uh transform it. And um you know some sometimes you might say oh I'm in the telecom space or I'm in the insurance space or I'm in the manufacturing space.
(1:02:28) uh but we are coming to an era in which every company will first be a data and AI company and then their respective uh functions are going to be layered on top because if you can't uh go fast enough in your data and AI journey you kind of lose it uh to your competitor at that point. So uh genie and dashboards we had covered as part of your TVSQL and warehousing uh lectures.
(1:02:54) So that's your agentic BI and these dashboards will tell you what has happened. They still give you some uh history. They give you an ability to talk to your data. Uh Genie especially is very powerful and we are going to look a little more of Genie tonight as well.
(1:03:11) you move into AI functions and we had seen these as well where in a select statement you are calling out to a lot of um uh geni capabilities like um uh similarity or um summarize or sentiment analysis or extract information just one line in in SQL just like you call average or max or min now you're just calling an AI query function and it's it's giving you the data Isn't that powerful? So, you're incorporating that into your pipeline directly. The next space will be the knowledgebased agent.
(1:03:45) So, uh Eric was just showing you an example where you have got a lot of unstructured uh documents. So, it can be PDFs, it can be images, it can be um you know um a lot of forms from which you need to extract information. So this is going to digest that information and help the human or be directly uh customerf facing to give the responses that they look for instead of having to scroll through um huge reams of paper.
(1:04:16) So call centers for instance were the first use case that LLMs provided for. And it's not that the LLMs were directly interfacing with a customer who's calling. They were helping the call center operator to be able to give the responses more concisely, more accurately to summarize the previous call. So that's the knowledgebased agent and this is also got significant ramifications in the industry.
(1:04:44) There are some um areas where um young people do not want to um work on like say somebody says oh this is a cobalt program or a mainframe that you have to maintain. Uh nobody really applies for those jobs anywhere but those systems have to run. If everybody were to work on uh bleeding edge stuff and uh where is the domain knowledge? So all those people who have been maintaining these old systems have got a lot of domain expertise what we refer to sometimes as tribal knowledge and these newer people don't know any of it like unless you work in those spaces unless you actually deal with it you won't. So
(1:05:21) now these LLMs are going to act as a bridge to join the newer generation workforce coming in and the old generation who do not know how to use these newer tools and these newer people do not know how to use those older tools and neither do they have the domain uh expertise which once this person retires or goes away from the company is lost forever.
(1:05:44) So this this has uh this is very very well adopted now. Last year we saw a rise of it. Now I think it's fairly getting fairly stable. Uh agents with multiple tools. Um again we'll I'll show you a multi- um agent supervisor uh agent bricks uh demo today. And uh calling an a one function or tool as an agent is one thing but having a whole repository or a whole library of them and having a router which kind of understands which one to call to is the true agentic uh architecture and that is the orchestration or multi- aent that we will um it's like the pinnacle of uh where the AI maturity curve is today.
(1:06:28) Um Eric already talked about uh rag which is uh more common than say fine-tuning or pre-training. Fine-tuning is when you take a foundational model and you train it with millions of data point to be able to produce maybe something which is more specific for your organization. Uh so it understands language.
(1:06:52) It has a lot of vocabulary but it doesn't understand your uh uh context like uh what is your fiscal year, what is what does um customer mean, what does revenue mean in your context. So all of that uh and your business uh terminology is going to be fed but it's not little amount of information. When it's little, you use rag.
(1:07:10) When it's a lot, you fine-tune it. And when you want to build it completely from scratch, that is called pre-training. And obviously the cost goes up a lot and nobody really does it because it's a very specialized skill. So unless you are an open AAI or a meta or um I do not know something uh like uh um like really really hot in the market you do not produce your own LMS.
(1:07:34) But what will happen it takes so long to create these LLMs. You give it all this information um millions and billions of tokens. you take some amount of um months or whatever to produce these uh huge expensive models but they get stuck whatever whatever date or time was the training that's all the information they had you would have noticed that openAI is now very good about giving you up-to-date information because it uses agents to go out and pick up the information from the internet but there was a time when it would not know who's the current president because uh it was trained a little earlier and how do you
(1:08:10) um update it on a more regular basis. It's not possible. So that is where you use those models either a foundational model or a fine-tuned model but you give it some additional information and rag would be able to you know you can run rag a little more frequently and even better would be to use agents who can go out and get get you upto-date information.
(1:08:34) Sometimes it's not easy to understand which sources an LLM is considering when they're arriving at their conclusions. So you would have noticed that sometimes in in a rag application they will also tell you that this is the snippet or this is the document from which they produced the answer and a few organizations have financial and human resources to produce and uh deploy foundational models. That's what we were just talking about.
(1:08:58) you have to be you have to have a lot of manpower who are skilled and a lot of resources because it requires GPUs and there's always a dirt of GPUs. So that is why rag is one of the most cost effective easy to implement and low-risk path to higher performance for ji applications and how does it look like like behind the scenes what happens uh somebody is coming in and asking the question but before that something else is happening you've got a lot of documents tons and tons of information which is uh very specific for your organization so you will ingest it through um a vector
(1:09:36) search into a vector database which is where the embeddings get created and make it available. So they have been chunked, they have been parsed, they have been converted into vectors and they store they are stored in specialized databases known as vector databases.
(1:10:00) Even a postgress can serve as a vector database but some of the newer ones like pine cone are high in demand and they are expensive as well. Um then when your question comes the same set of things like the tokenization the chunking the creation of the embeddings also happens on that little piece of information like where is my claim blah blah or what is the policy for blah blah all of that happens and another vector is created.
(1:10:27) Now this vector is going to be compared in your vector database very very quickly to find out which is the closest vector that matches it and that similarity search is going to return um certain documents. So these are the documents which are then going to be sent to your large language model along with your instruction and the prompt.
(1:10:51) So your instruction will be like oh you are an insurance agent. The context will be these documents and the prompt is uh be factual uh answer within three sentences. Um do not uh answer something which uh is outside the scope of these uh documents. So all of that is put together and the total length of this is your input tokens that you are going to pay for and remember we talked about the AI gateway behind which you have your um models and you have your agents. So that's going to be put into work uh to do a variety of tasks for you.
(1:11:24) So these agents are very nimble. You can almost think of them as your microservices. One of them could go uh look into a database. Another one could uh uh look up the weather. A third one could just do some computation. Whatever it is, it's going to put it all together and the response is finally going to go to the user in natural language.
(1:11:43) So even if this is like a blob of JSON or or something, uh now the LLM knows how to take that response information and translate it into natural language so that it looks conversational to the end user. And that is exactly what Eric was showing you a little while back. But this is what goes behind the scenes.
(1:12:02) Now um of the folks who are here today, how many of you are already working on rag systems? Anybody? Yes, Jeremy. So what do you use rag system for? In what industry in what context are you using rag? um education and I built uh just with Gemini's API the the Google cloud API uh and it was a really interesting learning experience you know having to create a vector database and really just seeing you know how you can put all the different chunks of your of your corpus into these different discrete chunks and I was researching about I guess the logic in which you overlap some of the
(1:12:52) chunks you know can plays a role and and reducing hallucinations and it was just it was really insightful. Interesting. Yes. I think Eric you found a text uh recently right? I think you had mentioned 800 to 1200 uh is the chunking size which is optimal and 100 to 200 is the overlap side so that you do not lose any information but at the same time the information is between the various chunks is uh linked and interrelated properly.
(1:13:24) So that's rag and um very simple concept already very very mature in the market I would say so much so that today I'm going to show you agent bricks which is like an autoML to-do rack um but to round it off um we'll say that data ops ML ops geni instead of just regular is going to be LLM ops and um you can unify this LLM ops component to the rest of the uh ML ops stuff that we had done.
(1:13:58) So there are just a few additional blocks that we are going to add to that picture. Um and that could be this fine-tuned um LLM or it could be a vector database. The CI/CD part you're going to learn in two classes now a little more in depth so that you'll appreciate it. your model serving is important and this is still ML flow with the unity catalog and your promotion from dev to stage to broad um because your CI/CD system is the one which is going to test it and it's going to push it across these various environments and once it's available then your requests are going to come in and um as your results are being uh logged you're going
(1:14:38) to evaluate and you're going to continue to monitor for speed for accuracy for uh performance and so on. Um now the on top of course you have git because you do not want to lo lose any information. So this basic blueprint still remains the same but we've added a few things like as I said uh you might have a different set of models from which you fine-tune.
(1:15:03) You might have some agents, you might have some um vector databases and vector indices. Now there could be some sensitive information in those vector indexes. So just like you are protecting your data, your file, your tables, your models, you will protect your vector search indices the same way in Unity catalog using grants and permissions. So only some people would be allowed to see and some others may not.
(1:15:28) Um sometimes uh people get very very excited about uh Genai and they want everything to be done uh through LLM. Now LLMs can be very expensive and we were just talking about some of the side effects of it like you know additional hallucinations more bias and uh difficult to control more uh blackbox and so uh if you are in charge of such things and you are being tasked to do some things use case selection in your organization is very important.
(1:16:02) Why? because the first north um like you know the the the northstar use case is going to determine the fate of how quickly the organization is going to adopt and evolve. Uh you take one which is the hottest use case and you don't you are not able to show value very soon everybody loses interest and everybody will say LLM is just a fad. Whereas if you are able to show h like you know high feasibility, high value and low risk use case so that everybody learns from that experience uh understands how to templatize pilot there's some excitement around it everybody gets
(1:16:34) educated because the space is growing and changing so fast um then there's chances of stickiness is there and it's a cultural thing like one group adopting one LLM is not making the organization AI ready right it's it's a much much bigger uh task there data accessibility and quality.
(1:16:57) So ease of implementation, adoption, reusability to other problems. So once you build something remember we said the registry is important because people can come and see the model understand what it does and if it fits their need they don't have to go through the process. It's expensive it's timeconuming they'll just reuse it and you have to build expertise um AI expertise in the entire organization.
(1:17:17) So you would have um been sent to train programs and there would be mandatory lessons uh because in the beginning stages of um OpenAI um there were uh trade secrets and other things which were um exposed and uh LLM would uh say things about your competitor to your customers instead of answering uh questions that are relevant to them.
(1:17:37) So that was the board and GM goof up. Um and you could trick the LLM uh through all the injection meth methodologies we were talking about earlier to kind of give it its secrets because if that information is there at some point the LM might get a little tempted to respond. So having guard rates is also very very important.
(1:18:00) So your problems around securing the models just got a little higher. Like with LLMs, the amount of discrimination, exclusion, toxicity, information, miss, malicious use, um all of these problems remained but just got like magnified. Um the explanability part especially is uh even more difficult. We talked about hallucination and breaches of privacy is probably the most dangerous uh because now you just don't know how of these reams and reams of uh uh unstructured data. There was like one social security number embedded somewhere.
(1:18:32) Now all of a sudden the LM is feeling very uh chirpy and uh spitting it out to whoever asks for it. So regulatory compliance um GDPR requirements and so on. The other thing to kind of just take a step back and realize is laws can be enforced but ethics cannot be enforced. Ethics can only be uh adopted through cultural norms.
(1:18:58) So as a society we have a collective moral responsibility and you can't enforce it but you know if the culture is right then everybody does what is correct for the larger good and things uh uh remain or like order is preserved. Um nowadays companies are focusing on ESG score of a company. Does anybody know what an ESG score stand for? Okay.
(1:19:32) So E stands for an environment factor like how much carbon emission you're doing and um you know this these are also known as sustainability indices of a company and companies who um who have a stronger emphasis on focus on it just get a little more um good press goodwill um and they usually do the right things because they have governance bodies within them to kind of ensure that um uh the larger good is followed. S is the social obligations.
(1:19:56) So you know you will have a day in your company where you're supposed to help out and G is the governance. Uh so we you know all these things that we were talking about. So these are built into their internal organizational policies. So by incentivizing organization to prioritize fairness, transparency, privacy, accountability policies and so on. You know we will ultimately be building ethical LLMs that benefit society as a whole.
(1:20:22) You must have heard about the case where some uh OpenAI uh chat interface uh recommended that the child commit suicide and they actually did. Sometimes I don't even know whether to believe these news or not but it I heard it in several uh forums and so that there might be some element of truth.
(1:20:41) So if a person is not mentally very strong and they use uh these elements as companions and these companions are are not very good and they just don't understand um you know um what what is the line of control they might be advising on all kinds of wrong things anyway so potential safety nets and bandates we are a very iterative process you have to collect the internal interaction details because how else will you know what's going on that is why the AI gateway that we talked about which automatically logs The inference payload is importance model monitoring we've talked about it a little more in detail earlier and so we won't talk too
(1:21:15) much about it but this ML scoring which is where um you know these guardrail models are um there so what will serve as a guardrail model so if you're using um a model 3 then a bigger model a model 4 is going to which is supposed to be a little better can be your guardrail right you you usually Don't use a very teenytiny model as a guardrail because then uh it may or may not even comprehend what you are supposed to do.
(1:21:47) And RHFF anybody can tell me what is the full form of this. This is the human feedback, right? Um when when you are um having yourmes monitor uh through review apps what the LLMs are doing, they can't possibly monitor each one of them. So they are responsibility is kind of uh broken up between these large LLM guardrail models and um real time uh human feedback.
(1:22:22) then you know you kind of know if something is going too rogue and needs to be stopped. Careful prompt design. Um I'm surprised as to how crucial a little bit of text to the model to kind of tune it uh is. And if you your your um uh you've done a good job with prompting uh then chances of it uh ignoring all your instructions is uh is limited.
(1:22:47) I I think some amount of careful prompt design does go a long way. Um Europe is usually pretty good about being um a forerunner of all these um regulatory stuff. So there is already the EU AI act. Uh so companies are mandated to disclose what kind of data they're using, what kind of compute model deployment like everything they have to uh disclose.
(1:23:11) I don't think it's true of everything in the United States, but in Europe through the EU act it is getting. But if you look at it, it's both a good thing and a bad thing. Like every little thing you have to do if it is customerf facing or even if it is not, you have to spend significant amount of time documenting it and providing its u uh value and its worth and its fairness before you can actually use it.
(1:23:42) So which means the the pace of innovation will be restricted but hopefully you won't do too much of harm um in the name of innovation either. I saw a hand raised up. Was that you? Okay. So, the key takeaways is every organization will be a data and AI company in the future. Um, your core business comes secondary. Your data and your model will set you apart from your competition. So, if everybody were to use Jack GPT, they'll get exactly the same answers, right? So uh there there has to be a mood and you know if you fine-tune your model either through rag or through fine-tuning or pre-training and use your specific data so your transactional data your policy data then that is how you can
(1:24:27) differentiate from your competition. There are various levels of complexity of LLM and you can adopt the one that best suits your needs and maturity. So we start from prompt engineering we move to rag then we move to fine tuning and then we move to pre-training.
(1:24:45) uh a reap of narrow purpose fit LLMs are more useful to an organization as compared to hunking large ones. So a chat GPD can be very expensive and you don't need a chat GPD for all your needs, right? A chubby can tell you some actress's name or um Taylor Swift's cat names and so on. Who cares? Like I don't want to know those information. I want I want to address a specific tasks.
(1:25:05) And so from LLMs, which are still going to be the brains behind it, you're going down to tasks. and for this task I'm going to use a mini. For this task I'm going to use maybe um um a turbo uh and so on. Right? Models are improving and getting cheaper. So it's possible to have your own foundational model.
(1:25:23) Although I don't see too many people going down this path because it's a very very complex thing to do in the educational context which Jeremy is doing. It can help promote plagiarism. Uh but benefits far outweigh the risks. So I noticed uh my daughter constantly uses um chat GPT to um get her responses almost to the point in which that has become like quite a pretty good companion and the minute she logs in Chad GB would say hello blah blah nice to see you types like you know it's it is it is almost as if it's the human tutor on the other side but she's benefiting a lot from it. Yes, maybe she's not doing as much as she should on
(1:26:02) her own, but but that's like a good way to double check things to um to be able to look up instead of uh like watching videos or or doing something else. And um if you follow Khan Academy uh he's this gentleman who um created um all these courses to kind of help uh uh school age um students in a lot of subjects where they usually get very intimidated. Uh he was also of the opinion that yes this is here to stay.
(1:26:35) Uh like imagine the data bricks assistant that is available to you. I know a lot of you use it for your assignments and all but it's isn't it better than not having it at your disposal to tell you this is right and this is wrong. Um but at the same time like ABCD you should know so that later you can spell C A T and D O G right that that ABCD um portion hopefully we are able to instill uh some elements of it in class and some of it is just uh your own um um desire to learn and grow and then there will be things like cursor AI or uh um or claude um um agents and
(1:27:13) whatnot to help you write code. So writing code uh which was a very esoteric thing a few years back is going to get a little democratized like that's not a barrier anymore because there are tons and tons of these very sophisticated tools to be able to do so but if you do not understand fundamentals if you do not understand concepts you will not be able to use it so I do not think we should look at uh chpt or its equivalence as a form of plagaris plagiarism I think it's okay it's it's a helpful tool uh they do pose the risk of uh automatic um like automating some routine tasks.
(1:27:52) So humans will lose job and they are losing already and the trend is going to continue. So we talked about it earlier uh maybe towards the beginning of our semester as to how the entry-level jobs are gone but the number of jobs for experienced individuals continues to grow because somebody has to keep these uh AI systems going also right they need skilled manpower but it this is going to cause a tremendous imbalance in society.
(1:28:18) These um young graduates from schools and universities do need to be trained and uh uh yes their job can already be done by um AI systems but if they are not given a chance to enter the main work stream and get skilled um who is going to replace the the senior ones when it's time for them to retire or time for them to move on.
(1:28:43) So these these are very very philosophical questions and we'll see how time is going to answer some of them. Hey Anandita Gi has his hand raised. Okay. My question is just um I don't know if it's re it's not really but it's kind of relevant but I just want to understand you know how these things work you know because sometimes like you know my general opinion is that you know this um AI agents this um LLM you know GP they've got got to the point maybe when you prompt it you know it doesn't just give you um some general responses like
(1:29:21) feedback like he just wants to answer a question. It assumes that he understands you very well and there are certain things about it that I'm still trying to understand like you know you know there was a time maybe probably I I prompted it and then it was like you know listen to me I'm not just giving you this advice as a generic generic AI assistant you know I'm giving you like a data engineer so I was like you know I got to a point that you know is it that I've trained it with my prompting that it had gotten to Very true. Very true.
(1:29:59) Because sometimes I also notice that I would have started a new session. It of course knows who I am because I'm logging in. I've started a new session but it can link my conversation with a previous conversation in a different context which means there is some space where infinite amount of memory is being um or infinite number of these um sessions or turns are being saved and it is able to go back in time um to retrieve it or maybe you're right maybe each one of us is having our own personalized assistant I doubt At this
(1:30:36) point, perhaps it's able to just summarize all our past conversations uh across different sessions, across time, across geographies, and be able to connect the dots faster and more efficiently, effectively than any human can do at this point. Yes, Jeremy. Yeah.
(1:31:03) um on a little bit separate um question is one thing I found is that the a lot of these models um are very uh inaccurate when it comes to the the time frame. Um they like they don't know what is more recent and what's older. You know, when you're looking up API documentation, they always give you outdated information.
(1:31:22) And even when I, you know, did this little rag thing, it's it's it's hard for it to distinguish. And I was wondering is that just um you know um a current kind of fallback uh that they'll correct eventually or is that something that's more intrinsic to the way it functions where it's just it's a it's a computer science challenge? Yeah. Um so in organizations there are policies and policies get outdated and there's a version one, a version two, a version three.
(1:31:48) And imagine if all three uh documents or policy manuals have been ingested in rag. There is absolutely no way you can tell um the uh the rag agent or LLM that you are building uh to retrieve the most recent one. You can try but amongst all the other information I wonder if the date on the document is given like a special privilege and the other two are weeded out.
(1:32:24) What would you expect? You would expect that the other two should be um weeded out and only the most recent one should be used or maybe the first document was very large and only the update was sent into the second one and the third one had an update into a different part. So now it has to munch all of those things together as a single entity to be able to answer your questions. But we are not doing it. We have given it three different uh documents.
(1:32:47) And so it's going to first look into the first one, find the answer and give it back to you. Now you have to relate, you have to link it almost like a graph database to say that no this one has a um a successor and this is the more recent one and that's what you should use or no this one has a successor here by the way this one has no successor so you should fall back on this.
(1:33:11) It's a very complex problem and um some of it could be addressed if you are doing CRUD on the vector search index itself like the entire document is being replaced in which case you'll say this the latest version I want you to forget everything about the old one um and just look at this new one but if you're if you're updating only patches of it then there has to be some way in which you have to link documents which I don't think rag does uh at this stage So we have to get more sophisticated to solve that problem. Thank you. Mhm.
(1:33:45) You can definitely try in the prompt but I hope you I was able to explain um the difference between the different scenarios. Uh so on on data bricks we have our own vector search uh database and indexing mechanism and we support full crowd.
(1:34:03) So basically you put it into a delta table which then syncs into the vector search index and if it is able to identify it as the same document then it's going to replace it. Uh so you have to replace the entire document for it to ignore the past and remain only the future. But if you give only sections of it and say oh this is um V2 and I didn't replace the whole thing I just replaced the certain section of it then then the problem becomes very hard.
(1:34:29) All right. Um let's look at uh a little more on batch inferencing, playground, UC functions as tools, agents and AI gateway. Of course, I will not be able to cover it all in the time that we have, but in the lab section uh I do have like a very detailed uh PDF on how you can um build these agents and there are three distinct types of agents. The first one is just Genie.
(1:35:00) Genie behind the scenes is an agent and so it gives you step-by-step instructions on how to create it, what questions to ask. So everything in blue is basically for you to ask and everything else is like instructions on um on what to set up. So you you set up your trusted genie. The second is a knowledge assistant and the third is the multi- aent one.
(1:35:18) So I'm not going to have time as I said but let me give you a a sample. uh there is a um there is a notebook as well and uh it's called uh life AI underwriting. So there you will see that um you have widgets in which you get the catalog name. Uh you've done this in one of your assignments. So hopefully this should uh be familiar.
(1:35:45) So when you say dbuttils widget get whatever catalog name in this text area whatever is the value is now saved in these two. So that's your catalog and your schema and I gave it this because I'm doing it in my um demo shard environment. Uh you can run this notebook in your uh free edition as well. So it will create the schema and um you will create all these uh tables.
(1:36:10) Uh it will insert values into all of these things. So it's a complete um data set along with the schema that you're going to get. But what I want to focus on is these functions. These are called UC functions. And you can create them in your catalog under your schema and say that this function does this.
(1:36:34) It returns all the code details for a specified client including coverage, premium, estimate, pro uh product option and health rating. And it's basically a select statement but it is um it has got parameters. So the catalog and schema parameters and um it's uh it's going to say that um this this is the name um of the person.
(1:36:57) So you might be might have passed some uh name. So this is the client name that is passed as a parameter to this function and it returns all of this stuff. How it does it is through a very deterministic uh SQL and this can remain in Unity catalog. So let's look at how it looks like. Um that's my schema.
(1:37:18) These are the regular tables and sample data that we have put. This is acturial, client submission, code, code status, risk and so on. Uh this is the volume in which I've put a policy document that we are going to use as our rag which is like a knowledge assistant. And these are the three functions. So get a quote uh get quote status and get risk score.
(1:37:37) So in get a quote uh this is the same SQL that we were seeing in the notebook. It's right here. Um this is the parameter. It takes the client which should be a string and it returns this uh set of data. So quote id is a string, client is a string blah blah and it is yes it is very very deterministic because it's going to look in and uh get you the data. All these are additional metadata and of course I created it and it's used in SQL and so on.
(1:38:05) So this is what we refer to as functions. It is within Unity catalog and it has permissions. So I can grant somebody else in the organization to execute this function because it could have some sensitive information. I don't want everybody to do it. I can give all privileges or can can manage privileges.
(1:38:24) So somebody else can come and edit it if they want. Right? So these are functions. This is volume which you have done quite a bit of. So there is like an underwriting guidelines PDF that is uh put in here. Uh that's the first one which is get quote that's the second one. Um so this is just basically running it to ensure it get quote status.
(1:38:49) Um uh running it to make sure that it works and get the risk score. So that's the value. Um once I have run this my schema is created my functions are created my tables are populated. Now I can go into genie. Um again in this section remember this is common this is for the SQL persona this is for the data engineering persona and this is for the ML persona because it genie works on structured data it somehow is considered a SQL thing although it has got natural language and it's you know you can you can query the data you can integrate and so on so forth so that's a new genie room that we
(1:39:27) were create how how do you create one again to remind you if we go in here you can get to a table and uh you can say create query notebook dashboard but this is a genie space this is a vector search index this is a synced table this is a UC metric view and so on right so that's how this um genie room has been created and um in this um these are the three functions that have been registered here uh so in the configuration section you can give it additional instructions so all of this is there in that PDF
(1:40:03) document. So if you want to follow it step by step, you can just copy from there and and uh put it here. So you're a specialized life insurance assistant. This is what you can do. This is what you shouldn't do. Premium means this. So these are all uh instructions to give the LLM some context.
(1:40:23) You might want to specify that of the tables. Um it can figure out through the metadata what is the relationship, but you can help it give additional details. So here you can say um I want to add a join criteria. I want to um filter on these things and these uh you can edit this to say that on this table and this table it is this join conditions and what kind of relationship.
(1:40:49) So all of this is basically additional metadata to help it do a better job. You can give it some benchmarks too. So you can add a benchmark. Basically benchmark is like you give a SQL question and you give the ground truth answer and that's your evaluation step.
(1:41:07) So every time you tweak it, you bring in new data, these are like your ground truth and it should like just like you had unit test cases and your unit test cases have to pass. So when you modify your code and your unit test succeeds in one scenario and fails in like a different sector, you know that you had some regression and you go and fix it.
(1:41:25) So uh what's the average of premium by gender and uh you know this is the SQL that it should generate so it knows um that these are your evaluations and these are the questions that it should care about. Uh you can um you can look at so we looked at benchmarks you can also look at the monitoring to see that these were the questions and what was the response what was the rating and so on and you can share this with other people u if you want somebody else to come to your genie room you can do so so back to uh back to the chat so this interface
(1:41:59) you can ask your question and I think we can go back here and pick up a question that is Let's see. So, we have set up all of this. Provide instructions. Uh we can say things like compare the risk profiles of our clients with occupation software engineer versus electric and explain why their premiums should differ. Right? You you I'm not going to go through it.
(1:42:25) You you can just do another thing you will notice uh is this has got two views. One is a chat view and the other is a research agent which is also in preview. So again I'm not sure whether you'll see it in your free edition but at some point this is going to become available in the research agent side is interesting because you remember how I said gene is a little deterministic because it works on metadata it works on schemas and uh it doesn't look at the data per se it is able to generate the SQL which then gets executed to give you the answer here you can say things like what will happen if interest rates goes by such and such now you are you're doing these exploratory
(1:43:03) type things which are completely um research oriented which are completely like for that people would have hypothesis people will go down one path second path third part fourth path so that is exactly what it will do you will say um if my interest rate goes up by this many points can you look at the ramifications to all my portfolios or um you know what is the um product line which is going to be affected the most so it will it will take much more time because it's it's doing something very heavy and it will tell you all the research that it did and what's the
(1:43:38) chain of thought in how it came up with all those uh answers. So that's very powerful because u genie by itself answers the what questions and the research aspect of it answers the why questions. So root cause analysis type of scenarios is what uh this one will be more geared towards. Uh now what I realize is this agents which is very very powerful and is in beta is not available in your free edition but it's it's all the rage today. So it would be a miss to not show that to you. So agent bricks is like our
(1:44:16) autoML for agents and this is where you can click just a few things and be able to create. So there is this information extraction that PDF that I gave you uh is uh is something that you can provide here. You can say use PDF. So you select the folder where the PDFs are. You put it into volumes and you select that.
(1:44:42) You select what is the destination table and um um the the table uh the schema and the table and um the information extraction is going to actually I take that back. What I've given you is for the knowledge assistant, not information extraction. Information extraction is for a form. If you have form with lots of fields and you want to pull up um like smoker, no smoker, cholesterol, high, diabetes, uh all all of those kind of um information that has to be extracted um which is basically forms and surveys.
(1:45:14) Then that is what um this one is going to do information extraction. Um the knowledge assistant is the rag and I got confused with it. So you give it a name, you give it a description, you give where your knowledge sources. So in this case it's either in a vector search index if it is unstructured data or um or maybe it is raw in which case it is in UC files.
(1:45:34) You give the source. So I can say that this is my catalog and this is my uh volume in which it exists. Give it a name. Give it a description and that's it. You can create an agent and I have done one. So I'm going to show that to you right here. So let's go to agents and I'm going to say owned by me. So that is the life underwriter agent and it's going to look something like this.
(1:45:59) That's the volume in which I put the policy document. It answers the questions. That's the last sync that we did. Uh you can update an agent and this is where you can start asking questions of your uh agent. And again you can refer to this uh document to get some questions to ask of that agent.
(1:46:18) So let's go to the second agent. So that's datab bricks agent one agent two um average premium by occupation type and uh risk level for instance. Right? So you can you can just copy these and put it here. You will get this. It's going to come directly from the uh PDF document and you can compare to see now at some point uh if you are going to have an a newer data source so you can sync it. It will show you when was the last time that was synced.
(1:46:53) You will update the agent and then you can see your newer responses here. Now if you want to improve the quality remember the human feedback that we were talking about. So this is where you can add proper questions which has said that if a person asks you this type of questions this is how you're going to do.
(1:47:15) So you can of course import and sometimes you can even export you can add this is your labeling session. Uh well in the future in the previous class we talked about how important the ground truth is and these labels are what helps the model learn. So all of that you can do here. Uh now let's go back. Let's go again to the other agent. You've got an underwriting decision copilot.
(1:47:40) How did we do that? That was this multi- aent supervisor which is tool calling and agent orchestration. So here again those instructions are there. You can choose either a genie space an agent endpoint a unity catalog function or an external MCP server. All right. And you can have I think a maximum of 24 20 different agents and tools that the supervisor can go and uh once the intent is understood like when a question comes the supervisor will first understand the intent and it will route it.
(1:48:11) So let's say I can choose a genie space here and I will say this is my life underwriter um and then I'll describe some content. I'll add it and then I can choose another agent. This time I'm going to say it's an agent endpoint in which case I'm going to give it um one of my knowledge assistant. So let's see takes a little bit of time. So this is a knowledge assistant.
(1:48:30) I can choose that and you get the idea. That is how the other one was created. And if we go back to the agents and look at owned by me, then you can see that this was called the underwriter decision copilot. And you can test it here. Um the there are two things. One is it has the um the AM life underwriter and the knowledge assistant.
(1:48:57) So that's a genie room and that's a knowledge assistant. So it is going to depending on the question it's going to get routed to one of them. Um the playground is also a very interesting area to test things. So from there if I bring it into the playground uh then you can say that this is an underwriting decision copilot that's the agent and you can apply on some data you can use curl API you can use Python API and you can start ask answering asking your questions here.
(1:49:31) Um now uh sometimes you might have different pieces of um code different LLMs and you what you can do is in the playground you can use this uh and you can maybe use compare your code here with like an open- source uh model a GPT5 or a Gemini a claude or whatever right so these are ways in which you can check uh the same question and see how many tokens is it using how long does it take what is the accuracy of the results and so on.
(1:50:02) I went a little fast. Let me try it one more time to show you. Uh you you click on the agent. Let's go back to your life underwriter. Um so this PDF is your knowledge assistant. It's um manual that lays out the details of life insurance underwriting guidelines covering underwriting philosophy, eligibility, coverage and so on.
(1:50:27) You can test it right here or you can open it in the playground. In this case, I did not open the copilot. I opened the um knowledge assistant underwriter. So that is the one which comes here. I can flip it and go to something else. Um and I can get the code and so on.
(1:50:48) But here I can start to ask questions like uh what is your um policy on uh I don't know what is an life underwriting uh policy on um late submissions or something like that has to be relevant to the document but I am just putting it out here so that you can see that it is thinking it is using tokens. It's showing me how much time it's taking.
(1:51:21) Uh and then he said that the requested information is not covered in in the underwriting guidelines. Please escalate this to a senior underwriter or underwriter manager for clarification obviously because I I give it very um a vague one but you get the idea. And here um you can have the AI judge is on. So if it did answer something properly which I think we should try. Let's try that.
(1:51:45) That's the knowledge assistant agent. Okay. So, make sure that you get it. So, this is all the setup on all that you fed it. There will be some questions later. Validated. How are applications categorized into preferred plus, preferred, standard or substandard? Okay, let's fix that. Okay, it says it uses a multi-ter risk classification system to categorize applicants based on their overall risk profile and the things and this is how um they get categorized and these are references to the PDF like this is cited from page one
(1:52:42) this is cited from uh page one as well. So these are this this is fantastic because you know it makes it uh highly credible and the underwriting process follows blah blah it took so much time so many things and these were the suggested uh questions uh these are the sources these are the custom uh trace so that's very very robust and um there is view thoughts here so it says uh how did it come up with this because this question asks about this and these were the underwriting guidelines and this is verifying the knowledge. So these are all the things that it did to give you the answer. Uh and that's why the
(1:53:22) playground is like a pretty nice place for you to be able to test um this particular asset that you have. Um but alongside other endpoints that you may have as well. Uh what else did I want to show you? I think that's it. We showed you Genie, the knowledge assistant, multi- aent uh UC functions.
(1:53:46) Yeah, I think that's pretty good. All right, I'll stop here. Any last questions? Okay. All right. Good night, everyone. Thank you.