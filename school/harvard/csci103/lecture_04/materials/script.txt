103 day 4 - YouTube
https://www.youtube.com/watch?v=z0S9FuG2nn8

Transcript:
(00:02) Okay, welcome everyone. Um, this is our fourth lecture. Tonight we'll be talking about data transformations. We'll review some design patterns and some common design patterns for data processing. We'll look at u compliance. We'll discuss some concepts around partitions and zorders and spark joins.
(00:35) We'll also look at some APIs and and userdefined functions and also look at um CRUD uh change data capture and also slowly changing data dimensions. and then the second part of the class will be um looking at lab 03. Okay, to start let's um just quickly review some concepts from last lecture. So two streaming types um filebased and what's the other streaming type that we've seen? Eventbased. event. That's right.
(01:20) And definition of checkpointing. Recovering from the last save point. Yeah. Offset. Yeah. Checkpoint is like the last well-known location that we can go back to if things go wrong. Watermarking. to accommodate latess of data. Yeah. So with watermarks we can we can um determine or specify how old data can be before we discard it.
(02:04) streaming output modes. There's three updating, writing, and appending. Yeah, updating um or append, overwrite, and update. And then um what is the processing trigger? and give you a hint to do with um microbatch processing. Okay, the interval in the read stream. So how how often should the microbatch run? Lambda versus Kappa.
(03:06) Uh Lambda is batch plus streaming and Kappa is uh just uh streaming. Yeah, very good. So slow slow so lambda allows both slow and fast processing where kappa combines into one um streaming pipeline. Data warehouse plus data lake equals lake house. Yep. and managed versus external tables. What's what's a primary difference between the two? The metadata will be deleted and managed but not in external.
(03:48) Yeah. Schema external we just store the schema. Yeah. Um Yeah. And the the location of the data is with manage tables it's managed by the uh the framework and then um for external tables it's that the location of the data is external to the framework and meta metadata brings better discoverability and and lineage of well uh yeah so the word I'm looking for is governance so uh like you can you can control the data better with um using metadata and a materialized view is
(04:58) We looked at different um table table types and views last week and materialized view. It's like a snapshot of the table, but it can be like stale if not refreshed. Right. Yeah. Yeah. Exactly. The um so it's precomputed. It could be stale, but it's very fast and more efficient than computing the view each each time it's accessed. Good.
(05:34) All right. Just um check to see if there's any questions or anything from from last week or anything else. Okay. So let's let's jump into the lecture. So uh tonight we'll uh first part of the lecture we'll be reviewing some big data design patterns. And um you may have be familiar with design patterns.
(06:10) they were became kind of popular in the early 2000s. uh um thanks to um a book called design patterns and um that was the the um and they were object-oriented design patterns but uh for object-oriented programming but uh design patterns the general idea of design patterns is they're reusable patterns and templates um that can be used to help address common problems that we face in um in generally in software engineering but also in in um data engineering.
(06:56) And so they um design patterns promote um use of design principles. Uh many design patterns incorporate these uh design principles into their uh solution. So and common design principles include abstraction, divide and conquer and separation of concerns and also um the idea of keeping it simple like reducing complexity and and um providing simple elegant solutions.
(07:29) So, and all of these design principles can um also be uh used within data engineering. Uh design patterns provide a vocabulary for um discussing uh problems and and solutions that can be used within a team environment to help understand and share ideas and and and solutions. And so once once you become with familiar with some of the design patterns then you can you can refer to them and don't have to explain them each time. So that makes um discussions much more efficient.
(08:08) And um it's important when thinking about applying design patterns, you need to make sure to um apply them uh judiciously. So apply the like don't just apply a design pattern because it it you know about it but um you need to think about whether or not the design pattern applies to the problem that you're trying to solve.
(08:37) So some highle rules of thumb around uh design patterns. Sema you have a question. Yeah. So these design patterns apply to both structural and as well as non-structural data. Uh yeah. Yeah. Um and we'll see we'll see a bunch of examples of different uh design patterns that we can apply to data engineering and and many of them uh will apply to either type. Okay. Thanks. You're welcome.
(09:04) [Music] And I have a question is just and I think you sort of answered it a little bit but how it differentiates from data modeling that it just it's it it's a more it's a broader definition. Well um it's a good question. So in actually in software engineering um a lot of the design patterns are used for um data modeling.
(09:36) in in data engineering, we'll see that that maybe um it can be used to help with the modeling but also in the design of um the data pipelines that we create. And that'll become clear once we see some of the examples. But in in software engineering, the um design patterns are are often focused on how to model um um like domain models for example, which could be useful for um data engineers as well.
(10:11) And yeah, so this book um that I have a picture of up in the right corner, it's design patterns, elements of reusable object-oriented software. This is a very famous book and which kind of opened up the doors to this whole concept of design patterns and um it's a it's a great book to have on your um as a reference and and to know about and and the um design patterns community is still very active and and spans um lots of different domains now.
(10:44) But uh definitely good as a just as a uh something to be familiar with as a software engineer or data engineer. Okay. So um so in the domain of big data um there are architectural patterns and design patterns and principles that have um surfaced to help us with our data engineering challenges.
(11:13) So basically again templates and and solutions that reusable uh wellproven uh solutions to pro common problems that we can use to help simplify our lives as as data engineers and and they're focused on primarily scale, velocity, volume and um data processing needs. And so these are different from the original gang of four design patterns which um this book is the authors of this book are refer referred to as the gang of four.
(11:49) Um that those were focused on object-oriented um design. Uh the the um big data design patterns are more focused on the um the problems that we face as as uh data engineers and and um creating data pipelines. Okay. So um we can apply design patterns for modeling and examples in the modeling domain uh include the star and snowflake dimensional um modeling uh that we've seen already where um you have a fact table surrounded by dimension tables and also vault dimensional modeling where you capture um all the data in a vault and then work
(12:45) work on the data use the vault as a source for um further data processing. Uh there's uh design uh patterns for ingestion. Uh we um uh using uh like uh connector like as we'll see in a little bit. Um there's a a pattern for connectors to allow us to connect to different data sources. Uh there's um pipelines for batch versus streaming like lambda and kappa which we've looked at already.
(13:24) And also um this concept of separation of compute from storage can also be considered a a pattern where we um we configure our solution in a way that we can scale independently from our compute needs. And this is often important for efficiency and cost effectiveness of our solutions.
(13:50) So we can manage large amounts of data but um don't have to pay um high prices for compute just because we have a lot of data if we're not actually doing a lot of computation. Um there's uh patterns for transformations uh handling schema changes and schema on read versus schema on write where um we may know the schema that's coming the the data that's flowing into our pipeline may be well known and that would be um considered schema on on read where we know the schema coming in.
(14:27) But um sometimes in there's situations where we don't know the the schema as the data flows in and we would determine it on the fly as we as we read the data. Um asset transactions, updates, deletes, merges at scale and then also multihop um pipelines and we'll look at this in more detail in a little bit but different SLAs's for different use use cases.
(14:57) So having different stages of processing uh the data and where each stage can the the data can be still consumed not only by the subsequent stage but also external users. uh persistence um different um different styles of persistence like compressed columner um formats for for speed and efficiency and handling large amounts of data.
(15:27) denormalized wide tables and avoiding joins. And then also multiple destinations where we have um our data pipeline may um basically fan out the the data for um different consumers and and different um with different needs and then analytics um uh instream analytics. So let let's look at in more detail at some of these So the um I mentioned lambda and kappa.
(15:59) So the lambda architecture is basically a design pattern that um splits the data pipeline into a batch layer as well as a speed layer and then um both both batch and uh speed layer are combined into a serving layer. And um so and this is useful when we we have both real time and and historical analysis of data that's required.
(16:26) So we can we can service that with the lambda architecture. A kappa architecture combines everything into a single streaming pipeline and um and then outputs the results to a storage uh location for consumption. And uh this is um best for real-time processing where we don't have a lot of um batch or historical data processing something like IoT um processing an event driven architecture where um we need to respond to events like uh like processing um e-commerce uh transactions or things like that where we have event producers, event consumers and then event streaming
(17:23) that make up the um the solution. There's also um design patterns for storage. So um and this is a good example of where the um design pattern incorporates some of the design principles and the for example CQRS the command query responsibility segregation in um just in the name you can see that it it helps um promote the concept of separation of concerns where um you know different parts of the system are responsible for different functions and and and the system is divided that way.
(18:10) In the CQRS the um responsibilities are between the writing the data and um and also reading or querying the data. So you can you can design the system to support um like optimize the way that it handles writes or new new incoming data and then or updates to the data.
(18:45) and then and then the and and then al also optimize the the query capabilities of your um system so that the um queries can be um as efficient as possible. So, so and this is um this way you can balance if you have a high throughput of data you can you can scale that appropriately to handle that that volume of incoming data and then you can uh separately configure your system to support the um the query demands on the system.
(19:24) how many users and and and what sort of frequency are the users hitting your um data store in terms of doing queries. So if it's if it's if it's a small amount you can you can configure it accordingly or um maybe if it's if it's if it's a high demand system then um you can also scale that. the that key idea here is to split or separate the um the the intake from the um out output and uh and and be able to scale them independently of each other.
(20:01) Okay, the next is polyglot persistence. So this is basically uh like a way of think a way to think about this is use the right tool for the job. Like uh all every if um you may have heard the expression when you're a hammer everything looks like a nail, everything's not a nail. So um maybe a hammer isn't always appropriate. So the same is true for persistence.
(20:31) And we've seen so far we've seen lots of examples of different types of persistent systems. Some some are very good at handling uh structured data like relational databases and others are better at handling uh data with um less less well-defined schemas or maybe large um amounts of data. So the idea is use use the right database for the solution and your your solution may even include multiple types of databases.
(21:03) Um so relational databases for transactional data, document stores for unstructured and and graph databases for relationship centric data. So um basically choose choose your um data storage uh wisely and and also it's possible that you may have multiple different um storage solutions in in your um data pipeline.
(21:36) That's okay. And that's um could be considered a design pattern. And then data lake pattern uh the last one on the page here is um basically hold uh large amounts of data in its raw form. So that idea here is that you may not know when you when you receive the data you may not know exactly how you're going to use it.
(22:06) So you want to store as much of it as you can and in its in its original form. So without um any transformations on it because you may want to go back to it and um reexplore the data as it was when you originally received it. Um so the uh the storage in this in this case the data lake pattern stores unstructured and structured data and then um there's uh different tools like um Hadoop or Spark uh can be used to to process the data.
(22:43) So again, if you're if you're not sure like how you're going to eventually use the data or you want to the you think the data could be valuable for um maybe use cases beyond the current use cases, then uh using a data lake to store all all of the data in its in its raw format is um could be a design pattern that you could follow.
(23:14) I'll just stop there and see if there's any questions so far. Okay. If not, we'll we'll continue. So um so there's patterns around uh data processing microbatch processing which we've we've seen. So we where we basically treat a stream of data as um and process it in small batch increments uh and um and over and over again. So there's uh something referred to as a batch interval that we looked at last week.
(24:07) Um that which determines the frequency of the data collection and processing. And then the um and then the the the tools that we might use for this could be uh spark streaming for example. So th this um pattern can be applied when when we um we don't have a a streaming solution but we we need real time insights.
(24:41) So the the data is coming in and we need to have near real time unnecessarily real time but near real time insights to the data so we can use microbatch processing for that solution. So you can kind of see um as we go through these these are all like um these patterns or um design patterns are all um readymade uh solutions for how we can we can process data and manage data in a um big data uh solution.
(25:13) uh cap theorem um we looked at before it says um of the three consistency availability and partition tolerance choose two you can't have all three but you can have two so there's um two popular uh selections one is uh CPbbased systems that prioritize consistency over availability and these include things like re relational databases where um you always know that the data is consistent, but you may um uh may not always be available.
(25:55) If if um there if there's some sort of disruption in in in the um in the partitioning then um it may stop working until the consistency is resolved. And then um APbased systems where you prioritize availability over consistency. The systems always available but not necessarily always consistent with itself. So um and this so this this may and these AP systems were evolved out of the need needs of big data processing solutions where the amount of data was so like pabytes of data that was so big that it was impossible to um manage it in traditional uh databases like relational databases. So they um went to different different
(26:46) architectures like Big Table and and had um and HBS and others that that um allowed you to manage extremely large amounts of data, but um but they um gave up the consistency for um that that ability to handle large amounts of data and to do it um in a way that you could guarantee availability but again at the cost of um consistency and then another uh pattern is the zero data loss or zero clone pattern.
(27:27) So you basically um uh limit the amount of um uh or I'm sorry provide the ability to reproc reprocess the data um and and and um reduce um duplication of the data but using distributed systems that that internally replicate the data so that um the d there's always multiple copies of the data.
(28:09) So if even if you lose a node from a cluster in in for example Kafka um the data is still available in some other node. So um so and this this is important for situations where uh the data is very critical and you can't afford to lose any of the data and examples include like financial systems or potentially healthcare s systems. Okay, data mesh. Um so a way of of thinking about um data data as products.
(28:46) So and having like within a large organization you may have many different teams working independently on their own data solutions. So um a data mesh is a way to allow those teams to collaborate and and operate together but also independently. So um each team manages its own data pipeline end to end and then um but using standardized tools uh that are defined by the organization to um implement these uh data pipelines and and also how to share them and then um then once the data is produced then it can be shared as as a data product and each team can be responsible for that
(29:38) that data result as as as like a product and and consumed across teams as well as externally by uh business users within and maybe external to the organization. Agentic architecture is a fairly recent design pattern where um you combines large language models um to coordinate um tools and agents um to support specialized task like the um generally the agent would be able to do something like uh provide the weather and maybe um by querying some external weather API and then providing the results back and and doing this in parallel with other agents that could do other other task
(30:33) and then work together as a as a team of agents to um provide like a larger solution. So um so and this is um it's gotten a lot of interest recently in the um industry and an interesting way to combine AI or large language models with uh existing APIs and databases that can be um combined and controlled using AI.
(31:13) So the connector pattern is another design pattern that allows us to be able to um interact with uh lots of different um data sources but in a similar way. So using a a single um API we can through connectors we can connect to multiple data sources but still access the data for as a consumer in sim similar way for example like um using um spark SQL or um spark streaming or um other other packages to access um very diverse u data sources.
(32:02) So this is uh in um software engineering this is also referred to as the bridge pattern and um JDBC connectors is a is a popular example of a of a um application of the um the bridge um uh u design pattern. So here I think this is a this lower diagram is a good way to help visualize it where you have different um solutions all all know how to interact with um JDBC and ODBC connectors and then um underneath you have Spark SQL um that supports the JDBC ODBC connectors and then under Spark SQL you have different um uh data stores that you can interact with. But from from the perspective, but
(33:01) all of these data stores, you can interact with them as if they were a JDBC data source and that's through the um this connector pattern. Basically, you can think of it as a bridge. you're bridging uh different uh APIs to um or maybe the same API to different um data sources. Another pattern that we'll see a lot in the course and you'll have the opportunity to apply is this multihop um access pattern which um we also refer to um as the medallion architecture in in in some cases. So um where you have basically your the is your data
(33:54) pipeline and you have a landing area a refined area and an aggregation area where you have the data ready for consumption by business analyst. But because we've broken it up into uh a multi-hop solution, the um the raw data can still be used by data engineers and the business the refined um data can still be used by um data scientists.
(34:28) So as you flow along from each hop basically the data quality increases um but as you go backwards the data availability increases. So um you as soon as the data lands you have access to it but maybe it has to be refined before it can be used for business logic and then aggregated before it's ready for business use cases.
(34:55) But um but this this is a way to um help divide up your data pipeline into logical units. And this going back to the design principles, this is a good example of uh divide and conquer and and also separation of concerns for helping um design our data pipeline where we're we're not trying to do everything in one step.
(35:21) We're we're first uh reading the data uh storing the data, making sure that we've captured it, refining the data, and then finally doing aggregations on that refined data um for um to support business users. So um important to to support different different um consumption patterns among our users. Another um pattern is the microbatching pattern or near real time pattern of of processing data.
(36:10) So here um rather than having a continuous stream of data, we um basically have a a an interval that's defined. So every in this case every 1 second the um the the batch processing is triggered to take whatever data has come within that last second to to capture it um do some sort of processing on it and then um output the data.
(36:40) So, and then um we wait another second and then do it again. And each time you'll be receiving different amounts of data, but um the intervals are always the same and um processing and and this way we're um we're supporting um um exactly one semantics. So data doesn't get processed twice. It's resilient to failures and um we can do instream analytics where the analytics are done as the data arrives and and made available almost immediately within a second and um it can also deal with handling uh late arriving data and uh supporting the the concepts that we were reviewing earlier
(37:34) in the lecture tonight. checkpointing, windowing and water marking. So another uh design pattern is um minimizing movement of data. So time travel is is one uh technique uh where we use um versions. We version the data and we've seen this with um delta tables that uh basically on top of parquet format.
(38:10) They manage um basically a journal file that keeps track of changes to the data. And so that allows us to um go back to a particular point in time or to a particular version of the data. and um very useful for going back and maybe um re re- um processing a segment of data um from a particular point in time and then zero copy clone.
(38:46) So this is a way to clone the data um without actually copying the data. We when we clone the data, we um basically make a um copy of the a a new set of metadata for the data, but the original data remains the same. So this is more efficient and and faster and um and also requires less storage because you're you're only creating new storage for the metadata, not the original data.
(39:12) And um there's delta clone that can be either a shallow clone where basically a zero copy clone where the metadata is copied or a deep clone where we're we're cloning not only the metadata but also the the um the data itself. And delta share is another way of sharing data but um without without any copy to the um to the data itself or or to the metadata for that um it's just um being able to share access to the data to um external users.
(39:57) So re and this the idea here is we can reduce the cost of the storage. we can um and and not only the storage but also the access to the data and also uh reducing transfer costs because we're not moving pabytes of data around the network unnecessarily and it helps keeps the the original data together.
(40:33) you're not making um like by like the zero copy clone, you're only you're only making um a copy of the metadata. The actual data is is still the same. So you're not having um the data become u like drifting um different data sources drifting because the underlying data has changed. Okay. So that's um so that's that's it about um design patterns for um data processing. I'm just wondering if if anyone has um any experience with any using any of these design patterns at your work.
(41:33) Yeah, I've used some of these I think but my use cases are mostly operational data. We also have the analytics side where data bricks is a major player in our company and uh I have heard some of these uh or heard or seen others do some of these things. I actually was going to ask uh the question that how how do you see it in terms of operational versus analytics data? I mean generally this addresses analytics but then operational area is another uh especially with microservices you have data pipelines that are CDC pipelines and it's the same same
(42:15) problems exist there too um data quality replication do you see those as fitting in this uh like yeah I think um I think these design patterns can be used in uh many different uh scenarios with um when you're creating data pipelines. So not not there really no restriction and a lot of them are not just for analytics but for like for example this microbatching.
(42:50) This is really a a way of def defining your um pipeline that can handle uh near near like kind of like streaming but doing it as as batch processing but micro batches. So this can be used for um not necessarily analytics but other other use cases where you're maybe moving the data or transforming the data or other things.
(43:16) So these um I think all of these patterns could be applied um in many in many different um use cases. Yeah. I mean in our on the operational side we like out of the ones that you called out I think we use the streaming like Kafka or tools that operate on Kafka um and create those real time streaming pipelines. That's what I have done a lot in the last five years.
(43:49) Um, good. Yeah. Good. Yeah. I think in in practice in the industry uh just like in the introduction to design patterns, it um these these patterns become u like a you can think of them as like tools in a toolbox that we can um apply. Um it and but we have to be judicious in our selection of which patterns we use and when.
(44:24) But being familiar with them, having the vocabulary and understanding how they um support uh good design principles and help us um like divide up a problem into smaller parts and and solve solve the individual parts versus trying to solve the entire solution at the at once. Uh these are all um very useful and and um and we'll be using these in the class for the assignments and and more labs uh in the future.
(45:02) But something that you can take away from the class and and use um and and help um help your understanding and also help your confidence in um building solutions because the um you know that the design patterns are well proven. I you know the tradeoffs and and um and advantages and maybe disadvantages of the design patterns and when when they can be um best applied to your advantage.
(45:36) So um so and and helps helps with the designing of say a data pipeline because you don't have to start from scratch each time. you have uh these building blocks that you can use to help um design your data pipeline. And then as your as you and your team become familiar with um these patterns and then it's easy to discuss and and like tradeoffs and advantages and disadvantages of of different approaches to how you um design your your pipelines.
(46:16) Okay. So just um just a note on compliance. Uh compliance is um something that's important and as data engineers we should always be cognizant of. So there are two um two regulations that we need to be um aware of in in our day-to-day jobs like GDPR is the European Union's general data protection regulation that um helps protect um consumer rights around their data.
(46:57) And then the California Consumer Privacy Act is another one specific to California. These both provide um frameworks for um understanding our um are kind of like the requirements on us as as data engineers to manage data for consumers and and doing it in a way that that doesn't um you know uh tread on their um individual's rights.
(47:35) So uh privacy laws for information of all citizens even even in other continents. So if the if the data maybe the data is stored in the US but if it's for um if it's for individuals of the EU it applies uh uh customers can demand um their personal details to be erased from the system. So, at any point, a a customer could call up and say, "I I don't want you to to store my data anymore.
(48:04) I want you to remove it from your system." And you have to be um able to do that. Otherwise, you could be fined um a significant amount um for not for not doing that. Um it's a way to um standardize data protection laws across um markets and across the digital economy. And um and then it's it's the responsibility of organizations to ensure that um this this personal data is gathered in a a way that's legal and um conforms to the regulations.
(48:51) And um and so we can't when we're storing data, we often say, "Okay, well, let's store all of the data, all of the data that we collect and whether it's on users or IoT devices or whatever." But when when we're collecting data that um you know about data that's about individual people then we have to be very careful and and um aware of these uh regulations that could um affect us and and so um so the the things that we have to think about mostly are the right a right of erasure the right to be forgotten So this means that if a customer says I don't want you I you have my data but I don't want you to to manage that data
(49:40) any longer. You need to be able to um as an organization you need to be able to remove that data from your system. And that could be um you know fairly difficult especially thinking about like a data lake where you just have masses of data and maybe um don't have a good idea of of you know what data contains user data and especially for a a specific user like where is their data located? how how are you going to go in and remove that user's data without affecting all the other users data? And then uh the the second thing is right of portability. That means the ability for
(50:26) um rather than just deleting the data, the user may u ask for the data to be exported to some maybe your competitor so that the data can isn't just lost but it can be um reused in some competing solution. So you not only need to be able to identify the data that belongs to the user but extract it from your system and then share it some way.
(50:52) So that's the right of portability. Um so ability to locate and remove personal information fine grained updates deletes as supported by Delta pseudo non anonymization um is a way to help um solve this by using um basically like non-traceable identifiers that can be used to identify the user.
(51:24) like some sort of token or gooid that's used to identify the user and then the map of that token to the actual user. If if you need to remove the user from your system, you can simply remove that mapping and then um in theory the um the the um there's no way to access that the user's data any longer because the the mapping from the user identifier to this um synthetic um identifier is lost.
(51:55) So um the data may still be there in your system but it's no longer traceable back to the user. How many um how many of you have to deal with um compliance issues where you work? I I come from a healthc care background so it's very serious for us. uh mostly like with respect to such systems it's usually fed ramp moderate or fed ramp high so we have to get our systems certified with with those standards I think that is a US uh standard mostly and then there are others of course HIPPA and other healthcare standards around patient data PHI PII um but yeah it is
(52:46) it makes the job increasingly hard sometimes But yeah. So yeah. So another another aspect of our lives as data engineers that we have to be um aware of of the um kind of our responsibility of of data custodians to to manage the data in a in a responsible way. And these these um regulations that have been created by the EU as well as California, they're designed to help make sure that companies remember their responsibility to do to to to you know be good data custodians, especially for p personal data and and
(53:33) not not abuse it. And there's been um different different examples of where data has been misused and and um and at the consumer's expense. So um as as um data engineers, we need to be aware of our responsibility and and take it seriously and and and that that leads to when we're creating data pipelines or or coming up with designs for how to manage um uh like um person's data.
(54:13) We need to keep in mind these um like how how will how can we remove the um individual users data from the system and then how can we make the um data portable so that it can be extracted and maybe shared with um some other service and do you have any any additions to that about compliance Yeah, compliance is hard and coming from a regulated industry like healthcare, I can imagine all the HIPPA and um Fed Ramp things that you would have to deal with.
(54:56) Uh so once we get through our basic uh data engineering and classification, data classification um knowing you know where we are going to store that data, how we can even provide audit trails on who has accessed it and who hasn't all of that becomes additional layers. So yeah towards the later half of the class we'll consider some of these more esoteric aspects of data. Now we just getting started.
(55:22) So um we'll we'll see where we land. Okay, would you like to take it from here? Yeah, sure. Okay, now we're going to talk about some not so uh exciting uh things. This is a little bit um dry stuff, but since you're going to be dealing with the Spark, uh you don't all have to be like um uh deep in the weeds in Spark, but you should know how things are happening at least just one layer uh deep um from your program um writing in the notebooks.
(56:23) So let's say you're writing some Python code, Pispar code, some um SQL stuff. Um now remember in the very first class we said that the spark architecture has got a driver and can have multiple workers. So that's the spark driver and the executors are on these workers. So what what happens is multiple jobs get created from here that get submitted through the driver and they are broken up into smaller um units called stages and each stage is like a logical unit and that might be an operation like a map, a filter, a group by so either a narrow transformation um like a map or a filter or a shuffle transformation. Shuffle if you remember is when the workers do not have all the
(57:07) data they need. So they have to ask their neighbor can you give me that data and so there's some crisscross of information between worker nodes which is again expensive. So ideally shuffle should be minimized but you can't help it. Sometimes there is some shuffle and um uh you know operations like group by and reduce by cause shuffle.
(57:28) So we would say how are these done like what are the boundaries for these? So these stages are determined by the spark engine as it is going through the various u um series of optimization based on the dependencies. So if you remember RDDs are the uh base layer they stands for resilient distributed data sets and those were hard to work with which is why data frames and APIs were created.
(57:53) But under the cover everything is happening still through oddities. It's just that we have nicer APIs to deal with and we just deal with uh data frames. Um and these stages will again get broken down into tasks. So sometimes when you are hitting your run button and the query is taking a little bit of time, you can actually see these being submitted.
(58:17) So this particular cell that you are executing might have so many jobs, so many stages, so many tasks, right? Um but sometimes it's very quick. You may or may not see it. Task however is the smallest unit and that represents the single operation that can be run on your um executors. So the real work is not happening on the driver. The driver is just distributing the work. The real worker work happens on the worker nodes.
(58:45) And so the cores here are going to each take a task. And it's the tasks that are going to run in parallel. Suppose you do not have so many tasks or they were not able to divide it up as nicely. There might be some which are running and some which are not not used.
(59:04) Right? It's a very small task but you have a big compute um uh with lots of worker nodes. Maybe a couple of worker nodes could be used. So right now these greens are showing that okay tried to divide it. It was a very small task. It didn't need its full capacity. So tasks again are executed by these worker nodes in parallel.
(59:25) And that is where the distributed computing power of Spark comes into play. Um, and it's all about crunching data. So each of these tasks is going to take a small portion of the data and whatever transformations you require, wide or narrow, they are being applied and then finally the results will all come together.
(59:43) They will collate all of that, pass it back to the driver, the driver is going to pass it back to you, the user. So one thing that I want you to remember that your code gets translated into multiple jobs. Each of those jobs gets translated into multiple stages, multiple tasks that those are the ones that are um uh use up the course of your uh drivers in the executors and that's uh parallelism. Okay.
(1:00:09) Um the word partition is very overused. So you'll hear it in so many contexts. So this is again another clarification that we want you to be very clear about that when you have tables you partition tables and then when you are talking about the spark ecosystem spark has its own partitions and this one is happening without you realizing it.
(1:00:37) This is very heavy um dependence on the data and your query whereas this is your design decision. um how you decided to partition a table uh is determined in your DDL statement and how Spark is coming up with its partitions depends on some Spark configurations some skew in the data the kind of query that the person is running and so on.
(1:01:00) So let's look at it again a little more carefully because you should know that these are two different things. table partitioning is at the database or the table level whereas Spark is at the processing level and you do want your processing to be as fast as possible and because the query execution is um done better if you have the right table partitioning as well as the right uh processing partitions.
(1:01:27) This table partitioning is actually dividing up a large table into more manageable chunks. uh you might partition on um country or if it's a sales organization they don't care about what's happening in somewhere else in the world they care about their particular region so that might be it or sometimes you may not uh worry about data that has come a long time in the past like a date um or maybe HR does not really care about um every function maybe they care about certain uh divisions a little more so those are all um possible ways of partitioning the data And when you are querying it and you constantly use a wear clause on these particular columns then you are avoiding
(1:02:07) um scanning large volumes of data and you can get to your data that the the relevant pieces on on disk much faster this way. Whereas in Spark it refers to how data is distributed across the various nodes. Remember in the previous diagram we said how your query is being distributed across the worker nodes as tasks.
(1:02:28) So spark partitions is all about the distributed processing of spark. The purpose here is data organization query performance and um on disk in physical storage you have your files and directories. Whereas here um in the uh spark partitions it's about uh your optimizations your parallel processing your fault tolerance.
(1:02:52) It's very logical division. Um now from a control perspective when you are creating a table you can say partitioned by these keys right? So it is managed by the user very explicitly here. Spark typically decides the number of partitions that are dynamically uh going to get created based on your input data.
(1:03:11) So we'll later talk about skew in data uh and you'll see how skew gets um you know kind of throws a wrench in and some workers may have to work much harder than others would be just uh lying idle and computers being um misused right and it might have something to do with your configuration like um you you can have some spark comps that set or the size of your file the number of tasks and executors at your disposal so many things can affect it.
(1:03:39) uh now this enables something called as partition pruning. As I said the best way to be able to get to the query fast is to avoid as much of unnecessary scans as possible. So partition pruning means if you are looking for countries equal to United States I don't even have to my query doesn't even have to look anywhere else. It goes straight into the partition and gets me the data.
(1:04:02) So this unnecessary scans is what helps the query be more performant. uh whereas this is regarding data shuffling when you're doing your join your group by your reduce by and so on data from these different partitions may need to be reorganized that's the shuffle that I talked to you about across nodes for more accurate results leading to network overhead okay now this is just again a regurgitation of the same thing so this is a single table now if you've partitioned you kind of quickly are able to index it think of a folder in which you have some labels you know how to get um now uh in Spark
(1:04:41) you have the RDS at the bottom. We talked about the data frame API which makes it simple to use it and then these are your programs. Another way of looking at it is you could have your data frame API. You could have your SQL query. Um this is not just in Spark. This is any uh database. You would first come up with a logical plan which is unresolved.
(1:05:06) And then you look at the catalog, look at some descriptions, come up with a logical plan that gets optimized. That's the beauty of the engine. It's constantly figuring out what's the best way to give you the last ounce out of it. Then there is a physical plan. So when you say when you're running a query and you say explain extended or whatever and you see the actual physical plan then a cost optimizer sometimes comes in here to find out which of the physical plans is the best.
(1:05:32) So that selected physical plan is what um is used for the final code generation uh to run against the RDDS. So again this is not very specific to just u data bricks or spark or uh this ecosystem. This applies to all database technologies. These RDDs are split across multiple machines in the form of spark partitions.
(1:06:01) Dataf frame is an abstraction on top of these RDDs across these multiple machines and kind of trying to make it seem like a uniform uh single uh way to access your data. And APIs are again a convenient way to manipulate these data frames. So easily you're able to say um collect.print dot um you know um reduce what have you and partitions can run on worker nodes in parallel. They are not splitable across nodes. So that is why how you choose some of these may be important.
(1:06:29) The number of partitions used in spark is configurable by default. You should not play around with it unless you know exactly what you're doing and you're moving the needle in the right direction instead of making things worse. So uh data partition to avoid full scans when quering. Now you would ask what columns to use.
(1:06:47) So there's a link here which talks about best practice. But if you were to summarize then it's the wear clause and what columns you typically use there. Um like heavily used is what you should use. The most frequent columns in the wear clause is your rule of thumb. And also do not use a column whose cardality is very high.
(1:07:10) um you know then you know every there'll be so many partitions that you really don't get too much of a savings from your file scans. Expect data in a partition to be at least one gig. Um now this was the slightly older way of doing things and then um we noticed that people would make so many mistakes and then once a partition is set they can't change it if their query pattern changes and so on so forth.
(1:07:36) So currently there are ways by which uh if the amount of data is uh um not more than one terabyte uh this I'm talking just about the spark in the data bricks ecosystem do not even bother partitioning because sometimes if you make a mistake there then the query just gets unnecessarily uh slow uh people in the beginning get very excited about partitions create too many partitions and that's also equally bad.
(1:08:02) So not having any partitions in very very large data sets is bad. Having too many partitions is also bad because it causes us small files to be created. So another uh rule of thumb here is that you should have at least 1 gig of data in each partition. And the newest advice is that if your data is less than 1 TBTE, don't bother about partitioning.
(1:08:26) The underlying engine will do a much better than you with all the optimizations that are kind of built into it. Um now this is a SQL reference link talks about all the common things that you would talk about. There are some them some examples that you can look up um when you're reviewing. But typically in your um uh magic command of SQL you'll specify your your catalog your schema uh maybe your table if you have or you can create a table and say partition by a state and a city.
(1:08:59) So you have two keys that are contributing to your partition. You could insert into the customer and then specify on this partition these values. So that's a that's a way of um putting it into a specific partition. You can show partitions. It'll tell you exactly how many partitions. Um you can show partitions on the table with just this partition details. It'll show you some more details.
(1:09:26) You get the idea that yes, if you want to play with it and understand the the reasoning as to why partitions were created in the first place and I think you will have some uh examples or some assignments in the future uh where you would be asked to partition. You should know how to be able to create your DDL statement and what keys to use for partitioning.
(1:09:43) Hey Anandita, it looks like Sema has a question. Oh okay. Uh yeah so I have a couple of questions. So first is about the partitions. So we are using here a partition instead of wear clause. Is that right or so? No no no you will have your wear clause. Your wear clause is guiding you as to which partitions would be most valuable for you to define on that column.
(1:10:07) Okay. So for instance if your query is uh select star from customer where state is equal to blah or where city is equal to blah. That's that happens over and over and over again. very very common very frequent occurrence that's an indication that state and city are good candidates for partitioning provided the amount of information in each of them is significant it's very little data it's not worth it okay got it and another question is in slide number 15 I guess uh you are showing about the driver and executors [Music]
(1:10:46) yeah here so uh um in in the the in the current diagram the green one says like it's not utilized right so my question is like green one is running the red one is not being used okay red red one is not used okay so question is like how how should we know like is there is any alert or something we can set to understand if my u cluster capacity is utilized or it is underutilized or overutilized so that I can provision it more very good question very very good question actually I take that back I I think I also got uh confused looking at the red and the green. The coloring is
(1:11:22) just done uh to make a distinction that there are two jobs and these jobs have gone here and these tasks belong to this job. So it's not about utilization or underutilization. But your question is um very true. Now when we were using the classic compute we had access to ganglia metrics right from the compute uh panel where you could see all the resource um utilization.
(1:11:48) uh so whether it is u uh 75% of CPU or memory or uh disk or network you could see all of that for for the class though we are using the free edition which is serverless in which case databicks takes upon itself to do that monitoring and uh um maximize the resource utilization so you don't have to worry about it. So we had that as an additional thing that we would look into.
(1:12:16) Make sure that your job is proper and I'm I'm sure that if you're using data bricks in your workplace, not all your jobs will be serverless. We advise that way because that that's another administrative overhead that's removed from you. But if you do have classic compute then from the compute panel you can go to the metrics tab and look at uh the driver node the worker nodes and look at the CPU utilization the the basically the resource utilization and the again the rule of thumb is if your compute is u not utilized 70 to 80% range then it's
(1:12:52) probably an underutilized cluster and you should try reducing either horizontally or vertically maybe you don't need five nodes maybe four nodes more might be enough or maybe you don't need five nodes of that core capacity you can reduce that core capacity in either way you would be able to save some money and when I uh for example if I uh plan if I plan to add more uh computes like if I'm going to uh add some nodes so will it be like stop uh that cluster will be stop or it will still partially serve and uh so that my
(1:13:25) streaming pipeline will not Yeah, see again you have these questions because we have never gone over the classic compute. Um when you set up compute that way uh there are options you can specify fixed number of worker nodes like five nodes say and it will stay that way.
(1:13:51) If suddenly you feel that no you needed to make it seven or eight you would have to stop edit the configuration restart the cluster for it to take effect. The other way is you have autoscaling defined and you say that um you start out with three nodes because I think that's the bare minimum that is needed and it can go all the way up to eight nodes if required. So in that case that elasticity will be built in.
(1:14:15) Uh when the thresholds um are hit it will automatically scale from three to four and you can see it in the event logs that the driver is upscaling uh the the sorry the cluster is upscaling and when the load reduces it will go down. We also have the concept of um uh auto termination.
(1:14:34) So if nobody uses uh the cluster for x number of minutes and you can configure whether it's 10 minutes or 50 minutes or what have you then the cluster will automatically shut down so you don't incur compute courses. Okay, got it. Thank you. Thank you so much. Mhm. Um now we will look at um delta when we're talking about data lakes uh in the next uh chapter. So this is kind of a little bit of a teaser.
(1:15:02) Um but there are ways in which you need to compact files. You need to be able to vacuum them like things which are no longer uh necessary. Um you know some time travel stuff and whatnot. Um those again we had to do earlier. Now with uh some newer I would say offerings even that is kind of taken away.
(1:15:31) So there's a feature called predictive optimizations uh which if it is enabled on a cluster uh is automatically going to do the file compaction. What is file compaction? As data is coming in it's not going to come in big files and we talking about a big data system. So every time small files come and there are like so many of them just accessing the files and reading them takes more time than actually doing the query.
(1:15:56) So if you compact 10 files and make them one I'm just giving 10 as an example then your disk fetch is single fetch as opposed to 10 fetches right and that's uh much better for your query performance. So that's what is bin packing um and when you optimize on a table you can specify um that everything was already optimized before this time. So when I when you run optimize that takes compute I don't want you to look back just do it beyond this or u maybe current time step minus interval one which says just the last day just compact it and you will have it as a autocron job that runs every day um and zorder is um so these are different
(1:16:35) types of compaction uh file compaction is when the files like small files are put together and you get one big file right Z ordering is the same file the data is moved around to make it better for some queries. So this is these are all different techniques by which your query speed is going to improve.
(1:16:57) Um a good example of this is if you're looking at doing network thread detection. Uh you always look at a from IP address and a from port a two IP address and a two port. Like when you do thread hunting those are very common fields. They are not really related.
(1:17:17) It's not uh intuitive just looking at the data that those four fields should come together. But if you zorder on them then all that IP uh addresses which are similar which are not monotonically increasing or anything come together on disk and so when you are fetching that you don't go have to go to multiple places to fetch it.
(1:17:37) You fetch it all in one chunk and which is why again your query speed is going to go better. Um we talked a little bit about uh time travel uh earlier. We'll look at it more in our next class. Um but sometimes when you are bringing in data doing the transformations, you suddenly realize there's something off about this. There's maybe uh some bad data that has come in and I want to be able to go back to yesterday's uh uh time stamp or yesterday's version because um u maybe upstream there was some change and they didn't catch it in time. you didn't catch it in time. So you can just say restore table which is a delta table to
(1:18:14) version as of say 26 or 29 or you can say timestamp as of yesterday. So it's going to ignore or it's is going it's going to give you a snapshot back there and then you can do a reingestion. So no damage done.
(1:18:36) Earlier you would have to figure out okay how much do I rewind how where do I have to go back to but now you have the concept of versioning and time stamp to be able to do so and that's the beauty of time travel so vacuum uh now these are older versions you don't really need there's a retention period if things are not vacuumed consistently then your query performance will also suffer um we'll also talk about other delta features such as uh clone uh and there are two types of cloning shallow and deep.
(1:19:02) In shallow clone, you're copying only the metadata. And in a deep clone, you're copying both the metadata and the data. Uh why would you need it? Maybe when you're doing some ML operation and you just you care about the metadata to be transferred across or you're doing um a disaster uh uh recovery scenario where you're where you have to clone it from one zone to another or one region to another.
(1:19:30) So these um these functions will come in handy. Uh default is deep clone and um if you specify shallow um uh then it's copying just the metadata. Again use this uh cheat sheet and familiarize yourself because that will going to uh become pretty handy for you. Let's talk a little bit about joints.
(1:19:57) Um a long time back when we are dealing with uh big data joints were frowned upon because joints do make it uh make uh things slower and even now I think you should have that mindset that these are systems which can um tolerate really wide columns. In fact the only thing that uh constrains you is the amount of memory at your disposal. Um you can like there's no real physical upper limit to it.
(1:20:21) Whereas if you look at any database system whether it's a SQL server or whatever they will specify I can tolerate 1,024 columns or 2,000 columns or what have you. They will have an upper limit on that. Um so in the big data system you should take full advantage of it but we are in SQL land as well and so we can do joints. what you had uh answered in your um first uh assignment or so was about outer joint, inner joint, uh left joint and so on so forth.
(1:20:55) Right? But these joints are joints that are happening uh underneath uh your um knowledge. So you are not specifying it. The spark engine is doing it and you should just know what they are. So there's the concept of a broadcast hash join. So what this does is you have a very small table um and you have a very very large table. So instead of doing the actual join if you broadcast the small table to each of your nodes then it might be a faster operation than trying to do the join and trying to fit this large table in memory.
(1:21:28) So you break down the large table you send it to all your worker nodes and this small table is sent to basically each one of your nodes. So the requirement for this is that one side is very small and you just uh you like broadcast it to all the worker nodes. So then there is no shuffle no sort and it get becomes very fast.
(1:21:45) But it will not work in all cases. It will work only if one of the tables that you're joining is very small. The other is um um shuffle hash join. Um now here it can handle large tables but you can encounter um an out of memory if your data is skewed. So one side is smaller and a partition of it can fit in memory. Right? So th those are the requirements.
(1:22:13) Maybe it's not as um much of a disparity like between small and big as the broadcast. But here you have one side which is at least three times or more smaller than the other and has a all its partitions at any time can fit in memory. So then you get the shuffle hash join. Remember in this case there was no shuffle no sort.
(1:22:36) In this case there is no sort because you are a partition is able to go into memory but there could be a little bit of shuffle right? So it's slower. Then you have the sort merge join which is very robust and by default it is um used you can it can handle data of any size but it requires maybe sometimes it requires shuffle and sometimes it requires the sorting. So which is why it is slower.
(1:22:58) So this is the fastest this is next this is next and by uh just looking at it you know that cartition should be avoided at all costs. Um it's like um a huge cross join and should be like you know if you don't really need it don't do it. uh you had been introduced to UDFs again in your first or second uh class.
(1:23:23) Um now the simple way of doing it is like um just creating um a table uh creating a little function and registering it as a UDF being able to call it from Python being able to call it from SQL and so on. Now you have to remember that Spark was written in a JVM language and we are now working with Python. So every time this um you know a crisscross happening happens between Python and Chv there's lot of serialization and des serialization in the process.
(1:23:51) So a better way to do it is to use what we call as vectorized udfs. They're essentially pandas UDF and we we will go through many more elaborate examples of this but if you have a decorator like this then that's an example of a pandas udf and that is more performant. Um udfs can allow for multiple column inputs.
(1:24:18) So it's not you can you can have define it um you know whatever complex function you want and the outputs can also be um pretty complex. So if you have a lot of things that you're returning, better to encapsulate it within a strct type. Um now udf is userdefined function and it works on a row and it produces a row. UAF sounds very similar but it is userdefined aggregate function. What that means? It works on several rows and produces a single row like a sum or a count.
(1:24:45) Now udtf is userdefined transformative functions. So here this is many to many. The first one was one to one. The second one was many to one and the third one is many to many. So you might have an explode function. You might have a pivot function.
(1:25:06) So you're working on multiple um input rows of input and you you're also producing multiple rows of output. Um now when to use it don't overuse udfs because if you create a bug or if there is inefficiency in it then you know you using it again and again will cause issues. So first look within the spark API documentation to see if some similar functionality already exists.
(1:25:30) If it does just use it next best is the pandas udf because it's vectorzed that we talked about. And next is a regular python udf. Now if somebody were to say CRUD, what comes to mind? What would what uh does the word CRUD stand for? Create, read, update, delete. Exactly. And that's when we are working with data. CRUT is like our best friend and that's what we do over and over again.
(1:25:58) So when we say create, you were creating cataloges, you were creating volumes, we're creating schemas, tables and so on. Right? Those are all the create options. read similarly same thing you're reading each one of these update sometimes you're not able to update you may not be able to rename something once it's created because it's already in use and so on and delete again is a dangerous operation so have to be very careful as to how you're dealing it uh uh also when you're creating the other flavor of it is are you really you know appending to a table
(1:26:28) or are you overwriting a table um sometimes uh lookup data or reference data is completely uh rinse and repeat like you completely overwrite it. But transactional data is is an append only operation. So you should know when to use those mood uh append, update, overwrite. So you should know when to do that. Read and retrieve.
(1:26:53) We just talked about all the partitions, keys, zorder and all of that stuff. Update, you can replace um um you know you you can update a new column that is coming in that's a metadata. you you are updating your data alongside it. You're replacing partitions and there could be fine grain like entire um country data could be updated. So you're just dropping the partition and recreating it or you're just making a very fine grain.
(1:27:18) So where user ID is equal to this. Eric was uh talking earlier about compliance data and GDPR requirements and imagine when you're in a big data system if you are going to look for a particular user among millions and billions of rows that's a very expensive operation.
(1:27:37) So you should be able to say um delete from this table where user ID is equal to blah or update this to say blah. So that fine grain thing which we are so used to from a relational world is very hard to do in the big data world but thanks to protocols like delta um we we can handle it at that level. Um deletes again um and upsert is also very interesting.
(1:28:02) Upsert comes from two words. It's an update and an insert. Uh as newer data comes in you'll have to look at the data. So it's not always an append or it's not always a replace. There might be some new data which is going to be inserted.
(1:28:19) There might be some existing data which may be updated and there might be a need to delete some data. So that is an upsert command and traditionally it would have required like seven steps to do so. Uh but if you were to do it atomically in a single command then we have something called as the merge command which would uh which would be smart enough to know that these are this based on this key uh these are new records these are all old records that need to be updated and these are records that need to be dropped.
(1:28:44) So when you are adding data you insert into um table. So this is select star from this. So you're getting all the rows from here and putting it here. You can insert overwrite that means you start from scratch. All your older data is gone. Um when you write uh you you remember the three modes we were talking about uh append, overwrite, update um you can um have additional options and then you can you have a path as well.
(1:29:12) Um when we are talking about uh schema by default your schemas cannot be um changed because if that were to happen then you will be caught unawares and you do not know when some bad data has come in. So by default it is tight and if you put an additional option saying merge schema is equal to true that means you're allowing your schema to evolve.
(1:29:38) That doesn't mean you'll put an incompatible uh format. That will definitely throw an error. But like say an additional column comes by or something was a string and now it has come as an int. Those are those are gradually um evolution that is supported and merge schema is equal to true would allow you to do so in your pipeline.
(1:30:04) Typically in your early stages you allow for schema evolution and in the later phases you allow for schema enforcement. That means I my contract is final. My consumers will get really upset. I don't want to change. If the data changes, I need to know about it. I want the pipeline to fail. So remember these are very common terms.
(1:30:23) There's inference, there's evolution, and there is um schema enforcement. I want you to remember each one of them. Now let's talk about CDC. That's change data capture. Uh so maybe many times we do just appends on big data systems. that's fine. Um, but if you are bringing in data that needs to uh be evaluated as to whether it's an update, a delete or not, then you have to use this upsert or merge that we talked about.
(1:30:49) So you insert your new data and you update the old data. Um, and this uh you'll see two words um fairly close to one another. Uh you'll hear the word sedd which is slowly changing dimensions. Um and then you will hear CDC. So CDC is usually from the uh source side when you're talking about it and sedd is is when you are referencing it from the target side you'll say sedd.
(1:31:17) Um so CDC stands for change data capture that is coming in. Uh here for instance you have the change data sets data is coming in as is. You have a staging table and then you're going to do a merge operation into your final table so that you don't have duplicates and you know you have just a single row for each of your unique uh primary keys.
(1:31:37) Um change data feed right so this represents the rowle changes between versions of a delta table. So there will be a a folder if you if you set that uh option uh read change data feed to true then a new folder gets created change data you don't have to worry about it but that's where all this uh information is being uh written out.
(1:32:00) It will include a row data uh along with the metadata that we all know um whether that specific row that was coming in was an inserted row or deleted row or an updated row. So uh enable change data feed is true. Uh you can select from your table um from 0 to 10. So those are the starting version and uh maybe ending version.
(1:32:26) Um you this is your table and you have those options set and you can start to see the change data feed coming. So that's the original table. You have A1 A2 A3 and B1 B2 B3 just as some some data. Now your change data feed comes in. So you have a2, a3, a4, z2. So obviously you can see that a1 did not come here. So it's left alone. So because a2 came and because a2 was already existing.
(1:32:53) So you have a before row and an after row. So your pre-image had the value of b2 at this time. Maybe it was a version two. And then your post image had um a different value of z2. So that's a case in which something was changed. Now A3 existed but now you're saying that it needs to be deleted. So you say that the previous value was B3 but it's a delete operation.
(1:33:19) Um and then A4 is brand new so it's an insert operation. It's coming as B4. So this is an example of what a change data feed looks like. Okay. Um we talked about SD. So that's slowly changing dimensions. Now first let's give you some mental reference to it. Um say you are staying at a particular location um and your social security um or your you know maybe some some kind of a service that you uh have been using for a very long time refers to you to that address. Then you moved somewhere else and you're still having your transactions. So your transactions is
(1:33:56) like your fact table and your um um your um dimensions are around you. So address could be a dimension and there are different ways when your address changes how different vendors or how different data uh companies treat that data right so facts will always be like one after the other your transactions are going but dimensions like they need to know a single address you can't have like five different addresses so there somehow they have to resolve what is your current address and that is where sedd comes in type zero it's fixed. So there
(1:34:33) is no change. So if your address changes, it's just uh slapped on top. You're doing an update. Um and if somebody asks you what was your address um in like 5 years back, they would not be able to tell you because they have not retained that data. So that is type zero. I'm I'm sorry. Type zero is no change allowed. So I'm sorry, they don't even allow for change. That's like too rigid. Type one is it overrides.
(1:34:58) The advantage of this is no additional memory or storage is required. the corn is of course you can't trace back. Type two is the most common. Uh you are going to create additional record for each change that is coming in. So the pro is you can trace back in history. The con is of course there is going to be more uh processing overhead apart from just memory and storage and we'll show you few techniques that is commonly used in the industry because this is a standard problem. This is there everywhere. Type three is when you keep
(1:35:28) adding a new column uh every time that something changes. Uh so you have the advantages over type one but this is not sustainable like if um you're slowly it is slowly changing dimension but still it might change a lot and then you'll have so many columns to kind of take care of. Type four is used as a historical uh table separately.
(1:35:52) And type six is combines one to three. Nobody really talks about it. The most uh common ones that you should know is type one and type two. Okay. And this is an example of sedd type one. So it does not store historical data. It overrites it. Um it's almost as if uh so this is like um the in initial incoming data customer name age customer name age 1 a21 uh 2 b31. This is um as such. So that's your target table.
(1:36:23) Now some new data comes in with your change data feed. So one is now a a and um two is now remains as b. So two is actually a delete and three is an insert. So how would this look like in your target table? So one is going to get updated from a to a um two is going to be just deleted just removed and three is going to be inserted with with the newest value c and 16. So that's the final state of the table. Very simple, just things being overwritten.
(1:36:57) And this is SEDD type one. In SED type two, this is one of the common ways in which it is handled. You have three additional columns. One is the start date, the end date and a flag to say whether this is current or not. So you have two uh unique rows and as of this date um you uh fill that in.
(1:37:23) So you had a start date and you have a very very large end date or sometimes you can even have that as blank. It's up to you. Um and this flag will say t to say that this is true. So if somebody were to say give me a detail. So you'll just say select where name is equal to a and current is equal to t. So it will be quick to be able to retrieve this.
(1:37:44) Um now how does it handle change? So if again like the previous case you've gotten some data then this uh row remains as such. So from this date to this date its value was this but now it is no longer current. So now my current flag is false and I have introduced a new row with its new values have this date here and a big value here and a true flag here.
(1:38:19) So this was my old date data its validity was from this period to this period but it is no longer valid. It's no longer true. It's no longer current. Um but if somebody wants to join what is my address when in uh 2020 I would be able to go in and uh you know look up this date range and uh I can get what the values were. Similarly uh two was a delete.
(1:38:45) So from this period to this period it was valid but now it's f and now there is no other record of it because it's been deleted. So it's no longer there. It's there's no no flag here which says it's true. and three is a brand new record which got added. And similarly, it has a big number to say that it's it's true. It's current. But uh you know since there is nothing else, I'm going to either keep this empty or have a very large value to say it's true forever. So do you see the difference between sedd type 1 and type two here? You can go back to history.
(1:39:11) But you have to have these additional columns to kind of manage uh that those operations. So that's all for today. Um, assignment two is out. We discussed it last Thursday. If you have not turned in assignment one, remember there is late point, so you should uh make sure to get to it. Yes. Was there a question? Okay. What's the best uh file format to use for type one and type two? Aro, park, delta, etc.
(1:39:48) uh what I showed to you um can be applied for any um type. So just the uh fact that this is sed type 2 does not matter. Um we will use delta for a lot of our uh examples because of uh some other advantages around asset compliance and time travel and schema evolution and so on so forth.
(1:40:15) But what I just showed you like fine- grained updates and deletes cannot be done in parket. So you would have to do a lot of work to kind of get to that fine grained updates and deletes. So if paret can't uh do just an insert of a row simply enough and it can handle only at the partition level perhaps park is not so good. Um Abro is a row format. You could try it but I would say delta is perhaps better.
(1:40:40) Um now um other formats which handle asset compliance are hoodie and iceberg. So they would also be good choices. Um but yeah that's that's I think good. Uh we'll talk a little bit about autoloader in our um lab today. Um now what is autoloader? It incrementally processes new data as it lands in cloud storage.
(1:41:03) So this is not event based this is file storage but it does not require any additional setup. So you don't have to do a lot of lift and that is why autoloader is so popular and it's very efficient and um it can uh get the files from any one of these um popular cloud storage systems so S3 or ADLS or Google or so.
(1:41:30) Now how does it work? As newer data comes in, as that metadata is discovered, those checkpoint location remember so it's a form of streaming and it's going to take care of what are the new files that have come in and what needs to be uh ingested. Um there is autoscaling, there is data quality checks, there is schema evolutions. There's a lot that is built into it.
(1:41:47) Um there are tutorials and uh but what since we talked about um design patterns today let's first look at some common data loading patterns. So you could ingest data from cloud storage as a variant. Um now in a variant um um type the the schema is extremely flexible and if the type changes um then you know you do not have to parse it right away.
(1:42:18) So that's one thing to keep in mind and we'll look at variant much later. But when you're giving your um uh location of where to ingest it from you can use um patterns like you can match any like regular expressions. Uh you can specify uh things like maybe you do not know how the file is going to come. So you can provide a prefix like this is the prefix.
(1:42:41) Anything that comes in here pick it up. Or you might say um uh I want to consume binary files but it has to be um PNG. So these are some pictures that you might be ingesting. Uh we talked about spark readstream. So you know the difference between batch and streaming. So this itself should tell you that this is streaming.
(1:43:06) So it's no wonder that it's able to um automatically do incremental reads. But there is also this additional thing called format of cloud files. The minute you see this you would know yes this is reading through autoloader from cloud storage. And like um before you've got a couple of additional formats you have to say what type of format is this? Is this CSV? Is this JSON? Is this binary? Um what's any filters any additional filters that you want? Uh and so on. Um there might be some easy uh ETL that you can do.
(1:43:38) So you you once you specified cloud files and this happens to be a JSON you can give it a schema location you can load so that's the reading part of it and then you can write uh you can have merge schema is equal to true which means I can handle a little bit of schema evolution your checkpoint and then your start loss now sometimes uh you might expect be expecting a schema but uh somebody formatted something badly and so if you don't um address it you could have loss of data. So how do you prevent data loss in a well ststructured data format?
(1:44:12) So you have an expected schema uh you are ingesting you've given that yes it's JSON um you'll say schema evolution mode has got rescue. So if you provide this and you know what I will not be able to see your questions if somebody wants to uh just go ahead and say what the question is that would be great. Go ahead.
(1:44:37) I can see that somebody put in something in the uh chat but I can't see it because I'm sharing screen. Go ahead. Uh sorry I don't mean to uh uh interrupt you. No problem. The question is like since we get a different schema like we it's a schema can be changed later once data is in the system. Is there is any way we can version a schema or absolutely so you I will show that to you in the lab that we are going to look at.
(1:45:05) So here of course just like you have um data versioning you have schema versioning also um and this is an example example of expecting a schema but also saying um I'll come to that I'll I'll show that to you in the notebook in a minute. Did you have another question because I saw two things pop up. Maybe somebody else. I was that was me.
(1:45:26) I was I was asking like is autoloader similar to like the AWS uh glue crawler? Is that the similar feature like adding new files into into the data base? Um I'm not very familiar with exactly how crawler works. Uh glue crawler works. But here if you drop keep dropping files in a S3 location and you have autoloader it will just know how to pick it up and and uh process it from there.
(1:45:53) Gotcha. Mhm. Thank you. Uh okay. So um this is having this rescue column is important because your bad data which could not be parsed is now going to be available so that somebody can look at it. Anytime your rescue column is not empty is a indication that some data was not parsed properly. If it's oneoff then maybe it's like a bad data.
(1:46:19) If it is happening in bulk you know that something has changed and you need to stop your stream. You need to talk to somebody upstream and so on so forth. Um if you um want your stream to stop processing if a new file is introduced that doesn't match your schema you can say schema evolution mode fail on new columns.
(1:46:37) So you have a lot of knobs, control knobs at your disposal. Now you might have um uh semistructured uh data pipelines. So here for instance you are um you have JSON um but you're giving schema hints. Uh you could infer the schema, you could enforce a schema or you could give some schema hints because if you don't sometimes it looks and thinks it's a string whereas it needs to be a float or it needs to be a big.
(1:47:07) So in this case um uh you have given and said that header has to be this and status code is a short so it's going to honor it. Um uh so bas later you it will be able to infer newer things but at least it knows that your intention is for these columns to have these data types. You could also have nested uh JSON come in.
(1:47:34) So some complicated uh input data and you would give it um uh instructions on how to um you know flatten it out. Maybe tags of page.name is what you want. Um or you know tags is like a a a nested JSON and within it you've got another level and another level and it has to be of int type. All of those stuff you can do. You can infer it. Maybe in this case you gave some examples, maybe you want to infer it. So you can uh it will do it.
(1:48:00) Uh maybe in CSV file came in without a header. Um so uh you might want to um make sure that you don't uh load data like if if um the header did not come then maybe things have gotten messed up uh in flight somewhere. So you'll put the whole thing into rescue data. uh to enforce the schema we already saw.
(1:48:27) Um what is this? This is header is equal to true. Yes. Um ingest uh image or binary data. We saw an example before. So your format can be binary. You can read from it. Um and then later we look at uh LTP which is lakeflow decor declarative pipelines in which you'll have some decorators like this. So the older name for that was uh DT. um that would be the table name and it's going to it uses um autoloader um behind the scenes. Uh and uh it's going to have uh expectations built on top of it.
(1:49:01) That's why I said we'll come back to it. That's too much to handle in one go. Um it has the concept of streaming tables and materialized views. But at least you get an idea of what autoloader is, what are some of the common data loading patterns.
(1:49:23) And now um if you are able to import um the autoloader notebook um it will have two files. One is the notebook itself. The other is like an include file. So when you see the command like this percentage uh run uh it basically goes and um runs this additional um notebook. So there might be some setup that is there and you might need to include it in all of your other notebooks. So that's a easy way of doing it and you can pass parameters.
(1:49:48) So we have used this to kind of um simulate uh some conditions. So in the beginning you start by resetting it. So maybe you ran it a few times or maybe you stopped midway. So now you clean the slate. So that is mode is reset. uh here essentially it will be your catalog your lab name and some some basic uh things are being set up so don't worry about it too much um the first time um to enable schema inference in schema inference and evolution the following conditions needs to be met um you do not provide a schema for your data otherwise it's going to um
(1:50:28) assume that that's the only thing that it should honor you provide a location to store your inferred schema. So to your earlier question about schema versioning, you can have the schema location and in that path the different versions of the schema can be seen, right? Um and we'll show using the checkpoint path which is uh recommended.
(1:50:50) So even if you give a single path, it knows under one path to hold the offsets and another path to hold your schema locations. uh merge schema should be set to true which will allow for newer fields to be detected. Right? So that is recommendation for very easy ETL. You have your catalog name, you have your schema name. You this is also a handy way of knowing where you are.
(1:51:14) Sometimes if you have a very long notebook, you can lose um track of where you are. If you say current catalog and current schema, it will tell you exactly what you have used somewhere else. Right? So you know that that's correct. um you will set your bronze table and you see this is a threepart name space and so if you've set your table like this um your catalog is essentially the first part your schema is the second part and your table is the third part um now like the example that we saw earlier we are doing a read stream this is a streaming operation we see cloud files we know
(1:51:50) that this is autoloader we we see format as JSON okay that's what we are reading uh we are going to handle schema evolution. So that's that and that's the raw data source from which we have read and we are going to write it somewhere else with merge schema is equal to true. So as you run it you can see the the performance I ran it just before class and uh when it is running you can actually see the live update.
(1:52:20) So as newer data will come in you'll see the live data here. Um this is a utility function. remember the first line that we added. So this actually comes from there. I'm simulating newer data coming in. As you do that and as your pipeline is running, um you would see actually this will be so much more interesting if I were to run it.
(1:52:38) So let's let's actually run it even though we are a little short on time. It's connected. It's trying to run. Um, and remember percentage run is like a shock command to kind of uh include data um and processing or or you know your logic from a different place. So it's it's just printing out some names making sure that we are doing it right.
(1:53:18) Okay. So that's a volume that we are going to use for the source. All right. So that's done. Now we are saying use this catalog. Use the schema. Uh show me what my current catalog and current schema. That looks trivial now. But if you remember this command, it might come in handy sometime.
(1:53:38) Else uh that's my bronze table name. That's my catalog. That's my schema. And that's my table. Right. So all good. Now use uh this and then let's start doing our read string. So that's the you see these are the tasks uh we were talking about spark tasks earlier and then the stream is initializing.
(1:54:17) Uh let's wait a little bit for Okay, it's it actually is finished so quickly. Now let's simulate some new data and let's see this command and the stream is again initializing. Let's click on it. We see some just a new data point appear here.
(1:54:45) So it's very little data that is coming in and as the cycle goes away you can see it on the dashboard or you can see the raw data here as well. Um the table the bronze table that we were talking about which had been reset in the beginning if you were to see what kind of data it is then there is a device ID a device type so looks like it's an IoT type data it's got a signal strength and a time stamp.
(1:55:09) uh we can peek into the JSON data and uh you can see it's exactly the same. It's got the device ID, the device type, a signal strength, a time stamp. So yes, the data is being parsed properly and from JSON is being put into a tabular thing. Now we saw that underscore rescued data column is added to the schema by default whenever autoloader is inferring the schema because there's always a chance that it the assumed schema that it has it may not be able to handle. So then whatever it can parse it is going to put it into the um that particular column.
(1:55:42) Now you can change the name by by default that is the name that it gives. If we look at describe formatted on that table uh then it's going to give us not only the schema but it'll give us uh additional things. So here everything kind of looks like a string. That's not really what we want.
(1:56:02) We want a little more structure and we'll see of course how we are going to do and you you can see more details like uh the statistics columns the table information and so on. Uh you can look at the checkpoint and uh you see that um there is the offsets which is for your um um checkpoint the actual checkpoint offsets and this is the schema so you can have multiple versions right here.
(1:56:26) So we can run this again. Um there are there are other things like the commits and the sources and so on. Um now if you look underneath this uh that's the version zero. Now if when a new column appears uh this is going to automatically throw an exception of an unknown field causing the job to fail.
(1:56:56) But streaming jobs typically are put on continuous um uh rerun so that it's if for whatever reason it comes down you're able to bring it up again. Um so now we are going to simulate new field coming in and that's the utility function that we were talking about earlier. So that has come in and when we um when we are going to this we have to run twice because the first time it's going to see the field and it may fail um the second time if you yeah see it failed.
(1:57:30) That's because it's notice that field the second time when you run it which later when we talk about workflows you don't have to worry about it. The stream is initializing. Yeah. A new data point has come in. That's very few data. Now you're going to do some count on the bronze event. You'll see some value. Yes. Um now we um we want to uh order by the time stamp column.
(1:58:04) And you see so far there's been all all the parsing is good. Which is why the rescued data is pretty clean. But because a new field has been added, we now see a metadata field has also come in. So let's uh run this. It has a version. It has got a response. Um so some additional information um this time we are going to pass some bad data. Uh your time stamp has suddenly gone bad.
(1:58:31) Somebody has made some mistake and now when we are going to run the same stream. The the reason why we are running this multiple times is to show you the order but ideally this is just one time newer data is coming in and you should be able to use reuse the same code. Okay. Um now when we look at this um the rescue data is still not bad.
(1:59:02) Run the display query to see that this incorrect time stamp has been ingested. Yes. But let's put schema hints and rescue data. So in this case we are going to reset the whole thing. It'll take again a little bit of time. So these are simulations which are a little hard to follow along but we want signal string to be a float instead of a string time stamp to be a long and metadata to be a map.
(1:59:27) Um so everything is now string. So what we will do here is we will have schema hints in which signal strength is a float time stamp is a long and metadata is a map. And once this is done, let's run this. Okay. It when we run describe formatted again, so instead of seeing string string, we see signal strength is a float, time stamp is a big and metadata is a map, which is what we expect.
(2:00:04) And that is what schema hint does. Um, now let's get a bad time stamp to come in. Okay. Uh, so we simulated that we are going to read this again. And then if we are going to read from the table, what do you expect? We see that some time stamps. Actually, let's uh resort this. And because maybe there's just one or two bad things, so you're not seeing here.
(2:00:47) But the important one will be that this column, the rescued column, is going to have that field that you can then be able to pull from. Can you can you sort by the rescued column? Yeah, let's try that. Let's try this again. So bad time stamp is true. Uh run this again. It's possible that this this did not really send a bad time stamp.
(2:01:26) But ideally you should see yeah this time you see all this time stamp was null because it couldn't parse it. It's a time stamp field and it's not. Now in the rescued column you can see why it could not be passed and what was the file that caused it to fail. So it it will help you to identify those um files which had the bad data as well instead of you looking for a needle in a uh haststack.
(2:01:52) Uh in this case we have got new field is equal to true. Uh we have the schema location we have the schema hints we have the checkpoint and we have merge schema is equal to true. So if that is the case uh unlike the first time when we saw the pipeline failed this time the stream should remain active. Um and let's display.
(2:02:36) Okay. So we are back to where we were with the time stamp and the metadata and the rescued column. Now let's look for some semistructured uh data. Uh within that rescued column you can see um some of the data had um a version and a file path. So we are now saying give me the time stamp and let's try to first cast it into double and multiply with th00and because that's exactly what I um debugged my code uh to be that you know that was the mistake which is why it is not uh a time stamp column and it's getting caught. So from the rescued
(2:03:16) column we get the timestamp field cast it um into a big and we call that as the repaired timestamp. So if we were to run this from the bronze table where rescue data is not null then we get a repaired time stamp and h this looks like what a real time stamp should be.
(2:03:42) So at this point you can bring this over into your um real time stamp and in fact uh rescue data rescue data time stamp timecast as repair time stamp from this where rescue data is not null perfect. Uh this is just yet another display. I don't think it's going to do anything more. Uh in the above example, this allows us to easily write logic to repair data that would have otherwise been lost by schema enforcement. True.
(2:04:14) At the end of all your streaming notebooks, um because streaming is continuous, it's not going to end. It's up to you to stop all the streams that you have like when you're going to be doing your maybe your third assignment or so, you will have um some streaming questions and you have limited compute for the day.
(2:04:38) So, make sure to have something like this run uh so you don't use it up in one session. You might have to go back to look at it again and again. So, for streaming all the active streams, just stop them. Okay. So hopefully that gave you an idea of what autoloader can do. It's very flexible, just has a little bit of syntax, but it is streaming behind the scenes. It is um file level.
(2:05:02) And we um took a little deeper look into the main concepts of schema inference, schema evolution, schema um enforcement. We had the merge schema, we had the checkpoint, we had the schema versions. So take a look at it and we will catch up again on Thursday. Sorry we were a little over any last minute questions. Let's see.
(2:05:27) um from homework uh when we are doing the SK uh schema drawings you know um I was uh not every table have a primary key or like is it normal to have tables that have two primary keys like a combination of two keys are the primary keys yeah sure um it's possible that you may not have any primary key but in this particular example I think most of them have some form of a primary key, but it's not required.
(2:06:04) And it's yes, it's possible to have multiple keys to con contribute towards your primary key. If there are tables that does not necessarily have this unique key, it's just like you know that combines two tables like this user have this thing but that user also have another thing but there's no key for that specific thing like there's no ID for that table.
(2:06:30) Um should we put no primary key or should we just select two things that are that should be always unique as primary key. If you are going to show a join relationship between two tables you have to identify on what they are joining whether it is one key or two key doesn't matter if yeah those will be foreign keys but should I select also a primary key for that one? No, it's not always required. It's not needed.
(2:07:00) Like for instance, you could have um um transaction data continuously flowing in although every transaction is unique. So it has a unique transaction ID key. But I'm trying to think of an example where you may not have a primary key. Um which one did you think did not have a primary key? Let's take a quick look at the TPCH data. R sub. So it has R key.
(2:07:25) It has sub key um and so part sub table. Okay, give me one second. Let me go there. I don't remember. Actually, I think line item is also the case. Line item does not have an line item ID. Mhm. Um, but does it have something as a line item unique name? I don't think so. Okay, then that's perfectly fine because this is just an exercise for you to kind of understand how these ER diagrams are done without uh looking at the data fully uh without finding out what are the um you you can do a data profile on it very quickly to see uh which ones are unique keys and if it's not then so be
(2:08:19) it. No problem. Okay. Uh, and you're listing couple tools to be able to create a drawing. I found a website called BB diagram.io. Uh, it does exactly that. Can I use another tool? Sure. Absolutely. Yes. Okay. Anything which is free and uh, you know, you can actually download it and put it into a Google Drive so you can have a public link is good enough. I I'll just take a screenshot.
(2:08:48) Well, even if you take a screenshot, how you going to pass it on to us uh as an attachment to your submission? Uh sometime that or just copy paste. Yeah. Yeah. Yeah, that would also work. Okay. Okay. All right. I think it's been long. Good night, everyone. Thank you. Thank you.