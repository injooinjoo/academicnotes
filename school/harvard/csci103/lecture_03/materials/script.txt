103 day3 - YouTube
https://www.youtube.com/watch?v=D-hsjUoVtfM

Transcript:
(00:02) Okay. Uh, welcome everybody. This is um our third lecture of data engineering. Tonight we'll talk about um data pipelines including ETL and batch and streaming pipelines. So we'll start with a review of some of the concepts from the previous class. Review um data data pipelines. talk about streaming um both the need for streaming and the concepts uh look at um batch and streaming and and also discuss Lambda and Kappa architectures and then uh discuss failures and um trade-offs and also best practices and the second half of the lecture will
(00:51) be lab 02 and um a note on assignment two will publish that um prior to Thursday and then Thursday's section will be uh focused on reviewing the assignment two requirements. Um any questions? I do have a question relating scheduling office hours. What is the best way to schedule office hours? Is it by the Slack? Is by email or is there something else that I'm missing? I think in the in the first lecture and also in the syllabus there's um links to Calendarly Calendarly links um for myself and Anadita as well as all the teaching
(01:39) assistants. So that's that's um how we prefer to set up uh office hours. So those are I think they're all 30 minute slots with um the teaching staff. So you can choose who you'd like to talk to and then um just schedule something that works for your calendar. Okay, great. Thank you so much. I will look for that. You're welcome.
(02:06) Any other questions also on that note um if for some reason you are there on the call and the other person has not reached do reach out on Slack and ping them because sometimes you know with multiple calendar handling people may forget so just ping them. Okay. Thank you so much. I will do that. All right. So, um just a quick review from last lecture.
(02:46) Third normal form is is what? believe this is when you have uh two levels of a strct of tables like normalized tables. Yeah, it's a a normalization process to reduce um data duplication and help help ensure data integrity and something that we do with relational databases. Generally denormalization is the opposite. It's combining all of the measures into one single table.
(03:24) Well, yeah. Um so it's basically the opposite of um normalization like third normal form where um we we and we do it for efficiency with uh no SQL or non- relational databases where the um information is uh redundant across different tables to help in increase query performance where we don't have easy access to joins.
(03:57) ETL is extract transform load. That's right. And um and then the variation of ELT is extract load transform. Yeah. Uh two popular data warehouse and data mart schema schemas star and snowflake. That's right. So um yeah in both of these there's a fact table surrounded by dimension tables and with um with snowflake there can be multiple levels of dimensions surrounding the the the fact table.
(04:39) example of a key value store a JSON file. And in this case, um, basically you have a key and a value. Um, and so S S3 and is is an example of a key value store where basically you have a key and then you have some sort of value that could be any any object that can be retrieved.
(05:24) It's a ve very very very simple storage but generally very um scalable and and usually very fast example of a document store MongoDB and and um couch DB is another one good where and with document stores rather than storing just an object we're storing a document and example of a commonar data store. Somebody somebody said said it before and I think I just heard someone say Cassandra Dynamo. Yeah. Yeah.
(06:13) Dynamo DB and Cassandra are um databases that store their data in in terms of columns versus rows, which is what we're probably more familiar with with relational databases. Okay. Um so why are binary formats um preferred over text formats? better compression, faster reading. Yeah. So, they're generally um faster and and and more performant. Uh parquet is what format and what's its advantage? Columner format. Paret. Yeah.
(07:02) And um it's binary stores the data in binary format. and um is therefore more performant. Uh four main properties of delta format. So Delta is a is a table pro format provided by um which is open source but also provided by data bricks asset um properties um schema evolution. Yep. Yeah. history. History. Yeah. Time travel and uh and and unifying batch and streaming modes.
(07:48) Well, um I'm not sure about the last one, but the um fine grained inserts and deletes. So, um so you can basically update uh data within the database easily at a very fine grain level. And then um the version versioning of the data is important because that allows you to go back and forth in in time.
(08:14) So you can go back to a previous version or or forward in time. Uh what is metadata? The data about the data itself like the additional data. Yeah. data about data that helps provide the context of the data and examples are schemas and and um transaction logs and delta. Okay, very good. So let's move on.
(08:50) So uh just a quick overview of uh different types of table types. So we have um permanent tables that are support um persistence of the data and then temporary tables are um as as the name implies are temporary tables that exists only for a a specific amount of time. And uh and we have two types local and global. and local exist during the duration of this session and global are shared um temporary tables can be uh shared across the session within the same cluster.
(09:31) So and they're and and temporary tables are not persistent. Then we have views which are um also referred to as virtual tables. Uh basically a view is defined by a query and uh and it stores the query definition not the actual data and each time you uh reference the view it basically recreates the um the result.
(10:04) So every time you um reference the view it basically executes the query and and returns the results. So materialized views are similar to views except um they actually store the results of the query which um helps um basically caches the query result which helps um improve performance but um can can result in stale data if if the view is not refreshed um it could be um u returned stale results.
(10:39) Um so definitely faster since you don't have to recomputee the query results each time it's referenced and um good for aggregations and with um joints and often used in BI reports. And then a streaming table is a special type of table for um streaming pipelines that basically as as events come in or data comes in the table is automatically updated with that new data as part of the streaming pipeline and we'll we'll look at those more um later tonight. Okay.
(11:20) So tables can be either um managed or external and um and this is something that we see with the data bricks data bricks um ecosystem where um you can either create a a manage table where the the data bricks um platform manages the table for you or you can reference an external table that's um created outside of data bicks but can be accessed from the from the um from the platform. form.
(11:51) So um with the manage tables the location is um is managed within the warehouse path um the database manages both the data and the metadata. And uh with the drop behavior um when when you drop a table a manage table it removes both the um metadata as well as the actual data. And um with the external table only the um metadata is removed because the the external um file remains.
(12:24) Um, managed tables are uh less easily shared than um external tables and risk of loss is higher with managed tables since you lose when you drop the table you lose not only the metadata but also the actual data and the code to create a managed table differs um slightly from the from the external table. It's basically simpler.
(12:51) If you don't specify the format and the location, then it assumes that you're creating a managed table which will be managed by the the platform. Okay. So, data pipelines which will be the focus of tonight's lecture um is an artifact of uh data integration and engineering. uh could be as simple as moving data from point A to point B or uh ingesting data from multiple data sources, doing multiple transformations on the data and then saving the data to to multiple output um locations or destinations and ETL pipelines which are a special
(13:37) case of um data pipelines generally generally um ingest data transform it and then um output it in a format that's ready for analytics or um consumption by applications or machine learning or um um AI systems and and basically ETL pip pipelines are designed to support business needs um and uh and decision decision making.
(14:11) So this um vin diagram shows that the the data pipeline is a is is a is a more general thing than ETL pipelines that are a special type of data pipeline. So to create a um data pipeline we need to design it, implement it, test it, deploy and then monitor it.
(14:43) And you can think of it much like a software application that we would design and implement and and deploy. it needs to be able to uh or we need to be able to handle changes and then update it to to support those changes and then redeploy it. Automation is critical so that um the the pipeline can run on its own and this is a major advantage of it is that um it doesn't have to be done manually.
(15:09) And then how the data pipeline is triggered is an important consideration. Triggers can either um the pipeline can be either triggered through um like a new file showing up in a file system or um some sort of schedule like a like a cron schedule or um can be manually triggered. So u those are things that we need to consider when building a pipeline.
(15:39) Advantages of having pipelines is um reduces load on the IT team by through automation and we can um make um the consumption of data and the delivery of data to users um more automated. Uh utilizes uh cloud-based dynamic resources. So when there's a lot of data flowing in, we can use cloud-based resources to and their elasticity to basically expand to support the increased load when there is volume.
(16:11) And then um and and then um for the business it supports um real-time analytics and um real time decision- making for the business which is um important as we'll see. There are challenges with data pipelines in including uh data issues. So schema changes or changes in the quality of data can cause problems and then fixing these issues um can cause outages for our data pipeline.
(16:46) And the the pipelines also tend to be tightly um or dependent on the frameworks that they're built on. And any changes to those frameworks can can create um issues for us as well. So um different just like um with tables we have different types of um pipelines. Uh the first is batch um where we we take a a set of data and process it and then um at some point in the time take another set of data and process it and things um use cases for for batch processing include reports um billing and uh historical analysis streaming or real time is um continuous
(17:41) um ingestion of of incoming data and um and situations where this would come in handy would be like uh fraud detection uh dashboards, recommendations and also handling events from IoT uh type devices. We also have ETL or extract, transform and load and also ELT extract, load and transform as uh special types of pipelines where ETL is is more traditional batch oriented processing where we extract the data, transform it and then then load it where um ELT is uh is designed for um situations where the the data schema of the incoming data is is less known and
(18:36) um can that way the the data can be ingested and then loaded and analyzed and then transformed and processed. We also use um pipelines for data replication uh machine learning pipelines that include the process of generating a machine learning model. We'll look at that later in the class. How to create ML pipelines and then um workflow and orchestration pipelines that are more configurable and can be used to um change change the sources and destinations of um the data that's being processed by the pipeline.
(19:26) Okay. So why um why would we care about streaming data pipelines? It's all about speed um we want to be able to provide uh insights faster um by um basically taking incoming data and producing analytics from it as quickly as possible. Um so um and that that creates a real time a focus on real-time processing and with large and the ability to handle large amounts of data.
(20:05) And this this is composed of the collection of the data, the analysis of the data and then the storage of the data all in all in real time. um works well with um time series data where the the incoming data is timestamped with and and comes in some some basically sorted by time and um so and there are situations where streaming is required.
(20:40) Um uh some examples include sensor data uh ad serving uh um uh security applications, clickstream data and um payment transactions. And also uh it's interesting to note that um we can even consider um batch jobs as um streaming jobs or basically implement batch processing or data pipelines that do batch processing as stream pip pipelines that have longer trigger durations or longer durations between invocation. and we'll we'll see how that can be done.
(21:24) And a important thing for us to remember when we're designing our data pipelines is we really want to focus on business needs versus what works well for the uh IT department. So thinking about uh the needs of our business users and that includes things like the the speed of access to the data, the accuracy of the data and um and and what data and and what not only what data but also the types of um questions that the business needs answered.
(22:02) So having a um business focus when we're designing data pipelines is extremely important. Some concepts um include uh so um every every data pipeline has a source um some sort of processing um component as well as a sync where the data is is written to. There's a file-based and event based um uh streaming where e the data is either read from or written to a file versus um either consuming or generating some sort of event.
(22:50) uh microbatch versus continuous streaming where microbatches process uh small chunks of data over and over again where continuous streaming just continues to process the incoming stream. Um there's a concept of a processing trigger that is used to um initiate the the microbatch. Uh there's different output modes. uh we can we can either append, overwrite or update the data when we're writing to a stream.
(23:22) Uh there's a concept of a checkpoint which um allows us um basically last known good point where the data was uh correctly written. So if there's a failure after that point, we can go back to the checkpoint and recover and maybe re reprocess the data that came after that checkpoint. A window is an aggregation interval. So for aggregate functions like sum and count and average um that window determines what what what period of time is used for the aggregation.
(23:58) Uh watermark is a um is a basically a a a an amount of time that we allow for the data to be uh considered valid or or invalid if it comes too late or beyond that watermark. um the the streaming pipeline can consider it uh stale and ignore it. Uh if it's within the time within if it's with within the watermark then it's um accepted and and processed.
(24:30) Um, stream operations are any operation that occurs within the streaming pipeline. And instream analytics are special type of stream operation where um the data is not written to disk or to a file. It's um it's um read from the stream and immediately processed. And um this can be um faster because you're not writing the data to the disk and then and then processing the data from the disk.
(25:04) You're just reading it as it comes in within memory. So it's it can be faster. So some use cases around batch near real time and real time. Uh so batch is good for things like um uh metering and billing um ad hoc analytics and and business intelligence. And then near real time is like where where the data is processed and made available within minutes.
(25:39) That could be um something like processing mobile or IoT data uh log ingestion um ad serving uh clickstream analytics where you you want um you want the results quickly but it doesn't have to be immediate or real time. And then there's other use cases that require second or sub-second response times. And those thing that includes things like fraud detection for credit cards.
(26:10) Um uh financial asset trading where you're you're racing to beat other traders to the best trade. Um multiplayer gaming. um ML in inference where you're um processing data through a um machine learning model and you and you want the um the latency to be as low as possible.
(26:37) And also think something like patient monitoring where the the maybe the life life of the patient is relying on um you know quick access to their health data. So um so there there's basically here we can see there's a spectrum of um of how quickly the um streaming data can be processed and that not not all use cases require real time. Um some can get by with near real time or or batch.
(27:12) And that's important to for us to consider when we're designing a a pipeline for a particular use case. Um because uh we want to we we don't want to go um you know support real time if if it's not really required. So So now let's look at Lambda versus Kappa architectures. These are two different um data pipeline architectures. So with the lambda architecture uh basically uh it's a way of supporting both batch processing and in um in parallel to streaming uh processing.
(27:56) So in this case we have a separate batch layer or batch engine that um processes the data in batch mode and then a streaming layer that's able to um stream the data in as as it arrives either uh real time or near realtime streaming layer and then the the results of the batch layer and the streaming layer are combined into the serving layer. And so that that's the lambda architecture and this came first um as a way to support basically the the benefits of batch processing in addition to um streaming.
(28:35) Uh, Kappa is a simplification of Lambda where instead of having um both a batch batch processing engine as well as a stream streaming engine, we just have a single streaming layer. And this the streaming layer uh supports both um uh batch processing in microbatches as well as um as and maybe not necessarily um micro batches but batch processing as well as streaming.
(29:09) And the the architecture here is simpler because um there's only one streaming layer to do the uh comp analytics um instead of having both a batch layer and streaming layer. So to um contrast the two, the Lambda architecture combines both batch and streaming um and supports both speed and and reliability where the batch processing is considered more reliable than the streaming.
(29:43) Uh but it does introduce um uh duplication in the data processing and um and and basically um combines the the outputs of the batch and uh and streaming into the output layer where Kappa um batch is treated as streaming and doesn't need a separate batch layer. So it's uh simpler um it's more scalable and um and it requires streaming with consistency. So exactly once processing.
(30:23) So even if if it receives the same data a second time, it will um handle it correctly and works well with high street high-speed um streaming and again can also support batch processing in addition to streaming. So that's um Lambda versus Kappa. And here's an example of um Lambda in Kappa in the Azure environment. So it it looks similar where with the the Lambda architecture, we have both a um streaming layer that's able to take in streaming data and process it as well as a um a batch batch layer that um basically does um maybe some sort of SQL processing to um um produce the results for
(31:19) um visualization. and um and apps to consume the data. So key thing here is is that there's both um a a flow for the batch data as well as the streaming data. In the Kappa architecture, we have um a single flow where there's just a single streaming layer that h handles all the incoming data sources and can support both batch and streaming. uh workloads.
(31:55) So it's um simpler because there's only one flow that we need to maintain and there's no um there's no um conjunction of the data like in the um in the lambda layer. you have to somehow rectify these two sources of data from your batch and streaming processes where with the um with the streaming um or Kappa architecture there's only one flow and only one output that goes to your uh apps and visualization.
(32:33) [Music] I'll stop there and see if there's any questions about Lambda versus Kappa. [Music] Is Lambda the same Lambda in AWS? No. Lambda in the AWS is a um serverless uh uh serverless um service that um handles um basically serverless deployment of services. So it's it's um lambda was used was referred to as a way of referring to a function and um and um like functional programming.
(33:21) So um it happens to have the same name lambda but it it it it's not the same meaning. Uh so here lambda and kapa are not um these are not products in data bricks or somewhere they're architectures and they're a way to do something. They're a concept. Yeah that's right.
(33:55) there are different um basically two different architectures for creating um data pipelines that can handle both batch and streaming uh needs. But um with Kappa um Kappa came later after Lambda and Kappa was basically a a simplification of the um pipeline so that you only had one flow through the system. And I I've worked with um a system that used this um lambda and it was we had lots of challenges and and uh the replication of the two two ways of processing the data.
(34:32) We had to do aggregates and in the streaming mode as well as in in the batch mode and making sure that they they were consistent was challenging. and then the um rectifying the data between the two streams was was difficult as well. So um the the the the kappa architecture is is is preferred because of its simplicity. I have one question also.
(35:00) So let's say you for an application u we don't need to combine combine batch and a streaming. All we need is a streaming processing. Um, so if you want to design an archite architecture that is dedicated for that streaming processing, how would that be different from the cappa architecture that is shown here? Well, I think I think what you're what you're um what you're suggesting is is basically the kappa architecture where you you just have a a streaming process Right. So for like you're saying the application for lambda and kappa is when you want to combine batch processing and
(35:43) streaming processing. And my question is what would you do differently than kappa if all you need is a streaming processing? What would you do differently from kappa if if what if all you need is a streaming? Well, uh, in that case, um, if you don't need the batch processing, then then you don't you don't really need the the the batch processing and and you can ignore ignore that aspect of your um streaming pipeline. You can just focus on the streaming pipeline, but it would look the same as as um as the Kappa
(36:20) architecture. It's just you wouldn't be taking advantage of the ability to do batch processing with it. Um, one thing that I see here is it is just concepts like streaming ingestion service and analytics service but it doesn't necessarily say what do they exactly do. It's more like names for the things.
(36:47) So um I don't think at least for me I I understand what is going on because as I said I'm just learning this name leads to that name which leads to the other name. Yeah. Well, again, this is an example of um Lambda and Kappa architectures within the Azure environment. And um and these are very generic um blocks or components. It doesn't really tell like it it's it's doesn't indicate exactly what it's doing.
(37:18) Some sort of business process, but um it's it it's very generic. It could be basically any business process that that consumes uh CRM, operations, finance, HR, and streaming and streaming data. It doesn't even say what the streaming data is, but it's um it's just defining a like the Lambda versus Kappa approach for defining the data pipeline. Yeah.
(37:47) And I want to um emphasize that these terms Lambda and Kappa are industry terms. they are not data brick specific terms or they are not in the context of AWS services lambda. So if as data engineering um practice if somebody were to say design a capa architecture for streaming you should know what they are referring to.
(38:06) Uh just like certain terminology like ETL and ELT and OLAP and OLTP lambda and Kapa just that and what is shown on the slides here is just the fact that Lambda is going to have two separate pipelines that need to be reconciled and managed and Kappa treats everything as streaming.
(38:29) So your batch is also treated as streaming and that is the newer concept that the world has embraced um in which you do not have to think of them as two different things. They are just different by the frequency in which data is coming and reconciliation is such a headache and so much of effort is spent uh handling it that it is best to treat everything as uh as streaming and you know we are just beginning our chapters on that.
(38:54) You will see in the future as to how tunable certain configuration parameters are. So that the distinction of batch and streaming is actually completely gone. It's like what is the frequency which with data is coming. Is it continuous? Is it 1 second? Is it 5 seconds? Is it 12 hours? Is it 24 hours? That's what makes uh something in our head as batch versus streaming. But business wants data as soon as available. So streaming is the way of the future.
(39:18) There's a hand raised Jeremy. Yeah, I think you might have answered my question already. I was kind of wanting a few more examples of each but so it's really a lot of it is the timing like you had given some examples in the first class about like autonomous vehicles and sensors and so where streaming becomes critical uh then you might would go with like a kappa versus use cases in which there could be a delay. Yeah. But you'll see eventually that you can use the Kappa architecture for both.
(39:50) So when you have a streaming uh system and you say that the frequency of data arrival is 24 hours or 1 month or one quarter, it doesn't matter. The stream will just wake up and um do its thing. Whether it is understanding what is the new file, what is uh already processed, what needs to be transformed.
(40:12) In batch, you actually have to do all this heavy lifting. you have to keep track of uh what are the files that are processed and what are not processed. The biggest advantage of treating batch um systems as streaming is to kind of push that that headache onto the platform and you'll see certain semantics like exactly once and at least once and all of those things which are hard to do manage through the checkpoint file.
(40:44) So you are in cloud environment at some point your cluster is going to go down and it will at some points it will go down. So when it comes up you don't want your files to be dropped or you don't want them to be reprocessed twice. So that exactly one semantics and the ability to use the checkpoint to determine from where to pick up the stream becomes important and all of that is already handled in streaming.
(41:02) With batch you'll have to do logic you'll have to write your logic to be able to do so. So all in all the key takeaway here is Lambda is an older architecture. Um at that time streaming technologies were not very um mature and batch was the way things were done but streaming was coming in.
(41:26) So what do you do when you have a fastmoving data to handle but you you just don't have the compute resources and and maybe you do not want a huge cluster. So that is why people came up with this oh this is batch which is like our bread and butter and this is this esoteric streaming use case and at the end of the day we'll figure out how to uh have some reconciliation between the two because there there will be discrepancies that we'll have to take care there might be some duplication there might be some some things that are coming on the fast channel as opposed to the slow channel and so on but once kappa architecture is mature it's stable
(41:56) I don't think lambda is much in favor these Yeah. So, yeah, Lambda was really a way of kind of like benefiting from the batch and its ability to process large amounts of data, but still having the capability to process incoming data, maybe smaller amounts real time.
(42:24) where now the um the the frameworks are powerful enough that you can do both batch and streaming with a single platform. Is there a cost advantage to use kappa sorry lambda because you're running in batches and you're processing much more efficiently. Um yes um but no at the same time because you'll see there are different options to kind of trigger your pipeline and so if your um batch is once a day you can have your streaming wake up once a day in which case your stream is not on all the time and hence there is no cost disadvantage.
(43:06) Yes. And it depends on the use case. So if your your business users are going to use the data only in the morning um or they they're not going to look at it even though you have processed it and made it available. Why would you have it as a streaming use case? You'll process it every 6 hours or every 12 hours or every 24 hours to make it available so that your compute is well optimized and does not have to run all the time. But the advantage is if your business user suddenly comes and says oh we just have
(43:34) a new requirement in which we want this data to be made available twice a day. It's very easy. You change your trigger uh configuration to say wake up every 12 hours or if they say every 15 minutes, you can do that. Every 15 minutes was going to wake up, look for some new files.
(43:52) There are new there are five new files, process it and go back to sleep or um there are new files. So I'm I'm I'm just not going to do anything. So you'll see something called as processing trigger. And there are other aspects of waking it up like if a new file arrives, wake it up. um if somebody uh calls it explicitly wake it up like there's a manual trigger as well.
(44:13) So we will like we will look at all those different ways of triggering a pipeline and a workflow um in in your coming uh sessions. But all great questions. But if if you're running it, if you have option to run it like once a day, isn't that at that point batch? It just anyway. No, no, no, no. Very good point.
(44:39) It is batch in your perception, but the way it is designed is a stream because the semantics of a batch is spark read, which you have already done. And the way you're going to write your streaming pipelines will be spark readstream. Now the fact that it wakes up uh once a day and processes only what is required and goes back to sleep is a different story. But you're not using spark read at that time. You're using spark readstream.
(45:04) You have to say what is your checkpoint location. you'll have all your streaming semantics uh you know lined up and it's very close to the batch stuff but with certain additional uh nuances um and uh as I mentioned the the most difficult thing in batch is to just keep track of what files were processed and what were not processed so that you are not in the business of creating those duplicates or missing some files in streaming you don't have to do it the checkpoint automatically takes care of it and Then
(45:36) um you know streaming has a lot of other advantages to it apart from like the real time uh dials that are in place um that you can easily switch. So if you want if the business wants batch like you can make the trigger interval 24 hours. If they want streaming like you can change it.
(45:59) Uh so it is more about uh processing the delta each time. Yes. Yes. it will do it a much better job and it it you have delegated that to the platform and the you know the framework instead of uh a data engineer putting in additional code to say oh this file was already processed I made note of it I put the status and I'm going to move it to this other directory so I I don't accidentally process it um all of that in a batch mode you have to do in streaming checkpoint is very powerful it'll do there will be other constructs like um late arriving data
(46:35) um doing uh um you know aggregates uh and processing while the stream is in flight because there are certain operations like imagine you are catering to patients in a hospital um they somebody's coming into an emergency you have to quickly see which room is available you have to get there thing so all of these are very quick activities and these are um like seconds um of latency if not milliseconds let's say seconds at that point a lot of these uh uh activities or these analytics is happening instream even before it touches the disk and you'll see uh the
(47:14) differences between true streaming and uh you know pseudo streaming in a bit we'll go over the different types of streaming frameworks uh right in the next slide okay and with that on detail I'll hand it over to you okay great discussion love So I have a last question. The uh Sure.
(47:41) So the the metallion architecture come both like lambda or kapa right? Yeah. Okay. Mhm. So these are again different uh concepts. Um one is um how you are treating like what are the constructs with which uh you're treating it. As I said in your head just say spark readad is batch spark readstream is stream is streaming and uh medallion can is a way in which you are refining the data in both cases you have to refine it like data might land and then get uh joined and that get normalized and get curated and so on. So medallion comes there.
(48:22) All right. So these are the popular streaming processing frameworks and it is very timely that you were asking about it. How many of you are working with Kafka in your maybe day jobs? Any of these uh technologies appear familiar? Okay, Shya, that's wonderful. Um so you definitely have a low latency requirement.
(48:49) Um Eric just talked to you about the nuances uh the the you know certain concepts. So one is uh when you have um streaming you can have filebased streaming and you can have event- based streaming when data touches disk and then is there is a spark.stream stream that is file based when data is still flowing either a event hub or a cafka or a kinesis or whatever and you're you're pulling it right from there that is event based right so here um the processing model is um events right you know there will be different techniques and then the state can be managed by
(49:26) raox db but typically you know the the duration of that data even when a kafka stream is growing is say around 7 days you can increase it but it's not infinite there is some fall tolerance um and it there's some Java library there are easy examples it's very lightweight library with which you can get started now flink is true streaming like people who are fanatics in this space uh they may not consider microbatching as streaming so true streaming is like it's continuous it there is there's no millisecond microc microbatch ing and so on. Uh and it is event based. It's very
(50:08) very low latency. It's for advanced systems. Um and it's strong. So your fault tolerance has to be exactly once. Uh ease of use is medium and it can support the most complex use cases. So C stands for um event processing, complex event processing.
(50:33) So remember as I I said as the event is coming logic is happening right there in stream and decisions are being made. So that type of use cases are served through flink although it's a little difficult to grapple with. Spark structure streaming is like a medium a good base. It was microbatch that's the default. It can support continuous mode but it is its latency is hundreds of milliseconds.
(51:00) Now there are some other projects that are underway there that we can bring it to tens of milliseconds but it is typically higher latency than these two. The checkpointing that we are going to see in an example is going to save the state of what was processed. So in an environment where things can uh where you can lose your compute the next time compute starts up no damage done you start out exactly um where you left off.
(51:28) Um ease of use is very good and uh you can use it for both batch and streaming. So that's the unified uh API. Uh storm is um it's a little born legacy. It was very popular maybe 10 years uh back. Uh it is event um at a time very low latency. It was extremely hard to grapple with like you know the cons were a little uh difficult and I would say it's more legacy now.
(51:53) SAMA came from LinkedIn that's also very low latency. Think of the kind of updates LinkedIn does. Uh it works primarily with YAN and Kafka. It's medium um you know in in its ability to use. Now KSQL DB is a layer on top of Kafka. It's Kafka SQL DB. So it supports continuous SQL over Kafka so that it becomes easier to use it.
(52:16) So it's low um it inherits from Kafka. uh they it's very easy. So instead of a Java library you can use SQL. Um then these three are the native cloud offerings. So in the AWS ecosystem it's Kinesis. In the Google ecosystem it's Beam. Um and then um Azure has its own stream analytics as well. This just goes into uh what we just talked about.
(52:49) But if I were to have a rule of thumb then when you're looking for ultra low latency and stateful C use Flink um for Kafka related uh you know you can have your confl Ka and you can have your managed uh Kafka and your open source Kafka um then unified patch and streaming. So when we are doing lakehouse our base is spark structured streaming.
(53:15) There will be other layers that are built on top of it like you'll see uh terms like autoloader and DLT and LDP and so on. But the base on which all of that is based is structured streaming. Um now if you want cloud native integrations you can have Kinesis and D dataf flow and uh um Azure stream analytics but Apache storm is on its way out. I don't see it as much. Uh we talked a little bit about event and file.
(53:40) So when something goes to disk and then somebody pulls from it that is file based and obviously because it goes to disk it is slower. So filebased streaming is slower. Eventbased means it's flowing and we are picking it from the host itself. So obviously it's going to be faster. There are two generations of this eventbased streaming.
(54:00) The first generation was with your message brokers like you may have seen all the code bases with active MQ and rabbit MQ they were based on message oriented middleware and I don't see too much of that these days. Uh instead Kafka is become almost like a de facto standard. Um you have uh you know in again in the AWS ecosystem Kinesis has a very evolved um offering as well. They support very high performance with persistence.
(54:32) They have you know they can support huge capacities and message traffic and they are tightly focused on streaming. So not so much of data transformation which C systems tend to do but mostly on just bringing the data to you as fast as possible. Now in Kafka there's the concept of a producer which is bringing in the data the consumer of course and in between it's these topics.
(54:55) So these consumers will be tapping into the Kafka cluster listening on their topics and those will be partitioned. We'll see some examples of this later on on datab bricks. This is where we bring it all together. You've got your various different data sources, you've got warehouses, you've got on-prem systems. Now remember, uh data bricks is a cloudonly offering.
(55:20) Um this was a big bet that the founders took a while back and it kind of paid off because the shift in the industry is cloud-based. That doesn't mean there is no on-rem and there are sophisticated specialized system on prem but by and large we see migration of these on-prem systems into cloud platforms.
(55:45) Um, you've got SAS applications, you've got machine and application logs, events, and then IoT data. So, they can land in file storage where it goes to the cloud um like your S3 or your um ADLS or whatnot. Or you'll have these message buses which are your Kafkas and your Kinesis and your event hubs and so on. Now, here we touched on Delta.
(56:08) We haven't done a deep dive into um um the aspects of the Delta protocol yet, but remember the data is still in open format in your cloud storage eventually when you persist it. When you put the delta protocol on top of it, you make it ready for analytics by assuring that there is asset compliance, there is schema evolution, there is schema enforcement where necessary, there is time travel where required um and so on.
(56:38) Then you have unity catalog which you have already started using which is going to be a catalog for your metadata but later you'll see how it can be used for governance as well. uh you'll see how it traps uh you know it captures your lineage when you do your transformations. It has audit logs.
(56:58) It allows you to give permissions to different groups and people. So I can access it, somebody else cannot. Um you know you'll go into row level and column level security as well. Just think of this as your governance layer. Uh photon is a rewrite of the spark engine. almost 13 14 years back when matesa wrote spark he used scala as the language then to write this and this is a engine you can think of this as a engine um and after a while uh because it's a JVM based uh system there are some inefficiencies in it so photon is a rewrite in C++ for
(57:35) faster uh data processing uh it's because it's C++ it there are some vectorized operations and it and take full advantage uh of the object-oriented aspects of it but without sacrificing any performance. Your structured streaming is right here and built on top of it is DT.
(57:59) The newer name for DT, this part is a little confusing because sometimes we go back and forth on names. It's now called LDP uh lakehouse declarative pipelines and we'll have a session on that at some point. But just remember that um uh Spark structure streaming DT all of these things here they're all open source. Now once you have your curated data what are you going to do with it? You might do some realtime analytics uh with a dashboarding and and um you know reporting. So that's where data bricks SQL will come in handy. Um we were talking about SQL and NoSQL the other
(58:32) day and we said that well this is almost no SQL data but we are able to have an SSQL on this kind of denormalized uh um tables and if required we can do joins as well. Earlier joins was very expensive now well if you can avoid it good but if you can't it's it's not too bad.
(58:54) Um we've got um a whole series of uh machine learning um options that you can use on the databicks platform. In fact, we have a runtime which is ML where all the common packages ML packages come pre-baked and they have been uh vetted. uh you are using serverless so perhaps that's um not as relevant uh now but all these packages makes the life of a data scientist easy so you don't have to spend time doing dependency hell like 14 version of this is compatible with 15 version of this or no I have to remove this and bring it down that takes a lot of time all of that is done there might be a special library that you might want to use you
(59:35) can always do a pip install at a notebook scope cluster scope um and so on. Then uh these are your enterprise applications. They can be BI apps. So if you want to use a Tableau or a thoughtpot or a PowerBI or a Looker, no problem. You can do that here. But if you want enterprise uh grade, that's like, you know, the the ones that a CEO wants to see, that's also fine.
(59:59) There are real-time AI apps that are very uh tightly integrated with your business or operational apps like somebody may want to be alerted when this pipeline fails or when they detect some fraudulent pattern or when there is some pricing discrepancy. Uh so remember this very simplistic view of an end toend flow of um a streaming pipeline.
(1:00:24) Now let's look at an example. The first thing I want you to remember is if you see a spark read stream that is streaming. If you see a spark read that's the older batch and both of them are perfectly fine and both of them are still used. Um now when you say dot format you can provide a series of options here and this is basically your source. Depending on the source you can have some options and you load it. That's it.
(1:00:51) You have access to your incoming data. If it happens to be JSON data then maybe you can cast it as a string. You can use the from JSON uh function to take that JSON data along with the schema. Um and you can select those individual files from it and call that your payload. When you're done with all your transformations, you may want to write it back.
(1:01:18) You can write it to disk into a table or you can write it back into another stream. So you can say write stream format is Kafka and whatever other options that are necessary you can provide there can be um a trigger there can be a checkpoint and then you start. So there are all these things are chained because of that API. I don't know why this is not showing on my screen. One second.
(1:01:50) Okay, something is off. Maybe I won't go into uh full screen mode because for some reason it's not coming up. Um lakeflow has got three parts to it. Connect will uh connect to the sources of the data and um handle CDC which is the incremental loading. So for instance, you can connect to an on-prem Oracle system or a SQL server or um you can connect to a SAS system like a workday or a Salesforce. There are different kinds of connectors.
(1:02:27) Spark itself had several but you had to manage the processing here. These lakeflow connectors are brand new and they will make the whole incremental like um ingesting of data simpler. This was a space that data bricks didn't play earlier. we always would rely on somebody to bring the data into cloud and then we would pick it up from there.
(1:02:51) So connectors is a newer thing that data bricks is getting into because customers say that that's also a very important part um that needs to be fixed. There are several partners in the ecosystem who do it. So for instance there are these informatas and the altrixes and the five trends and prophecies and whatnot. They they play in that space and they bring in the data.
(1:03:13) But now datab bricks can also offer these connectors. What that means is it's going to be more performant and it's going to be cheaper for customers. Pipelines is the heart of it all. This is where all the transformation happens. The medallion stuff that we were talking about. And then jobs is the workflow orchestration.
(1:03:36) Um when it is to be triggered, how often um what is the dependency, what needs to come after what and if it fails, what should I do? All those things are captured in the jobs and that is how uh the data engineering has evolved on the data bricks platform. Now because jobs is uh got so many um bells and whistles to it from an orchestration reliability and observability perspective.
(1:04:02) Let's take a quick look at some of the features that it offers. File arrival triggers. Uh so when a new file comes and it is able to kick off the pipeline that's the file arrival trigger that's pretty powerful which means sometimes I do not know whether my file will come at 3:00 or 6:00 or 8:00 and I'll have to have compute and I might have a processing interval in which I spin up the compute and look at it.
(1:04:28) So having the file arrival trigger definitely saves on compute conditional tasks. So in your uh workflow you can say if this passed or if the result of this was one then go down this path otherwise go down another path. So conditional tasks is very powerful to introduce that dynamism in the uh workflow. So it's not like a like you know slam dunk straight path.
(1:04:54) It is pretty aware of the results of that processing and decides to take action. You can use serverless compute. In fact, the free edition that we are using now is all serverless. Uh you can have job level parameters. So you can pass parameters into the job and the jobs um they can pass it down to the various tasks.
(1:05:16) Um this is important because that means your code is the same but maybe you have to do it 500 times each time with a different source and destination. So you don't have to build 500 different um you know pipelines. You build just one and then you pass different parameters.
(1:05:33) web hooks um when you have to provide notifications perhaps to your Slack channel or to an email to a page of duty or whatnot. Job cues uh sometimes uh you will send your concurrency to be a maximum of this or maybe you know you want any new uh triggers to be queued up. Um continuous execution we talked about microbatch and continuous task looping.
(1:05:56) Some tasks need to be done five times or 10 times. So instead of repeating that code u five times or calling that notebook five times you have a construct very cleanly in the workflow which says that this is the parameter that is passed for i is equal to 1 to n uh run this that many number of times.
(1:06:16) So there is a for each built-in table triggers when new data is populated in this table then trigger visual monitoring uh how did my pipeline do which jobs took more time which ones failed you can see that all in the observability dashboard uh jobs data in system tables. Now after the job has executed there is the concept of system tables which stores information or metadata about the job itself like when did it run what was the ID who triggered it how long did it take um all those kind of information can be viewed later um to see well yesterday the job took 1 hour but typically it
(1:06:56) takes only 30 minutes why did it take 1 hour or maybe it took 5 hours so those kind of um due diligence can be done later. But if you if you have the monitoring data modular workflows, so you can always stitch, you can uh you can call out to another job or another workflow. So they are all modular. Um duration alerts like so we were talking that this job typically takes half an hour.
(1:07:23) So if it has taken more than an hour, maybe something is wrong, kill the job. Otherwise, your computer is just hanging and you're paying for nothing. So uh gant visualization uh so you'll see it in nicely in green and red with a duration which is relative to say how long it took uh SQL tasks uh data bricks asset bundles and this will be covered much later.
(1:07:48) Uh this is when we'll talk about CI/CD and how asset bundles helps with uh code promotion from a lower environment to a higher environment. This is probably a lot, but I did want to give you um a flavor or a feel that ETL is serious business. Yes, it's transformations. Yes, it's medallion architecture, but so much goes on behind the scenes to have a robust pipeline.
(1:08:08) So imagine if a platform is able to give you all of this out of the box at the scale at the performance at the consistency then you know it's it's those are the kind of things that you should be looking for when you are shopping around um and looking for platforms which which is are good fit for your use case.
(1:08:32) So a pipeline is a pipeline at the end of the day but all of these things makes the life of a data engineer much easier. Now since we are in the uh context of streaming again there should always be optionality. Many systems get into a state in which there is one knob and that's it. You can't do anything about it. So if you if you want to tweak anything that's it. You have to either live with it or throw it off and go to another system.
(1:08:57) Spark was built to achieve these goals with tunable performance, cost and quality. Remember the cap theorem. So this is an this is a variation of the cap theorem in which you can't get everything. So if you want performance, you have to be willing to put in additional cost. If you want quality, you have to be willing to put additional cost.
(1:09:22) If you want speed, then you'll have to um you know compromise somewhere else. So that determination of what your requirements for that particular use case are is going to determine what kind of resources you are going to throw at it. And this is a very iterative process. So first you should understand the goals and requirements of the streaming use case.
(1:09:41) So stream one streaming use case can be very different from another. Um the criticality of the use case will determine how much resources you're willing to put into it. And for each of these goals you have to have a strategy. Um then for each goal you have to define how much resources is needed. You'll have to plan for it. then compute the cost of the resources that is employed and re-examine the tradeoffs to understand and balance your expectations.
(1:10:12) So again we are saying that um here you you have certain goals certain requirements certain needs and they will come in the form of like what is your scalability how much of data are you going to ingest what is your TPS um that's your your u throughput per second um if you know if you have huge throughput you can't have a very small compute you have to design your system accordingly what's your performance requirements that's the measure of scalability your end to end latency.
(1:10:41) How much of processing have you done? Maybe the data is already very clean. There's not much of processing to be done or maybe it's extremely dirty. So you'll have to do your due diligence in cleansing it, transforming it, normalizing it, curating it. So how many hops, sinks, joints? Uh is there some stateful processing that is required? Is there a reprocessing logic that you have to think about? So these are your processing needs. Then comes quality.
(1:11:04) Um yes, you know, data has come in. Um but uh you have to um see whether it is correct not only from the conformance perspective but from uh no drops no duplicates um it should be potent. So if you run the same stream same u pipeline again there should be no problem because it should know that that file is already processed otherwise you're going to go crazy with so many little files coming in the entire day.
(1:11:32) uh reliability and availability, ability to handle or recover from a failure condition and then um just disaster recovery. The whole region has gone down and you know these are very very sophisticated use cases. What are you going to do um you know in terms of putting it into another zone, putting it into another region.
(1:11:55) So you have to understand the state it is in your data has to be available in another region. your processing and your code and your pipeline and everything. All the other assets have to be available. That's that's a very very mature use case. But at a bare minimum, at least these should appeal to you as typical things that you ask of a use case.
(1:12:16) So you just don't jump and start coding because you heard a streaming use case. You have to say how what am I designing this system for? How much of data? What is the speed at which it is coming? Who's going to use it? What kind of processing is needed? Then you're going to look at the resources. How much compute do I need? What kind of compute the number and types of VMs pools.
(1:12:35) Now serverless you do not have all of that luxury but you know at some point you may not have just serverless at your disposal. You might have to do your own uh compute clusters and so you should know that that's an important consideration. Storage where will you store it? Is it warm? Is it cold? What kind of additional services you have to integrate with? How many other API calls are there? Are there rate limits associated with it? Then the people part.
(1:13:02) So it's all about people, platform and process. Uh so the people part is where you know who is available to do the job, what are their skill sets uh to not only develop but also manage and maintain it. Thank god we are in the generation of um chat GPT and perplexity and uh notebook LLMs and all.
(1:13:23) So this part will become a little easier but still a certain minimum level of uh um you know profile employee profile is important. Let's take a a quick scenario to say uh and this is by the way a real world scenario. So this is to say that you know you have to understand the situation and act accordingly. This is a huge company, huge, huge, huge.
(1:13:45) Uh they own onethird of the internet uh uh data say um and this was a network threat detection. So of course they were using data bricks to be able to determine you know where there could be a a scenario or pattern which is not very good. And so they ingested from event hub at low frequency transformed it aggregated models were applied to predict thread detection patterns and suggest recommendations. Wonderful. Now the customer pain was after spending way too much on storage.
(1:14:16) Yesterday we shut down the 24/7 processing this morning. Storage was 70%, VMs were 30%. What is going on? Computer is what is the most uh expensive part of the equation? How come storage is 70% and VM is 30%. Anybody wants to take a quick guess as to what the problem could be? Okay, no problem. So there too many duplications. Duplications. Yes.
(1:14:47) Uh duplications is a good one. But no, in this case it was um it was very esoteric. It was the access tier that they were um trying to access trying to get to. So we have things like glacier and then suppose in the AWS ecosystem you have S3 and then you have Glacier and this was probably a Azure client and so they have replication from for disaster uh recovery um like they have something called as local redundant storage geor redundant storage so these storage things are also important so when data needs to be archived we don't need the data then you put it into cool storage and it is less expensive than
(1:15:25) your warm storage But if you have to access it again and again, accessing it from the cold storage or the cool storage is actually very very expensive. Uh and that is one of the things that they were doing. Active data should be retained in warm storage and just being able to access it from the cool storage was was um expensive and then the kind of replication that they were doing geo redundant storage replication is more expensive than local redundant storage.
(1:15:54) Of course DR is important to them but they have to choose judiciously. It cannot be that their whole use case is shut down because the cost is prohibitive and sometimes things like this happen. They were also encounting encountering high egress cost. Um low streaming volumes cause lots of very small files resulting in high compute path.
(1:16:18) And in later classes and examples we'll see how compaction inflight and on disk is important. So storage access cost was high and the storage processing API costs were very high but I have never seen anything as lopsided as this 70 30 it's usually the other way around VM will be like 70 80% maybe and storage if you're dealing with very high volume then it will be like 10 20 30 All right the next one is sometimes you may need to reprocess let me take a quick look at time.
(1:16:53) Yeah, let's stop here. Um, you may have to catch up data. Uh, so there's a big system outage. Um, and you have to catch up. So what do you do? You scale up a large cluster. You adjust the throttles. You clear the backlog and uh this is possible if the additional time required to clear the backlog can be tolerated because you're now putting things on hold. Also, the partitioning should uh force to increase compute.
(1:17:16) This may or may not be as uh uh true. Now backed up data is lower priority usually in streaming because you are immediately reacting to it. Whatever is backed up is necessary for later reconciliation purposes but for now handle the the newer requests that are coming in. So in general this is not a good idea as it affects a checkpoint. You spin up the second cluster right now.
(1:17:42) You handle the offsets. your older cluster will continue as before trying to catch up and eventually they will catch up and then one cluster can be shut down. So you are changing the offset. You are saying I don't want to off start offset here. I want I want to increase my offset to 2 days or 1 day or whatever. It's a little dangerous because I'm manually changing the offset.
(1:18:07) And I say that one cluster is processing from here. Another cluster is processing from here. And at some point when the once the class offsets start to merge, you pull down the older cluster and let the newer one continue. Sometimes business may come to you and say they want to change logic. Um, and if it is not time-sensitive, you can let structure streaming do the heavy lifting.
(1:18:26) Spin up a separate job. Um, allow the second job to do the recomputee and catch up. And once it's finished, then you stop the first job, swap the syncs, adjust the configuration as is needed so that now you're you're going to the new sync. So these are very very sophisticated ones, but I just wanted to flash it so that you kind of think about it.
(1:18:49) uh multi-tenency is is a given. So earlier when you're on prem you you know you will spin up a new cluster for each of the um customers like say you are an AT&T you have so many customers and for each of the big ones you'll have a little node when you come to the cloud that becomes very expensive uh and so what you would do is you do some form of um maxing and then demoxing maybe in the final stages when your your results um are computed per customer You might isolate it by a schema. You might isolate it by a catalog. You might you
(1:19:26) might have different um grains of isolation at that layer. But having a compute for each of the customers is not really required. Plus we have partitions and when you select you can select by partition, you can select by query, you can have grants and other things.
(1:19:44) So to be able to handle multi-tenency, dduplication and watermarking. So, uh, watermarking is a scenario where, um, you know, the bus waits for you for 10 extra minutes because it knows that you're typically there and so it's going to give you some grace period to catch up, right? So, that's late arriving data and you have the drop duplicates within that.
(1:20:08) So when you have all the data and you are running um you know certain uh aggregates on it is a very different story from like a moving window in which certain certain aggregates are being computed. So you become very time bound and timesensitive. So best practices is understand your SLAs uh get a ballpark of cost estimate. You won't want your cost to be so high that it stops your uh project.
(1:20:30) Always use checkpoint. Introduce the process trigger interval. Optimize your rights and plan for capacity. Like sometimes you bring in all the data together. That's the MX. And then you do the demox downstream where everything lands in bronze and then depending on the customer type, depending on the the use case type, perhaps it is a split.
(1:20:56) Um read chapter 5 this time and lab 2 is in the uh files folder. So, with that, I'll hand it off to Moan. Hello. Just good good timing cuz uh my router went off and I had to switch to hotspot, but now it's back on. That's Murphy's role. Yeah. Share my screen. Let me know if you can see my notebook. All right.
(1:21:40) So today um we're going to do lab 2 which is uh it's called a network analysis notebook. We're going to analyze some some network data that's you know um available in the public data sets that uh data bricks has made available. Um this is available in your um like files. So if you go in here and go to files and go to labs, you should see lab 2 network analysis.
(1:22:05) So you need to essentially download it and then once you download it, come back to your workspace. Uh you can do it from anywhere you want, but uh ideally you would go to wherever you're storing everything else. Uh and then do import and then upload. Essentially just drag that zip file here or browse and click it. Right? And then if you're doing all that, you'll end up with a lab 02 folder like this.
(1:22:29) Uh inside which is this network analysis notebook. Um does everybody is everybody uh been able to get access to those or any issues? Okay. So I think we're good. Um Okay. So once you open up the notebook, um, anybody want to give a thumbs up in case they're like like ready to go or there's no reactions in here, is there? Yeah. Wait a little. Let's make sure. Take care. Yeah.
(1:23:09) Yeah. I'll just repeat that. So if you go to your uh you know canvas and go to files you should see lab02 network analysis which you can download uh and then come back to your data bricks workspace click on workspace your home folder and wherever you're you're storing stuff is fine but uh wherever you're doing it just go in there uh you can either do it from the top right this kebab menu the top right here or just right click in the middle of the screen and say import and then bring that zip file in.
(1:23:46) Okay. Did folks find it? Yes, we I imported. Okay, great. Okay, awesome. Awesome. So, I'll just wait one more minute and then I'll get started. So, once you import it and open it, it should look like this. Uh this first cell should look very familiar to everybody. We are doing this essentially the same structure uh in all of the labs.
(1:24:09) The only difference between this content and essentially the first 00 and 01 lab is I changed the middle two lines to say lab02. But apart from that everything is the same same catalog. You're just going to create a new schema within your own catalog called lab02 uh and create a bunch of volumes which are essentially paths on the file system.
(1:24:30) Um again just uh we we may or may not use it but we just will follow the same structure and looks consistent right. Uh so assuming you have your notebook open you can connect to your your compute uh to the serverless compute and then run the first cell. Um now I think Anendita mentioned something about a data engineering pipeline being itmpetent which means essentially the result of you running it twice should be the same.
(1:24:59) Uh this is a similar concept just from the construct of metadata which is running this any number of times should not cause any harm. Uh I in fact had all of these assets but obviously the command succeeded because we have these if not exist statements here uh that make it input right. Um okay awesome. So that that's done.
(1:25:24) If that's done, uh, you can navigate back to the catalog explorer, like right click on it, or you can click this little sort of, you know, triangle circle square shape here, which is a shortcut of the catalog. And if you click that and browse to your catalog, which is CSI 103, you should see a new schema called Lab02, which is the one we just created, which is this schema, right? So, we are in lab 02.
(1:25:46) Um, and you should see uh nothing much there except the volumes that you created. I will have other assets there because I reran this lab. But like it should look like lab 02 for you. Uh, and you should see at least these two volumes. So these two volumes, right? You won't see any tables because you have not created tables yet.
(1:26:04) Okay. So I'll just keep that open just in case. Uh, then let's let's start. Right. So uh what's the problem that we're going to work on today? Um we have some you know we have lots of sample data sets mounted uh essentially publicly by data bricks uh in in all of the workspaces. So we're going to read some of that.
(1:26:26) Uh we have seen some of these file system list commands before. Uh so you can again run it again see what's available in the publicly mounted data sets. Uh there's lot of you know data sets here. uh you can see this definitive guide which is like row 17 here which is uh you know the data sets associated with a lot of uh you know exercises mentioned in in the spark definitive guide book itself uh and there are other things obviously that are specific to data bricks um that you won't typically find anywhere else where you may find it everywhere else but
(1:26:58) we've just collated it into one mount one right this co data set was super popular for a lot of demos back in 2021 uh now nobody uses But yeah. Okay. So let's keep going. So once you've run that, you can um we're going to browse one particular directory which is log files. So let's uh we're going to define that path rather.
(1:27:19) And then we're going to browse that particular uh directory. U again this is just to show that you can you know define a path and refer it as a variable. I'm not quite doing that in this exercise. uh but it's something typically that you would do in data engineering where you know you refer to it right and where I am referring to is not in this command but it's in the one below that uh which is down here but we'll get to that so let's take a look at what's in sample logs so you can see that you know there are all these like essentially part files of data uh that are sitting there um and we
(1:27:53) we will take a look at what that is shortly right but that's what we want to analyze uh these are basically just sample Apache log log entry files right. Uh so let's take a look. So if you do head you can actually see you know the top uh rows uh just like you know you would do head on the file system.
(1:28:18) Uh you can do that here and see what we have in one of these files. So this is part 00. So we're taking a look at this data and and spit out a bunch of rows but take take a look at the first few rows. Right? So there's a user ID, there's obviously an IP address, there's a user ID, there's some type of time stamp uh that we need to parse.
(1:28:37) Uh and there's a request to an endpoint, right? So you know, maybe you have an application running. Uh you want to track which customers are calling, what type of endpoints at what frequency and what kind of um uh success rate they see, right? So 200 would be for example uh like an HTTPS okay right so that's the response you get 500 is probably an unknown error things like that right so essentially you can see some sample data and what we're going to do is try to parse this data into a structure in which we can operate with it right cuz
(1:29:10) right now if you look at this like this this is essentially uh you know space separated text file right uh and it's not very useful to work So uh the first thing we'll do is we will uh create a function to parse out the different network log files. So we'll get to that function here.
(1:29:34) But let's take a look at um um let's take a look at the actual data that's here and we're going to write it into a data frame. So now we have uh the same files that we read up here in sort of this you know blobl like output in the notebook that you can't really operate with. So we need to write that into a spark data frame so we can start operating on it.
(1:29:52) Uh so we're going to read spark readad.ext and again Anita showed you some of the you know spark read uh and spark format um sort of functions in in her slide deck. This is one of them. We're going to use essentially spark read.ext to say this is a random text file, right? Um so notice we're not separating header delimiter nothing of this nature.
(1:30:17) we just want to read it in uh and we're going to operate on it later, right? So, we're just reading it in and then we're going to display the this particular u uh data frame. So, display is an action, right? Uh sorry, a transformation. Sorry, an action. This is a transformation and this is an action. So, this really doesn't do anything.
(1:30:36) If I had not run display here, uh you wouldn't see this performance tab, which is actually a Spark job that ran in order to do the display, right? Um okay so now we have this raw log files data frame uh and we'll see what we can do with it. We want to essentially parse this into something readable. Now there's multiple ways in which you can do this uh but the way we are going to you know do it is basically use a regular expression um and and see if we can achieve that.
(1:31:06) So we're going to call uh this function called regular expression extract which is one of the many you know functions available in pispark.sql. SQL um which is like a plethora of functions, hundreds of thousands of functions um and we're going to use that for our purpose. Now this will require a bunch of trial and error, right? So how you would typically do this is you know maybe you take this thing and you you know consult the documentation to figure out oh how do I split something when there are like square braces around it uh and then I have quotes then I want to separate it. So you you are coming up with a pattern
(1:31:36) for regular expression which is you know not certainly intuitive to most people. Uh today what you would do is most likely plug this into you know chatdia or something and say hey give me the pispark code to do this regular expression and it would give it to you right and that's totally fine uh but you still need to sort of uh understand what it means.
(1:32:00) So you can once it gives you that code you'll say please explain each step to me and then it'll explain it to you right this is how most people are sort of uh teaching themselves these days um instead of you know consulting documentation right it's achieving the same result so let's run this uh cell level and what it's going to do is extract based on this logic that we that we gave we're going to split it into uh different sets of fields right so now looks like the regular expression was good because we get this sort of structured data. Uh whether you want to keep this slash in the endpoint or not is up to you. We decided to keep it. You
(1:32:34) could remove it as part of the regular expression. But at least we know okay we hit some endpoints. Uh these are our you know rest API endpoints. Uh there's a bunch of methods calling them. You know get put I see post here. Uh primarily those three. Uh there is some user information. There is also a lack of user information.
(1:32:56) So now like if I was you know if this was my app I would need to try and figure out where in my code I'm not capturing user information like what kind of requests. Uh we're not going to solve that in this in this class but again you you would analyze this data and then you would improve your app using what the findings you get from this data.
(1:33:14) Right? So it's important to capture logs. It's important to know how to parse the logs uh and then use those to improve your own application if that's what you're doing. Right? Um now this is a data frame. Uh so we have a new data frame called parsed data frame.
(1:33:33) So the first one I called uh was raw log files and now we have a parsed data frame. So we have two data frames that we can operate on. Uh let's see the count of rows that are here. Okay. So looks like there's 100,000 rows. Um and the text here says hey we have 100,000 rows. we still got to like you know uh how how can we like do calculations on this in a distributed manner right so there will be a reference here to uh spark session and all this stuff because you're in data bricks you don't really have to worry about this I added a note that said hey you don't have to set up any explicit sessions in in spark uh in data bricks cuz when you spin up a
(1:34:10) notebook and when you attach to compute uh we are automatically spinning up a default session for you and that's the session and only you are attached to that session. Nobody else is attached to that session. If you spin up a new notebook, that's a different session. So we are doing this sort of isolation for you.
(1:34:29) Uh if you were you know spinning up an EC2 instance and running your own version of opensource spark on it these are things that you would have to worry about except you don't have to worry about dat. So just purelyformational uh but over here you don't have to worry about okay so what am I doing next? I am going to create these tempus uh because I like operating on on SQL.
(1:34:48) Your preference may be to operate in Python in which case you don't really need to do this. Uh you can do purely dataf frame operations for the rest of it. Uh and we will see some data frame operations but I also want to show you SQL operations and it's a way in which you essentially jump from uh a pispark data frame to a SQL view.
(1:35:06) Uh and they're both giving you the exact same objects uh data but you can operate on them differently. Right? So let's run 15 and 16. Uh so now we have two uh temp views. Obviously you know nothing is really created right a temp view is just a pointer to that particular data frame at this point.
(1:35:26) Um now notice we use create or replace temp view. There are you can create and persist views in data bricks like with the create view statement. Uh and we'll show you that later. We just don't need that for that for this today's exercise. So this is basically attached to your local Spark session, right? It only exists within the construct of this notebook and uh nobody else can read from this particular template. It's unique to you, right? Even if you were in a shared environment.
(1:35:55) Um um also means that if you restart this serverless cluster, if I went in here and said, you know, terminate or reset and then come back, this temp is gone because it only exists within the session, right? Okay. And now let's count from the uh raw log files view. Again we have 100,000.
(1:36:14) So that was the view that we created on top of the data frame, the raw data frame. Uh and then we have the log data which is the uh view that we are creating on top of the parsed data frame. So we have these two. Again we have not dropped any records. So that's good. Everything in our raw is in our parsed data frame. And we have 100,000 records. All the data looks good based on our initial uh analysis.
(1:36:34) Uh we can query this just to be sure that it looks the same. Uh and this looks exactly like the parsed data frame. So all good so far, right? Um this note, this cell will not work in in um in serverless yet. So do not try to run it.
(1:37:01) But I just put it there to show that you can switch to Scala if you wanted to and refer to that same view that we created. This particular uh SQL uh you know essentially view that we created can be referred using Spark SQL in a scholar set right. So you can imagine how you can jump back and forth between these languages uh within you know within one notebook. Uh there are some limitations in serverless like I said before. So you know Scala's coming but it's not there yet. Okay.
(1:37:26) So let's take a look at what the print schema of the raw file was. It says oh it was nothing. It was just a string right because we saw it was just like one long line with nothing in it right. Um and we already displayed the data frame again before but you know you can do it again uh just to you know remember what it looks like right? We had a string and we sort of broke it up into fields. Right.
(1:37:51) Okay. So now we get to a cell. Uh everything worked great. Cell 28 is expected to fail. So if you got to this cell before me and you try to run it and it failed, don't be surprised because this is not uh uh this is not a a string value of a time stamp that can you can just normally cast time stamp without telling exactly what format uh this particular string is in. Right? And there are again multiple ways to do this. There's lots of SQL functions that you can use. uh in our example.
(1:38:22) So this is expected to fail like I'm saying. Um so it says hey try cast to tolerate the error. In which case if I had put a trycast I would have skipped this. It wouldn't have actually errored out but you wouldn't have got the right answer. Um but what we get back is basically a null right.
(1:38:40) Um so it says um that you know it's returning a null and what do we do like we need we need to solve this issue. So one way to do this uh and this is most probably the most common way if you if you're doing this a lot is to create a userdefined function and we'll look at two ways to create userdefined functions.
(1:39:00) So the first userdefined function that we're going to look at is going to use essentially pi spark code uh where we define a function and we register it as udf um and you know don't worry about you know too much of the specifics but you can see that again we are giving some like patterns here and saying you know based on this set of patterns that I'm going to give to you you know return the time stamp uh return it convert it to time stamp right um so that's the function that it's doing and if you run and then print it out. It is going to spit out the Unix time stamp, right? Which is like the number of uh epochs
(1:39:34) elapsed since 11970. So that's what it's given. But that's a it's a good format because now you can convert it to whatever you want. Uh you can convert it to you know UTC, EST, whatever you want using various time stamp fields, right? Um the the only reason we are showing you this is basically to say okay you have some particular format of some value in your data and you want to operate on it frequently how would you do it you wouldn't write the same function again and again and again like you don't want to run write this a 100 times so instead you want to put it in a UDF so you can call the UDF from
(1:40:09) wherever you want right so this is one way to create a userdefined function we'll look at another one shortly now Um you can um uh yeah you can uh register this UDF using this um uh UDF call and then use it uh in your in your select state. Right? So if you if you notice here we are defining uh this uh par state and then we're calling this essentially lambda uh syntax within spark to register it as udf.
(1:40:50) It's just the way uh the UDF syntax expects to be uh and you have to give a return type which is float. uh and so that's a required step and then you're using this particular UDF inside your select state right so you have a data frame which is your parse data frame dot select and I want to see only one field so I'm selecting only that one field but I want to wrap it in this UDF uh so that I can look at it here right and so now when you run it you end up with this time stamp value which is basically this but without the dot sign right so now now you can operate on is however you however you want right uh the other way to register udf is with the SQL context right so you
(1:41:28) can actually say spark udfregister and now notice we don't have to do this particular lambda call because that's sort of a pispark version to do it uh all you do is say hey spark udf register pass state which is uh this function that we defined up here and we're just registering it as a SQL udf again with the same thing uh what you have to do is compare cell 35 which is how you would do it in SQL uh in Spark SQL and how you would do it in uh pispark right which is uh this set this cell 32 and cell 35 is
(1:42:07) what you need to compare they are basically doing the same things it's just different syntax right so maybe you uh you like one way of doing it versus the other but they achieve exactly the same thing um and you can now use this function parse state in uh any sort of uh SQL construct even if you're writing it in Python, right? Uh so that's what we're going to show you here. So uh we don't need to do that. You don't actually need to run 36. Let's run 37.
(1:42:37) So what's it doing? We're going to define a new data frame called clean logs data frame. And we're going to uh obviously we still writing in Python but we're going to wrap a Spark SQL statement inside uh that Python uh dataf frame syntax and we are going to use this parse state which we defined up here.
(1:43:00) So up here we defined uh you know UDF as parate and we're going to call that par state over here in cell 37 uh and then call the output as date format. Right? So this is uh you know you could write this as a just a vanilla SQL cell uh or you could write it as SQL cell wrapped in Spark SQL inside Python.
(1:43:24) Um this is all just like you know essentially data frame APIs talking to each other. uh and then let's display and see what it comes like right so I'm doing some conversions I converted uh the content size to int um and then uh or I'm conver uh converting this past state timestamp uh to pull out uh just uh this particular one because we're calling the past state here and then we're calling out uh a more formatted version here which is here like uh we want the output in a specific format uh and and basically some renaming uh which is just for like
(1:44:01) you know compatibility with some of the other code that I did. Uh but you can do all of this aliasing. If you're familiar with SQL, you know, select a as x. Uh we're just doing that within the SQL construct, right? Um but now we have at least some clearly formatted dates that we can deal with.
(1:44:20) Um and you know these things are still the right response code and user ID. I just did some renaming. Now there's another way you can do this. So we're going to call another data frame, create another data frame called DFV2. again just to show you a different way to do it and we're going to run it and I'll explain to you what the difference is.
(1:44:42) Um so this is select expression and this is select right so this is a spark SQL and you're writing this sort of select expression within it uh but you can operate directly on the pispark data frame with this select expression without saying uh this particular string right with select you see the select word the keyword select within sparkql when you write anything you have to say select from something select something from something.
(1:45:14) But when you operate with the spark select expression, it can be any arbitrary sort of SQL expression that goes in there. Uh in in this case, all we're doing is calling those same functions, doing all of these renames and everything. Uh and you'll get exactly the same result, right? It's just uh a different way to do it, right? So uh the the the the functional difference here is here we call them as we are referring to them as these column objects uh but here we are referring to them directly uh sort of through the SQL syntax right so that's the difference but in the in the end result you get the same right just showing you different
(1:45:48) ways in which you may want to operate on your data depending on your preference that's all um okay one other useful thing I want to touch on here is like you can always look at what Spark is doing to evaluate whatever you write. So in this case, we created this data frame clean logs df. We gave some u some uh code in there on what this data frame should do.
(1:46:20) And if I hadn't put this display command here, it would have just stayed um as a transformation in memory, right? I haven't done any action yet if I hadn't run the display. But how do I know whether what operation spark is going to do right? So for that all you need to do is define the data frame or anything uh you know any code that you want to define as a data frame and then say explain it to me right so you're saying hey I define a data frame explain how it's produced right so it's telling me okay I'm going to do some scanning so you have to read from bottom up uh and then it's going to because it wants to use photon it's going to convert essentially a row of data to column
(1:46:54) format it's doing some projections uh these are a bunch of photon things And once it gets the result, it's going to convert back to row so that you a user can sort of read it in in the way that you are meant to be. Uh and so that's the physical plan, right? So this is what spark is doing under the hood anytime you run.
(1:47:16) Uh and there's way more details here which is like you know uh beyond what we need to know. Uh but again quick tip if you take this whole thing and you plug it into like chat GPD or something and say explain this to me, it'll give you a very nice explanation, right? Cool. So what did we do so far? We read some network logs. We created a bunch of data frames.
(1:47:36) clean uh you know clean v2 uh we registered some views but we haven't actually created any tables or anything like that right everything is just like data frame oriented so far but we have one data point that we don't know what it is and that is this particular um uh response code and like what what do these response code mean right because most likely this is like transactional data if you want to think about it but I need to load it to some dimension So I need some like reference table.
(1:48:06) So we're going to go get that reference table now again from public GitHub. Uh and we're going to bring it into our lab zero to input. Uh so you should see this come in and you can check what's in there. We're going to set up our user directory which we've done before. Uh we're going to copy from input to output. Again you don't really have to do this.
(1:48:29) Again I'm just want to keep uh reinforcing the fact that you can do file operations. Uh and then now we're going to read it. So, we're going to read whatever we put into status codes which we brought from GitHub just now. Uh, and we and we know it's JSON or rather I know it's JSON. If you didn't know when you did a list, you'd know it's JSON. And we're going to say it's a multi-line JSON.
(1:48:49) So, these are Spark options basically. And we're going to read it, right? So, let's see what it says and what it comes back. It says, oh, there's a list of codes, right? 1 XX 2Xs, whatever. uh 200 you know we I think all most people know that 200 means it succeeded and it says okay right so again you you got a data set maybe you need to join it to bunch of dimensions to make sure uh to to know what each data element in your data set means so that's what we're trying to uh show you here right so we got our primary data but now we also need some
(1:49:22) reference data um so now let's try to bring them together um we want to join these two right so now this is basically U how you would join two data frames in in Pispark. uh you would define a new data frame uh and you would say that hey I want to take the clean logs df uh which is the first sort of uh data frame that we created uh and we want to join it to the status codes df which is this one right so we have the status codes df uh and we're saying that we want to join on uh essentially the response code but
(1:49:58) as we know a response code could be a string 7 xxx not an integer right so we know that in status code it is a string But remember up ahead somewhere up top I had cast response code to int because in our data we only had 100 200 300 500 whatever we didn't have text fields but this join will not work because it is an int to a string so in order to make the join work I have to cast our the response code that's in my data in the raw logs to a string so I could join it to the status codes code which as you
(1:50:32) can see has strings right um so So let's run this. And so now we'll get a network df uh which is our next data frame. Um cuz now you know spark has done a join uh and you can see if you scroll over here uh this is our original data 500 and we essentially joined it to the code from the status data frame and pulled in the description and phrase inspect and spec title all of the stuff that the thing other one gave us. Now we may not need all of this.
(1:51:03) So we can obviously like you know choose the fields that we need in the data frame. We can uh select uh and say you know these are the only columns I need right but just to show you that uh now we have two data sets that we joined um and we're going to filter uh just to run another filter command to see you know I want to see what's happening.
(1:51:23) So these are the commands where the request succeeded but the content size was greater than 100. Why do I care about content size? Maybe I have some uh you know thresholds within my application where you know I I know that if I get a size greater than something uh it might work but it might be slow right I don't know I I know my how my application is designed so I want to know and analyze my data to see oh how many people are sending me more than 100 right uh and this this threshold could be anything but it's just to show
(1:51:55) you an example uh and what did we get we thought that the request has succeeded because it's a 200 but now we have a much smaller set of uh data sets uh like little more than 8,000 rows uh versus the 100,000 rows that we have right so only one code and only one set of sizes okay great so I I just want to uh deal with this going forward maybe this is the data set that I want to analyze further down for something right so what I'm going to do is I'm happy with all my analysis I'm going to write write it to my delta table. How do you write to a delta table in pispark? You would say,
(1:52:33) you know, I'm taking the data frame, my input data frame. I'm going to do write. I'm going do format delta and I'm going to say mode override because I don't care about making this incremental or anything. I just want to override each time and I want to save it as a specific table.
(1:52:52) So this basically makes sure that this table network filter is registered in our schema that we're working with, which is lab zero. So if I run this, this should actually create a delta table. Uh, and you can sort of see what's happening here. Um, you can click through and see, okay, I read so many rows. Here I read 100,000 rows. Uh, but I only wrote 57,000 rows.
(1:53:17) Why? Because we had a filter, right? In a filter DF, we had response size 200, I mean 200 and consent content size 100. So we only had to write those rows. You can see some statistics here. um you know we had 33 files in our raw data uh and we wrote out one file right we just wrote out one file because this is only 50,000 rows and that easily fits in a compressed format like park par uh in a you know maybe it took 10 MB right even less than that so it's a very small file um and you can you can sort of uh see the operations if you click click into this query profile it actually breaks it up a little bit more. Um, so
(1:53:53) you can see that it scanned some data. It did the row to column now to convert to because Spark essentially works as a column format internally. It did some filtering and then it converted back to the to the row format uh to do uh you know because there was a function there that required it to convert back to row and then back to columner and then wrote out a resistance.
(1:54:18) So you know these sort of des serialization and serialization between formats is expensive. So if you ever find yourself debugging something and you call in a specialist uh this is the kind they will be looking for and say oh what if we could you know avoid this step or what if we could avoid this step things like that. Uh cool this go back.
(1:54:42) So we're here and now we're going to select from that table and you should see some some data from that table because we obviously wrote it out. Um and we can verify the count from that table. So we can say select sorry select counterar and it should be 50,000 something uh 57751 which is what our query profile told us was written and you can find this in your lab 02 on the left here if you go to tables and click on network filter uh you can click it uh you can click on this little link here and it'll
(1:55:22) take you to catalog explorer and show show you more details about it. Uh again you can I'm not connected to compute here but you can see details about it uh that it is in fact delta um and it you know there's some statistics here the it wrote 57,000 rows things like that right um so I think we're almost at time but essentially we wanted to show you how you would operate with you know some amorphous set of data that somebody sent you uh you would add some structure to it uh try to understand it and then try
(1:55:53) to get related data sets that may be required for you to make sense of some data element in that amorphous you know data you got right uh maybe I don't know what all the response code means so I need a set of uh information uh that I can enrich it with and then I will write it to my uh delta table and then make it available to my data scientists or whatever so they can do more cool things with it awesome that is a great lab yes reminder Your first assignment is due soon. I think it's the 20th. So, um if
(1:56:30) you have not um submitted, uh make sure that you do so. Your second assignment has opened up and uh this Thursday um uh Paul will walk you through um the additional asks of that. If you have any questions, you can you can take a look and come prepared. Uh with that, we'll call it a night. Thank you everyone. Have a good night. Thank you. Thank you much.