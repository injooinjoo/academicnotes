103 day1 - YouTube
https://www.youtube.com/watch?v=apzloN8GGXs

Transcript:
(00:02) All right. So, um welcome everyone. This is uh CSCIE 103 data engineering for analytics to solve business challenges. And uh this will be our first lecture and tonight we'll introduce uh data engineering. So, uh first I'd like to introduce um the teaching staff. So myself and Anadita Mahapatra will be uh uh the course instructors will be co-eing the class. Uh Anadita is a graduate of the HS program.
(00:46) She has a BS in MS and computer science and she's currently a solutions architect at data bricks and has um 25 plus years experience. And my name is Eric Gizaki. I'm also a um graduate of the ALM IT program and um I'm an entrepreneur working on a couple startups and uh also um 30 plus years experience in the software industry.
(01:19) And then we're fortunate to have uh three outstanding teaching assistants starting with Ram Morali. he's a lead solutions architect at data bicks and Paul Signarelli he's a senior solutions architect at data bicks and moan Matthews is a lead delivery solutions architect at datab bricks and um their expertise in data engineering and data and as well as the data bricks platform will be especially useful um for all of us um during the semester Tonight uh we'll um look into the course logistics and um motivation.
(02:02) Also talk about the big data ecosystem, discuss data personas, distributed computing and also talk about machine learning and and the ecosystem around it. uh look at some industry trends and also um think about uh uh business justifications for um resources going into um tech investments and then we'll hand out assignment one and then the second half of the lecture will be a lab where we'll introduce um big data processing using the uh datab bricks um platform. form.
(02:50) So for logistics um there'll be um two primary channels for communication. The first will be Canvas where we'll place course materials like assignments and uh also um lecture slides can be found there as well as the syllabus and and also the schedule for the course uh can be found there.
(03:16) And then um this the second um is Slack and uh you should have all received an invite to to join our Slack uh workspace for the course and that'll be used for um more interactive communication uh uh questions about um you can ask questions about the assignments or the the uh course um materials there as well as and use it to interact with the teaching staff as well as other students for for example for group projects. There's two textbooks for the course.
(03:49) Uh the first is simplifying data engineering and analytics with delta and that that book was actually written by ana and it does a very nice job describing data engineering and and also how um how you can do data engineering on the data bricks platform with delta. And then um the second book is Spark, the definitive guide.
(04:14) And both of these books are um available through the Harvard Library for free um in electronic form, but you can also um you can also download them or purchase them online through Amazon or other sources. The um for labs and and also assignments, we'll be using the free edition of the data bricks platform. And tonight tonight's lab will introduce you to that and um and you'll get very familiar with um how to use data bricks to do um data engineering and um data bricks is is probably the the um leading uh data um platform available today. So um it should be um useful very useful
(05:02) for you and for lectures um and as well as lectures and sections we really want you to attend both either live um by joining the live Zoom session or by watching the recordings which the recordings will be available uh through canvas um through the gather website linked from the canvas site.
(05:34) So if you're not able to watch the um the live lecture or lab um definitely uh watch the recording later and um we understand that some of you are in different time zones and it makes it difficult to attend the um or live um live viewing but it's important to watch at least the recording and then also even if you see the um live lecture or section.
(06:01) The recordings are there for you um to review if you want to. The lectures will be um broken into two parts. The first half will be um the lecture where we cover um theory and then the um second half is for the lab where we we'll put the theory into practice and show you how you can um do various things uh related to data engineering through the data bricks platform sections are will be um on Thursdays from 6:00 to 7:00 p.m. Eastern time.
(06:38) And um the sections will be focused on helping with um helping with um assignments and and reviewing course material and maybe also reviewing um going over the the solutions for assignments as well. We offer office hours um for all the students to um to basically one-on-one sessions with the teaching staff to ask questions and and um get help or um if so if you're struggling with an assignment and and don't know how to um um figure out a a solution, uh remember that the office hours are available for you and they're fairly easy to set up with these Cali links that are available
(07:25) here and the these um slides will be um made available to you through Canvas so you can um get the links later. Our expectations for the students include um being engaged and inquisitive and um ask ask remember to ask questions if um something's not clear and um be open to learning.
(07:52) we we'll be covering a lot of um maybe new ideas for you. So be open and um and and willing to learn and remember to be courteous and professional uh to the not only to the teaching staff but the other other students in the course. You'll be working together in teams on group projects. So, it's important to um remember to be um uh courteous and and um respectful to your teammates and um lean in for group projects. That way, you'll you'll learn the most.
(08:27) it it's easy to kind of step back and let others do the work, but um to get the most out of the course if you lean in and help um that'll not only help your team do well in the group project, but also help you learn, which is important. And then um remember to reach out to your team members early and um introduce yourself so that you you um get to know each other as soon as possible.
(08:58) And these um group projects are not only um interesting and and fun to do, but they're also a great way to meet other students and expand your network at Harvard. So take advantage of it. This is our uh course schedule for the semester. So each week we'll be reviewing a different um uh topic around data engineering and then we'll be having assignments um scheduled that go correspond with the the material that we've covered in the course or in the in the lectures.
(09:35) And there there's a total of four assignments and um two case studies uh two quizzes a final presentation and and those will all be graded of course and then we also consider participation that's part of the score. A small amount of the score will be based on your participation. So that includes uh you know asking questions during lectures and and also um being active in Slack and and also participation in in the group projects.
(10:09) Uh all assignments are due on by midnight on the on the due date. So and it's always a good idea to submit um at least 5 minutes before the due date in case you have problems uploading your assignment. so that it doesn't um so so you're sure to get it in on time. And then um we do have a late policy of minus two points per day late with a maximum of a 10point redu deduction per assignment for lateness.
(10:45) So, even if you're late, um even if you don't get it in uh turned in on time, um make sure to turn it in because you can still get points for the assignment and not lose um all the points. So, um we we understand that everybody's busy and has um many of you have families and and and work. So, um so there's some leniency in in terms of turning things in late.
(11:11) Best practice though, of course, is to turn things in on time and um that way you can get started on the the next assignment as soon as possible. And also a really good practice um regarding the assignments for this class or really any any class is get started as early as possible and so that you can you can get the most out of the um get the most learning out of the assignment. I'll just stop there and see if there's any questions.
(11:48) So, yeah. Yeah. Yeah. Can I ask a question? Sure. Yes. So, um I just want to understand how the final presentation is going to be like. Is it a group final presentation or individual? Everybody's going to present an individual topic. Uh well um yeah Gab I I don't want to get too into too many details about the final project yet but um it will be a group project and and it will be a group presentation as well.
(12:25) So the group will um work together to present their um their results. So um yeah and this is um the I I like the group project that we do because it's um very very attuned to real real life um work environment where working as a data engineer you're you're work you're generally working in a team environment with others and and having to coordinate and um in the in the final final project um everybody will take on a different role or persona which will talk about in a few minutes tonight and um and then um present their aspect of the project. So does that does that address your
(13:08) question? Yes, it does. Thank you. Sure. Okay. So um yeah, so we're we're in an interesting um period of time right now with um the advent of AI. Things are starting to move very quickly uh in terms of innovation.
(13:39) Uh there's been a series of innovation cycles through history and and one thing that you might notice is that the the frequency of the innovation cycles is increasing and it's it's certainly true. um right now with AI. And so I'm I'd like to um share a video with you that kind of talks about the um the the where we're at right now and and what what what is important. So hopefully you're able to hear this. Um maybe somebody can say if they can.
(14:13) Can you hear? Yes. Yes. Okay. [Music] So, it's really interesting. The largest growth jobs are very much the backbone of the economy sort of jobs, the ones that we see in everyday life. They're much more so than the really technical uh jobs that we see. [Music] [Music] to farm workers and laborers is our top growing job. And there's a lot of things at play here.
(15:16) The biggest one is a green transition. And what we've seen is that as we move into a greener future, the demand for farm workers certainly changes. And a demand for sustainable practices is is clear from our from our results. [Music] It's not a technical piece about what theoretically might happen.
(15:55) This is about what are executives doing right now, what are the decisions they're making and what do they expect is going to happen over the next 5 years. [Music] [Music] So trends such as advancements in AI, robotics and other technologies are driving the increased demand of AI and big data as well as technological literacy skills.
(16:36) What does it mean? It means that in the upcoming years uh understanding how to leverage technology will become as critical as reading and writing. [Music] [Applause] With technology automating many many routine tasks, creativity has become our secret weapon uh driving innovation but also allowing us to solve complex problems as well as to adapt to change.
(17:09) [Music] Okay. So you can you you can see that um the you know techn it's interesting that there's kind of both ends of the spectrum like laborers as well as um technologists have an important role in the um upcoming years in terms of businesses and um as but they underscored the importance of AI and and um innovation and um and and data skills relating to data.
(17:55) So um so what we're learning in this course is um and and and probably in your degree program as well is you know very relevant um to where we're at today. So some motivations um for for for data engineering or right now we we're big data cloud computing and AI are all coming together uh to create like almost like a perfect storm for innovation and data data is at the heart of it.
(18:31) Um data drives um machine learning uh which and the machine learning is um is driving u business innovation and productivity and um and value and um and most businesses even small and large see themselves as as as digital businesses uh dealing with data. Uh the challenge challenge with data is harnessing it so that it can actually be used for something useful.
(19:03) And um and and also um another important point is that technologies will change but the uh underlying um problems and and um and principles remain the same. So uh we have to remember um you know a lot of the theory that we will learn will continue to be useful many years to come maybe but maybe the underlying technologies that we use to apply it will change.
(19:32) Uh businesses are very interested in in in accessing insights from the data quickly. So speed is important. Um data is an important asset. So we need to treat it um as an asset just like um we think of our software that we write. And um and also another um interesting fact is that that structured data is is a very small amount of the overall data that we uh work with.
(20:05) There's also semistructured data and unstructured data like audio and video that um that provides us even more data and um so and and all all of we we need to be able to use all of the different types of data um to be able to um derive business insights. So um and this this applies to all industries basically um uh cyber security, health, um vehicles, autonomous vehicles, uh autonomous factories, um personalizations and including social media, uh gaming and entertainment, farming, banking, uh forecasting, weather forecasting, all all these different industries and many others are
(21:04) using um data to help make them make those businesses more um efficient, more impactful, um more valuable and um so so the the ability to harness data and use it to gather insights can be really applied anywhere in in the industries. Uh so you may ask like well well what is data engineering? Uh in one sentence we can think of it as turning raw data into valuable insights.
(21:45) So taking taking data from its source and and manipulating it and transforming it and conf um maybe filtering it and and doing other activities until maybe we the end result is a machine learning model that can answer questions like whether or not um this X-ray shows somebody shows whether or not somebody has cancer or not. Uh so data engineering is um basically it it combines not only data engineering but it also overlaps with software engineering and also um the role of the data scientist. So while data engineers focus on uh data structures and uh distributed
(22:36) computing um programming and and uh all the different tools that we use as well as how to build ETL and data pipelines. Uh the data engineer also needs to know about the things that a software engineer would know about including uh design um development um web and mobile apps, DevOps and and and service deployments.
(23:04) And then um also for the data scientist um the data engineer needs to know about modeling, machine learning algorithms and business intelligence. So there's quite a lot to cover as a as a data engineer. Now um as we're as I um mentioned when we were talking about the final project, there's different data personas that work with with data.
(23:35) So data engineers are are focused on creating data pipelines and maintaining those pipelines to transform and and prepare data for other uses. Uh BI analysts are are interested in and taking the the refined data and and presenting it in um and analytic charts and different ways to um to help uh get value out of the data.
(24:07) uh data scientists and ML um practitioners work together to um create um ML models, features, build features and also um define models to um use the data and DevOps and MLOps help with the automation of the pipelines and then um data leaders uh like um a chief data officer, a chief information officer um they basically help drive um the direction that the um IT organization takes in terms of what what data to use and what what answers they want out of the data for to help the business.
(24:48) So all of these different roles work together to um to basically achieve um those insights from from the from the data the raw data that comes in and all of them um also overlap with the role of the data engineer. So I like this diagram a lot because it shows that um there's a lot of attention about the latest ML model coming out of OpenAI or or um or or some other um ML company or AI company.
(25:39) But um the there's a lot of work that goes into creating the the model um beyond uh just defining the model. So that includes configuration, data collection, extracting features, um verification of the data, uh process management tools, analysis tools, m um resource management, um serving infrastructure and then finally monitoring.
(26:09) So and all of these um task or jobs are um fall within the domain of the data engineer. So and then the um once the once the data is collected and refined and and ready, it can then be used as input into u ML um training an ML model for example. So um so there's a lot a lot to cover here and and this will really be the focus of this class is to um help help you understand these various steps and and and uh wrangling the data so that it's ready for um generating machine learning models or doing analysis with the data.
(26:56) So most most um data sets today are characterized as big data and um the characteristics of um data um can be described with um the what we refer to as the the five V's uh volume, velocity, variety, veracity and and value. So the value or I'm sorry the volume of the data refers to the size of the data and and often this is um you know uh in terabytes and and um we can measure the volume of the data by the number of records or the size or maybe the transactions per second.
(27:41) Um velocity can be either batch or real time or or sometimes streaming and um the variety of the data can be either structured data what we familiar with in like relational databases or semistructured like JSON or or XML where the um schema is is part of the data or unstructured data where there is no schema. It's and this um things like audio, video, images are all um considered types of unstructured data and which are very important um sources of of information today for for models. And then the veracity is really about the correctness
(28:27) of the data. Uh can we can we believe or trust the data that we're using? And then the value is what's the ultimate um value to the business of of this data? What's the potential value of the data? Is it is it worth um the effort of of uh refining it um for the business? So we can classify data by the the size of the data.
(29:03) It's either big data uh which could be terabytes of data that requires like distributed computing to be able to handle it or um can it fit on a like a uh someone's laptop and then then it would be um considered not not big data. Uh the the structure of the data um is it structured, semistructured, unstructured? Uh again the structured data the schema is well known semistructured the schema is part of the data and then unstructured there is no schema and this this includes things like image audio video um documents any anything that and
(29:50) a lot of th this unstructured data is the source for a lot of the um machine learning models that's being that are being um developed Um today the AI AI models the velocity of data is it is it batch is it re refi arriving in in chunks discrete chunks or is it is it being streamed like a continuous flow or maybe micro batches and we'll talk more about that later.
(30:24) And then also another important um characteristic of data is is how often does the data change? Is it um is it does it hardly ever change? Like you could think about uh census data that's updated once every 10 years when they do the census. Uh is it occasionally updating like um maybe the status of a machine on a factory floor? Is it is it operating correctly or not? And then um behavioral data is is the next level where it changes often like and this could be um u data derived from social um social interactions like on a social platform like LinkedIn or or Facebook
(31:07) and then um or is it is it um changing frequently and that would include um things like weather data where it's it's constantly changing and um and and producing large large volumes of of data as as it changes. Okay. So um at well I'll do one more slide and then hand it over to anita to to continue but for um we can also think about the data in terms of uh data storage and um whether the the and and the um how fast the data is being um processed and also um how how um new is the data and um and the data temperature can be hot where it's changing quickly or cold where maybe it's um data from a year ago and
(32:13) and and and hasn't really changed since then. And then the processing speed, usually if it's hot, we need to process it um more quickly, maybe real time. And then um sometimes um interactive or or batch, we might like um take it a data set from a year ago and process it as a batch process.
(32:38) But if it's changing real time like um like weather data for example, we maybe um need to process it more quickly to um again um gain the business insights and answers that the business is looking for. So So Anandita would you like to um maybe you can introduce yourself and and and continue with the lecture. Yes. Hi everyone. Nice to be here with all of you. Welcome to the course.
(33:10) We're going to have a lot of fun. But that means that you have to remain engaged and put in your due share. Uh with that, let me share my screen. Can you see? Yes. All right. We can get started from here. Um what this slide is trying to tell us is that when you think of data you should also be thinking about uh the temperature of the data.
(33:42) Uh so there is the concept of uh data that has just come from a source system. So that's hot data or cold data because it has been sitting around for a while. as um the temperature starts to drop the value of the data starts to reduce as well. So when you have real time and streaming needs, you want to be able to process the data as soon as it arrives.
(34:10) So compute is on a different dimension and there also the hot data is processed very quickly and the cold data takes longer. The three cycle circles that you are ellipses that you see here talks about the three modes uh that we would be eventually doing uh our pipelines with real time which is streaming or near real time. There is interactive where the data is sitting but you need to be able to query it not on a dashboard or a structured widget but something that you are exploring and you all come from the data science um uh program so this must be uh pretty um you
(34:48) know common to you guys. And then there is batch which means jobs get triggered at a certain interval maybe at midnight um all the data from for all your reports um and your model have made their way in and so at 1:00 or 2:00 or so on a big batch job is kicked off which processes all the data for that day. So that's an example of a batch.
(35:13) Real time is as the data is coming in maybe in a 5 minute interval or a 15-minute interval or or maybe instantaneously data is being crunched and interactive means the data has mixed uh temperature and you're just quering it ad hoc because you want to get answers to some insights.
(35:35) So in all three cases you're getting your insights and answers but the real time is where um compute is used the most and typically these insights are more valuable for the business not to undermine the batch processes but the whole industry is trending towards streaming. Businesses want insights and answers as quickly as possible but at the same time you should ask them do you really need this? What are you going to do it? Is it really actionable? you got the insight but if it is not actionable do you really want to pay the price for it or not? So those are considerations and questions that you as the keeper of
(36:06) the pipelines and um um you know the compute cost costs also come from running the pipelines should be aware of. Uh this is increasing business value. So you have your operational analytics that's the bare minimum. You want to know whether your pipelines are running or not. uh how much data have they brought in? Then you have got these diagnostic and um exploratory analysis.
(36:30) That's the middle uh um ellipse that we were seeing in the previous page. You've got planning analytics. So you kind of say that this is the project we are going to do and this uh data and this data should come together to give us this insight. Predictive analytics.
(36:48) Um of course you know that's the ultimate goal of every company. But even beyond that with the advent of um jai um platforms like data bricks are embedding it as part of the core platform. So which means you start to get these operational efficiencies automatically to a point in which it should suggest and recommend to you what needs to be done to fix a problem as opposed to the human doing it.
(37:16) So that's the ultimate goal of um everything running on autopilot. We are some ways away from it but perhaps not too far. This is a LinkedIn uh post which is a visual and tells the difference between uh raw data and if you sort it, you arrange it and you um you know aggregate it uh you are then able to use it more effectively to build something out of it and tell a story. And that's exactly what we were going to do.
(37:46) As data comes from source, you cleanse it. Um you maybe aggregate it. So we you'll learn about something that we refer to as a medallion architecture of bronze, silver and gold. Uh so that your business users and your consumers are able to see this picture. Obviously you can't see it in this uh heap here.
(38:07) And the analogy is the more transformations and refinements that you do um that is the refinement that creates a diamond from a carbon and similarly creates insights from your raw data. Uh it's not necessarily longer the processing layers but the more refined the processing the more curated is the value of the data.
(38:27) Um obviously you can see that this is a complete mess. Here you already see some value. Here you see more value even more value and here you can consume it directly and the process of creating this has thick spense but the ease of consuming it is more here. What is the evolution of these data platforms? The data has been around for a while and Eric was uh referring to this earlier that the use cases kind of remain the same over time.
(38:56) What changes is the technology stacks that we use to address those use cases and the data typically outlifts the technology as well. So both the use case and the data outlive the technology but as leaner meaner better technologies come uh there is tech debt that is acured to a point in which the business says no now it's time to bite the bullet and move over to a more modern stack.
(39:22) So in the 60s we had the rise of the database management systems and these are flat files and that were being moved into uh DBMS systems. In the 80s warehouses was all the rage. Uh so you have you might have seen dimensional modeling data marts. Um at the same time there was a class of databases known as MPP that is massively parallel processing.
(39:51) Um and terod data was one of the forerunners here which were able to get the speed and the um you know could handle the volume of data which traditional warehouses at that time were not able to do but they were um they were very reliable even though they were expensive and their primary use cases was BI.
(40:12) BI stands for business intelligence with these relational um data on proprietary systems. So you couldn't quite switch from one warehouse to another. Once you are locked into it, that's the proprietary format uh that you have locked yourself into. In the 2000s, we saw the rise of web and along with it came the unstructured data.
(40:33) So remember the early days of uh Mozilla and all where you had um you could go and see um the uh the spinning of the uh web browser but then it was gratifying to see some content. So audio video codecs you know that space exploded and the emphasis on metadata grew. So the data was important but the metadata was getting increasingly important.
(40:54) Streaming was in its infancy there and NoSQL databases also started to come in to handle some of the deficiencies of SQL uh processings. uh around the 2010s uh came this little yellow uh elephant uh Hadoop and this is where open culture started to soar because it's it now demanded that instead of all these expensive hardware you can be on proprietary system on distributed architectures and um get the same insights at a cheaper price.
(41:31) So business use cases however we'll see this in later um uh lectures that even though Hadoop had the promise of the data lake and could accommodate any kind of data structured unstructured semi-structured batch streaming it kind of opened it up and everything was open source. Um there were some concerns around data quality and data reliability.
(41:56) In 2010 like Hadoop brought in the data lake but it spark continued that adoption and it increased in popularity and because of its speed and agility. So initially it was packaged with Hadoop but later Hadoop was dropped in favor of just Spark because Spark could operate standalone as well and people started to move to the cloud.
(42:15) At that time the AWS's and the Azures and the GCPs were starting. Um now why did people go to the cloud? Because there was a separation of cloud and compute and um blob storage or S3 was much much cheaper with the same reliability as your uh traditional databases. This was also a time where specialized stores started to evolve.
(42:40) So graph databases uh there are certain use cases which lend themselves very very well to graph databases not everything. So they started to uh improve and then uh we saw the rise of deep learning models as well. Uh if any of you have tried those initial experiments of cat and dog like take a picture and determine whether it's a cat and a dog that's basically computer vision um CNN's RNNs they started to be on the rise.
(43:10) It was only in the 2020s that this space stabilized a little and um there was a new term that arose which was the lakehouse which combines the best characteristics of a data lake and a data warehouse. So that is how the two words converged and it was called a lakehouse. Um further on we'll look at other uh sophisticated offerings like what is a data mesh, what is a data fabric. Um they are all new entrance and there are variations of it.
(43:36) um they all focus on data domains because now that uh data becomes such an important asset um how do you give it to the uh domain experts who can use it the best but the important thing to keep in mind here is it's all about the data once you get your data right then your insights are going to follow now I'm not sure how many of you have uh worked on NoSQL databases let's just take a it to see the difference between what is SQL based and what is NoSQL based.
(44:11) Sometimes people think that NoSQL means anything which does not have SQL. But it's it's that's not the right definition. It says not just SQL which means NoSQL databases can also support some SQL like experiences but they can do more. Um so we have grown up with um MySQL servers or um you know the oracles and uh SQL servers of the world.
(44:35) Acid is a set of properties that is honored on these SQLbased systems. These stands for four distinct things. Atomicity. So when you do an operation, the whole thing will either go through or it will roll back. And that's very important because in a distributed system, you've got so many different nodes.
(44:59) What if three nodes succeed and two nodes fail? Just cleaning up that mess would be a nightmare. So atomicity is important. We kind of lost it when we went from uh SQL based system to big data and then now we are getting it back because there's additional protocol layers that are being put to ensure that asset is honored consistency. So once you know you set a value to two it remains two.
(45:22) Um isolation when you're doing two transactions they are independent they are happening in their own isolation. durability. So once set, it remains set. Um use cases with highly structured data um with predictable inputs and outputs. So for instance, if you're doing a ledger application, some kind of a financial system where you're transferring money, consistency is the main requirements.
(45:46) So when you uh when you're going to say that something is $7 and uh 23 um if it's always going to round it up to 20 cents or $7 $2 or whatever that's going to create havoc. So being very accurate being consistent is extremely important characteristic needed for those kind of use cases. Now let's look at NoSQL.
(46:10) Uh here you have a different acronym. It's called base and the properties for base are basically available. So the system is guaranteed to be available in an event of a failure. High availability is important. So when you're talking about um you know a different regions or within a region uh multiple systems that is why uh you know HA and TR that we'll talk about later in the course going to become important.
(46:36) soft state uh which means because there are multiple nodes um the state um may be changed because of those inconsistencies. So you might query one node, you've just made an operation, you query one node, it'll say the value is one, you query the other node, it will say the correct value of two because there is a some uh time lag for all the nodes to be uh to kind of catch up.
(47:02) And that is referred to as eventual consistency where all the nodes will eventually reconcile to the last state but there may be that period of inconsistency and these systems can tolerate that inconsistency. Now a financial um system may not be able to you know give you that leverage but a NoSQL system is for a different set of use cases and these use cases can tolerate these properties.
(47:26) So what kind of a use case would be for NoSQL? less structured scenario involving a lot of changing schemas. So for instance, if you have a Twitter application that is scanning the words to determine what is the user sentiment in the last 10 minutes, in the last 1 hour, in the last uh 1 week, etc. You need those systems to be highly highly available.
(47:46) So what if it misses one count and reports it uh second later or 2 seconds later that is not uh critical to the working of the use case. So high availability despite failures is the main requirement of these setups. Now I'm not sure if you have come across the cap theorem but this is um um a computer science um theorem where there are three properties and the claim is that systems so far that have been designed usually cater to or they can satisfy two of the three properties at any time. So you choose what kind of a use case you have and then decide which
(48:25) properties are important for you. So cap stands for consistency which is like you know up-to-date and synchronized information. Availability which means anytime you send a request you're going to get a response. How it gets the response whether it gets from that system or it forwards it to another system and gives you the response it's immaterial. Partition tolerance.
(48:52) So even if um some systems go down uh the overall system is still operating and um traditional relational systems typically will support the first and the last. So consistency and partition tolerance at the expense of availability whereas NoSQL systems will support availability and partition tolerance at the expense of consistency as we saw in the previous um example.
(49:18) As you can realize that it is even though it sounds simple it is very difficult to attain all three at the same time and as your data grows keeping the data consistent is actually a big big challenge. Now what's the difference between OLTP systems and OLAP system as they are called? They are one is an operational system.
(49:47) The other is an analytical system. So OLTP itself stands for online transactional processing. OLAP stands for online analytical processing. So transaction processing is when you are crunching crunching crunching your transactions are coming through. analytical system is you don't probably need immediate response and immediate consistency but you need to crunch a lot of data to produce those insights and in both of these systems you can have NoSQL technologies as well as SQL technologies for instance an example of an online transactional processing system using NoSQL could be like your MongoDBs and
(50:25) couch uh couch base these are document stores or it can be your key value stores. So when you work in Amazon S3 or when you you are in blob storage, they're essentially key value stores or if you are in the um Dynamobs or HBA bases or Cassandra, they're column family stores.
(50:46) These are all examples of NoSQL technologies but they support OLTP use cases. Now if you were to look at SQL technologies like the Oracle SQL servers, MySQL, they are also used for OLTP. If you go to the analytical side of the house, which is typically on the right, the OLTP is to the left. Uh you've got the Hadoop that we talked about a little while back. You've got modern data cloud um data platforms like data bricks and snowflake.
(51:12) They can serve both as a SQL technology as well as a NoSQL technology. I know the words will um sound a little confusing, but as you start to go through the course, you kind of will put it in the right place. So let's have a diagram to kind of show you where OLTP and OLAP come in. In all these ecosystems, there will always be data producers and data consumers.
(51:38) And sometimes the producers can be the consumers and the consumers don't always have to be at the end of the system. They can tap into the middle tiers as well. So ODS stands for your operational data stores. So imagine if you're running um um a store, an online store, your customers are coming in, they're making transactions, they're buying something.
(51:55) this system is being updated again and again. That's an example of a transaction processing system. Now this data can move into analytical systems which can be your older data warehouses and data ms or newer data lakes. From the data lake also you can sometimes push to data warehouses. And from these marts and these lakes sometimes if you need very high-speed um uh or low latency serving you might have a different system sitting here like a Cassandra or a Dynamo DB could be sitting here.
(52:32) Your consumers can come here, your consumers can come here, they can come here and sometimes even from the serving layer from your analytical system if you push the your insights back into your uh transaction processing system that will be referred to as reverse ETL.
(52:54) So ETL is the process of extract, transform and load where you are going from your OLTP system to your OLAP system. And when you go from your OLAP system back to your OLTP system, that's your reverse ETL. So how did we get to this modern data platform? Your data sources. Um maybe earlier you had fewer data sources. Now with a cheaper compute, you are able to bring in a lot more data sources, have a more holistic view of your customers.
(53:20) Um I touched on the different data types. So you've got structured where you have rows and columns and you know the schema. Semistructured that's your JSON and your XML where your schema is kind of embedded in your data and unstructured where where you know you have got text you've got bytes uh like images video it's not your schema is not quite known.
(53:45) Then in injection type you've got the range on one side you've got batch and the other side you have got real-time streaming and ETL is the process that you take to kind of bring it together in the past you're using data warehouses even now there are modern data warehouses but there were like a school of old data warehouses as well uh Hadoop reigned for some uh period we learned a lot from it but it was not as successful as expected it kind of is on its way out and then you have these modern um data platforms and examples of modern data platforms are the snowflakes and the data bricks. Your use cases remain the same. There has
(54:21) always been reporting scenarios and those use cases are still there. You were working off uh data warehouses, then you are working off lakes, now you are working off modern uh cloud platforms. Ad hoc queries is what we refer to as EDA, exploratory data analysis. There will always be analysts.
(54:41) there will always be be uh folks who are scrutinizing the data and trying to find inconsistencies or trying to find patterns or insights from here. And ML is those machine learning models um that are could be classical, could be deep learning, could be now GNAI, could be written in a in a wide range of languages, but the more common ones are of course um Python um and R.
(55:04) Um and all of this works off the data that you have brought in cleansed and transformed. We'll talk a little bit about Hadoop even though it's kind of fallen out of favor now because it was the beginning of the uh data lake era. Uh it started as um a Yahoo project way back in 2006 and it had a promise which is why it got so popular.
(55:32) It was inexpensive like you know when you're talking about your big Oracle honking servers paying so much of licensing costs. Now you have an alternate which could do the same crunching but at a fraction of the price. It was reliable. There was triple replication of data and it was horizontally scalable. So you know you don't have to buy another bigger server and a bigger server.
(55:57) You could buy two two nodes, three nodes, four nodes and they will horizontally scale. um there were distributions. So these companies came about and were very popular. Cloudera, Hotton works, Map Rar, EMR is still from um AWS. They were compatible with many types of hardware.
(56:15) So even Terod data had its appliance with uh Hadoop and they work with distributed file systems. So just like we have S3 which is a distributed file system, HDFS is the Hadoop distributed file system and it had triple replication for that uh you know to protect your data because storage was considered cheap. You the expensive part is the compute and so they use commodity grade hardware which is what made this so inexpensive and you could have a lot of serviceoriented architectures with open-source components and this is what the framework looks like. It had a master node and multiple slave nodes. We won't go into too much of the architecture. But basically data
(56:53) was broken down into blocks and a certain number of times that's the replication and sent to the worker nodes instead of like compute and um storage being together here they were definitely uh split apart and the data was being sent to the compute. There was the concept of a name node map reduce um yarn job tracker. Let's not get into that.
(57:19) But this architecture uh is something that Spark inherited from a driver and uh uh worker um perspective. And um the the main um word that you should carry away from here is this construct of map and reduce. So map is where things are broken down and then you'll have to sort all these um uh similar things have to come together and you have to reduce.
(57:45) And every time you do a map operation or a reduce operation, it has to go to disk and come back again. Um but but the important thing is the HDFS file system. The ability to split a large data into smaller chunks, be able to work with it's a divide and conquer approach, right? Work with the smaller thing, do your task and then combine your results.
(58:10) Now that we know relational systems uh you have um uh the NoSQL and the SQL and we introduced Hadoop a little let's compare what's the difference between the relational systems and Hadoop why why was Hadoop um so popular uh a few years back in terms of the data architecture relational was all about structured databases with rows and columns and anybody uh you know who knows SQL or a variant of uh SQL will be able to work with these systems. In Hadoop, you should think of it as a file as a massive distributed file system.
(58:44) It's not a database. And um the reason why uh people preferred uh it at that day and age was because it could handle any volume of data split across uh computers and open-source projects like Hive and Presto, they were very popular. They came from they were contributed from different companies. In fact, Hive came from Facebook.
(59:07) Presto also came from uh Facebook and um the whole ecosystem flourished uh because it was open sourced. Data variety in relational it's mostly structured maybe a little bit of semistructured but in Hadoop it's all types of data structured unstructured and semistructured. From a technical expertise maybe you can argue that this is easier because it has got fewer components.
(59:31) It's much simpler. Here you have to now manage the cluster you have to manage the Hadoop nodes. You have to understand when the name node is going down when the yan uh resource allocation is not happening. So it got definitely more complex. Uh security issues were well understood. Security is always complex but it's much more well understood.
(59:52) Here they referred to something called as keraros and it was very hard to implement. Uh from functional issues this supports transaction. Remember how we said uh talked about the asset compliance. So this supports transactions.
(1:00:11) If something fails the whole thing is going to roll back and its primary use cases was for BI reporting. Here this is write once and read many. So it was not very conducive for frequent updates. In fact frequent updates would cause small file problem which was u harder from a performance and manageability perspective. Um we talked about uh specialized systems like Cassandra as well. So let's compare Hadoop and Cassandra.
(1:00:36) Now Hadoop you should think more as an analytical processing system and Cassandra as a transactional processing system because we learned those two words. Now let's uh put it in the right bin. Cassandra architecture looks like this. It does not have a master. Whereas Hadoop has got a master node and these slave nodes.
(1:00:54) And this this architecture is more popular but you can imagine that this becomes a single uh point of failure. Um both deal with large data. HDFS has got master slave architecture. Um and Cassandra is masterless. It uses something called as a gossip protocol where they exchange information. Uh Hadoop supports partitions. Cassandra supports indexing.
(1:01:21) um they can coexist where Hadoop can be used as the data lake and the serving can be of Cassandra. Both HDFS uh like Hadoop and Cassandra adhere to CAP supporting availability and partition tolerance because if you look at the SQL and the NoSQL aspects, they kind of tend more towards the NoSQL uh bucket which is why availability and partition tolerance are the features or the characteristics that these systems adopted.
(1:01:52) Then comes Spark which is a unified analytics engine for large scale data. It was an open source uh project started in 2012 um at the AMPL lab in uh Berkeley. Uh originally it was written in Scala but it was polyglot. uh it had support for other languages as as well and it started with a wide variety of connectors for disparate uh data providers and consumers making it very easily readily uh usable by a variety of folks.
(1:02:23) So if you look at the stack there was the spark core and then you had uh support for spark SQL and dataf frames. You could support machine library, you could support graph uh data frames and you could support uh spark streaming. So this is structured streaming which is the original um version of streaming that was provided.
(1:02:48) Now we have DT and a few other which is far superior and the connectors is basically what was um tying it to a wide variety of data providers and data consumers making it very readily um accessible to a lot of developers to work with. And if you recall the Hadoop architecture from before which had a driver node and master nodes is essentially sorry uh slave nodes is essentially the same.
(1:03:13) That's the master node and these are the slave nodes also referred to as the driver program and the worker program. When Spark starts up, there's something called as the Spark context and you have to have this to be able to run your program. The cluster manager is separate and it track keeps track of all of these nodes and who's alive, who's doing what. The work gets distributed to these worker nodes as little tasks and each task will get a slice of the CPU through the executor and there might be some cache. The worker nodes can talk to one another. We would try to minimize that as much as possible because this is
(1:03:48) called shuffle. Every time this happens there is additional delays. um you want the data to be well partitioned such that each worker is quite uh self-sufficient and can run as much uh independently as possible. Again to summarize why Spark was so popular, it's much faster.
(1:04:12) So it's it speed um it's got a good caching mechanism uh ease of deployment support for real time support for multiple languages and like Hadoop this is horizontally scalable as well. So if your data size increases, you can add another node and it knows how to be a how to scale it. Uh let's compare Hadoop and Spark because um Hadoop was initially uh Spark was initially part of the Hadoop stack.
(1:04:37) So you can think of this as the HDFS. YAN was the resource manager. I I don't want you to remember too much of it but just see how complex it is and what a disparaged set of technologies that were bundled together make making it um you know so complex that people found it ultimately too difficult to use.
(1:05:02) Um why is Spark so much faster? Because remember how I said that uh map reduce uh each operation goes up. Spark retains a lot of it in memory in in stuff which is called as DAG stands for directed asyclic graph and all the operations all the transformations are being captured there only when it is absolutely required which is during an action.
(1:05:28) So you'll do a lot of transformations and only during an action does it go down and touch disk. Hadoop reads and writes files to HDFS. Spark can process data in RAM and occasionally spill to disk. So uh when uh you deal with spark you might hear the word RDD. So at the base there is the RDD and that was the original data frame. You had to deal with resilient distributed data sets. They're completely immutable.
(1:05:56) Uh now we do not uh deal with RDS directly. We deal with data frames. Data sets was introduced but it's not popular. So you can uh you you can uh choose to ignore but data frames is what you will use in class over and over again. It's a newer abstraction to the original RDD.
(1:06:16) So remember how we said that when we have big data it needs to be chunked up and things have to be paralyzed across the nodes. So data chunked going through these transformations in RDDDs which are immutable and are kept in memory as DAGs and finally an action comes by. So what is an action? something like a persist or a collect or a count where you know you you you cannot do that in memory anymore. You'll have to like bring all the whole thing together.
(1:06:40) What we were referring to as the reduce operations in Hadoop now has to take place and then you know um you're going to get your result. Uh we should also since we're talking about data understand that data platforms are of three types. You've got the infrastructure as code.
(1:07:01) You've got the platform sorry infrastructure as a service, platform as a service and software as a service. So if you start at the base um infrastructure as a service could be like your AWS. It gives you uh the ability to use cloud computing but you have to know what you're doing. It's most flexible but you'll have to um uh you have to be able to stitch it all together.
(1:07:30) platform as a service abstracts away the computing resource aspect of it but you'll have to develop your own um customizations on top of all it without worrying about the infrastructure. So obviously it has made it a little simpler and then the third is software as a service. So data bricks for instance is an is built on cloud infrastructure all three clouds. Um and so it's an example of software as a service where you have the ease of use.
(1:07:52) You are going to have you're going to be charged uh on subscription basis. Uh by the way you don't have to worry because you are going to use the free edition but if you were to use a commercial version then it's subscription based uh um charge and easy customization.
(1:08:10) But what you can't do is you can't go down and be able to tweak um a lot of the uh infrastructure related parameters because those are layers that have been hidden away from you giving you a setup that where you can focus on building your pipelines, your use cases and not have to go into the nitty-gritty details. So your data center is an example of an IAS.
(1:08:34) Your database um management and your some of the tooling that you use could be platform and your hosted applications could be SAS. Now this is a fuzzy area because you can build your pipelines on top of the datab bricks platform. You could be building apps on top of the data bricks platforms. So that could be another layer of software right on top of it. Uh so what are the top challenges in the big data ecosystem? It's around data quality and staleness of data.
(1:08:56) So if you're not if you're taking too long to bring in your data and data stale, its value reduces. Remember how we started with temperature of the data? That's what it means by stale data and quality. Even if you brought in a lot of data and you brought in just in time, if uh if uh the data has got a lot of um flaws um challenges, you know, things are have not been uh scrubbed properly, then you know the insights that are going to be generated out of it is going to be bad.
(1:09:25) So those are the two big um uh challenges and according to uh Gartner there's this cycle of uh you know different groups having their own uh data silos and they're not talking to each other so you don't have a holistic view of the whole um profile of a user for instance. Uh sound reliable data pipelines could be a problem.
(1:09:47) Maybe one um uh line of business is creating a silo in a tech stack which is not consumable by another uh business. fragmented tools uh like somebody would decide like today I'm going to use AWS. Somebody will say no I think I'm going to use just native services from AWS or I'll just use uh data bricks or maybe I'll use Tableau or I'll use PowerBI. There might be some incompatibility.
(1:10:08) So it's it's not just about uh um being able to draw the insights but you know how will your people uh be used to that setup so that their productivity can also go up. So the third one is shortage of skills.
(1:10:27) You have the data, you have the tooling but do you have the folks with the right skill sets to be able to leverage the tools. So the result is inaccurate insights and delay hence unusable results. So what are some of the best practices for these big data platforms? We won't go into too many but just a little from uh the first lecture that we have uh seen thus far. You should aim at building decoupled system.
(1:10:50) What does decoupled mean? Uh your storage should be decoupled from your compute because storage is going to grow and grow and grow. So if your compute has to grow even when you're not using those pieces of the storage, then that's not a very sustainable strategy. Serviceoriented.
(1:11:09) So when you are building your pipelines or whether you're building your applications, they should be reusable modules and each should have like a particular task or a function. uh so that when there is time to uh uh you know reuse you can just call it uh so that's serviceoriented and you should leverage cloud storage in open format.
(1:11:29) Anytime you are in a closed ecosystem you've kind of already gotten yourself into a vendor lock situation and your tech dot debt uh clock has already started ticking. So open format is a big deal. You may or may not be able to appreciate it immediately, but imagine that you bought a car and then right after that you saw something better. Can you go and you know uh get rid of your old car and get the new one? No.
(1:11:56) You kind of have to use the old car to a point in which maybe it's time to buy a new car and then you go. But had you been in open format, had you just taken an Uber perhaps, then maybe you can switch and get into uh a better car. So you have the optionality here and you can you dictate the terms of like what's the price what's the um what are the terms whereas when you're in a closed ecosystem you cannot uh there will be lots of uh good platforms good tools but you should always look at your use case and see which tool is for the right job.
(1:12:27) Remember when we talked about use cases for SQL versus NoSQL, for OLTP versus OLAP, for um Cassandra type scenarios versus um um Hadoop type scenarios, Spark versus Hadoop. Each of them offers certain pros and cons and you can't just say something is bad and something is good.
(1:12:49) No, you should look at your use case to say what does your use case demand? What kind of a uh system are you looking to build? So, you have to consider these trade-offs. Is it latency? Is it throughput? Is it access patterns? And there may be multiple use cases leveraging same uh data but with different tools. So you have curated your pipeline, brought in the data. There would be some system which will be consuming it uh as a reporting um endpoint. Somebody else could be using it from an ML lens.
(1:13:19) Uh somebody else could be using an app to consume some data. So multiple use cases can leverage the same data as long as you have curated it, kept it in a manner that it is readily consumable. Logcentric design patterns in a multi-tenant setup. Um now because you're on a free edition, in fact this is the first year we are using the free edition.
(1:13:38) Up to this point we were using a proper full uh commercial version. Um you are going to be the owners of your own system. You can invite other uh users to join you. You can have roles and privileges. So that is what is referred to as multi-tenant. But for all practical purposes for this uh course unless it's a group project, you will have your own workspace and you would do your assignments individually.
(1:14:08) Um, immutable logs is important so that you can go back in times to see what is uh what happened and if there is a reason that your business logic has changed and you need to replay it, you would be able to do so. Later when we are going to look at the medallion architecture of bronze, silver and gold, it'll become obvious as to why the bronze exists.
(1:14:27) It is to protect you from some changes that might happen in the future that you're not even aware of now. Multiple views of the data depending on the consumer needs. Sometimes you might um need to uh mask the data for some users. Maybe an auditor has come and they might need to see it. Maybe a um a business analyst has come and they should not see it. So all of those things will have to be taken care of. Cost to build.
(1:14:47) So there's always speed to to insights and that should guide whether you build something or you buy something. You have a short window of opportunity. Your competitor is already catching up. You do not have the right skill set at that time. You shouldn't be thinking of building.
(1:15:03) you should think of buying to kind of narrow the gap with your competitor and win your market share. There's always a cost to build which is your time and a cost to buy which is your dollars. So in-house expertise is leveraged at the cost of time and you have to weigh these things when you're making decisions um to invest in a particular tech. Also, as um tech personas, sometimes we get a little carried away and uh we want to build for the sake of building, like the thrill of trying out a new tech.
(1:15:32) That's always a bad idea because typically these projects never make it into production. There always needs to be a business justification for tech spending because um tech should be viewed as um an innovation center, not a cost center.
(1:15:51) It's a it should be a growth center, something that is able to make your business stand out from the rest of your competition. And you should always partner with your business so that they will support you and your tech stack and your pipelines when it comes to going into production. Um as opposed to let's there's a um money crunch, let's kill this project or you cannot uh demonstrate a proper ROI, maybe this is the project that needs to be axed.
(1:16:16) So understanding why this is important for business. What are the key KPIs which is basically your key uh performance indicators. Um and so every tech investment should have um a calculated and preferably a tangible return. Capex versus OPEX. So because we are in the cloud, we don't have to immediately pay for um a big server for whatever time we use. We we pay for it.
(1:16:42) So that is OPEX versus capex as in the capital expenditure that you initially had to do when you were on prem systems. The risk assessments, tunable cost platforms um and data is an asset. So it has to be governed and protected from inappropriate access and beach breach. Right from the get-go today, you will be exposed to something called as a unity catalog.
(1:17:02) Um which is where you can put um privileges. So you can grant some people access, you can revoke those things. So that's what it means by giving the appropriate access permissions. Then we can go go into fine grained access around row and column level. But that is not for today. Um usually you have a techn technological lens on the uh problem and a business lens on the problem.
(1:17:29) What is your current state and where are you trying to go? So there has to be a particular challenge which you're not able to address which is why you're you're you know suggesting a new way of trying things and there's always a uh you know um a prototype that people will work on to demonstrate the value. Then from the prototype, you're going to do a projection of the cost because you'll have to explain um how you're going to fund for it. Uh how your business is going to afford it so that you can be better.
(1:18:03) Uh one system that we use um even in data bricks is known as the medpek system. Some of you uh may be aware of it. So when you are analyzing a setup, you look at the metrics. So we talked about the KPIs. You look as to who's the economic buyer, who's going to be willing to um foot it, foot the bill.
(1:18:23) What's the decision criteria? Is latency is important? Is cost important? Is um you know, resiliency important? What's the decision process? Uh who are the people that you need to partner with to make it happen? Is there a certain challenge? Remember, if there's no challenge, if the existing system is working fine, for you to go and say, "Let me try.
(1:18:41) let me show you how this is to be done is never going to be met well. Um then you have to build have some champions and you have to understand your competition. So this is not perhaps important right now but later when you're going to be doing your case studies you should keep this in uh in mind. So with that uh for homework you're going to read the first three chapters of the simplifying data engineering uh uh book.
(1:19:07) Uh now the spark uh definitive guide is um is a reference so you you can read introduction of spark and data frames but if you don't have the book just uh catch some things online do your um assignments you'll start to get it little by little. Uh we will go over assignment one uh on Thursday and the due date for that is going to be September 20th at midnight. So there's plenty of time.
(1:19:32) You do not have to stress about it. These are all going to be individual submissions. Um, and you will export the completed notebook uh and upload into canvas. We are going to show you how to do it. Uh, but again, this is just to tell you that that's the process. Um, lab zero is linked here, but perhaps I'll drop the link as well so you can you can start.
(1:20:05) By the way, by show of hands, how many of you have um installed the uh the free edition? Um Moan, do you want from here? Yeah. Is it just the lab zero walk through? Yes. So, um, Mo has his own setup and we are going to give you this link right here. So, let's put it in the chat here. Um, and this is what we are going to cover just uh introduction on how to execute the code there.
(1:20:58) work with um simple data types, have your first Spark data frame, create a table uh create a database and table and then um if time permits we'll go through the um asset evaluation. So I think we need to adjust the permissions for this one. Okay. Uh we'll have to change it to anyone with the link I think. Hold on. You should be able to present now. Yep. Thank you, Eric.
(1:21:32) I changed the permission on that link. So, it should open. Um, that link is um accessible to whoever has the URL. So, it doesn't matter what. Yeah. Okay. All right. Let me know if you see my screen. Yeah, we can see it. Okay. Awesome. So, I think we have about 30 minutes. I don't want to go over. Um, so I'll give you a quick run through of the lab, lab zero, which is just like foundational. Uh, before that I'll just introduce myself. My name is Moan Matthews.
(1:22:21) I'm a lead delivery solutions architect at data bricks. So I work uh with an at data bricks. Uh so I've known her for a few years now and super excited to get the opportunity to ta this course this year. Um that being said okay so if you have uh created your free edition accounts and logged in uh it'll look something like this. Um you know I have the dark theme on but maybe yours is a white screen.
(1:22:46) Um but you should see data bricks free edition on the top right here. If you don't see data bricks 3 edition on the like the top left rather uh you are not in the data bricks free edition. So uh you know you'll incur a cost if you're on a regular data bricks uh account you will incur a cost after the 14-day trial.
(1:23:05) Uh so make sure you are in the databicks free edition which if you go to datab bricks and say data bricks free edition should be able to create an account and you can use any personal account or uh hardware account for that. Um okay. Yeah. Do you want to share that Anita? Like I don't have it handy. Uh okay.
(1:23:30) So let's uh let's just do a quick walk through here since it's only 30 minutes. Uh if you are uh you know new to data bricks, this UI may look a little bit overwhelming. There's a lot of things going on on the left panel here. Uh but you know as the course progresses, you'll get more and more comfortable with it if you're new to data bricks. uh otherwise maybe you're already familiar with some of these things.
(1:23:52) Um so the link that we gave you to the Google Drive basically will have a zip file in it. That zip file is basically an archive of the code uh that I'm going to open up now and show you what it looks like. Uh if I was doing this um basically I would go to the Google Drive and I download that zip file and then import it into the workspace.
(1:24:10) And the way to do that is by clicking on the workspace which is here on the top left and which will land you into this kind of menu which says hey there's a home which is like every user has their own home basically uh and then a workspace which has a list of folders for all the users in that workspace.
(1:24:28) So in this workspace I'm the only user cuz this is my free edition and I haven't invited anybody else. Uh so I see my own folder here. You will not see all of this until you start importing stuff. So don't don't be confused by that. Uh what will happen is basically you'll create a folder. However you want to structure your data bricks workspace is up to you.
(1:24:48) Uh what I'm doing is under my home which is like my you know my username. I created a folder called labs. And if you wanted to do that you would just click on your username here and you would say create and you can create any of these things. And I created a folder. So when you create a folder you give a name.
(1:25:07) So I created it as labs and so that's my labs here and then you would go in there and you would import that zip file. So how you would import is you would just rightclick on the screen and say import and when you say import you can drag and drop a file you know because you downloaded that zip file into your local you would just drag and drop it here.
(1:25:24) Uh you can also you know provide a URL but we are not going to do that for this particular instance. You're just going to drag and drop the file. Um, and you can get to this menu like from the top uh of this kebab menu on the top right as well.
(1:25:40) You can say input, right? But if you just right click anywhere on the right side of the screen, that's your input. So you can input from there. Um, so if you've done that, it'll data bricks will automatically unzip that zip file into this folder structure and you'll end up with a lab 00. Uh, and then if you click into it, you will see these five notebooks.
(1:25:59) Uh so there's a readme, there's an initialize, an intro, a readwrite, and a spark, right? And this is what you we'll be looking at in this lab. Um it's it's all very simple stuff and I wouldn't u you know, stress on the fact that you understand every cell that we're going to go through in the last 30 minutes.
(1:26:16) Uh but you know, you have this material. So as you run along, I'll also show you how to use the assistant to ask help. Uh which is quite helpful. So let's open the first one, which is the readme. Uh, and I kept it really super simple.
(1:26:32) I said, "Hey, all you need to know are the other four notebooks, right? There's no like elaborate instructions here. It's just to give you like sort of an anchor point to know like where to go." Uh, and I've named the notebooks in a way that you can understand sort of logically. So obviously you would you would run the initialized notebook and then we would go to the basics. We' go to the readr and we go to the spark now.
(1:26:52) Right? So let's go to the initialized one. Um and if you don't see the UI the same way on the left here there are these like uh expand you know um and contract sort of menu items. So if I click this workspace it contracts. So you see the folder uh list go away and if I click it it expands.
(1:27:15) So if you're new to data bricks some of the things may seem a little bit uh foreign or or alien but you'll get used to it. Um and you can like sort of hover over it to understand what's going on. So obviously there's a table of contents uh if I have a table of contents. I don't have a table of contents for this notebook.
(1:27:33) Um and then the workspace and then the catalog is where we will be storing our metadata which will link to the actual data that we're going to create. And then you can actually you know shortcut to the assistant to ask questions. Uh but we'll get to all this shortly. Okay. So the first thing if you are in the initialized notebook and you don't have to do it with me, you can do it after the fact as well. uh is my the first instruction I gave you is hey you can run this notebook so like don't be afraid to run this notebook.
(1:27:56) Um so what is it doing? Uh we're doing basically three things in this notebook. We're going to create a catalog and as we you know go into some of the other uh lectures you'll understand what a catalog means better than this one cell is telling you. Uh but a catalog is an upper level uh in data bricks hierarchy of how they store metadata. So you have a catalog and then you have a schema.
(1:28:21) In a traditional RDBMS you may call that a database and a schema and database are sometimes interchangeable sometimes not. Uh data bricks landed on the terminology of a catalog being the top level of how you store your metadata and then a schema being a child of the catalog and then tables and other assets falling under schemas and which we will look at in the UI shortly.
(1:28:41) Right? So the first thing you'll do is you'll create a catalog and it says if not exists. So it's basically doesn't fail if it already exists. And then you'll say use catalog. Uh so use catalog is setting the context of what the rest of the commands in this notebook.
(1:28:59) Uh where should they run? They should be in this catalog because you may have access to many cataloges. And for this notebook, you may want to use one catalog or for one cell you may want to use the catalog. So use catalog is just a convenience command to set a context for whatever the next command you're trying to run is. Um in a similar fashion now that we have set up the catalog we'll create a schema.
(1:29:16) So that schema we're going to call lab 00 because we are in lab 00. Uh and then we're going to use that particular schema. Um and then um if you're not super familiar with data bricks in the past, we had something called the data bricks file system uh which was sort of uh you know it came with uh you had to you had to create the file system based on whether you were on Azure or AWS or GCP.
(1:29:47) You would h you would bring your own S3 bucket or ADLS container or you know GCP storage container. Um but because this is a free edition data bricks is managing the storage for you. So you don't have to worry about bringing your own uh external locations. You don't have to worry about bringing your own cloud storage even though we're working in the cloud.
(1:30:04) Data bicks is automatically provisioning an S3 bucket behind the scenes in this case. Um so all we're going to say is hey data bricks I know you have some storage I want to carve it a certain way and so create volume is how you basically carve that right and I'll show you how it looks like in the UI.
(1:30:23) Uh but just for now just understand that these commands uh I've I've put them at the beginning of each notebook and they should not error out for you as long as you are in your own free edition account. If you're in somebody else's account, if somebody invites you, they may have to give you permission to their catalog or their schema, right? But if you're in your own free edition, uh you will not have any errors running this. Uh so let me just run this. Before that, let me show you how to get um to the compute.
(1:30:48) So because we are in the free edition, this is a serverless only environment. By that we mean you don't really actually spin up um you know allpurpose clusters u like you may have done in the past. Uh you just click on this compute tab here and let me actually make it smaller so you can say connect right and if you make it bigger it just becomes the button here.
(1:31:12) So depending on how zoomed in you are you'll see the text or not. So I want to say connect and I want to connect to serverless and what I'm saying is hey data bricks I know you have some compute running uh that I'm not controlling the size of I just want to you know give me a space on that right uh so this is your own essentially compute uh and nobody else uh is running on your compute you're getting your own context your own session uh and nobody else even your own other workloads don't run in that same compute uh so this is carved out for you. Don't worry about the internal
(1:31:45) mechanics of it. Uh but it's your compute, right? And now in your compute that you just did by saying clicking on the serless button, you can run this. Uh how do you run a cell in data bricks? There's multiple ways to do it. There's a convenience butter button here that says run cell or you can click the menu on top here and say run and then run and debug, run all, run selected cells. There's all of these options. For now, I would just recommend uh the first cell.
(1:32:10) So we're going to run the first cell uh and it says waiting you know it's establishing an execution context uh on that compute that you know is magically created for you by data bricks. Uh and it's going to run this command on that for you. Uh and if the catalog exists it's not going to recreate it. Uh and if it doesn't exist it's going to create right.
(1:32:35) Uh and then if it succeeds, hopefully it succeeds, uh we can go then look at the catalog explorer to see what was created, right? So let's just wait for a bit. Now you can see the green check mark here, which means it did succeed. Uh and it it's telling me it took 24 seconds. Most of that was establishing the context. Uh you can actually click down here and see the performance of each thing.
(1:32:54) You can see all of these things took less than a second actually, right? The longest one was uh create catalog if not exists, right? So it's sort of descending um in terms of the order, right? Um now I'm telling you a catalog was created.
(1:33:11) How do you believe like why do you believe a catalog was created? There's two ways to get to the catalog explorer. One is you click this convenience button here on the left uh and you can get to the catalog or you can go all the way on the left panel here, the left sort of uh tile menu and you can rightclick and open a new tab.
(1:33:30) Right? So just so you have uh typically I find it convenient to open a few different tabs so I can go back and forth between all of these things that I open in data bricks. So if I open in in the new tab I will end up in this view which is the catalog explorer. Now you see I I I don't see my files. I don't see that workspace uh I mean the set of code that I was running.
(1:33:52) I just landed up in the catalog explorer and this catalog is what we just created. Now, don't worry if you don't see all of these schemas, but what you should see is a default schema, an information schema, and a lab 00 schema because we had um the command to create lab 0 in that first cell.
(1:34:10) Uh you will not see lab 01 yet because that was something that I was doing um you know to prepare for the next class. So you you should see these three uh schemas default and information schema is automatically created for you by datab bricks and then you should see lab 00 and right now we've not created anything in lab 00 except for something we go back to our notebook.
(1:34:27) What was the thing we created? We said use that schema lab 00 and we said create volumes. What's a volume? It's carving out a path in cloud storage. So if you go back to your catalog and click into lab 00 and you can follow the breadcrumbs up here catalog explorer then the catalog and then the schema name uh and you can copy the full name of you know your fully qualified name uh to clipboard if you want.
(1:34:55) Uh and then you'll see tables, volumes, models, functions. So these are all artifacts that you can create within the context of a schema in data bricks. um you know and then you know set up permissions on these things uh for your organization or for others. Uh don't worry about tables.
(1:35:13) We did not create any yet but we did create volumes in that first cell and you should see two volumes like this input and output. And if you click into input it says there's a path. What's that path? It's just uh you know slash volumes which is essentially a convenience pointer to somewhere in cloud storage that data bricks created default storage for you.
(1:35:35) You don't control it because this is essentially the concept of manage storage. Uh now most organizations are not using the free edition right they're using their own enterprise edition. In those cases those organizations will decide where to set up their S3 paths or their ADLS parts and then you will create volumes within that. Uh but since this is a free edition, you're not thinking of all of those things.
(1:35:54) So volumes essentially is just a convenience sort of command to get to the uh sort of get to the second half of the path on which on on which we've carved out from S3, but you don't see the actual S3 path. You will if you know how to look at the logs. Um and then CS CI3103 is the name of the catalog that we created.
(1:36:16) And then lab 00 is the name of the schema we created. and volumes always end up in that path. So it's volumes slash catalog schema volume name, right? So it's always that path. Um so if you go back a step to 00 and click on volumes again and go to output, it's again that same path, right? Again, you won't have anything in here.
(1:36:35) Um you know, if it looks empty, don't wor don't be worried about um let's keep going. So the next few cells are essentially just running you through um some uh you know common convenience commands that will be good to know as you progress through this course, right? You don't have to understand all of it today.
(1:36:54) Um so the first one is basically data bricks ships a bunch of free data sets uh that are mounted and shared with essentially everybody which is including free edition including enterprise customers. It's basically uh a bucket that data bricks has made publicly available and mounted certain data sets to u and percent fs percent is called a magic command in data bricks is essentially how you switch between languages.
(1:37:21) Um as you know or or may know like datab bricks supports multiple primary languages which are python, SQL, R and Scala. Um if you you know click this top button here you'll see these four languages. Um now remember we are in serverless so uh there are some limitations on R and Scala. So we'll be sticking to Python and SQL for the most part for all of these sessions.
(1:37:41) Uh and right now I'm in a Python sort of notebook but I can always switch languages in every command. Um so here I'm switching to the file system uh essentially convenience wrapper which is a magic command and I'm saying ls which is list uh mounted data sets uh definitive guide data by data. Right? So if you uh read the definitive guide or you're planning to read it, there will actually be a reference uh to a GitHub repository in which the bike data data set is stored. And what data bricks has done is taken that and mounted it and
(1:38:11) then made it available to everyone. Um so you can go directly to GitHub and download the data set as well. Um so you'll see you know a nice like sort of tabular output for this. Um and you'll see okay what's available there. Okay.
(1:38:30) Okay. So if you keep going now I see this 20158 and I want to do the same thing ls I'm going to run it with that particular file to see hey tell me more details about that file. So it says okay there's a size uh you know this is bytes uh and then I can keep going. Now remember we created a volume in the first command which was lab 00 output.
(1:38:48) Uh you don't really have to do this because it's empty for you, but you can run commands just like you would run in, you know, in a shell script or in your command line on your Unix file system to like delete stuff from a directory. You can do the same thing with the file system.
(1:39:06) So just to show you what you can do and you can delete everything in that volume, right? So it says, okay, I'm going to go ahead and delete everything that again all of this is gated by permissions. This is you're able to do this because you're the owner here of your own catalog. So you get like full access uh to your own catalog. Now we'll go to something more interesting.
(1:39:24) Um so here we're going to define a variable uh pointing to that particular CSV that we looked at earlier which is like up here and we're going to write a basic sort of spark command uh using the construct of a data frame. So if you're not familiar with data frame don't worry about it. you know some of these courses uh some of the subsequent sessions we'll touch on this more. So this data frame is very simple.
(1:39:47) It's saying spark read, right? I'm I'm it's a read operation. Uh and uh depending on what kind of file format you're using. In this case, we are trying to read a CSV file. Uh so we're we're giving all of these options on how we should interpret that CSV file. We're saying, hey, it has a it's actual comma separator versus a pipe. For example, it has a header. I want you to infer the schema.
(1:40:13) And if you go look at the docs, there's like, you know, a bazillion of these options. Uh, so we're saying, okay, read that. Uh, and then we're going to like essentially list all of these uh columns in it. Um, and we're going to um, you know, print the the schema that that we want out of it. Right? So, let's run this command, see what it gives us.
(1:40:35) Uh, again, don't worry too much about like what the actual command is doing. It's just to show you uh essentially like okay if you had a file you would have to define a header you may not have to define a header you know all of those convenience things and then how would you check the the schema of this thing so here spark inferred the schema what if it inferred it wrong right so then I would I would you know take the choice of like supplying the correct schema or would I supply the correct schema I know my data I I looked at my data and say oh spark I don't want you to info this as an integer I want to you know force it
(1:41:06) into a string because I know in the future, you know, some customer is going to send me a string for this for this particular column even though it looks like an integer for myself. It's because I know my business. So that's the kind of thing that you would do when you want to enforce a schema.
(1:41:23) Uh when you know your data, when you know what's coming, when you know your requirement, you would enforce a schema and you would define a very strict schema. In this case, we are letting Spark essentially infer the schema and just printing it out. So Spark read that CSV file and said okay looks like trip ID is an integer etc etc right um okay so now we have a data frame and we can do things with this data frame uh which we we'll get to so we're taking that data frame uh we have a bunch of operations here um which is cos is basically we are trying to like essentially do this in like one
(1:41:54) partition um don't worry about again the fundamentals but like you're just trying to do it in like um you know you know one you want to write out essentially one CSV file uh and we're going to save that in a new path. If you remember our original path, it's bike data. Now I'm going to store it in uh bike data CSV 2.
(1:42:13) Right? So if you run this, essentially we're creating another CSV output uh but not a partition one. If I had a 100 CSV files, I would read all of it and I want to write out one CSV. That's the kind of operation I would do in this case. And so it wrote out a CSV file.
(1:42:32) And if you want to go look at this now, you can go to global data bike data CSV under lab 00 output. So let's go there. So lab 00 uh output I mean output I mean global uh data bike data CSV2 uh and then we wrote out essentially uh you know this this particular data out right so that's that's what that previous command uh did um uh so so we wrote that so we wrote that data out now there's a bunch of other outputs that you can do uh so we're going uh essentially overwrite as JSON and as spark and all of these other formats. Again, this is just to show you that you can read the same data frame. So your
(1:43:17) data frame input could have been any format. You read it, you did some transformation or you you know enforced a schema or you did something like this and you just wrote out a different formats. Uh and in this case we were writing out to JSON then let's write out to par. Uh and you can click uh this C performance tab to see exactly what's happening.
(1:43:40) um and you know just get familiar with essentially navigating this uh this uh menu. So this particular performance tab is basically called the query profile which it'll link you to the query profile uh which is essentially data bricks's new flavor on the traditional spark UI.
(1:43:57) So we're we're trying to make it easier for customers to understand what's happening in a query. Uh and you can you can continue running this uh this one should also work. Um and then we we're stopping with this first notebook with just to show you how to create a temp view. So if you have a temp view like a uh again we don't have a command here but you could essentially write a SQL statement that says select star from a right because now you have the temp view in the context. So that's how you would flip back and forth between like Python and SQL if you wanted to.
(1:44:24) Uh but again this is just like a primer just to show you a few commands uh that you can run off of public data sets that data bricks has mounted. Um uh there was a lot there. So I know like not everybody may have kept up uh but you can always you know uh rewatch that according to catch. Um so if you're done with that let's go to intro basics again.
(1:44:49) I will connect to my serverless instance and I will run the first cell. Again I've said the the cell is item potent which means um you can run it n number of times and the same thing should happen right. Um so because these catalog exists it's not going to recreate it. Uh, and it's not going to create a schema or the volume if it already exists.
(1:45:05) Everything's going to stay the same way. Um, and it's so like the the file that you wrote out in the volume in the last notebook is still there because the volume was not recreated, right? Um, okay. So, there are all of these magic commands. That's how you use to switch between context.
(1:45:25) Actually, this particular text that looks very nice and pretty is marked down. So, if you double click it, you'll see it's percentage markdown. And you know uh if you know how to write markdown you can make a really pretty table of contents and markdowns and stuff like that. Um and so let's let's go down.
(1:45:43) So again we are in this Python context because this is a by default a Python notebook and we're going to run a basic Python thing. Sprint okay printed out what you wrote right just how you would do it in your you know sort of local Python install. But now I can switch language and say, "Hey, select some random string." And now you've it's actually running a SQL statement, right? If you say print now with a SQL contract, it's it's going to fail because SQL doesn't know what a print statement is. Uh but SQL does know what a select statement is.
(1:46:11) So we're doing exactly the same thing, but we're running it as SQL. Um so this is now essentially you're in a SQL context. So you can run, if you know SQL commands, you will run that in a SQL context. You can also run shell commands like in in the first notebook we saw percent fs which is file system commands but you can run arbitrary shell commands that are allowed uh to the extent possible obviously you cannot like delete the entire file system uh and you can see like hey what has data bricks installed like oh there's all of this stuff what processes are running uh and you can you
(1:46:43) know uh you know investigate further if you want uh but anyway you can try other shell commands that you're familiar with uh Maybe you want to write a sleep command. Maybe you want to write a top command. All of that, you know, stuff will work here, right? You can also write HTML.
(1:47:02) Um, so to make your, you know, notebook look pretty before you present it to somebody, you can add HTML, uh, to make it look prettier. Uh, we briefly talked about markdown, but the syntax is like, you know, there's these double asterises for bold. There are, you know, header is like a hash or double hash or triple hash.
(1:47:22) uh and you can also have these convenience things if you don't know how to write markdown yourself and this is a little bit tricky. So we have this section on DBFS but just I want I want everybody to understand that DBFS as as it existed in a world before Unity catalog and before serverless and all of these things in data bricks was a different thing and it's a different thing now and by that I mean data bricks as supports DBFS commands uh essentially DBFS utility commands that you can run on top of the file system but you will use that to access data sets that are either publicly mounted or mounted to volumes and not to
(1:48:04) you know create your own data in DBFS. uh if you try to do that it'll say something to the ext uh to the you know effect of you are not allowed to you know the DBFS writing data to DBFS is not supported or something like this because datab bricks wants to get away from this sort of legacy construct of what databicks file system is but it's keeping the wrapper of those commands because it's actually helpful to navigate the file system like S3 or volumes on top of file system so you can still run sort of uh DBFS commands uh
(1:48:35) but not actually create DBFS mount data sets in right so these these are percent fs commands like we saw now this one is actually referring referring dbfs public data sets but if you try to you know bring your own s3 path and try to mount it to dbfs in this particular serverless environment you'll get an error saying it's not allowed right um again just some uh some common commands uh there's a head command you know obviously uh things you know typical Unix commands and like you know uh file navigation commands. Uh DBFS uh
(1:49:10) has convenience functions for all of those things or you can rely to your you know typical shell commands if you wanted up to you. So again you can just run through it and it's going to walk you through some of these things. Uh this is a different way of listing. Remember in the first command we did percent fsls data bricks data sets. Now we're doing db utils fsls.
(1:49:34) So it's a slightly different wrapper, but it's going to give you the same information. The other one gave you a nice tabular output. This one just spit it all out as one text, right? Um, now you can also uh use uh this convenience function called display to make the output of that unprett, right? So this is exactly the previous command, but we're adding a display and display is very uh it's a data brick specific command and it's going to make it look tabular, right? Um cool. Uh okay.
(1:50:05) So let's get to at least creating our first table. So there is a public mounted data set data bricks uh data set samples. Um and you can when you do percent fsls you'll you'll come across this if you were to go down a few levels. What we're going to do is create table if not exists. It means if the table already exists don't recreate it.
(1:50:25) If it doesn't exist create it. But notice we're not providing a catalog. We're not providing a schema. We're not providing any of these things. So where is it going to get created? Uh your clue is in the first cell where we are actually setting context to saying use this catalog and use this schema.
(1:50:49) So when you actually at any point say create table X the context is which catalog which schema and you have to set that ahead of time or you have to fully qualify the table name. You have to say create catalog sorry create table catalog name dots schema name dot table name. So you can always refer the full uh fully qualified name which is the threele name space or you can set the context of the catalog and the schema ahead of time and then create a table.
(1:51:12) So if I was to run this command now this create table uh I may have already created this so it it's not going to recreate it but basically what it's saying is it's selecting all of that data which is actually paret data and creating a table.
(1:51:31) Now it's it's always going to give you this num affected rows don't as as empty don't worry about that but if you wanted to if you ran that command and scroll down and then say select star from that table you'll actually get some data back. And what we actually created was a delta table. And delta is a default data format in data bricks. It's basically a paret with a transaction layer on top. Uh and so this is a delta table.
(1:51:51) Uh and you didn't have to say that data bricks just assumes that if you say create table, you mean a delta table unless you tell us it's something else. So you can say using CSV, using park, using JSON to create other formats, but the default is delta. So what we ended up with was is the delta table um that we selected data into from a list of park files that was sitting from a public mounted data set.
(1:52:16) And if you wanted to see where this table ended up it should be in your L00 schema. So if you go back there and go to that L00 schema which I'm here uh this is the one that we just created right and I created ahead of time but you can navigate uh to that same um yeah I can select compute here I don't want to start a warehouse yet so but if you have if you are connected to the serverless you would uh see your data here as well and you will see other details of so managed means you know uh you get all the benefits of a managed table data bricks which is a bunch of under the hood optimizations
(1:52:53) um and you'll see that it's delta right even though you didn't specify it so you'll get a bunch of details about the table itself uh you can see history delta tables store history again I should have started my warehouse so uh let's actually see if I can start uh and then it should should come uh pretty quickly and then we can see the history of that um uh so yeah because this is a server users. It came up came up pretty quickly.
(1:53:25) Um and it should show me some history uh which is actually pulling from object storage. We write the history to object storage and then pull it out from there. Uh and this is version zero because I just created the table. It has only one version and that version it says was created as a create table as select which is a command we did.
(1:53:44) Uh and it has some uh you know some properties here which you don't have to worry about, right? Uh cool. So you can write more complicated SQL commands obviously, right? Uh Spark SQL is AnSQL compliant. So most you know typical SQL commands will work here. Uh some of the most advanced things like store procedures and stuff very new to data bricks. Uh but that kind of stuff is also possible. Uh but we won't go there yet.
(1:54:11) But you can write other SQL commands. And once you write SQL commands, you can create your own display here actually visualizations uh which we'll touch upon here, right? Uh how we doing on time? I think we only have 3 minutes and have two more notebooks.
(1:54:29) Um yeah, so you you can take another five more minutes perhaps and then we'll stop there. Anything else we'll cover during Thursday? Yeah, cool. So I jumped to the third notebook which is the intro read. Right. Again, maybe I won't go through all the cells this time. You can navigate on your own. Uh again it has a bunch of file system commands. Again we're creating um you know a data frame.
(1:54:49) Uh in this case we are actually going to manually define a schema because we want the schema to be a certain way which most organizations will do. Um and then you know keep going. Let's see if there's anything new here. So here's uh a way in which something we haven't seen yet which is directly reading JSON files.
(1:55:10) Right? If you remember in the first couple of notebooks, we had commands that said spark read dot CSV because our base was a CSV file. Now we can say readjson file. Um and then you can write it out uh to any format you choose. In this case, we're writing out to par. Um uh you can also write out a delta. So in this case, if you notice, we're writing out to par and in this case we are saying a format delta, right? So you can read essentially any data format uh that spark supports and you can write to any data format that spark supports. Uh the thing that you would do in data bricks is every time you write out data you
(1:55:48) would ideally choose delta because delta gives you a bunch of performance benefits. Um and if you're keeping up with all the opensource format wars we also support iceberg sort of you know fully natively at this point. Um, and if you do not want to run it cell by cell, you could go to the top here and say run, run and debug all and run all, and it's going to run everything.
(1:56:18) Uh, and we recently launched uh this little, you know, scrolling button here, which is really nice. In the top right, if you see, you can sort of navigate cell by cell and see which cell is. I think this just rolled out like a week back. So, this is pretty cool. was kind of annoying earlier to know which cell was running but you can see here all of these greens mean all of these cells ran uh and you can you know see what's happening right so these are all ls commands let's go to the actual table creation one so here created a a data frame uh here we essentially read the
(1:56:47) JSON uh from from the bike data JSON path that we wrote out in one of the earlier notebooks uh and then we are writing out these two here we're writing out park and And here we're writing out a delta. Now we're not actually creating tables here.
(1:57:07) Like by that I mean we're not registering the table as an official uh you know table in Unity catalog. Uh you can just operate on the file system level and write delta and park uh tables but unregistered tables if you want to think about it that way and work in that. Uh typically you wouldn't do that. Um and then this is what you would do.
(1:57:25) So you would say hey take the data frame write it out and save as table. Now in this case you're actually registering with the with the metas store like in Unity catalog which is why if you go if you if you actually wrote this I mean ran this command you would then find that table when you actually went to your tables and went to your catalog.
(1:57:44) So this table by data is the result of that particular command uh from here right. So this particular command uh is actually creating the table by data and registering it. Whereas this command where you just wrote write delta is not actually registering with the uh the metas store. That's the difference between those two things. Again don't worry too much about it. This is just to show you various things.
(1:58:10) For the most part uh you will be registering tables and then setting up permissions on those tables as well. Uh and then there is a a lot of these uh convenience things to show hey show me the tables in my in my catalog. Show me the tables that I have access to. you know, all of these commands uh that you can sort of explore on your own.
(1:58:28) There's also these convenience ones like where you can say select uh let's say current and then you know data bricks will essentially prompt you for hey what do you want you want current catalog database data you know all of these convenience functions in this case I get database uh but you could easily have done select current user and it should give me back my username uh which is like one right because that's my free edition account uh again the last notebook is uh is an extension of this one. We'll start with the same cell just to anchor again. I'm going to run all in the interest of time
(1:59:05) and this navigate through um where we are defining a variable here uh which we can use later. So if you ever get tired of filling out filling full paths uh everywhere uh define the path as a variable and then refer to that variable. So you can see in the next command here we uh referenced a path a volume as a variable and then we're using that variable uh to then use say okay write something out there or read something from the subpath of that or whatever right um so it's programmatically you can have a bunch of these variables and make your life uh super easy um by setting the context
(1:59:44) right up front and you could have it in a different notebook even as long as you refer to that same variable essentially you have you'll reference the context of the other notebook in this notebook if you wanted. Um, but yeah, there's a few commands here um that uh if you're familiar with Spark will make sense.
(2:00:02) If you're not, you know, you'll have to read the manual a little bit. Uh, but essentially uh you know, it's like uh adding a column uh or redefining a column data type, things of that nature. Uh if you scroll down here, um you can do a full sort of select expression if you're familiar with that.
(2:00:24) uh you can drop column from a data frame right you can say hey I don't want these columns in my data frame uh because I I don't need it subsequent so all of this is happening in memory these are not action commands u action command is basically when you uh like display would be an action command but just defining the data frame is not an action command uh it'll only be evaluated when you run an action command uh and then this is how you would add columns to a data frame so you took a data frame dropped a few columns you added a few columns So you ended up with a new definition of a data frame and then you would use that going forward for your subsequent processing. Um let's see if there's
(2:00:58) anything else I want to touch on here. Uh yeah that's pretty much it I think. Uh yeah a few examples of how you would read different formats. You know we had CSV park and JSON. Uh this is how you would read a you know a set of park files in a directory. For example you would just say spark read.formatp right.
(2:01:22) Um uh oh yeah one other thing before we leave you can define userdefined functions. So this is not a function that we are actually registering to unique catalog. It's just a function that we're going to use within the context of this notebook. So you can register a function uh and then use that function uh when you you know want to do something with the data frame.
(2:01:41) Right? So in this case you're selecting trip id uh but you also want to convert duration uh using the uh using a function essentially to make your life easier and you can see the definition of this function here right so you can do stuff like that you could have a list of functions that you use all the time uh and plug and plug and play it wherever you want uh and ideally you would register these functions to unity catalog which we we will show you at some point uh you can also do explain to see what's actually happening
(2:02:11) um you require a completely different class to understand what's happening with this explain uh but essentially that's how you would do to see like what's actually happening how is spark evaluating this data frame when I try to tell it to do something right so in this case we said subset df is this read a data frame select a column convert another column and rename it to something else and how is spark actually doing that uh you can look at the physical plan that Spark is actually doing, right? So, it has a logical plan and then generates a physical plan and it's telling you this is what we're doing. Um, and it's
(2:02:47) telling you that Photon is basically like a performance pack we have in Spark on top of Spark which is a different engine and it's saying Photon doesn't support a few things which is expected. Photon doesn't have full coverage of everything Spark. Um, yeah, that's all I had.
(2:03:07) So once you once you define the data frame uh you can create a temporary view out of that data frame and then you can you can refer to that temporary view as if it was a table right so we uh created a temp here based on the data frame and then now you can start interacting with pure SQL if you're familiar with SQL uh yeah I'll I'll stop there awesome moan that was fantastic um I'm just curious uh Was there anybody who um had their free edition set up and were able to attach to compute and run a single cell? Yeah, that's uh yeah, great. Good. Good. Able to step.
(2:03:49) So once you get used to the platform, it's it spoils you. It's actually very easy to use. In the beginning, as Moan mentioned, when you see so many options on the left and the right and the top and all, it looks uh overwhelming, but you stick to just the tried and tested path in the beginning. And um um yes, Ivor, you said I'm getting errors, but will uh troubleshoot offline um errors from running the command or just being able to get in.
(2:04:24) As long as you can get into your um your workspace and import the file, the free edition, as long as you're using it for academic permiss uh purposes, it's free forever. It's an amazing amazing uh offer. Yeah, I can I can get the files in and um I just get an error when I go to the initialize uh file and when I run that it when I open it it just says notebook failed to load. So I wasn't sure what was going on. Okay.
(2:04:55) Do you want to show the screen? Yeah, if you want to share we can troubleshoot it quickly. Otherwise, you know, uh one of the officers I think we can discuss. Yeah, sure. One second. So it says uh host disabled attendance uh screen share. Oh sorry um uh can I can I learn when will you be publishing the modules? Will it be like before the class? Basically you if there's any chance you can publish the modules in canvas before the class that would be great because I can read and prepare. Ah yes uh you mean the lab modules.
(2:05:48) Yes whatever is available for that class basically that we can read before. We we will be posting the um the lecture notes um ahead of class and and the lab should be posted as well be beforehand. Okay. Yes, sure. We'll do that. Thank you so much. Interesting unexpected error.
(2:06:33) Oh, the first thing I see is that you are not are you in the free edition? Doesn't look like the free edition. This is the link that I received on that checklist. So, I clicked on that and created the account. Um, failed to load. That's interesting. Okay. Yeah, I was able to import it fine. Uh, and and looks like other people were too. So, I'm curious what happened.
(2:07:04) I I was able to follow all the steps. Okay. Okay. Now, right on the top where it says data bricks, usually it should have free addition there and it's not. Uh so is there any other way to check? Yeah, actually it's interesting because I check my account or settings. Oh yeah. See that that preview tab that you see here that should not be there in the pre free edition. So you are not in the free edition.
(2:07:45) Okay. So I'll let re reset I mean create another account I guess. Yeah I think what what you ended up with is a trial account. So for 14 days when you create a new it's quite confusing. So data bricks has two types of accounts. One is a free edition which we just launched a few months ago basically. Uh and then the trial account has existed for a long time and anybody can create a trial account.
(2:08:10) Um, but it only lasts I mean it it'll last forever, but like after 14 days it'll ask you uh to pay for a credit card or something. But the the confusing thing to me is the top right when it says workspace that's usually a a string that databes uses for the for the free edition, but you don't have the symbol of free edition on the top left. So I'm a little bit confused as well. Got it. Okay.
(2:08:38) So I guess I will uh use the other link that was shared on the chat and create recreate the account. If if you don't mind, can you send your full workspace URL like from the browser top uh like from the Yeah. Yeah. Just put it in the chat if you can paste. I'll I'll try to check internally to see whether what kind of account it is and I can get back to you. You got it.
(2:09:06) But the other link you were showing Iber is the right one. Yes, this one. Mhm. Got it. Okay, I'll recreate one here. Are we going to learn more about the assignment uh later or are you going to publish the full assignment on Canvas? Yeah, we'll publish the assignment but on Thursday we have section in which we'll go over the assignment and the expectations.
(2:09:34) So just like you went through the lab um we would go over the assignment so you kind of have a general feel for what is expected. So we are expected to actually join both the class and the sections throughout the whole class and the sections are not like the other part is not optional. This the sections will be recorded. Um, so if you're not able to make the time of the section, it'll be recorded so you can watch it later.
(2:10:07) But but the expectation is especially like this week where we're reviewing the assignment, um, you'll need to you'll need to watch it. Yeah. And then I would also suggest to folks after that, you know, schedule time and office hours, if you're stuck on anything, it's a good idea to get your like legs under you on this first assignment cuz then it'll make the other ones easier.
(2:10:32) So, um, you know, between Mohan, myself, and like like schedule some time if you have any questions about it. Don't don't wait towards the end of the period when it's due. um if you have any questions and it's not clear so happy to and or just put questions on Slack and happy to answer them but so if you don't get to see it live you know on Thursday watch the video but also you know schedule time on office hours All right.
(2:11:17) Anything else before we close for today? Um, and Jeremy, to your question, uh, we were not able to secure the discount code this time, but you can always get a free copy from the Harvard Library and the links are there. Thank Thank you so much. Yeah. All right. We are actually way way over, but uh, great class. Thank you everyone. Uh, see you soon. Thank you. Good night. Bye-bye.
(2:11:44) Thank you.