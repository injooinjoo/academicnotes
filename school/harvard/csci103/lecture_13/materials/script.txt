(2) 103 day 13 - YouTube
https://www.youtube.com/watch?v=rgL_r3RfZ7g

Transcript:
(00:04) Okay. Hello everyone. Um, welcome to our um, 13th lecture. Today we'll be talking about um, how to get the most value out of our data um, initiative investments through um, continuous improvement. So the agenda covers um we'll first talk about the final assignment um and then talk about the software development life cycle and then how to create u robust and scalable data products and services looking at the um some of the reasons around that and then also um talk about the importance of automation for infrastructure.
(00:57) and also um discuss observability and monitoring. And then um also how how to maximize ROI um and with a um center of excellence and uh as well as uh creating intelligent platforms um with Gen AI. And then um the last last part of the lecture will be um a lab covering um data bricks asset bundles that um we'll um review.
(01:36) Okay. So first the final project um it was released last night. It's on it's on Canvas and you can you can find it there. It is a team project and we created new teams for it um under the people page and uh so um make sure to reach out and um get in touch with your teammates as soon as possible and get introduced and make sure that you can communicate with them um and get an early start.
(02:12) I know it's the holidays, but it's um will be um it's significant amount of work. So um give yourself the most time to do it. So um and then um the um presentations will be um given by each team um and at the um the last I think the last lecture on the on the 15th I believe and um the um so and the the presentation should include um problem statement uh introducing the team by role and uh you should all um pick a role.
(02:57) I think um the teams are large enough that um each each role will probably be composed of two people. Um so but but introduce yourself and then review the um design and architecture and um that should be done by the um data architect. Think think about scalability, quality, performance, reliability and governance.
(03:24) And then um someone the data engineer can review the data pipeline. The ML practitioner will review the model and um how it's developed and then the um the BI dashboard can be reviewed by the data analyst um and then uh summarize the insights and um and then uh next next steps in terms of refinements. So keep it to 20 slides.
(03:59) Um, keep your presentation to 15 minutes with five minutes of Q&A. Um, everyone should present. There was a question about that on Slack, but everybody, all the team members should present. One person can drive the slideshow, but um, everybody should have partake in the presentation. Um they'll we'll have a some guests um who will provide um in uh basically review the um the presentations and give some feedback and um and you should give your team a name.
(04:45) Um and um and the plan is to go in order of the group number ex but we can make um exceptions if team if teams have team members in um different time zones where it would be um it may be very late for them in the evening. So we will give them um priority to go first. But let us know if that's the case. Okay. Any questions about the final project? >> No, I don't have a question.
(05:17) >> Okay. Okay. So, um let's let's start with the lecture then. So um on the topic of um pipelines and their oper operies operate [clears throat] um we'll um we'll review some of the um some things to consider. So first is um understand the use case um think about the business challenges and um and things that are specific to the domain and then um make sure that you understand the requirements both functional and non-functional and um and as well as um service level agreements around performance and and scale and things like that.
(06:19) Make sure to um build for scale. It's um one thing to be wary of is building something and then that works but under load um it it falls down. So which could put the whole project at risk. So think about scale up front. Think about the um number of consumers or users that will be consuming the um model and make sure that you can um support it properly.
(06:51) Um you use u repeatable patterns um and frameworks uh that are configurationdriven. [clears throat] document best practices um for for your team as well as maybe other teams throughout the organization. Do um early performance tuning to make sure that the um the pipeline is efficient and and cost effective. um make sure to do things around um infrastructure efficiencies including uh storage life cycle.
(07:40) Um making sure that you're using the appropriate storage for the um the type of data that you're using if it's not um used often can be put in lower cost storage and then also um auto termination of your clusters. So if the clusters aren't in use, make sure that they can um automatically terminate. And then um think about security and um business continuity like ensuring security of the data and also of the models themselves and making sure that the um the models are um consistent with the business and um don't um age out.
(08:27) Um observability is important. Um you want to be able to monitor the um the pipeline as well as the models that get produced and make sure that um everything's working properly. And also you can monitor usage whether or not um your users are accessing the models and and um what sort of performance or latency there is.
(08:57) Invest in um automation and continuous integration, continuous delivery. We'll talk more about that tonight. Uh you can do things like uh infrastructure as code uh to um to make sure that things are repeatable and also automating testing is important so that it's easy to attest and um ensure that your models are working as expected.
(09:27) And then um creating a center of excellence um where there may be a central group within the organization that uh helps um promote best practices, provides expertise in in terms of how to create um robust um pipelines and um and also maybe um consolidating around um some methods or means of doing that. and also providing support.
(10:07) So, uh there's various phases of um software development for the um pipelines. The development phase comes first where you want to understand the requirements. Uh implement business logic. Um um do some testing and creating test as well. And then um I generally using a smaller um set of resources like maybe a single node cluster and spot uh instances which are um can be as much as 80 per or 90% less expensive than um normal instances.
(10:51) And then the next phase is um staging. So here um we're moving out of the development phase into a a phase where we we're preparing for production but it's not there yet. Uh we can increase the size of the data set to um approximate what we expect or more closely to what we expect in production. [clears throat] do more um testing um and maybe increase the um test coverage.
(11:29) Uh automate the um automate the pipeline. Make sure that the pipeline is um checked in to um get or um source control so that it's repeatable and then um move [snorts] um begin to um move from an interactive uh triggering of the pipeline to automated and then also um set up um service principles for uh for the OP pipeline to operate under rather than individual credentials.
(12:08) Um you can create and define and use service credentials that are used to um that are specific for the um for the pipeline. And then finally uh we migrate to the production phase where everything's fully automated uh using the correct policies and um ser and only using service principles to um to provide credentials for the pipeline to use.
(12:44) And it and the um and of course in the production phase in the at this at this point the um the pipeline and the models produced are um available for um public consumption or at least consumption within the um business. So let's talk about service level agreements. Um there's different types of um SLAs's for different types of workloads.
(13:16) Uh batch is usually um defined as the the volume of data that we need to process as well as the total processing time uh required to um to to do the batch processing. streaming is um generally um uh we use uh transactions per second or TPS uh to um measure our um streaming SLA. So that could be like a,000 transactions per second or maybe 10,000 transactions per second, but um some volume where the the pipeline needs to be able to maintain that up up to that limit in terms of um processing.
(14:05) Um for um business intelligence uh generally we're looking at the latency the time it from a button click to the time that the the visual appears to the user and um as a as a performance mark and it could be like half a second or a second um or longer depending on on the need. And then concurrency refers to the number of concurrent users using the system.
(14:38) So one user using the system will um be a lot less demanding than say a thousand users um interacting with the uh system at the same time. So um that a larger number of concurrent users will require um maybe a larger infrastructure. So another thing um that's al often um referred to as the availability um like how and basically this refers to how how often the system uh goes down or inversely like what is its availability and that that's measured by the uh total uptime the the the time that the system is available and functional.
(15:30) U um divided by the total time which is the total uptime plus the total down time and then um time to get availability that's in percent. So then um something like um 99.99 percent um availabilility is referred to as 59s of availability. And this um this is really only allows for 5 minutes of downtime per year.
(16:04) Uh so this is a pretty pretty tough um availability number to reach, but um it's um often often the case. If you wanted uh to be a little more relaxed, you could you could say 99.99 four 9s instead of 59s and that gives you a little more time to work with. But um basically it's it's up to the business um generally what they're willing to accept in terms of of downtime.
(16:41) And then disaster recovery is is also important. If something goes wrong, how long does it take uh to um recover? And there's two metrics here. One time, one is RTO or re recovery time objective, which is the amount of time it takes to restore the system to a functional state. So that could be like if after after after if if something goes down if the RTO is 15 minutes and that means that the system should be back up and functional within 15 minutes.
(17:20) And then recovery point objective is another um timebased metric but this this indicates the time like um going backwards where you can get uh the maximum time going backwards where you can get uh reliable backup of your data. So um and that could be say 15 minutes but that would mean that um no more than 15 minutes in the past you have a um accurate um data backup that you can use to restore your data processing.
(17:56) So, um yeah, and uh disaster recovery is um important can especially when you're thinking about high availability. Uh, and there's ways to um help avoid uh uh like um like some sort of down downtime because of um your data center might go down. You can use multi-reion um uh like a multi-reion system where if if one region goes down, you still have part of your system operating in another region.
(18:40) So to help avoid um downtime. Okay. So we want to um choose um the platform that provides the best out of out of um out of the box um price performance um but gives you at at the same time gives you the right control. So a balance there between performance and cost and and um control. Um there's um some industry um benchmarks that we can use to help identify the correct platform for our needs.
(19:24) Um and there's there's two that are um are important which is um the transaction processing performance council uh decision support benchmark um which is designed to um provide a benchmark for analytics and decision support and basically it it it's a measure of how fast uh you can query the data um from the data warehouse once once you've had the data.
(19:54) So, going back to the um kind of like the the um BI, if we um go back here to the um ET latency and maybe the concurrency uh would uh help give us an idea of um this metric would or this benchmark would help us understand that. So [clears throat] and then um datab bricks has a um has has set the record for this and then TPCDI is a data integration uh benchmark.
(20:35) So this this gives us an indication of how fast a platform can um can we can effectively um build and and load the data warehouse with data. So how long the um data ingestion process takes on a partic particular platform of course we want to try to minimize that uh time. So in terms of um reducing cost uh uh don't make assumptions uh uh take the time to do experiments and and um validate validate your um your thinking [clears throat] um um requirements around performance um come in the form of development ecosystem and business We want to um
(21:38) focus on writing better code. Um better code or more efficient code will help make our systems more efficient. For example, we can leverage uh dataf frame APIs over homegrown UDFs. Um the data frame APIs are highly optimized and work within the framework and don't um don't require like a UDF might require um starting a a a Python engine to to execute and there's a lot of inefficiencies there.
(22:18) So if um the um idea here is be familiar with the APIs available by the various data frames and use them and um rather than writing writing your own use um caching uh during ML training the cache will maintain u previously computed um values and and data structures that can be reused and um greatly enhance the um the performance of your um ML training and also um use binary formats for um data for example um delta or parquet uh which will um be smaller smaller in size and also more efficient and often can be um queried directly where um something like CSV
(23:15) file files require um each time you you process data you have to basically um read the entire record um and to understand what's there. So binary formats are u much more efficient and and faster and will result in lower cost. uh use the right infrastructure um terms of the the um the size the the type of instances that you use and also the size of the cluster the number of instances within the cluster.
(23:55) uh you know um if it's um if it's too small the performance will suffer but if it's too big um performance may be good but you may be spending a lot um on it. Um govern um make sure to think about governance uh around um cluster sizing like uh the the number of instances in a cluster as well as the uh types of um instances that you can have within the cluster.
(24:29) make sure that that's um specified in a way that um people don't create huge clusters um unnecessarily that uh could rack up um cost and also uh you can tag um environments and things like that to um indicate the type of clusters that can be used. Uh use autoscaling which allows the cluster to grow and also shrink. um based on the current current um needs.
(25:04) So if um basically if you need additional uh compute uh the cluster will um scale up to what's necessary and then once that demand for the computers um gone then it will scale back down and also uh enable auto termination. This is important. So if people um if users forget to uh turn off the um cluster by themselves with auto termination after 30 minutes ex for example of inactivity the cluster will shut down itself and release those resources.
(25:46) So you're not um left um spending for it's um basically um paying for a cluster that's left running over the weekend or something which could amount to um thousands of dollars. Um you can save substantially on on instances if you use spot instances. This can save as much as 90% of the cost of an instance. Uh and um but they're only um recommended for use cases where if the instance is taken back, which is part of the agreement of a spot instance, if if the if the um data center needs additional computing, then um they can basically reclaim that
(26:32) instance. So you don't want to do this for um something that's mission critical, but for model training and things like that, you can use spot instances and and save um a lot of money. Um use the latest and greatest version of Spark. Um that uh each each update to Spark will include bug fixes as well as um many um performance enhancements.
(27:02) So the the higher versions of Spark, the the the better quality it will be, the more features will be available and also the more efficient it will be. um consider moving away from a using a cluster to using serverless. Similar to what um we're doing in the class with the free version of data bricks which is on a serverless environment.
(27:35) You can also um migrate from using a pulled um cluster resources over to um a serverless environment. And there the advantage there is with serverless you're only paying for what you use when you use it. And with um a cluster as if even if you're you're not doing any compute as long as a cluster is up and running you're paying for it.
(28:01) So um serverless can can result in um lower cost and serverless also uh has some advantages in terms of uh the time it takes to spin up 5 seconds versus maybe a um a minute uh for a um cluster to start up. Also make sure to um provide monitoring and alerting. Uh look for um look for um clusters that are are have been running for a long time or um or maybe are out of someone size them too large or larger than they should be or something like that.
(28:51) so that you can um stop stop the cluster before it's um spent a lot of money. And also um use tags to identify who's using the resources. And this way when you're looking at your for example your um data center bill you can identify more easily um where where the uh cost is coming from in terms of which projects or teams or or um applications are uh causing um maybe anomalies in the um spend.
(29:34) And so uh continuous integration, continuous um delivery. So uh CI um basically the idea here is is that as soon as developers make changes and check the change into their uh git repo uh that will trigger um um maybe a a build as well as testing automated testing. And if that um succeeds then it'll be um maybe um like pushed to the next level maybe into the staging environment automatically.
(30:11) So um as as soon as there's a check-in everything's um compiled and and built and then um tested and uh if it either succeeds or it doesn't. If it if it fails then maybe it uh it the developer would get an indication that he needs to go back and fix something otherwise it'll be automatically um promoted to the next level.
(30:37) And then continuous delivery and development. Uh so any code that's um ready for that's been checked in and um tested and has made it through the staging area and is ready for uh deployment. Then uh basically continuous deployment and delivery um allows that code to be um pushed into production and um and and used right away.
(31:06) And this this that push to production could be fully automated. If it's passed all the um regression testing, then um it could automatically be pushed. Otherwise, it may be um some human in the loop that determines whether or not to to allow it to go to um production. So, so here just a a graphical representation of this.
(31:41) So that the developers commit changes the uh the the build automatically happens and it goes into a staging in um and and and also running tests and then if that that's good it goes into a staging environment where more more um testing is done and if that um succeeds then it it go it's migrated to production. So code, build, release, and then deploy, test, and and operate. Yeah.
(32:13) And often when you push something to uh staging or production, you may have some additional checks there that says if something goes wrong, um have some way to automatically back out that that new version and and um continue with the older version. So some automatic um fail over to the original um or the code that was running before in case of issues.
(32:42) Okay. Um at this point I think I'll hand it over to um Ron. >> Right. Thanks Eric. >> Ron I'll I'll stop sharing and you can share. >> Yeah. Okay. So [clears throat] when you are essentially developing uh with a data platform like data bricks you have multiple environments that you are usually considering right like a development environment a staging environment a production environment.
(33:32) These kind of loosely map to what we call as workspaces and you know typically c some customers or some uh developers might want to do some kind of local development uh with their IDs. Uh so there'll be some kind of ability for you to use your ID and hook up to that environment and then import and export artifacts when it comes to development.
(33:56) And then you can check in the um you know the artifacts that you've developed into a version control or a source code control system and then you know follow regular SDLC practices like you know checking out you know uh merging pushing uh changes committing changes etc. Um and then the CI/CD process that Eric talked about starts to come into play once you have this kind of a setup that is uh kind of um uh you know in place.
(34:28) Right. So when you want to move your artifacts from one environment to another, you will have some kind of action that you know necessitate necessitate necessitates that move. And so you will actually move those artifacts from let's say uh development to staging and then from staging to production. Now the one thing to note here is that you know typically notebooks are used for your uh highle logic but if there are things like shared libraries or uh you know things that you've developed uh as a as a shared asset to be used across
(35:04) teams uh those are typically developed as wheel files and those wheel files are uh also you know kind of built on the build server during that continuous integration uh phase. So you build the wheel file, store it in a in a artifactory or in a in a kind of a repository and then from there you can actually push it out to the various environments during your deployment stage.
(35:32) The idea here is that you know you don't want to keep doing your um uh you know your builds during every for every single environment, right? So typically those wheel files don't change uh from environment to environment. So you can build it once and then you know if there are no changes you can continue to use that wheel file in in uh you know various environments.
(35:54) That way you avoid the overhead of having to build that uh each time. So let's talk about infrastructure as code. So infrastructure as code is essentially infrastructure automation. So as you know a platform that's built on the cloud will rely a lot on things like uh you know the the cloud um you know assets things like VMs, VNETs, security groups etc.
(36:25) And then from there you actually have workspaces for example which is the data platform itself that needs to get built. So you know which leverages some of those uh cloud assets that you built. So the idea behind infrastructure as a code is to make it repeatable uh as much as possible with uh by using code rather than going to let's say the AWS console or the Azure portal clicking and you know building stuff there uh or or having homegrown scripts that help you do that.
(36:57) We essentially think about that whole process and infrastructure as a code right. So there's a whole uh kind of uh established practice around that and the core practices of IA are to define everything as code that you that you want to build out. You continuously test and deliver all your work in in progress and then build small uh simple pieces that you can change independently. Okay.
(37:22) So if you want to change some part of your infrastructure, you can again go back to the uh infrastructure as a code, change the necessary code that uh controls that component and then redeploy it which means that you do a in place kind of a change there. So with the data platform like data bricks for example like I said infrastructure as code can pertain to both cloud infrastructure as well as the workspace infrastructure the data platform infrastructure. Okay.
(37:55) So this essentially what are some of the automation options that are available right? So some of the automation options that are available that you will hear about in the industry are things like chef puppet anible things like terraform cloud formation on the AWS side uh things like bison uh on the Azure side. So there's a variety of um uh tools out there uh that are available.
(38:21) The thing to note is some of them fall into what we call as configuration management and some of them fall into what we call as provisioning. Okay. So, Terraform for example form falls into the provisioning type of uh automation whereas Chef Puppet and Ansible fall into what are known as config management.
(38:42) And you can see here you know there are there are things called mutable and immutable. So what is the difference between config management and provisioning right? So provisioning essentially is used to provision your infrastructure so from scratch. So essentially you want to build something out or you want to change your infrastructure, you will use some kind of provisioning uh system for that.
(39:05) But config management is one you want to change software kind of components within an already provisioned uh you know workspace. So kind of an in place change which is why it's called mutable. So if you want the latest library to be installed for example uh you would use something like a config management to go in and uh you know do a pip update of of a certain library and that will update it.
(39:29) Whereas with provisioning which is immutable you're actually creating that code which will actually in the next run deploy everything as needed uh from scratch. Right? So that's kind of the big difference between config management and and provisioning. Here you're changing software components in already deployed uh infrastructure and here you're actually deploying it from scratch.
(39:51) Okay. And another characteristic is you know usually config management tools are procedural kind of in nature. Uh you have well-known languages like JavaScript or Ruby etc that will help you you know build out these config management kind of practices. So you can imagine for a for a you know uh for a organization maybe with thousands and thousands of workspaces how do you go about and manage these workspaces if you want to add an agent to the workspace or if you want to change something uh in the infrastructure of that workspace you now
(40:28) have a centralized way of doing that and pushing changes out right so rather than having to log into each workspace and making the change you can you can you know create procedural kind of uh scripts that will go actually push those changes. Whereas with provisioning you're talking more declarative in nature.
(40:47) So uh you have like scripts which are very um uh declarative uh and and not like you know using some kind of language. I believe uh Terraform uses uh something known as HCS. I believe it's their own proprietary kind of declarative language. Um that's that's becoming quite a standard now. So uh with with the datab bricks uh uh you know with the datab bricks provisioning kind of um strategy we rely heavily on terraform.
(41:20) Um we have like uh you know kind of um libraries that that are provided by Terraform. Whereas you know there are some customers who would use things like AWS uh cloud formation or or something like that also to to actually build out their platform that is supported too but the first class support that we provide with terraform uh with data bricks is is pretty strong.
(41:46) So different data platforms might align themselves to different kind of provisioning uh u you know uh strategies. So um with with uh data platform like data bricks it's very terraform heavy. Okay. Um and then this master versus masterass and agent versus agentless again is a characteristic here. So typically with these config management you have agents and serverside constructs uh that kind of services uh that are available to you that are that are actually not only available to you but are needed in order to push changes right so when you're pushing changes you
(42:29) require to have an agent on on those machines so that you know it accepts the change and actually uh executes on the change. Whereas with provisioning you don't require such kind of agents because you're doing it from scratch. The API layer itself is your server or your service layer. Okay. So that's kind of what the these two actually kind of uh mean. Okay.
(42:56) So let's talk a little bit about workspace organization. Okay. We've been talking about this term called workspaces. Now in the context of something like a datab bricks or any other data platform people look for isolation boundaries right. So what is the smallest isolation boundary that you can get in order to isolate your environment or your workspace from other workspaces or other environments.
(43:25) Okay, here environment is a little bit of an overloaded term because as you can see here people traditionally used to think of environments as you know for example as a dev environment or a staging environment or a production environment but now you know that term environment is probably extended out right so for example now you have lines of business and each line of business might want its own isolated kind of an environment so you might have line of business one.
(43:56) They might have their own dev environment and then they might have their own staging environment. They might have their own production environment. You might have different projects within line of business one that all want their own kind of isolation strategy. So very quickly the number of workspaces can increase right.
(44:15) So like I said traditionally I would say you know maybe 7 8 years back you know people would just have three environments right? like a dev, a staging and and a production. And maybe with with you know organizations that are just starting on this data kind of platform strategy, that might still be the case, but mature organizations will very quickly start to evolve and build out their workspace uh organization strategy based on things like line of business, right? So it it's no longer based on your development strategy like development staging uh and
(44:48) production but it goes much more beyond that. You might have you know fine grained kind of isolation based on you know projects within a specific zone for example. Okay. And it's not uncommon to see you know hundreds to thousands of workspaces in in very mature kind of an organization. Now the good thing is the users, groups, service principles etc.
(45:16) needed to drive that isolation because people have to log into you know each of these workspaces is all going to be common and comes from something known as an active directory kind of or a or a intra kind of a uh you know structure. So people provision their users and groups at an enterprise level and then that centralized SSO is used to log in.
(45:38) You know the users that have permission can log into their specific projects. Okay. You might also come into situations where you have shared uh kind of projects between two line of business uh owners. So you know uh that's also something that we see. Um so if there is like common users or common kind of a data domain that you know two lines of business are are kind of sharing there might be a shared project that that spans the two line of businesses that can come into play.
(46:13) So as you can imagine when you have thousands of workspaces like this that repeatable and provisioning kind of uh becomes becomes very uh important u because you cannot spend time trying to manually go and you know deploy these workspaces. You want it to be not only repeatable but you don't you want it to be pretty standardized right? So um because so far we've been talking about workspaces but workspaces also have a notion of storage behind it where the data for the data platform is stored.
(46:43) So that also needs to get standardized. You just cannot create like arbitrary storage accounts because you'll have an explosion of storage accounts which you cannot control. So maybe you have storage account but within storage account you have the hierarchy that that these projects will kind of use. So there is some thought that needs to go into that kind of organization capabilities.
(47:06) So a related concept here is called as a data mesh. Okay. So a data mesh is essentially you know treating data as a product. Okay. rather than treating your uh you know your entire organizational infrastructure and your project as a product, you treat the data itself as a product.
(47:31) Right? So that's how the the underpinnings of data mesh came about and and a data platform like data bricks fits very well into it. Again I go back to that previous one. What is the lowest level of isolation that you can provide? It's a workspace. So the workspace forms that isolation boundary where you can create a domain.
(47:49) So there are four characteristics to a to a data mesh. The first is you know you have well organized data domains. So a data domain could be like a you know something like a marketing data domain or a sales data domain or a you know like a supply chain data domain. So something that's you know well specified and has data.
(48:11) This data domain will have its own data owners. It'll have its own federated governance strategy. So they are responsible for the data and they're not responsible for the governance of that data. And then it should be pretty selfserved from an infrastructure perspective which again goes back to the provisioning right.
(48:32) If you want it to be self-s served from an from an infrastructure perspective where data owners can create their own uh you know data domains then you need to have a pretty standardized way of provisioning. So that again goes back to that having that wellknown uh strategy for uh provisioning. So, Unity catalog here is the governance kind of a catalog um that kind of is the glue that binds all of these data domains together, right? So, you cannot have data here that is generated by this data domain completely isolated from this data
(49:06) domain because a data product at the end of the day might be a mash of those two um you know data data data from two different domains. So, Unity catalog provides that kind of glue that can help stitch together and and form those data products. Now, if you think about it from a from a data platform perspective, some of those concepts that you heard about medallion architecture moving from bronze to silver to gold will also fall into that kind of a paradigm where the gold layer is a data product that is coming from multiple data domains.
(49:41) Okay. So going back to you know the CI/CD approach um this is this is on Azure DevOps. So you have you have your source code control system like GitHub uh you know but then you have also like a bit bucket for example but you also have a a DevOps kind of a mechanism that underpins that right.
(50:09) So the combination of these two is what makes CI/CD or DevOps happen. So uh with Azure you have a system known as DevOps that also comes with its own built-in source control source code control system although you can attach you know external ones to it if need be. And what this DevOps system provides is the ability for you to create these pipelines.
(50:30) This is a slightly older uh kind of a strategy where you have two pipelines the build pipeline and the release pipeline. Now they have a single stage a staged approach where you have all of these pipelines in one stage. Um but the idea remains the same right? So with a build pipeline you can run your uh you know your unit tests, your integration tests, you can build out your um uh you know your wheel files, your libraries and then with a release pipeline you essentially move the assets from one environment to to the other based on
(51:04) certain actions. So the action again could be based on your source code control system itself or you could have manual gating. Right? If it's manual gating, some some person has to go and actually manually kind of approve it. Uh but if it's automated, some action within your source code control system can do that.
(51:23) So for example, when you push something uh to your source code control system, the build pipeline automatically kicks in at that point of time as a trigger and then runs a sequence of steps there that it has to complete for that test to be um uh okay. Why is this important? Again, from a historical perspective, people used to have quarterly releases of software. That's no longer the case.
(51:49) You know, people want uh faster and faster releases. And a CI/CD DevOps system makes that possible because you're testing as soon as your code changes are pushed in, you're doing tests um you know both unit as well as integration tests that ensure that your code is compatible and is is is okay without having to wait for everything to be you know compiled and then finally you know you doing the test testing.
(52:17) So that is kind of the uh idea behind you know this very agile uh CI/CD DevOps kind of process where you're continuously doing the uh you know the testing and the integration testing and the builds and then deploying you know based on either automation or human gating uh to different environments. So like I said the source code control system and the CI/CD server um you know Azure DevOps for example is very popular.
(52:48) There's another one called Jenkins that's pretty popular. Uh those those are kind of like uh you know fall under the CI/CD server source code control uh systems you guys are probably pretty familiar with. And then on a data platform things like a data platform because you have to export out uh artifacts import artifacts during that you know the build process as well as the deployment process.
(53:14) You need very nice API support along with the platform so that your DevOps build server. You know the build server essentially can um you know build take take those assets out into that build server either do some compilation do some unit testing on that and then push those assets out uh you know to to to uh you know downstream to to some other kind of a uh environment.
(53:42) Right? So that's what the rest API has enabled you to do is to interact with that data platform so that you have the ability to you know bring in information and then export out information in that during that build process. Okay. And then uh um you know you have to pay a little attention to things like clusters because clusters for example um also form like you know what kind of uh automation you want to do with uh your project.
(54:13) So for example when you're doing your in production you might want your cluster to be an automated job cluster. you don't want it to be an interactive cluster. Whereas with development, you're probably toying around with uh interactive clusters during your development period. So the switch between interactive and automated will occur during that CI/CD process.
(54:35) Uh yeah, so this is kind of a view of the Terraform landscape. So as you can see, it's pretty rich when it comes to the datab bricks uh ecosystem. So datab bricks and hashi corp which is the which which you know manages the terraform uh product uh you know have a strong partnership. So almost every aspect of data bricks that you can think about from a workspace and an infrastructure perspective can be automated using uh terraform.
(55:09) Okay. Uh Terraform is also cloud agnostic. uh you can reuse code um you know between modules for example you can create small libraries that you can reuse gives you very good repeatability so that you can you you know you can provide that repeatable it's very predictable uh supports rest APIs so you can build your own wrappers uh where there is u you know kind of rest APIs that are available and the nice thing here is it's not only doing your datab bricks workspace this kind of automation but it's also doing the underlying cloud
(55:45) automation right so um an example for you know if if you quickly want to see what that looks like would be something like this right so this will be that declarative script that we're talking about so here you can see on the Azure side we're using a subscription and then this is the region and then you know we using uh we are telling the in this particular region I want to create a um you know a um you know a network domain that's you know following this kind of DNS uh kind of rule.
(56:20) I don't want public IP. Um so you can control almost every aspect of the underlying cloud because you're you first have to create your virtual networks on which the data platform resides. So essentially you're creating your virtual networks, you're creating creating your subnets, your vinets, you're creating security groups and then finally you you you will actually come to the location where you start to create some of your datab bricks uh related uh you know artifacts.
(56:51) So the final output from this module will be a URL that you can use. Okay. So how do you actually run this? It's it's it's quite simple. So you can go into something like uh you know your UI here and as you can see in the UI you have this provider.tf which is the script that I showed you. Um and then you also this autogenerates what is called as a TF state.
(57:16) So Terraform essentially can create state based files where it it understands and and maintains a state of what has been created. So if you want to come in and swap something out in between, you can go to the state file. You can create a new script which will go to the state file and remove that and and and insert a new kind of a u uh you know product there.
(57:38) So you have the ability to do some state management and uh you know um individual changes so that you don't have to do everything from scratch all over again. But typically you would do something like a Terraform plan and then run the uh planner. Terraform will go through the script, make sure it'll do a dry run, make sure everything is fine and then after everything works uh and it says everything looks good uh from here on um you know so the plan is you know I want to add nine things I have zero to change I have zero to destroy right this is coming from the state file
(58:14) so the next time you run the script if you make a change it knows from the state file what has what has already been added what needs to be changed and then you know automatically make those changes. So that planner is now you know in action and then you can go in and say terraform apply and then it'll actually apply the script and run the script and at the end of the script.
(58:35) I'm not going to wait till it runs the script. It takes a while but it it's going to create all the Azure resources and then the datab bricks resources and you're going to have a workspace. So as you can see it becomes highly uh you know kind of repeatable at that at at that point of time. Okay. So that's kind of the idea behind the um Terraform.
(58:59) So with um repos API and git based jobs so datab bricks as a data platform right has first class integration with uh you know your source code control systems. So that makes the whole DevOps CI/CD process you know much much more simpler and nicer. So, so far we've been talking about kind of ETL jobs, but you know there is also this whole notion of ML that comes into play u and ML follows you know a slightly different kind of a DevOps uh kind of a process which we call as ML ops uh which will include things like you know experimentation
(59:43) feature stores uh the ability for you to actually use models within model registries and actually promote your models from challenger to champ from from champ challenger to champion or whatever the case might be. Uh there might be you know slightly different variations with ML ops and the ability for you to create these folders within data bricks and the ability for you to tie your source code control system with various assets using APIs is what makes the system you know very tightly integrated with with uh
(1:00:15) DevOps. So when you're thinking about DevOps, you got to think about it from the ground up. Uh you know, it can't be an afterthought. Uh the platform itself has to provide very good support for it. Uh and and uh you know, your your DevOps uh folks will will develop pretty sophisticated scripts to interact with the platform and then you know be able to do testing and uh uh integration testing.
(1:00:41) Okay. So if you want to see what that looks like within a datab bricks uh system. So let me log in here. Oh actually I've already logged in here. Okay. So I can go into my workspace. So this term workspace here is kind of overloaded. You think of workspace as a single URL from what I talked about from an isolation boundary perspective.
(1:01:11) And why is there an isolation boundary there? It's because a workspace has its own set of clusters, has its own set of jobs, has its own set of notebooks, has its own folder structure that no other workspace can look into, right? So whatever you're doing within that workspace is kind of, you know, local to that workspace.
(1:01:31) So you can think of this URL as a as a workspace. Okay? So if again going back to the traditional sense of this let's assume that you have three workspaces you know for dev staging and production you might have a E2 demo field dev data bricks E2 demo field staging data bricks and E2 demo so you have three URLs which are three different workspaces this workspace is nothing but a folder structure like a folder structure that you see within you know Windows Explorer or whatever.
(1:02:04) So uh it's it's kind of an over slightly overloaded term but when you come into your workspace or a folder structure you can go in here and create either a folder or a git folder and and the difference is the git folder here you can go in and actually put a git repository URL. So if I log into let's say GitHub uh let's take this guy here and code I can copy this I can copy this here and then I can create a git folder.
(1:02:36) So what this does is oh actually it already exists so let's put two there. So what this does is it creates a git folder and this git folder now as you can see is linked to that git uh you know that git repo right so I'm already on the main branch here but if I want to go in and actually look at the git editor within here so we datab bricks as a platform provides like some g integration and some native integration capabilities so for example I can create a new branch from here I can switch branch branches if I want to. Um, I can
(1:03:14) pull the latest artifacts from a single branch. I can do commits once I change. I can actually push those changes. You know, see there's a commit and push button here. So, there are some native integrations with git. But the nice thing is the git and uh, you know, this folder structure here are kind of in sync with each other now. Okay.
(1:03:35) So that becomes important because you know when you're doing pushes etc and and you want your code to be synchronized and then pushed to a subsequent environment this this kind of integration helps a lot. Okay. Um so this is kind of the the the way the the git uh you know integration works within within this data platform.
(1:03:56) So you can come and make changes here and those changes are automatically reflected in in uh you know your local environment just like uh so for example if I say test for agricore 2 here and then now if I go back into my git uh location uh so go back into my workspace and my g location here. So you can see now that I have one changed file.
(1:04:21) If I want to discard those changes, I can discard those changes or I can commit it. Uh changed readme and then commit and push. So now you know these changes are pushed. Again this shows colorcoded what has changed, what is deleted etc from the from the git version. So it's it's a very nice seamless way of uh working with uh this.
(1:04:41) Okay. So let's go back here. So that's that's that that's what this slide is trying to say is that you know there is like very good integration and remember that you know again from an MLOps perspective uh if you guys are interested in uh you know doing ML uh reading more about MLOps like I said MLOps will include things like experimentation feature stores etc.
(1:05:16) So there is a nice uh book uh called the big book of ML ops by datab bricks I believe. Yeah. So this you know the big book of MLOps that it covers you know the various strategies. So for example if you think about it you know there's two ways that you can do MLOps right? So one is essentially you can create a model and then move the model from one environment to another.
(1:05:48) So you train the model once and then use the security or the registry on that model to push that model from one environment to another which might not be you know ideal in certain situations right because the data that you train a model on is specific to a spec environment. So you might have dev environment data there and that might not be sufficient to train the model.
(1:06:12) So what sometimes people will do is take a second approach where they actually move the training code from environment to en environment and then retrain the model uh uh in that specific environment. Right? So there are nuances there when it comes to actual u you know DevOps versus MLOps. So that that's covered pretty well in this book if if you guys want to get into some details there.
(1:06:36) Uh let's see here. So let's get into a little bit of observability here. So datab bricks offers you know excellent observability. So um you know for example for anything from billing all the way to auditing all the way to looking at what is happening with jobs within your uh workspace. It provides what are called as system tables. Okay.
(1:07:09) So these system tables essentially again let me go back and show it to you in in practice. So in your unity catalog which has your cataloges and schemas and tables which you're familiar with there is a special catalog known as system and the system catalog has all of these schemas and so for example if you look at the billing schema you will get all of the billing information that you need to see what kind of usage has occurred right now from a from a data mesh perspective or from a you know workspace organization perspective. This becomes
(1:07:47) very important because there's chargeback models uh you know there's a central bucket of uh you know uh that you can use there's a central pool of dollars that you can use but you have all of these various organizations and you know you want to charge back to these various organizations based on the amount that they use.
(1:08:07) So that's when observability monitoring etc becomes pretty important internally also uh and this this kind of system offers that capability to slice and dice usage so that you can you know appropriately charge back. So if you look at the sample data here um you will see you know at a very granular level it provides what is the skew that was used uh what is the usage start time what is the usage end time um any custom tags that you put right which becomes very important from a monitoring perspective because a custom tag is what is the is
(1:08:42) the way you can slice and dice usage um so one lo for example or line business or a data domain owner might put his own set of tags there which you can then say okay show me how much usage has come from that specific tag. So these custom tags become extremely important. Um and then the usage quantity right so from what time to what time you used it what is the usage quantity you know how much can we charge back etc that kind of information is what you get from these system tables okay so that becomes kind of uh pretty
(1:09:17) uh important. So another one would be something like a compute. So if you want to find out hey what kind of clusters was used by this workspace um you know between this time and this time you can go in and query this table and you get uh you know all of the necessary information. So this is the cluster name, who it is owned by, uh what is the create time, what is the delete time, what kind of node it used.
(1:09:42) You can see that this uses AWS's R5X large for workers, what type it used. Uh it also goes into pretty fine grain detail about you know what is the memory usage, what is the uh you know the the you know usage from a CPU perspective. So you can start to build some some kind of monitoring around that to see whether you're making effective use of your clusters or not. Okay.
(1:10:08) So system table is a very CPU system percent CPU weight percent etc. So it it captures all of this and then pushes it to each of your um organization. So if you have access to the system catalog now uh admins can go in and start to do all sorts of you know kind of analysis on on uh you know monitoring the the workspace.
(1:10:32) The other part is auditing. So auditing becomes very very important because you want to see if something you know who accessed what data uh when did something get deleted etc. those those kind of things can be uh you know found from the auditing table okay which is right here the system access table okay uh so yeah as you can see there's like wide variety of telemetry that a data platform will provide to you that that you can use for you know various uh uh tasks so the other part is uh maybe maybe I'll get get back to that in a second. But um
(1:11:15) there are also dashboards that you can create. So data bricks provides some pretty nice out of the box dashboards. So if you go into the dashboarding section and look at like let's say uh uh some kind of job system dashboard here. So this job system dashboard will give you like you know uh slice and dice of you can see here you know the custom tags that I was talking about based on tags.
(1:11:42) You can see how much has been used etc. who's using, when did things spike, when did things uh you know suddenly uh you know if there was a job that took too long. Um you have the ability to monitor all of that. How many jobs actually failed? What is the failure pattern look like? So you can come in and start to see all sorts of uh metrics and things in a in a visual way which which is always very appealing to a to a admin.
(1:12:08) Okay. Uh let's go back here. So that's that's kind of the the dashboarding, right? And the these dashboards are essentially all built on system tables, right? So there's no magic here. So this is all widgets that are quering your system tables and getting this information back. So uh the system table like I said very very powerful.
(1:12:32) So you know uh a lot of interesting information can be gleaned out of it. Um so data teams right typically have SLAs on uh delivery right not quality. So um you know they essentially want to say hey there is an SLA where we need this data by this specific time because there are downstream pipelines that require it.
(1:13:01) Nobody goes to the data team says look I need awesome data you know perfect data but you know I don't care when you give it to me you know that that that never is the case. So people are always waiting for data and they assume by by very nature that the data quality is a is always good.
(1:13:21) So what happens is you typically have reactive management whenever there's a data quality issue. Um and then you know if there are multiple data quality issues it starts to get bottlenecked. Uh trying to get a root cause analysis of what went wrong with the data what upstream pipe actually broke etc sometimes gets you know pretty uh you know tricky to to address immediately.
(1:13:46) So what happens is there are kind of couple of things that are built into the platform to help you with that kind of monitoring. So the first is called anomaly detection and what anomaly detection does is again let me go in and show that to you. So you can go back to your catalog here and let's say you know you have a catalog and under that catalog let's say you have schemas.
(1:14:14) So at a catalog level if you want you can enable anomaly detection or you can go to a specific schema and enable this catalog uh uh this anomaly detection. So if you go into the details page here, you can see that data quality monitoring has been enabled here and then you can see the results here only. So it creates a dashboard and what this anomaly detection does is it does checks on multiple dimensions.
(1:14:40) So it does check on freshness of all the tables within that schema. It does checks on completeness of data within that schema. And where is it getting the rules for freshness and completeness? It uses AI, you know, in the back back end because it knows what the trends are, right? So, it knows, hey, this table is always getting refreshed every half an hour and this table approximately always has, you know, uh, you know, 2,000 rows that are added to it or 6,000 rows that are always modified approximately.
(1:15:12) So, it it starts to gather all of these metrics and then applies this metrics based on trends, right? uh in the future and comes up with what are known as freshness and completeness metrics which will very quickly tell the tell the uh you know proactively tell the admin that there is a problem. If a freshness doesn't occur then an alert can be sent out.
(1:15:36) If a completeness is kind of skewed then you know a a alert can be uh sent out. So essentially using the power of you know some kind of AI uh engine in the back that's learning continuously from your pipelines uh it can monitor and and give you um you know pretty interesting uh um uh pretty proactive and interesting you know feedback about your pipelines without you having to do much work right to set it up.
(1:16:02) Uh so I believe there's a screenshot here. Let me see. Yeah. Uh you know so essentially this this is what it'll look like. It'll say hey there is a problem with this schema. The table under the schema you know the reason is stale. Um you know it's not updated since since a while the impact is going to be pretty high for you and the root cause right because some jobs have failed.
(1:16:29) And the reason you know we can get to the job level is because we know from a lineage perspective what the table is actually um you know responsible for right so uh essentially that's that's kind of the lineage is being being associated so for example if I go to any table here and look at it let me go back to the catalog or let me go to the system table and show the lineage actually system they will uh so if I take a system like uh billing I know billing is probably used as a source table for many other tables right so if I come into the lineage here I can
(1:17:13) see the lineage of that table uh uh all the different dashboards that use the table all the different jobs that use the table all the different tables that use that table as a source so I I get pretty interesting metrics Right? Like this. So this is the usage table is being used in all of these tables.
(1:17:32) So I can very quickly get impact analysis. If this table breaks, all of these tables are going to break also. Right? So and I since I also have job level metrics, I know exactly what job is feeding data into this. So I I I I can start to come up with that kind of analysis too. So um so this this root cause analysis is essentially using the lineage to say what is you know kind of could be the culprit of your job failing.
(1:18:01) So with as you can see with very little work you know just by toggling that button you start to get very interesting monitoring and observability in the in the system based on AI models that it generates in the back. Yeah. Uh the second kind of monitoring that's used is more you know uh table focused and the way data bricks as a data platform thinks about it is you know whether you're doing data quality checks on let's say a table or a or a pipeline or a ML model right eventually both of these write something to a table right so if we actually
(1:18:44) monitor the table. We can monitor pipelines as well as models, right? So no matter whether it's a model or a pipeline, if there is a monitoring system that actually is monitoring that eventual table that uh these two are dependent on uh you you get you can you know establish monitoring pretty effectively.
(1:19:06) So with one system you can kind of you know uh you know you can kind of do both of these kind of uh things right. you can u uh get both of these things uh you know covered. So typically what happens with this kind of lakehouse monitoring is and I'll show you show it to you in a second but here you have your table. This table is associated with a monitor.
(1:19:32) This table has what we call as profiling table. It this monitor actually generates this. Okay. It generates a profiling table. It generates a drift table and it also generates a dashboard by default. Okay, that uses the profiling table and drift table and it gives you various metrics. So what kind of tables like I said the table could be a snapshot table or it could be a time series table where you know you have a time kind of a column that uh driving the table or it could be an inference table where your predictions are being written to a table
(1:20:06) right so from a model. So all of these three type of tables will generate a monitor. This monitor will generate these uh supporting tables which can be used for monitoring this table. Right? So you can now start to say things like hey I have a profile for this table. I know what the min value and max value for my columns are supposed to be.
(1:20:30) I know what my categorical column uh values are supposed to be for a column. If anything deviates or anything drifts, right, I I I can immediately get notified. Drift becomes all the more important from a model perspective, right? Because you might have seasonal changes in the model.
(1:20:48) You might have, you know, inputs that are changing that are changing the behavior of the model. So you want to very quickly get to drift and seeing if the model is actually drifting. And this particular drift table calculates for models things like KS test you know k square test um you know various other tests that uh you know are are used for model drift uh detection and then comes comes up with you know metrics that uh can be shown. Okay.
(1:21:18) So I won't show you the model uh you know metric but I'll quickly show you you know the um um if I can find it uh I'll show you the inference one. So the inference one was in uh yeah so I had a table here. I believe it was in uh this guy. I had the silver transaction table that uh you know I had actually enabled quality on.
(1:21:57) So this quality tab here you can go in and actually enable and configure your data quality monitoring. Okay. So you can say whether this is a what type of table is it? Is it a snapshot table? Is it a time series table? Is it a prediction table? And then the monitor that is generated in the back back end based on your selection will automatically know how to behave or how to act on that table.
(1:22:23) So here we've generated this table uh this uh this uh monitor. So this monitor can keep running based on you know some kind of schedule. So uh maybe every 1 hour if you need it that fast or maybe once a day maybe once a week it'll it'll run its profile and then it generates these two kind of tables support tables like I said right so it generates the silver transaction drift metrics table and the silver transaction profile metrics table and these tables contain drift statistics as well as profile statistics for the table and it generates it at a
(1:22:59) very granular level so it's generating it at what time. Uh if there's any slicing parameters that you want uh how is it deviated from the baseline for example. So lot of interesting uh highly granular statistics are are are captured in these support tables. Now these support tables by themselves might be very uh verbose to read.
(1:23:23) Uh so nobody typically uh goes to those tables directly. you you know you either write SQL queries against it or the dashboard that's generated gives you a good idea of what's going on. So if you go into the dashboard for this table uh you can see that it has created like you know pretty interesting things like what was the row count in the last time window.
(1:23:44) You can slice and dice it right you can say hey from what is my start time and what is my end time any slice keys that I want to provide any granularities like default or 1 day or 2 day or 1 week or whatever the case may be and then it'll tell you what the row count was over a period of time.
(1:24:03) So I can see here on this day the row count suddenly bumped up for whatever reason I can go and figure it out. Are there any columns with a lot of nulls in it? Um and or a lot of columns with zeros in it which might be indicative of problems. And here in the graph I can see based on these column names right suddenly I see this particular column which is the preferred payment method suddenly had a lot of nulls on this particular day.
(1:24:32) So now again this is giving you a visual cue about saying there is a pro possible problem because why did this column suddenly get so many nulls right similarly percentage zeros over time on the same day I got like number of reviews as lot of zeros uh zeros right so why did that happen maybe upstream something happened in the pipeline maybe the schema changed that's you know giving me a lot of zeros in this so it's giving you know pointers to where to look at uh to to to get those problems.
(1:25:02) So from a profile perspective right again like I said lot of profile uh statistics. So here you can see the drift you know some of the drift level P values and K square values that are calculated per column. So you can know the column is drifting from a deviation perspective. But from a profile perspective for numerical kind of columns it it it generates the min value, median, max, standard deviations, lot of profiles, right? So that you can look at and for categorical values it'll it'll generate things like distinct
(1:25:33) count, percentage, distinct counts etc. So um you know uh interesting metrics like this that lakehouse monitoring. So you can see the difference between anomaly detection is anomaly detection just required you to you know but click a button and then let freshness and completeness checks happen in the background whereas with this you get much more information but it requires some work of you going to the table and setting it up setting up that monitor okay and then monitor can also be set up using APIs and things like that so you
(1:26:08) know u all that is fair Okay. So this is the transaction uh and observability kind of uh thing that is happening. So let me see here. So this is what we talked about. It generates these two support tables, uses these support tables to generate a dashboard. You can go in and modify the dashboard. The dashboard is not locked or anything.
(1:26:32) Uh you can also look at the SQL queries, pull the SQL queries and and and tailor the dashboard to to what you think. You can since dashboarding also has alerting capabilities if like let's say a standard deviation goes above a certain level you can have it alert you etc. So you know those kind of things are possible too.
(1:26:52) So all this slide is trying to say is that you know when we talk about a table you have to think about one level higher as to what generated the table. Like I said um that table could be coming from you know some kind of pipeline. It could be coming from some kind of job. It could be coming from some kind of model.
(1:27:14) Uh it could be coming from a feature store. So at the end of the day the the common abstraction there is the table right but you know there could be different subsystems within your lakehouse platform that are actually generating that table but since a table is pretty standard abstraction concept you can actually attack the table and then monitor the table which in turn is going to monitor you know all of these processes.
(1:27:40) So that's kind of the idea behind Leos monitoring is is to try and do it at the table level so that you can now start to monitor different processes. So DQX is another interesting um uh you know kind of concept. Uh DQX essentially deals with uh data quality and this is a framework that [snorts] was developed by databicks labs which is codebased. Okay.
(1:28:09) So as you're generating your pipelines in code uh and doing your you know bronze to silver to gold kind of uh transformations can you actually have code in there to you know do checks. There is a system known as you know spark declarative pipelines which um you know I believe Paul walked you guys through that has things like expectations right? So it has that quality kind of stuff built in.
(1:28:36) But if you're developing your stuff in Pispark, you can use DQX to enable some of those quality things in there. Let me see if I have an example of that that I can quickly show you. Uh DQX. Yeah. So here uh the way you would use this is essentially you know you would import the DQX library in and then um you know pip install this uh DQX library.
(1:29:05) Don't worry about this cell because this cell is just showing you source some source data that has been set up. So it has three columns ID, age and country and uh you know this many number of rows. Uh so quick and dirty way of just setting up a small test uh sample data. And now in YAML right um this is one of the ways you can set up your rules quality rules.
(1:29:31) Here you're saying hey this these three columns ID, age and country should not be null and not empty right so there are built-in functions like this in dqx and you can also write your own functions right but there are like a there's a big library of built-in functions that are available to you here is another function we're saying uh you know the the uh you know the country should be the the countries Germany and France The age should be between 18 and 120.
(1:30:03) Right? So again declaratively you're explaining what the quality rule should be and what the criticality should be. Right? In this case the criticality is an error. In this case the criticality is just a warning. Um and then it this should be in the list. So country should be in either Germany or France.
(1:30:23) If it's any other country then flag it as a warning. uh this particular column should always be unique. So there's a variety of functions like that. So you can build out your YAML with you know quality rules declaratively without having to write any code. And then you set up your DQ engine and then in your DQ engine you apply your checks right against the data frame.
(1:30:46) So this is the spark data frame that you guys are familiar with. So you might have a spark data frame as part of your processing. You pass that Spark data frame in, you pass your rules in and then it'll give you back what rules succeeded and also what rules didn't succeed. Okay, so for example, here you have your valid data frame and your invalid data frame.
(1:31:07) Okay, so if the rules don't succeed, then you'll get all of the rows that didn't match those rules in this invalid data frame. But the rows that matched, you get it in your valid data frame. uh you also get back you know what the errors were that it detected. So it's a it'll say this particular row what are the rule that failed.
(1:31:28) So you can do you know some SQL and and and get stuff to it. So this is a another way of doing um you know kind of uh uh quality in your code. Uh so variety of ways to do quality right. So you know we touched on anomaly detection, we touched on lakehouse monitoring which are kind of built into the product. Uh you can do it via code.
(1:31:52) Spark declarative pipelines are something known as expectations that are built in. So most of these data platforms have you know these kind of data uh uh observability data monitoring kind of uh things built in. Uh yeah. So that's that's kind of the uh uh DQX portion. Uh now the last thing I'll show you is something known as uh datab bricks asset bundles. Okay.
(1:32:22) So from a CI/CD perspective, right, we quickly glossed over, hey, you know, we need to export some things out of we need to maybe check out some things out of the repo like data brick stuff, work on notebooks, etc. Work on some job etc. and then we need to move it to another environment right now.
(1:32:44) Yeah, we can use APIs to do it. So datab bricks has like a you know very very rich set of APIs. Um so you know for every almost every system every subsystem within the platform right you you have some kind of API. So for example on the job side on the compute side how do you create a cluster? You can create a cluster by using an API.
(1:33:08) How do you create a job? Uh you can go into your workflow section and create jobs. uh if you want to anything within Unity catalog that you want to do so it's got a very very rich sec rich set of APIs you can interact with the APIs to do these jobs and to do these export of notebooks import of notebooks but kind of gets tedious because there's a lot of scripting involved so what datab bricks created is this asset bundle kind of a concept and what this asset bundle does is essentially gives you a declarative way to say what your project comprises of.
(1:33:41) It comprises of some jobs, comprises of some compute, it comprises of a folder with your notebooks in it. And then when you say deploy that dab right to us some other uh environment, it can take that package structure, you know, and and push it out to another environment and create the job and the clusters in that environment without you having to do it.
(1:34:03) So it forms a very kind of a portable way uh of of you know bundling your project in one kind of a entity and then using that bundle to actually move things from one environment to another. So it's kind of self-contained with all of the assets that a project needs. So let's see how that kind of works. Uh and this is going to be all demo kind of stuff.
(1:34:31) So let's see if I I can go to my demo section maybe it's yeah so here uh before I do that right let's show you some things here so datab bricks also has a CLI uh so the CLI uh can be used to interact with the databicks platform from my local terminal. So I have a local terminal. I have my datab bricks environment somewhere in the cloud. Now I can interact you know locally to the cloud by using these what we call as the datab bricks cla. Okay.
(1:35:12) So let me actually activate data bricks so that it has the right datab bricks. Yeah. And now what I can do is I can start to interact with my uh environment. This profile essentially is telling which workspace I'm interested in. So as an admin you might have thousand workspaces, right? So which workspace are you actually communicating with is what this profile is telling me.
(1:35:35) Okay? So there's a way to set up this the this profile. Um and now I can go in and do anything right. So I can say clusters list for example and it'll go in and give me back a list of clusters that are there on that particular on the workspace. Uh so similarly there is another thing called bundles. Okay. So this is datab bricks bundle.
(1:35:59) I can create a bundle here and say in it and it'll say what template should I use. Should I use a default python template a default SQL template? So on and so forth. I can say use a default Python template. I can say this is uh you know our word uh labs and then I can say does do you want to include a stub? I can say yes.
(1:36:25) Do you want to include a stub for a declarative pipeline? Do you want to include a stub for a sample Python package? These are all artifacts that you would normally create during a development process. Uh do you want to use serless compute? I'll say yes. And what this does is if you notice here it'll create a folder called Harvard Labs and under that it creates all of these entities right so it creates this databris yl file which is the important file and then creates subfolders for source for tests for resources etc etc. Now this datab bricks
(1:37:02) YAML file is where you are packaging the entire thing as a bundle. So it's a YAML file that has you know for example here I'm I'm creating all of my jobs which I'll show you in a second. Uh and then my targets do I what is my dev target look like? What is my production target look like? uh any kind of art uh you know characteristics or attributes of the dev um you know environment I can put in here and then I can refer to it as a target right so I can say datab bricks bundle do something with this target or datab bricks bundle do something with
(1:37:41) this target and then it knows which target to attack and uh work with okay so with that background let's go back to the dabs folder because that's kind of a little bit simpler and Here I have a Python file here. I have my YML file here. I have a resources folder and I have my source folder. Right? Um so let's look at the YAML folder first.
(1:38:07) So vi databicks.yamel. So here in the YAML section I have my bundle name which is like a just like a friendly name. I'm including under my resources folder this thing called demo jobl. Okay. And what what this does is it takes my job that I've created. And just to refresh, right, a job is nothing but a um uh anything that you have as a as a as a uh you know as a uh orchestrated job within data bricks.
(1:38:42) So you might have like a set of tasks that you have to complete. Each of this might be pointing to a different notebook, right? So how do I take the definition of this? I can come in here edit as YAML and I get my YML file for this particular job. So this YAML file is exactly what is used here in this particular YAML uh section.
(1:39:04) So I took my job definition that I had created. I kind of got it as a YAML. Now I'm putting that into the bundle so that the bundle understand next time you want to create the job in some other environment use that definition to create it in. Right? So similarly I I can also put things like variables because one environment I might have behavior a little different from another environment and all that can be controlled by variables like names of projects or catalogs or whatever the case might be.
(1:39:35) And then here I have my cluster ID. This cluster ID is a variable. I'm looking up a specific cluster name here to get this cluster ID as a variable. And then I have my target deployment en uh target deployments. Right? So under development I have which workspace it is. In this case I'm working only on one workspace but if I uncomment this out for example this could point to a completely different workspace.
(1:40:02) So I could be working on one workspace and exporting it out to a different workspace. Right? So it's like that. So similarly in the production side I have my mode is production. what workspace I'm working on and I can override things that I did in my previous jobs right like I have my jobs definition but for dev I might want to override that with certain values with production I want to override that with certain values I can do that overriding here so as you can see this YAML file is very very flexible and powerful where you can define what your
(1:40:37) cluster is what your jobs are uh what are the overrides for a specific environment and all of that right? So let me quickly show you what that resource looks like. Um it's nothing but the resource that I showed you earlier that you can export from your UI. So in this case I have two tasks.
(1:41:01) One is called create bronze table. The other is called create silver table. Create bronze table points to this notebook. Create silver table points to this notebook. Both are in the source folder. Right? And they take it takes parameters which means that it takes an input. These two inputs these two inputs are driven by variables again.
(1:41:21) So uh very flexible you know to customize it based from one environment to another. Uh in the source folder I have those two notebooks the create bronze and the create silver. So now that I have everything in place, I can come in here and say something like data bricks minus minus profile. I want to do this against my E2 demo field workspace.
(1:41:49) I can say bundle uh validate, right? So it'll go in and actually validate, make sure everything looks fine. [clears throat] And because I'm in the dabs folder, it's looking at the YAML in this folder. Okay, so it's going through the YAML. It's going through all of the subsections in the YAML and and then it's going to do a uh validation and says validation is okay.
(1:42:16) Now what I can do is I can say now I want to deploy this right. So I've developed this locally with against my development but this time let's say I want to deploy to a different development environment. So I can give the target name development to point to the development section of that YAML and say deploy it.
(1:42:41) So now it's going to take the development section and then look at all the overrides there, etc. Look at all the variables, bundle that thing up and send it up to that environment and and create everything that's there in that YAML, right? It'll send those two notebooks up there. It'll send the it'll create the job with the silver layer and the uh bronze layer.
(1:43:05) Uh it'll create the compute if it's not there and then uh you know uh work on it. So the deployment is complete now. Now if I go in and check in my environment which is this E2 demo I was pointing to this environment. If I go to the job section here I should see that there was a job that was created. So this job development demo3 dab just got created.
(1:43:30) And if you look at the task you see the create bronze table the create silver table. This points to a notebook. this points to another notebook. So everything like you know you can see the automation kicking in here where things are getting you know automatically generated um from here. This is different from Terraform, right? Terraform generated the whole workspace and and in in a certain predictable fashion, but this now is pertaining to actual projects, project artifacts, right? So that's where DABS comes into
(1:44:02) play is how do you take projects and move it from uh you know one environment to another as part of your DevOps process. So now that I have it there uh I can also run if I want to run that I can also run it right. So I can say minus I can say run minus t development and which job do I want to run.
(1:44:26) So I can say I can give it the job name. I know the job name here is called demo03 job in the YAML file. So I can run this and what this will do is it'll actually start kickstart that job within that environment. So, if I go in here, um, that job should show up here in a few seconds. So, it's running. So, there it is. So, you can see that it is starting to run now.
(1:45:06) It's doing the create bronze table, the create silver table and it's actually running the job, right? So from a you can think about this from a unit test perspective. So you want to deploy it and then run some unit tests against it, get back the response to do something that you want. Uh so in this case it's just showing a simple example of actually running the job.
(1:45:24) But that job could have some unit tests within it. Um uh that it's actually doing some tests against. Now let's assume that uh you know let let this complete and then uh I'll show you the production part. So this was this was let's say in a development environment. [clears throat] In this case I'm going to use the same environment as a production but like I said the dab could point to a completely different workspace uh for production if if need be. Okay.
(1:45:58) So this completed uh so it it you know it did it got a successful message back but now if I want to run it in production I can say hey I want to deploy this but now I want to deploy to my production right and this profile could be pointing to a completely different environment in this case I'm going to point to the same thing uh and then deploy it in production so now it's going to use the production section of my YAML file and the overrides within that to customize the the job to that.
(1:46:32) Um so you can see you know it becomes a pretty powerful uh CI/CD DevOps kind of a mechanism to manage datab bricks level assets right things like jobs clusters you know notebooks uh pipelines ML models um data bricks apps all of that so here deployment is complete so if I go in here now and look at my job section I should see a new job here yeah right here production demo3 dab was created.
(1:47:04) Uh the tasks again should be the same but this time it created it in a different production environment and I can run it if I want to right. So I can now run it to do unit test integration tests etc and get responses back. Um so that kind of is the idea behind this uh you know this whole data bricks asset bundles. Okay.
(1:47:28) So forms a very kind of a core component of supporting CI/CD and DevOps uh within that data platform. So there there's a variety of mechanisms here. One is the git folder that gives you direct integration with uh source code repo. you can bring that down to your local work locally and then um use uh uh datab bricks dabs for example to move stuff from one environment to another as a complete packaged unit.
(1:47:59) Okay, so that was the end of the demo. So let me pause there. Uh I think we have like a few minutes. So if there's any questions I'm I'm I'm I can take it. Um any observations, questions, thoughts, feedback? You guys do you guys use uh uh CI/CD DevOps in your organizations? What tools do you typically use? Okay, good.
(1:48:45) >> Hey, Ram, I had a question about like integrating the the DAB utility into a a pipeline, >> right? >> How does that work? >> Yeah, that's a good question. So if you go into data bricks uh tabs uh I believe there is a section here where they have a sample uh GitHub action right so yeah here so yeah this is this is the dab But then here you can see this is like a this is like the uh Azure DevOps pipeline that that uh is the YAML for that.
(1:49:41) So this is part of your entire uh you know uh uh your your Azure DevOps pipeline. So this Azure DevOps pipeline as you can see here um you know it has some steps that are done. So this this trigger is when it's released. So when you whenever you click on the release uh a branch and release it uh as a release target then these steps start to happen right.
(1:50:07) So the first thing it does is it checks out the the repo into the build server and then cleans it up. And then out here you can see that it does like a um uh datab bricks dabs kind of a uh uh hopefully this has it uh because this might be using some of the APIs. I don't see the dabs here. Um >> I saw a bundle mentioned. >> Oh, you did see. Okay. Okay, good.
(1:50:36) >> It was down down low, I think. Maybe search for bundle. I'm not sure if that's the same. >> Yeah, there is uh there is an example that shows exactly how it's uh done here. Let me see if I can find it. Uh tutorials. Yeah. Uh hopefully this has it. Uh Python will build I'm not able to find that example here
(1:51:44) but >> well that that's okay but it looks like you using that command line syntax you can you can >> yes using that datab bricks bundle command here you you you will be able to put that in the DevOps pipeline And then in that pipeline right you will have the actual um uh uh you will actually call that uh uh datab bricks asset whatever I did from the command line here datab bricks bundle deploy datab bricks bundle run right you can call the same command line utility from within there because part of the pipeline what it'll do is
(1:52:19) it'll install the databicks cla also in that build server so it'll [clears throat] install the databick CLI and then follow the same commands that I did here manually within that build server to actually deploy it. >> That's cool. >> Yeah. >> And I I suppose the assistant is smart enough to help you define a bundle too.
(1:52:40) >> Yes. Exactly. Yeah. So that is uh another very good point because the databicks uh assistant within the notebook can help you you know configure that bundle that YAML bundle very easily. If you think that that YAML bundle is pretty verbose, etc., you can use the uh you know the the uh um the YAML file to uh the the assistant to help you.
(1:53:05) They're also introducing UI based uh helpers for bundles. So you'll be able to do stuff within the UI, you know, configure the bundle within the UI using UI rather than having to go and manually type in those YAML. So yeah, both of those mechanisms are very good mechanisms. one is datab bricks assistant to generate the YAML file for you or um there are some private preview features where there is actually UIs that will help you build the uh YAML out.
(1:53:33) >> That sounds really useful. [clears throat] >> Thank you Ram. Yeah, no problems. >> Nice work. >> Yep. Thank you. >> Hey, thanks. >> Well, you want to close it? >> Yeah. Well, yeah, let's wrap up for tonight. Um, thank you, Ram. And, um, and I hope everyone has a nice Thanksgiving, and we'll see you next week.
(1:54:10) >> Yay. Happy Thanksgiving. Bye. >> Thank you. Have a good night. Bye. Bye. Bye.