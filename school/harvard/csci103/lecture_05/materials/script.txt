103 day5 - YouTube
https://www.youtube.com/watch?v=v6CBUzKtKUk

Transcript:
(00:01) All right, today we are into lecture five and we're going to cover data lake. Um, as usual, we'll review um a previous class. Just make sure some concepts and some words stick. And the main concept in a lake of course is how do you hydrate the lake uh so that it does not turn into a data swamp eventually which means pushing data is great but you need to be able to pull data out from it as well.
(00:36) And so the two ends of the pendulum here are data silos in where um traditional data mods and data warehouses have landed us into. And then if you uh swing right too quickly without due consideration to governance and um access patterns and quality and so on, you can land into a data swamp. Um and so it's um uh what are some of the best practices? In today's lab, we are going to look at just delta as a format as a protocol.
(01:03) Um and sometimes I may make the mistake of calling it format, but I actually mean a protocol because the underlying data format is park. So Delta is um uh right on top of it uh to give it certain uh uh guarantees like the asset compliance, the schema evolution, the schema inferencing, the time travel and all of that stuff. So always refer to it as a protocol to be able to grapple with to tame the big data in a distributed computing environment.
(01:36) uh we have um much more interesting use case around customer 360 uh which we'll probably not have time to cover today but Ram is going to take care of this in our uh Thursday section where you will actually look into uh full-fledged orchestration of um multiple data sources of different types coming in through the medallion architecture of bronze silver gold and we are inching towards a real use case um real world use case.
(02:05) Uh so uh what does DAG stand for in spark word world? DAG should be at your fingertips. So what is DAG? Directed cyclic graph exactly. Yes. Uh directed means there is it does not go both ways. There is a certain order of things. As cyclic means there cannot be it cannot have cycles and of course it's a graph because it's got nodes and edges and we talked about spark with its lazy uh transformations and its reliance on memory.
(02:38) So DAG is a very very important construct. Last class uh we just took a little bit of a peak at what uh a command that you type on the notebook translates to in terms of spark job. So what does it consist of? A spark job consists of many dash and then those dash will consist of many dash stages and tasks. Exactly. Um and each stage consists tasks.
(03:09) Um you didn't get a chance to kind of see the monitoring infrastructure but some of you did ask how it is done and we kind of talked about it. Does anyone remember uh that if you were on traditional compute and you had um uh access to view it uh where would you see? So infrastructure means it's not your um datab bricks constructs it's the underlying cloud constructs.
(03:37) So there it would be your storage your which is like cheap S3 or blob storage is like really ADLS is cheap the real thing is compute uh so your CPU your network um latencies or um you know how hot your memory is going. So those will be the resources that is going to throttle um or cause some bottlenecks in uh processing large files. So, anybody remembers? If not, no worries.
(04:09) It's ganglia metrics. There are different um uh dashboards that you could use. And today, I'll show you a snapshot of what it looks like. Um you may have heard uh the term schema on write and schema on read. Uh does anybody know what a schema on read means? And uh what kind of a data store exhibits this property? You guys are reading the book, right? Because you know some of your concepts concepts will get clearer that way.
(04:42) So schema on read is you dump whatever you feel like you don't care when it is time to consume when you're trying to to read then the schema all of a sudden becomes very important. So obviously there is a problem with this but data lakes um right from Hadoop days which popularized the concept of data lake did exhibit schema on read which is why injest became super simple but consumption became a nightmare.
(05:12) Uh the opposite of that is schema on write which is what your databases typically use. You know you cannot write into a database without a table. You cannot write whatever you feel like. You cannot add an additional column or drop some things. There are very strict uh rules around it. So that is a schema on write.
(05:30) So every time you write it you the schema check happens. Here when you're reading that's when a schema happens. So in in those days this was very novel and you know perhaps people took too much of advantage of it and kind of misused it. Compartmentalized data leads to which is what the data lake solves. We've talked about it a couple of times. Data silos. Exactly.
(05:57) So where you know every organization will have a little uh bit of the information and your customer data is spread everywhere and nobody has a holistic view of it which is why we will do the customer 360 in the um lab example. Um and today also we'll cover a little more around um you know just the um intuition around these data links and data silos.
(06:22) So ungoverned data lake will of course lead to data swallow. Exactly. So these are two words you should remember and think of a pendulum and kind of remember it at both the ends of it. And this should be everybody's dictionary. Data warehouse plus data lake is a exactly. Yes. Very nice. Um now we talked about partitions.
(06:48) We talked about spark partitions. We talked about file partitions and the difference between it. One of them is um the engine automatically grouping data and sending it to the worker nodes. The other is um um you as the user when you're designing um your tables, you're modeling the data, you have an idea about um you know how the query patterns are going to come, you partition.
(07:12) Uh so is it a is this a true or false? You use a column whose cardality is very high to use for a partition to help you improve the query performance. No, exactly. Because it kind of defeats the purpose. You'll have too many partitions with very little data and you know the file skipping or partition scripping or partition pruning as we refer to it uh is is not achieved if you have too many of those.
(07:43) Um metadata we talked about um again the need for metadata. Data is wonderful and we collect it all over the place but metadata is also needs to be very rich and it needs a management of its own. We talked a little bit about unity catalog uh which is where metadata on data bricks kind of resides and it's in the control plane.
(08:01) We haven't really talked too much about the data plane and the control plane because uh in some ways uh not spinning up your own clusters kind of um uh you know isolated you from all those nuances. Uh but it it would probably be a good idea to show you the um the original architecture and how serverless is definitely an in thing um where we are trying to make it as simple as possible for the user. Uh but I've kind of digressed.
(08:30) Metadata brings helps to bring better discoverability. People come to one place, look up uh some data sets or that they want to use for their use case or they might just be searching to see uh a little nugget of gold in piles and piles of data sets. So discovery is one. The other is um governance. Exactly.
(08:58) because not all data should be viewable or you know should be um you shouldn't have all privileges to all kinds of data. There will be sensitive data. So access privileges is important um whether it is row level or column level. We look at it a little um more later on which is like finer grain. So once you say yes you have access to this table maybe within that table you might have certain rows which are sensitive because it's meant only for HR people or maybe it's meant only for the finance people there may be some columns that could be sensitive because it might have a social security number or maybe an email um which is okay for an auditor to
(09:31) see or or somebody else who is privileged but not for everyone. So with governance is supremely important and it is through the metadata management that governance can be established. Uh so discovery and governance unstructured data um we saw that uh databases weren't very good with uh unstructured data but something else was better at it. So that's a quick one.
(10:02) Actually this is a little big question. Yeah. Yeah. It's data lake. Uh ideally all of this is in um cloud storage. So somebody could have just said cloud storage but we here I gave you the hint of like it cannot be in a database. It cannot be in a warehouse because they are not very good at it.
(10:20) It's data lakes um which actually introduced the construct of like semiructured and unstructured data being first class citizen along with alongside structured data. Um we looked at a spark job but now this job that we are talking about is more as a workflow as you are building up your code in your notebooks.
(10:44) There might be a data engineer doing something. There might be an ML person doing something. There might be a business analyst um doing some SQL dashboarding. So all of this put enter is going to constitute your use case and that's your full pipeline. So this job that we are talking about is from a pipeline perspective not the spark job right um sometimes the code that you are writing uh may need to be put on schedule so you and from the those of you coming from Unix world or otherwise would have heard about the chron expression uh which will say like so many minutes or so many
(11:22) hours or the third week of uh the first quarter whatever you know you can as complex as you want and the chron expression is extremely uh robust to support it. So you can put your code on schedule but there are a few other things like a file trigger. Um so some other event could wake it up as well.
(11:48) Um the older name for this pipeline used to be workflows and sometimes they call now they call it as a job. So bear with me. The full thing stitched is the job or the work line workflow. Um and pipeline is like a DT aspect of it. Uh so we'll come to DT or LDP in the next class. But for now just think of this as a job. Job has got multiple nodes or tasks.
(12:13) Some of these tasks can run together because they are independent. Some of them have dependency. So in this example for instance um you have click ingest um which might be calling autoloader to ingest clicks. Somebody has come to your website and is clicking all around. You need to know where they are clicking.
(12:32) Uh and then uh there is an orders ingest. So this orders ingest and this click ingest are completely orthogonal activities. So there's no dependency between them. They can happen independently. But you want to sessionize the user. Um, you know, I've been there. I've been tinkering with it for 15 minutes and then I get up and go away, come back after 2 hours.
(12:56) Then it's going to be a new session. So there might must be some logic as to what is the period of inactivity after which my clicks do not count as that session. They contribute to a different session. So sessionizing of these clicks can only happen after you have ingested the click. So this is a dependency.
(13:17) Um then once you have the orders and the sessions then you can match uh maybe build some features on it. Maybe people like the purple hats or the blue shoes or whatever. Um you may want to persist these features to be used elsewhere or you might use a training pipeline. Now these features could be used during inferencing as well.
(13:41) Um the long story short is that some of these tasks uh as part of your job or your workflow are independent and some of them are dependent. So you will see that uh from this there are two tasks which are forking. This job of course has to finish but these two can run independently. Now this can run on a schedule. So at a certain time you can run or it might be continuous um which is more expensive because compute is always available and running. Uh if it fails then there should be a retry mechanism.
(14:11) There was a temporary glitch in AWS. It failed. It was not on account of human error because the pipeline ran perfectly fine yesterday. There's a timeout. some uh thing happened in the either in the environment or in the code which caused um a timeout like you were reaching out to a rest API somewhere but the other endpoint was down and so there was a timeout and you know nothing else could happen.
(14:41) So you need alerts to kind of let the owner of the pipeline or the stewards of the pipeline aware of what's happening. Um so all of this definition around the job uh that includes not just the code which is in your notebook which we consider as very precious but what kind of resources were used what were the permissions around it and all of this put together is what defines that workflow and this also looks like the DAG right so this is multitask um we have new feature called repair and run so for instance if you finished all of this and took an hour uh and this failed because of some silly error on your part. You
(15:18) don't want to wait one more hour. So if you do repair and run, it knows to pick up from this point onwards. So it's going to reuse this, which means you save time as well as compute. Um this is shown on the portal. It's visual, but you can do everything that you see on the portal.
(15:37) You can through through APIs and CLIs. Uh so later when we talk about DevOps capabilities we'll introduce you to um the data bricks CLI commands. So uh where uh what you created as part of that free edition is your own workspace. So that's your layer of isolation. uh earlier um semesters we had one large um data brick sponsored uh workspace and all of you were given um users you were you were onboarded onto the workspace which means you had your house there but one user cannot see any other users content so that multi-tenency isolation was already built in now what is the
(16:25) main purpose of this workspace is to house some compute so when we say cluster, it's essentially compute. Um, you can have multiple uh clusters running. One of them could be running your job one, the other could be running job two, maybe one cluster is being reused for two jobs. All of these uh possibilities exist and not all jobs um will have the same needs.
(16:59) uh earlier when we were opening up creation of the cluster there was some important decisions that the data engineer had to make in terms of what kinds of instance type to use. So when you're on AWS you'll have a certain types of VM nodes. When you're on Azure, you'll have a slightly different naming. The concepts are the same but maybe they're called differently and so you need to be able to translate it.
(17:17) Uh but when you're using serverless uh it's much easier. So when you start out it's going to ask for a t-shirt sizing and based on that t-shirt sizing you're going to get that many number of nodes and if you don't use it for some time it auto terminates. A lot of the things are simplified for you. We were talking about the infra metrics infrastructure metrics.
(17:36) So it's around CPU network throttling and memory use. Um now sometimes if your jobs are running very slow um it means that maybe not enough uh resources infrastructure resources were done apart from like bad code these are some other issues. There could be a spill.
(17:59) So what does spill mean? When spark is very memory hungry but when the amount of memory that you have given through the nodes is not sufficient then you it has to save the result somewhere. So it'll go and write partial result into the disk, come back and do some type of computing. But remember every time you go to disk and come back, it is an expensive operation.
(18:18) So a spill should be avoided. Um but sometimes uh when you you're you do not know and you are having these spills because of memory pressures, you'll see sluggishness in your jobs. Shuffle we had talked about earlier. Does anyone remember what is shuffle? Is it spreading the task to different nodes and then bringing it together later? Um yes. So shuffle is when uh you have asked me to sort K.
(18:54) Um but some of my K data is with my neighbor. You have asked him or her to sort N. So I have to not talk to my neighbor to say hey if you have any case can you give me because my job is to sort case. So that uh communication between the nodes to exchange some information or data is what is shuffle and that's expensive.
(19:19) So if if the in the beginning when the driver was allocating the chunks if it knew exactly which how to partition and how to send the data there would not be a need for shuffles. So and now that doesn't mean shuffles don't exist. Of course they do. But if you can avoid it that's better. So if you you have your partition strategy or you write your code, you make sure there is no data skew and all of those other good things and you if you can minimize shuffle then your job sluggishness is better.
(19:45) Skew and straless. So these are all s words but they are all very different from one another. skew is um maybe if you're considering say United States and a very small country like um uh say um Indonesia is not by no means a small country but let's say Indonesia so the amount of sales or the transactions that happen there is going to be very different from the United States and if you have just country as a partition and you've got these two fields then one node is has to do a lot of work and the other node has very little data so that
(20:21) node is going finish and it's just going to wait because the cluster is still running. That node is idle, doing nothing, taking up compute. Maybe it might um you know uh scale down a little but an active task is going on. So it's it can it dare not scale down um and this job is still struggling.
(20:45) Whereas if you had done uh like country with some other tribute um like um uh may maybe the first two letters of the state or something and then distributed perhaps uh the all the workers would get equal share of work here. Some nodes are struggling and some nodes are taking it easy. So that's that's what means by skew or stragglers uneven workloads causing delays. um small files.
(21:11) We talked about how optimized does file compaction and how zordering and um um liquid clustering um actually physically changes the uh file data layout. And so the second one is different here. We talking about very small files which need to be compacted so that you don't go to disk so many times to pick them up.
(21:37) you go once and you pick it up assuming it's an optimum size because there is t task overhead from the IO operations. Again, this is just to give you uh uh an intuition and understanding of um how to debug. Uh you've written your code perfectly fine. Your workflow is good, but your job is not uh is is is struggling to finish and you feel that it should take only 25 minutes, but it's already an hour and it hasn't finished.
(22:00) So you start looking in the metrics to see what's your utilization, how much is it struggling, which of this is causing it to uh fail and so on. And I told you that um today we can't see the ganglia metrics because of this uh serverless version that you are on um but uh this is an example of how you would see a multi-node cluster.
(22:24) You can click on each of the nodes and look at the graphs for uh memory, CPU and uh uh network. IO bound operations are also um uh very tricky because you see your job stalling but your compute is like uh your available compute is like 80% 90%.
(22:46) So throwing more nodes at it or throwing a larger node at it is just going to cost you more money. It's not going to make your job any faster. So uh some perspective um question now. Mhm. Uh on on the spill issue like so what would be the strategy to like avoid it from uh happening like is there any calculations or something that we run before we allocate the memory for the nodes or you just set like a high memory by default so it never No, it's the other way around.
(23:19) you take a small sliver of the work and first see what the characteristics are. Nobody can kind of fully predict exactly how much memory it's going to take, right? And then you say, okay, for this much of data, this much of memory is needed. Um, so maybe when I double it or triple it or quadruple it, then that much of the memory is total memory is required like the driver node plus all the worker nodes. And you you go backwards.
(23:45) you in fact become very stingy and then slowly start to because you don't want wastage. Okay. Mhm. Got thank you. By now this diagram should be you know back of the napkin type of diagram for you. Um batch and streaming together as in the cup architecture raw data landing as is in the bronze which is your historical injection.
(24:10) Sometimes you may have to go back to it for uh reprocessing. Silver is where you have done a lot of heavy work. A lot of compute resources has gone into filter, clean, augment. These are where the joints happen, the lookups happen. Um and things begin to make sense. Gold, you can have multiple gold layers uh for different use cases also of the same silver layer. These could be views, these could be materialized views.
(24:37) and your consumption whether it is AI or BI could could tap either from the gold layer or the silver layer. Uh about the memory um for serverless, do we still have memory? And if so, is there an option to auto adjust? For example, every time it runs this job, it also runs this debugger in a way and looks at it and next time it basically is there, you know, in S3 there's intelligent earring. You have something like that.
(25:10) Here you have t-shirt sizing like say you have a hunking job and you just chose an extra small maybe your job will not run at all and you would go into the query history and look at the query plan uh or the statistics like on how it is performing and see that it's uh it's not able to move very well but we still do not have the ganglia level metrics um uh at the serverless level because serverless is a newer offering from data bricks Maybe at some point we would have uh better dashboards. But the idea is um most regular uh data engineers get
(25:46) overwhelmed just looking at it. They just want something out of the box which is going to work for them. Uh so um if you specify a large cluster and um it's going to have um multiple uh nodes behind it uh which is going to balance and when it is not needed it's going to go away.
(26:09) So I'm not answering your question directly because no you cannot control the memory uh the same way in terms of computing it and uh looking at your results. All you have is a t-shirt size to go with. So you have to understand your workload and your future capacity to say that um I may be able to scale up or scale down and later when we talk about lakeflow decorative pipelines you'll see that that infrastructure management gets a little uh easier or a little smarter.
(26:40) Okay. Um data consolidation tools. So since we said that the whole purpose of the lake is to break down the silos and bring in different sources of data and unify it and give you a single view of the truth um data consolidation can um happen in different ways. Last week we looked at the autoloader. So continuously you are bringing in file.
(27:05) Um it's very scalable panabyte scale. Uh people have been using it for years now and it is streaming friendly right so it is a spark. Um remember the format there is cloud files. So minute you see cloud files you know autoloader is being used behind the scenes. Copy into is a SQL option and it is used for one time uh bulk batch injection. It's got very simple syntax.
(27:28) It's almost like um um like similar to autoloader uh except that um you don't use it again and again. There's no streaming concepts. It's like one time big uh bulk batch injection. Um the other thing is it is item potent. So say you ran it and you rerun it again, it kind of knows that you ran those files and it's not going to do damage as opposed to a pure batch uh in which you might end up with duplicates. Local file upload.
(27:58) I'll show that to you on our screen. Uh and then DT or LTP is like um our newest generation of um a descriptive uh ETL which has got quality constraints built into it. Um and you know this is going to become like our def facto standard going forward. And by the way all of this um is open source uh including DT or LDP. with that.
(28:24) Should I actually show you what we're talking about or maybe go a little further? Maybe just show you here. So, in data u engineering, you've got this data injection and um you see the these are some connectors that data bricks provides to connect to Salesforce because that's a very popular source or SAP business clouds or workday or service now, Google Analytics, SQL Server.
(28:50) So some very common ones are here and this is to take a simple file and just upload it. So if I click here it's going to say drop the file. So if I have it somewhere here I'm just going to drag it but you can have a maximum of 10 files and a total size of 2GB. So these are for smaller things and um you will of course need to have either a SQL warehouse or um datab bricks runtime and the supported formats are CSV, TSV, tab, JSON, AVO, parket, txt, XML, right? But it's very quick. So if you're a business user and you you do not want to be bothered with like you know the nitty-gritty details, this could this
(29:26) could be a quick way in which you can bring in a new data set that you want to analyze. Um similarly we've um uh talked a lot about volumes. Uh volumes is almost like your S3 except that it is managed by Unity catalog and you can dump any kind of data there.
(29:47) Um but it was meant primarily for unstructured data. Uh you could create a table from Amazon S3 Fiverr and many other um partners that we have. Uh you could have connectors from there as well. So that's what I mean by the local file uploads. Uh DT would need like a session to itself. So we'll we'll look at that a little later.
(30:13) Uh copy into and uh autoloader are the big ones, but autoloader is the more uh refined, more sophisticated one. And um this lab we'll cover on Thursday. Um and it's going to be very detailed and it's going to be a a good uh run. Uh we will also look into the assignment three uh pre-eread. Um I have not uh published it because there are certain nuances to it. We will do so soon.
(30:39) And on Thursday, Ram is again going to walk you through those various sections and what is expected of you. It's going to have some amount of streaming. It's going to have a CD type one and type two. Um all right. Now a recap on autoloader. Um we said that it can incrementally ingest new files which is a big deal.
(31:01) Um it can scale to billions of files which is again a big deal and it can infer and evolve schema. Um so your schema is of course inferred when you have like structure or semiructured data right and sometimes when you know partial columns you can you can add hints as well let it in for everything else but for these I want the these to have these data types that that is also possible um it's easy to get started and um uh autoloader is again a very very u battle tested uh uh way of ingesting uh data We've been using it for like uh four or five years uh easily. Um these are the different file types that it can handle. So JSON, CSV,
(31:43) park, arrow, ORC, text and binary file formats. Um we looked at into the rescue data column so that you never lose data even if there's some parsing issue. Um the streaming or batch and detecting same machines. In fact, we went through all the data patterns also last time. So I'm going to skip.
(32:04) But to just refresh this is a batch operation. This is an autoloader operation. But this the semantics the syntax is very very similar. Uh copy into um again um as I said this is item potent. So if uh the the files in the source location have already been loaded then there's no damage done.
(32:29) Uh so you create a table and then you say copy into that table from this S3 location. Maybe it's a CSV format. You can give some other options like merge schema is equal to true. Delimiter happens to be a pipe. This is CSV. So it has a header. Now copy options also you can provide. So copy into copy options uh CSV file and additional format instructions. So this should begin to start look very familiar to you.
(32:55) Um this again because we are not going to cover LDP uh in lab today I'm going to skip over it. uh all uh it goes to uh state here is that spark structured streaming uh is our base and using that with autoloader uh is how uh LDP or DT has evolved and um because it's declarative and because it is built on top of all of these things it takes advantage of it plus it has some bells and whistles of on its own that is why structured streaming has got all of these and this is all manual whereas DT will have all of this and all of this um
(33:34) automated because it's it's like our next generation. Eric, do you want to take over now? Hello, Eric. Yeah. Yeah. Can you hear me? Yes. Okay. Now we'll look at um lakehouses and um so um data lakes um lakehouses data lakes and and and also review data silos. So we started with um uh data silos where we have um data in different technologies and and um holding different types of data but um the problem with them as on and deta stated before is that they weren't very good about um for sharing data like um
(34:46) it was difficult to access and and also just know that the the data was accessible when they were in silos. And then um and then Hadoop came along and we um we combined a lot of data together. But then um Hadoop had its limitations in terms of um managing the data and providing um being able to control access to the data.
(35:17) And then um and we've uh migrated to uh data lakes where um you can put structured as well as unstructured data into the um data lake and um then and use it to manage the data and and also be able to provide um better fine grained access to the data. So it it saves um it's more efficient, saves time and effort and cost um and um can can support multiple data sources and um importantly support unstructured data which is now 80% of the the data that we deal with is either uh semi-structured or unstructured data and can be um configured in a um coste
(36:08) effective um and scalable environment. So uh so as I mentioned the data silos are isolated um usually only accessible from a single um uh line of business within an organization and um it's because the data can't be shared. it can't be used and um provides uh results in inefficiencies for the business and lost opportunities in terms of uh using the data to help the business succeed.
(36:54) So there's um structural issues with how like how the data is stored and accessing the data. political. Um, sometimes like businesses don't want to share the data because they or business units don't want to share the data because they feel it's their data and and and why would anyone else want to look at it uh limits growth and and also um because these data silos are often proprietary systems, it can it can create opportunity for um vendor lock So we've migrated from from data silos to um data lakes because data lakes are
(37:38) better at sharing data. But um uh however if um when we when we go to the data lake if we don't have sufficient governance over the data lake it can um easily devolve into a what we refer to as a data swamp. So um so so it's on to us as data engineers to to manage the the data lake and make sure that it it's under sufficient governance and and controls so that uh it doesn't um doesn't devolve into a data swamp. Let's let's look at what that is.
(38:25) So here's our data lake on the left. Um we have um it in includes metadata. So we can we can um manage processes, properties, relationships and we can tag the data and um and then we have streamlined ingestion processes and uh the value that's provided here includes um self-service um datadriven.
(38:57) it's it's always available and and accessible. Um it scales well because of the um the infrastructure that separates the the storage from the compute. Um it's flexible and it also helps um improve the quality of the data. However, without sufficient control of the metadata and also governance, it can devolve into a what we refer to as a data swamp.
(39:28) So um and that happens when we the the metadata um management um is broken like there's there's not control over who's adding data, what type of data is being added and also um the processes for for cleaning up that data. So it can become quickly become basically a mess for the organization and and um you can your beautiful data leak um can be um basically corrupted and and um become a source of problems for the business.
(39:59) So um lost opportunities in terms of business value um takes longer to to do things with the data doesn't scale well um it's not flexible and then the quality is is degraded. So we we definitely want to avoid that situation. So um a a delta leak can help us um um meet these challenges and the challenges are it's um difficult to append data um existing updating existing data is hard.
(40:46) Uh um what what happens if jobs fail midway? How can we restart them? Uh real time operations are difficult. historical data is um keeping historical data around and being able to access it is is hard. Um large amounts of metadata may be challenging. Uh maybe um over time we we um create many uh many files which um slow slow down access and result in poor performance and then overall basically data quality issues.
(41:23) And once once um the quality of the data is degraded then the business loses um faith um in in the in the in the data lake and and um basically uh loss opportunity in terms of providing the business value. So um a delta lake um which there's a special type of data lake um can h help and delta um provides acid um compliant transactions so we can um ensure that the transactions are successful and and um consistent.
(42:05) uh all the transactions are recorded and um so we can re go back in time u using this concept of time travel. We can go back to a specific point in time where we knew um where we had some sort of checkpoint so that we knew the data was good at that point and then restart if we need to. Um Spark is designed to handle uh very large amounts of data.
(42:31) So that's um uh built in can it supports um open formats um specifically the parket format um and also um the data is cached um in memory. So that helps um with performance and um and the the metadata um exists along with the with the actual data. So that um that helps ensure that we have the um the management necessary to help again to help prevent us from going into a a data swamp scenario.
(43:15) Uh we have um support for schema validation and evolution. And uh we can also um it also supports something called data skipping which allows us to through um through stat statistical analysis of the data we can skip over certain records and also zordering is supported for efficient um partitioning and and and indexing of the data.
(43:49) Now let's go back and compare um the a data warehouse versus a data lake. So um we have a a picture of a Ferrari and and um and then a tractor trailer. Uh so you can you can kind of think of the um Ferrari as being very fast and nimble but um limited in what it can carry. It's basically yourself and a passenger and that's it.
(44:22) And you can zip all around town and and do things very quickly, but you're kind of limited in terms of um what what what you can carry and what you can do with that. Where the um and that's that's analogous to the warehouse. So the uh tractor trailer on the other hand is very large and and fairly fast, can zip down the highway with um a large amount of um storage and and also you know different types of can carry many different types of things um in in that um thing.
(44:59) So that's a good way to think about a data lake. It's more um maybe not as fast as a warehouse but but it's large can do a carry a lot and the speed is is not as good as a warehouse but um can still get the job done. So some differences um with the schema on right where a data warehouse is um structured uh only supports structured data.
(45:30) So you need to know the structure of the data before you can write it into the data um warehouse where with the data lake um it will accept any structured um semistructured or unstructured data and and even structured data can accept um changing schemas. So it's more flexible and and therefore we can support um rather than the ETL process, we can support what we refer to as the extract load transform where we um basically land the data and then um and then read the data and and analyze the schema at that point and then transform it.
(46:13) So it's more flexible in the the types of data that it can receive and and then what what you can do with it. Um the volume as I mentioned the volume of the uh data warehouse is limited but the um data lake is orders of magnitude more because of the way that the um the the storage mechanism is separated from the um from the compute and also that the storage mechanism is is um very um scalable and also um cost effective.
(46:47) Uh with warehouses we um do things in batch mode where a lake can support both batch and streaming. Um again the the type of data is structured versus in the data warehouse where um the data lake can be structured, semi-structured or unstructured. Um the data formats for the warehouse are generally um proprietary um and um specific to the technology used to support the warehouse.
(47:26) Whereas a data lake is using um open open formats and can support um many many different types of formats and um the cost of it warehouse um scales with the volume of data. So the the more data you have, the more expensive it's going to be because usually the the compute um needs to scale with the um the size of the data. So that can be expensive where with a lake you've separated um the data storage from the compute.
(47:56) So you can you can store large very large amounts of data um efficiently and and in a cost-effective way. However, um as I mentioned the the the if you're do if you want a very fast response and and you don't have a lot of data and it's structured then the warehouse is the way to go because you can get responses much more quickly than you can with the data lake.
(48:22) But um but but then the data lake is providing you a lot um a lot more flexibility in in what you can do and and the size of data that you can work with. I'll stop there and see if there's any questions. Okay, another view of um data lakeink versus data warehouse. So or the data lake is affordable um supports unstructured and structured data.
(48:59) user users are generally uh data scientists um and provides um usually used for um supporting AI and ML modeling and and also schema on read is important because the the data lake can ingest any any format of data but when you read it out of the data lake then you have to analyze the the format of the data to understand it.
(49:27) So in contrast the warehouse is expensive only supports structured data users are generally business users that want to do um business reports or analytics and um and then the um schema on right. So you can when you're loading the data into um the the data warehouse, you need to know the the expected schema ahead ahead of time and write the data using that that schema otherwise it will be um it you'll have some sort of error. Um you won't be able to load the data.
(50:04) So it's much more restrictive in terms of what you can put into the warehouse. So um so when to use a data link we we can say well the when the the data that we're adding um we we are not sure what we're going to add to the data lake. So we we may have new sources of data that we didn't expect.
(50:35) Um when the data sets are huge um and to keep help keep the storage cost down the lake is much better. um different formats of data. We can you know put put data of any format in and um and this is useful also for um when we have raw data sets where we we we know that there may be some value in the data but we're not sure what the value is ahead of time.
(51:02) Well, we can we can collect it and store it in the data lake and then analyze it later to see um how it might be useful for machine learning or other use cases. And then um also we may not understand the relationships among the data beforehand where with a data warehouse um we know we know we have a much better understanding of the incoming data and um can can manage that.
(51:37) The data formats are much more static. they don't change. Um usually um usually for generating um business reports or doing analytics and and also um where the where the um the tables have been defined in a way to support the business needs and um yeah so um it is possible the last note here is it is possible to combine a data lake with a data warehouse by putting basically putting a data lake in front of a a data warehouse where the data lake is used to ingest the data and transform it and then um output it to a data warehouse where it can support
(52:28) business um business reports and business users in a more traditional and faster uh way than than just serving it directly out of the data lake. So, um there is there is something though called a data data lakehouse which combines the the best aspects of a data lake with um the data warehouse. So um with a lakehouse uh supports all types of data structured, semistructured, unstructured, it's cost effective. It um supports open formats.
(53:11) Um can it's scalable like a a data lakeink. It can hold large large amounts of data and do it in a cost-effective way. uh it unifies um access like it's can support access by data analysts, data scientists and um ML people. Uh it's it can because it has metadata um it can support high quality and reliable data and and it provides a simple structure for um accessing.
(53:44) So it's the ease of use is is good and that's um something that a a good good um aspect that comes from the data warehouse side of things and the performance is um high. So basically you take um a data lake and data warehouse and put them together and say okay I just want the the best best of both worlds and you have a lakehouse.
(54:15) So the um uh innovations that uh Lakehouse is um founded on um include acid transactions that help with reliability of the data, indexing for um good performance. Um access control lists and and access access control to support governance. And then um high high expectations for quality.
(54:51) So the business impacts include um data um better uh innovation, lower um cost of ownership with due to its simpler architecture and also a avoidance of vendor lock in. And um it's through automation it can um increase productivity and also reduce um security risk because of better um governance and access control. So uh let's look at a lakehouse in more detail.
(55:29) So it basically as I mentioned it combines the best characteristics of a warehouse with um the data lake and um allows us to take raw data ingest raw data and provide uh basically a curated data lake. um can connect um to BI tools. Uh um so the the data lake is open so you can access it directly from BI tools that business users are used to.
(56:07) Uh it helps um reduce um redundancy of of data and also governance by having all of that within a single system, the lakehouse. And um and that helps reduce um management overhead and reduce and and then through all of that it helps reduce costs for the business. And then um some some things to think about if you're adopting a lakehouse is what do you do with the existing warehouses? Um maybe migrate the data to the to the um lakehouse.
(56:48) Um, are are the tools all um mature and unified? And are we is it is it going to become a monolith or maybe um even um become some sort of swamp like we saw with the data leaks? Amir, do you have a question? Yes, I have a question. Um so just wanted to understand the relationship between the lakehouse and Delta Lake. Um, is using Delta Lake always required to build a lakehouse or is it one of many options? Question one.
(57:18) And then question two. So when you say with lakehouse we have the best of both worlds with as you know best of data warehouse and data lakes. Um I mean normally there are tradeoffs in any application. So how do we achieve the best of both technologies? Is it solely those the the use of the the the four benefits that you listed in the previous slide? Is that how we are getting the the best of both worlds or how exactly are we not having to deal with trade-offs? Well, it's basically combining the two two capabilities into a single platform.
(58:01) So the the um the the delta delta lake or data lake is basically um being you're aggregating the two things together. So you're not only supporting the data lake but you're also building the capability to do queries and and um with um SQL like queries with from the data lake as well. So but then we are saying there is no limit to storage and there are no limit to like types of data.
(58:31) We can have unstructured structure but then we are saying there is no hidden performance. We still get the high performance of a data warehouse. Yeah. Because well it assumes it assumes that the um data has been transformed into nicely presented tables that can be quickly queried by um business users. But but what does that nicely presented table mean? So like is that like our case is that nicely presented or is it the delta? Well, actually I I'll let I'll let um anita address that.
(59:09) Yeah, sure. I think what Eric was saying is um you with park you had some deficiencies which is why the delta protocol was invented and with the delta protocol and the unity catalog governance you now have a governed uh data lakeink with um relational like constructs. So could you do asset transactions before on you know distributed compute? No.
(59:34) And that was like that is where half the quality issues started to creep in. So Delta was like the first step, Unity catalog was the next step and serverless is what got us the performance. So if you go to the portal, each one of you have your own portal. Uh whatever you could do on a warehouse like in terms of being able to write u uh SQL uh code um visualization, you can do all of that and at this almost the same speed.
(1:00:06) Maybe the only case is when you have really really tiny ones. Maybe an oracle or maybe a SQL server might do a little better uh on a very very small size which is not the case in a real world. In a real world you will have uh good amount of healthy amount of data and there are multiple engines.
(1:00:27) So there is the SQL engine and then you'll have uh um photon acceleration and serverless uh uh compute kicking in. So that is how it feels like a warehouse. anything any of the things that you could do on a warehouse you're now able to do on a delta lake and yes the delta protocol is a big ingredient to it uh how about indexes I like partitioning I can understand lakehouse can do it but um can it do indexes um in the concept of index is not exist on a big data systems so we had talked about um clustering um and um data layouts. So those are some of the strategies like in Hadoop world we had binning, we had uh indexes,
(1:01:09) we had partitioning. Now with the way compute engines have evolved and with the way our optimizers have evolved, there's no need to do so. So at the best I could say is um uh liquid clustering with specific fields uh to identify your u wear criteria is the most that you should uh consider.
(1:01:35) It's not like uh create index on um relational database that we were used to. But um when I was thinking the data warehouse I was thinking always red shift in my mind. Okay. And um as so far as I know we can build indexes that only has like certain columns and if you actually query in the index you can get so for example there's three columns to indexes and you're only asking three columns and nothing else. It actually doesn't even go to the data it just searchs the index itself.
(1:02:08) So do you have that kind of abilities? No. uh with Zordering we could specify um like these are the wear clauses and um there are it it is a maximum of say four to five columns but the reason why it's not as as popular is because these are really really wide uh tables and you will have hundreds of columns.
(1:02:32) How many indices can you practically create? Creating it beyond four or five is going to have performance ramifications. Okay. You also cannot have two different sorting. You like if you want to have two different sorting, you create two different tables. It just Yeah, that's what Dynamo DB does for instance. Uh when it Exactly.
(1:02:59) So what it does is it's a physical copy of the table with slightly different formatted uh data so that you can get that speed. But you you really can't do it with hundreds of columns. That's not the intent. the intent is to make it more flexible and if it is negligible performance um and you're getting all the benefits then why not I guess in data bricks that is equivalent to having two gold tables with different sorting um well that is gold tables are different gold tables you're aggregating on different dimensions precomputing those aggregations but that's not what an index
(1:03:41) What I was saying that's another way to physically have two copies like if you have two different tables. Yep. Yep. You could you could have materialized views in which you premputee stuff and make it available for certain types of use cases or um uh consumption patterns and also this requires a mind shift.
(1:04:07) um certain things that we were used to in relational world you're not going to be able to do it and I do not I know Dynamo TV is still very nice very fast but every time I have to create a different query and I get stuck or I didn't have it in my global index or I didn't have it in my secondary index let me create another and let my cost go balloon up again that's like you know I feel it is cheating of sorts because they call it as index and of course the whole copy of that entire big cable gets created.
(1:04:36) So, uh um that's that's actually very expensive. And I think this picture, this diagram in the lower right of the current slide is is a good way to kind of help visualize this as well because with the um lakehouse platform provided by data bricks, you have all all different types of data um living within the data lake and then um you get the data management and governance um from Delta Lake and then On top of that, you have the capabilities for data engineering, BI and SQL based analytics, um, real-time data processing and also,
(1:05:23) uh, the data science and machine learning. Real time data does not refer to low latency. Correct? like everything in um data bricks or like still you're talking about long latency compared to Dynamo DB which is single digit millisecond. That's correct. Here it is t of milliseconds uh as opposed to a cassandra or a dynamob.
(1:05:57) But is there a way that with serverless we can get like run a query within like 100 milliseconds? Uh yes you can. In fact we have benchmarks um where hundreds of milliseconds is definitely possible. U comparable like if another performant equivalent is able to do it in hundreds of milliseconds we should be able to do so as well. It's not tens of milliseconds which is where sometimes flink and ray and all might be uh superior. There is some more effort that is going on.
(1:06:31) It's called light speed uh to bring it down even further but I wouldn't claim do you want to add something? Yeah. Um I I actually before I take this class I never realized but are you saying that there is possibly a way to run data bricks as your primary database that you're like doing the cred operation against and it will work in production.
(1:07:06) Um not for operational data sources because data bricks has uh traditionally been an analytics system. So it will keep up with an ODS but it is not used as an ODS at least not yet. But data bricks never stops to innovate. So there is a new category called lake base that it is coming up with which is essentially postgress behind the scenes but um uh built to sync with delta tables and with that um yes exactly.
(1:07:35) um mon you can come off mute and add here uh but with uh lakebase that's going to and if there is a postgress behind it for instance that becomes like an ODS it's in early stages uh we were working on it and we also acquired a company called neon which had um uh some similar technologies there right now we use it in limited storage which is uh 2 tab or 4 tab or something like that And when it is going to be at pabyte scale or larger then we can claim to be an ODS. Um and you know you can have built-ons um like
(1:08:14) data bricks built-ons for all the popular things that are today uh being executed on an a typical RDBMS system. Okay. Yeah. I I think and I think the this this is a nice segue for this um next slide about the lakehouse maturity. um this fairly fairly recent innovation. Um but this um hype cycle um diagram this this the one on the left is from 2021 and uh this one is an updated one for for this year actually.
(1:09:01) So you can see um not only how lakeouses are progressing but other other technologies as well are moving forward. So here in 2021 the lakehouse was on the just on the upside of this um innovation hype cycle. Yeah. Hype. Yeah. And um so it was gaining in popularity but um but eventually um all all technologies fall into this trough and then if if it's a worthwhile um technology then it will eventually um climb up this um slope of enlightenment that they call it.
(1:09:42) But here we are in 2025 and and it's the lakehouse has gone through the trough of disillusionment and is is about to um progress up the slope of enlightenment. And it's also this is a report from that gardener puts out and it's um uses a a a white um dot to indicate a lakehouse.
(1:10:08) And so they're they're saying that it'll plateau in the plateau of productivity within uh 2 years. So this is a a fastmoving technology that's um going to um take on uh or take off basically in in popularity with um organizations uh within the the next two years that'll be um you know accepted technology.
(1:10:37) So that's that's pretty impressive and I think um companies like data bricks have really um helped um promote this concept of a lakehouse where you can basically the idea is you can have your cake and eat it too. you can you can um you can have huge amounts of data and and and not not just structured but unstructured data as well and um and then um have have the ability to do um data warehouse type operations with it.
(1:11:11) So basically, if we go back and think about the picture of the the um Ferrari versus the um tractor trailer, basically you can think of some sort of vehicle that would um encompass both of those um like a fer maybe a tractor trailer that looks like a Ferrari somehow, which is hard to imagine, but but um that's the that's the goal of the lakehouse. Exactly.
(1:11:34) And actually what is even more interesting is look at where transactional databases are and data lakes are even though they kind of existed many many years before a lakehouse. Lakehouse is a fairly uh innovative um uh concept and paradigm and the rate of adoption has been phenomenal. Yeah. And the data lake has alreadyached reached the plateau the plateau where um I think here it was data lakes were down in the trough of of dis disillusionment um in 2021 and now they've um you know been accepted as a as a mature technology.
(1:12:12) So in fact a report from MIT suggests that almost 70% of u uh CIOS that were interviewed said that they have adopted lakehouse in their organizations. That to me sounds like a fairly high number but these are coming from an MIT report. So in theory or in a concept everybody agrees that that is how the um the data world has to evolve because it's extremely expensive to maintain so many copies of the data or maintain so many different platforms.
(1:12:43) Prior to this, people would have a streaming platform. People would have a uh ML platform, a batch mode and um a warehouse like a lake and a warehouse. And then they would go crazy. Forget the kappa and the the lambda portion here. Just keeping four or five systems uh stitched together and in sync with reconciliations and everything else is a challenge. And then to make things just more interesting came geni.
(1:13:10) NJI has its own sets of tools, own sets of things. But here we are saying that data gravity is the most important thing. Once you have massaged it, once you have cleansed it, curated it and governed it, that's it. After that, all use all kinds of use cases should be able to run from it because there are specialized engines to be able to do so.
(1:13:31) uh with the innovation of Photon which was like a rewrite of the spark and um serverless and unity cat catalog along with uh delta it kind of made things a lot more compelling. Um we were not in the or I should say data bricks did not play in the warehouse space at all. it was and uh it was invented around Hadoop era and it continued to think of itself as a lake till um the realization hit that multiple uh people in our customer base actually care only about SQL and warehouses and that's a huge market which is when uh data bricks had to
(1:14:11) pivot and start from scratch again it had some semblance of SQL just to keep things together but it wasn't compet competitive sequel. It went to the point of actually tipping the benchmarks on the most established players. So that shows like the the underlying um foundational uh layers with which this was capable for a warehouse to become a um lake is much much harder than for for a lake uh data lake to become a warehouse. Um, of course having a very strong engineering team does help and having a very innovative approach uh to
(1:14:48) technology helps. So every year um the way things are discarded and um embraced new technologies are embraced even within data bricks like they are not afraid to say like okay even though this is only 2 years perhaps this is not going to stick ready to pivot and create a new one.
(1:15:08) So Delta itself was um game changer. But Delta is not the only one of its kind. There is hoodie and there is um iceberg. So they're all similar. They all kind of grew uh from a need that everybody in the industry was feeling but it it happened independently and now Delta and iceberg are trying to come together. Not many people use hoodie so perhaps it's not as relevant. So that's one big innovation.
(1:15:32) The second innovation was uh the SQL engine. it had to be performant even on data lake scale. So that once that was achieved then moving on to governance because no regulated industry will ever come if you cannot assure um data privacy, data access and all of that stuff.
(1:15:57) Now once that is assigned then it is like we uh it was all about like can any model run can any model be managed and governed and uh can chain applications go to production fast. So just because of the data gravity all of this um following you know subsequent use cases becomes a whole lot easier. Uh it would be interesting to see lake base is brand brand new. It would be interesting to see if lakebase indeed becomes the distributed transactional database right now.
(1:16:23) This is still very very early. We cannot claim it yet but perhaps one day it will. All right, enough. We It's I was just going to point out from this graph that data engineering is also here on the graph and and in 2021 it was in in this trough of disillusionment but um nobody believed that it was a necessary thing but today it's it's basically about to reach that the plateau of productivity and has been well accepted.
(1:16:53) So I think in the first lecture we were pointing out the um the importance of of data engineering to especially with all the work that's going into AI at the moment. Um it's really important field that we're learning about. Absolutely. Okay. Okay. Sorry to interrupt.
(1:17:15) kind of just go back to um my earlier question about relationship between Delta Lake and Lakehouse. Um and I think you answered this already but I just wanted to verify. So is Delta Lake a required technology or component of every lakehouse or is it just one of the options? No, it's one of the options because as I said Delta has its counterparts in iceberg and hoodie. Okay.
(1:17:43) So you could have yeah you could have um those formats but that specific need has to be met without it you're going to land into a swamp. So once that was achieved that's not the only thing then you need solid governance and uh UC is also extremely battle tested at this point. Um and a lot of people actually uh adopt data bricks for unity catalog uh because it becomes so much easier. And by the way, Unity catalog is open source as well.
(1:18:10) So you could have it not being on a data bricks platform. It can be a standalone just like Delta, just like MLflow, just like uh uh you know a whole lot of other things that come out. Okay. All right. This was interesting discussion. Very good. Yeah. Good. Uh, Anadito, why don't you take it from here? Okay, sure.
(1:18:50) Light. Oh, sorry. Um there are some guiding uh principles. Um a few colleagues of ours wrote this uh blog. It's got a more details but we liked um the the gist of it and so decided to put at least the six founding principles here. One is that you're curating data and offering trusted data as products.
(1:19:23) Many of you come from the data world and you might be doing um um something in your organization related to it. And we will talk about data mesh soon. You will hear the terms data as a product, data as a service. Uh because data should not be a byproduct or it should not be a secondary concern. It is very much a primary concern.
(1:19:44) So when we talk about um the bronze, silver, gold, uh of course it's a terminology which is there in the industry. In fact, sometimes people also refer to the fourth layer which is like a platinum layer where you bring in business ontology, business semantics and define KPIs and things like that for the business user.
(1:20:04) But the most important thing is from this point to this point you have used compute which is expensive. Now what have you achieved because of that the quality of the data the trust that you have put in the data has gone up. So that is what data engineering is about and that is what you have achieved as part of going from left to right um whether it's three hops or two hops or one hop or what have you.
(1:20:24) So you're curating the raw data and offering it as data as products. That's the first principle. The second is you're removing data silos and minimizing data movement. you know anybody even on native uh cloud offerings if you see um why does data why is data brick so popular or why does it even exist when you know it it is just a a layer on top of the clouds because when you are using the cloudnative uh capabilities you still have to move data significantly you still have to stitch services and they are it is not intuitive so when the
(1:21:03) movement of um data is actually like a core core principle. Um when we were talking about what if you know people just get too attached to their old systems and are not willing to move. So for those we advise that have the lake in the front so that you at least bring down your cost and you you keep all your data and if you're hellbent on having serving of your warehouse, do so so that at some point you can face it off. But sometimes people do the other way around and it's it's an absolute um disaster.
(1:21:34) You can't have a warehouse feeding a lake. You can have a lake feeding a warehouse and eventually pays out the warehouse because you're paying so much extra and there is movement of data. Um, so this is the final layer. From the final layer, you might be a masking the data. You might be filtering it.
(1:21:54) You might share a portion of it to a partner. You might have some of your business users query um some versions of it. Like earlier somebody was saying multiple gold versions or uh uh views of the data. So that's good. in in any case they are still reaching out to the lake to get the data as opposed to the data being shifted somewhere else and somebody doing it.
(1:22:21) So what are apart from fragility can anyone think of other problems that appears uh when when you when you're just moving data from one system to another that could be like compatibility issues right? There are data compatibility issues like in one system uh when I say decimal it means something in another system I might have to say float um and here by default it's six precision there it is two precision all in terms of currency and all this becomes a big problem so source of truth is one you spend all this compute to curate it you keep it in one place the other point I was looking for is real time when your
(1:23:00) business user is uh wanting to get to the curated results you don't want to tell him wait that pipeline hasn't finished I need to move the data first then you can query it now you're looking at older data now you're looking at newer data no so if the data is available and it has been processed business users should be able to do so and remember we spent a lot of time to say that because of asset uh compliance the older data as it is being updated is never viewable by the consumer it is happening in a different version until the transaction is complete the user is going to see the older version. The
(1:23:36) minute the transaction is finished, then they are going to see the newer data and you have not moved it and they are getting it as soon as the data is available. Democratizing value creation through self-service experience. Now, what is the simplest way to protect your data? Don't give anybody access. Your data remains protected.
(1:23:54) But what's the flip side? There's no innovation. There's nothing useful anybody is doing to bring value out of the data. you just spent all that compute to run your fantastic pipeline but it was just a waste. So you have to um self-service and governance kind of go hand in hand.
(1:24:16) You have to enable uh people to look at the data uh use it in their um specific uh scenarios their use cases um and and see how it affects your ROI of the business like you know what can they find in the data that can help the business otherwise this is just a exercise for the sake of it and to do self-service properly to democratize properly the governance layer has to be solid you cannot have loopholes you cannot have like backdoor mechanisms and so on so forth Uh and that is the next one or uh adopt an organizationwide data governance strategy.
(1:24:47) Uh what should be your quality metrics? How should uh ownership be defined? Uh how should your catalogs be uh constructed? What are access privileges and so on. So all of that uh quality checks uh should be built in. Encourage use of open interfaces and open formats. Now we talked about how data warehouses typically uh it's only very recently that some data warehouses are able to adopt open formats but traditionally they were all locked in formats which is why they got their superior performance uh because of course they can do some hanky panky stuff and get that performance which nobody else in the
(1:25:26) outside world would know about or be able to use that data. So again it's a solid vendor lockin situation and that is a no no right from the beginning because we come from the lake perspective we want everybody in the ecosystem to contribute meaningfully uh to the data product data service uh data assets uh ML assets that you are creating and so if it is not if it doesn't have an open interface or an open format they cannot play well uh with the others so that's another and then build to scale and optimize for performance ments and cost. So if um if
(1:26:03) you're on a datab bricks uh platform typically you should not have to worry about scale at all. The platform is built for scale and whatever use case you're doing on top should also uh be considered like scale should be an important consideration because you're on a big data system.
(1:26:21) If you don't have that much of data if it's little data then go back to a simpler system. You do not have to uh use lakehouse. you you can just be on a one node system and life would still be pretty good. Uh optimize for performance and cost. In the beginning you might say I don't care I've got this much of data and Eric was referring to this earlier your warehouse will run beautifully and you will not be upset about the cost.
(1:26:47) Now minute the that volume of data increases and you look at your uh bill and get a sticker shock that is when you realize that no you should have thought about uh you know the consequences of what will happen when your use cases explode when your data explodes and so on. So build to scale have big data and optimize for performance and cost. So it can't be just cost or it can't be just performance.
(1:27:07) There has to be a balance between the two. Any questions? Okay. So, use the data lake as a landing zone for all your data. Mask data containing private information before it enters your data lakeink if you can. Secure your data lakeink with a role and view-based access control. Uh we haven't done too much on all the UC features, but we will do so soon.
(1:27:33) Build reliability and performance into your data lake by using the delta format. catalog the data in your data lake. So um cataloging means uh understanding and like having hygiene in your metadata. Um naming your tables uh properly, tagging uh your u columns and uh tables properly.
(1:27:59) So there there is so much of hygiene that data stewards need to do so that even if I didn't I'm not the owner of the data when I look at it I can at least understand somewhat what the data is for uh what um characteristics it has and what quality it has. So without that I would I should not build a use case on top of it. Now let's move to data mesh.
(1:28:26) Anybody here in your organizations have you implemented a data mesh? No. Okay. All right. So these are newer terms I would say and this was popularized by Chamak maybe uh four five years back. Um and it talks about decentralization and distribution of responsibility. Gone are those days when we had these uh Oracle database administrators and if they didn't come to work one day or they were on vacation, you know, everything halted. Yeah, we can't possibly have that.
(1:28:57) There has to be centralized rules that are adopted but decentralization and distribution of responsibility so that there is domain ownership. Uh so people the reason why there's a sales organization and a finance organization and a marketing organization is because people in there in these domains know certain things which is very specific. So an HR person can't go to marketing and start becoming productive overnight.
(1:29:21) Right? So domain uh refers to the kind of use case and the kind of data as a product or as a service that is being created and having these decentralized and autonomous teams means that they will be responsible for their business what they do within the boundary of their little silo uh in terms of like how they operate, what tech stack they use um should all be okay like provided there is central uh approval from it.
(1:29:51) So it's it's not that they're going rogue and you know building something as a silo. No, it's tied to the main organizational principles but still there's not too much of interference from a central IT team. Uh there are certain dos and don'ts at some level and then there is um some free hand that is given to them.
(1:30:10) uh responsibility is owned by those that closest to the data because they understand their domain well and they will map to these uh domains are mapped to business organizations like I said sales uh finance u uh maybe in the context of um of a company there might be like an insurance claim center or an underwriting center. So those can be different uh domains. Data is a product.
(1:30:34) So um there has to be a order of that and uh they have to have customers. So they are solely responsible for the happiness or the um you know satisfaction of these customers and that measures the success of the products. So if I'm doing a lot of innovation but my customers are unhappy then this data as a product uh concept is failing.
(1:30:58) selfs serve uh distributed and scalable so you can easily create and terminate resources on demand just like you guys are able to create cluster when you need it and when it's done it's uh gone you're not relying on a central IT team and opening up a ticket and saying can I use it from 9:00 to 10:00 now that that would be too constraining uh compute and data locality so because they own the data and because they are working in their own little spheres um in their own little workspaces they there is compute and data locality. Federated governance
(1:31:28) um governance can't be like um uh these little silos decide what their governance strategy is. No, it is defined at the central level but then it's decentralized in the in terms of like I determine uh whether per group X within my team gets access to this data or read access or read write access because I know exactly what they're going to do.
(1:31:52) So instead of a central team kind of guessing what should happen you know these teams uh understanding those kind of um uh regulations or guidelines is perhaps much better interoperability within these little um data domains uh there could be different stacks but if they are such that after that I cannot work with your data anymore then that would be a miss.
(1:32:17) So interoperability is still a very big concern and global standard standardization in terms of like naming conventions or uh dos and don'ts those things uh it's very hard to explain but there's a fine balance between like rules of the house and rules of the room.
(1:32:38) So within the within my kid's room he can probably mess up his bed a little or whatever whatever but I don't it doesn't matter as much as long as certain other decorum is followed. So think of it like that. There's global standardization and then autonomous uh uh teams. Any questions? So four main tenants of data mesh domain ownership, data as a product, self-s serve and federated governance.
(1:33:03) Now if we look at those same pillars and see what features in data bricks allows um datab bricks to function or allows lakehouse to function as a data mesh. The first one was domain ownership. So the these data producers they will take responsibility for their data and its outcomes. So it needs to be open and flexible architecture um and distributed ownership of data assets and data pipelines.
(1:33:30) So they get the data maybe from a central team. Maybe the data comes to them directly. They are going to build their transformations. They know exactly uh how the data should look like. They understand the domain. They're going to cleanse it.
(1:33:47) they're going to produce it but the contract between what their consumer wants and what they have agreed to produce that is going to be very strong and binding. Okay. Second is data as a product. So this is product thinking um data consumers beyond the source domain. So I'm creating the data. It's not just within my domain. Someone outside my domain might use it.
(1:34:06) So that that's why the data contracts are so important. uh open standards, open format, asset guarantees, versions, audits, um fresh highquality data with uh DT or LDP tables uh and several more self-service. Um you are able to add more users to your workspace and you can say what kind of role they are.
(1:34:28) Are they going to be admins like you or are they just regular users and can they spin clusters? Lots of things you can determine. So a unified platform serving these analytical workloads managed orchestration autoscaling serverless infrastructure as code. Uh usually what happens is in large organizations the central team will come up with the terraform code to spin up the infrastructure otherwise it might be abused and they will create the workspace um as opposed to free free edition where you have limited uh compute per day and within that workspace they will create a catalog
(1:35:01) perhaps but then what schema is going to be who are the users who are going to onboard what kind of permissions will they have how many pipelines they are going to have all of that is determined by the local uh federated computational governance. So discovery, access and lineage with Unity catalog.
(1:35:19) Anybody can explain what lineage means the previous steps that data went through coming to the final. So exactly in another table before and it was living in another table in a different format. Correct. Correct. So the the transformations that have happened to the raw data to bring it here uh is the lineage and I might look at the lineage and say hm that doesn't look quite right and maybe I don't trust it.
(1:35:51) There's quality metrics associated with each transformation. If the quality has dropped a lot, maybe I wouldn't even look at it. But lineage uh is the transformations and it can come from source system. It can go to your um uh you know um outside like somebody from a uh PowerBI or a Tableau could be consuming it as well.
(1:36:15) So it's end to end lineage and it's very very important to understand the uh pedigree of the data. Right. Um, when you talk about lineage, is it does it include row level, column level? Because I feel like they may be all very different or or when you make a row level, it may actually take a very different shape. Um, when you're transforming data in a big data system, you don't do it at a row level.
(1:36:43) You do it um there's a transformation which is happening on data that has come in. Now you you will have column level because you can change the schema. You can add a column and this column was created by joining this data and this table together or by adding an S before this table. Some some logic like that might be there but the row level I'm not so sure.
(1:37:01) Column level I understand. I mean if the data is coming from for example in this one we join this two rows uh is row level all the way down if yes so when you have two tables and you're joining on a key you know that this wide table that you got was because its pedigree or its parents were these two tables and it was joined on this column. Okay. Yep.
(1:37:35) Um then comes um like say there's a central team. It might bring in some data sources which are common to everybody else and I might consume from it or maybe there's some specialized uh data streams that comes to me alone because I have exclusive uh rights over it. But it will be a miss if it is just retained within that team.
(1:38:01) uh they are creating it because they are the experts and then they are the producers and somebody else is consuming from them. Now the consumers themselves they cannot be static right they are asking for that data because they need to do something else with it. So they are going to produce they are going to have their own pipelines.
(1:38:19) So those are called as derived data products and uh so from the source system you will have um like say here you have PLM manufacturing and so on you create some products and then you have um derived data products from there. So it it can go to whatever level of granularity as is needed to satisfy the use case of that particular domain and the consumers will be aligned to data products and they might be pulling from different domains.
(1:38:44) I might be pulling from um the manufacturing domain. I could be pulling from some order and sales data as well and so on. Now let's look at another term which sounds similar to mesh but is a little different and you'll see data fabric. Um the the key point here is that uh this is more about the actual technology whereas um uh data mesh is a little more about organizational maturity process people and so on.
(1:39:15) Every customer I talk to wants to be a data mesh. But you ask them what is the level skill level within the individual um teams like the data teams and the skill level will be so low that their dependence on that central team is too high they cannot the organization is not ready to adopt a mesh architecture. Just giving you an example in a data fabric setup.
(1:39:35) It is about understanding all your data as state and creating that as a unified whole. Maybe some data is on cloud A, another one is on cloud B, some of them is onrem, some of them is underneath your table in some server. So all of that edge uh cloud everything is going to come together and your fabric kind of determines when you're going to ask it um which pipeline, which data, all of that is going to magically do.
(1:40:06) So it's more technologycentric and it gives the concept or the illusion or the perception of a single environment and um it's definitely a more of an architectural construct um rather than an organizational construct. So here are some dimensions against which we can compare and contrast.
(1:40:27) So when you talk about data mesh it's all about who owns the data and manages the data. So that data ownership is very important. When it's fabric, it's about how the data is flowing, how it is connecting and so on. So the core idea is decentralized domaindriven data as a product. Here it is technologydriven architecture, metadata and AI powered integration. Uh focuses on operating model and organizational.
(1:40:49) Here it is technology and automation. Um ownership is each domain team owns, curates and services data products. A central team has to manage the integration and governance fabric because this is too distributed which again seems improbable. So there will be some delegation. This is a federated computational governance with some common standards.
(1:41:12) Here it's a centralized governance embedded via metadata cataloges and automation. So if you look very naively at the lakehouse then standalone it will sound like a fabric. somebody can build mesh on top of it in the construct of like um um the different functions within an organization. Strong emphasis um on data as a product.
(1:41:35) So products have to be discoverable, addressable, trustworthy and so on. Here it is all about seamless access. Um like I should be able to discover the whole world through that fabric. uh scales with um organizational complexity, scales with technical complexity.
(1:41:55) So because this is hybrid, multiloud, there could be some legacy integration um technology agnostic um but needs some self-service. This is very very bound to uh tech um large enterprises where centralized data teams is a bottleneck and so the lines of businesses have to kind of step up uh get properly skilled and take more ownership. Here enterprises needing cross environment uni unification.
(1:42:19) um example a bank with a mainframe and then some of it is in the SAS and some is in the cloud and like each team is at at a different level of maturity and so the central team is constantly being asked to step in. The analogy for this is a federation of cities each managing its own infrastructure under some common rules and this is like a high-speed railway and a highway system woven across all cities. So it's all about connecting access.
(1:42:45) Uh maybe from this point to this point this train goes then you have to take this bus then you have to take this train just mental image. So together they can accelerate trustworthy governed and accessible data at scale and these are just different uh paradigms. All of both of them can be built on the lakehouse. Now we talked about such esoteric stuff.
(1:43:03) Now let's look at the real state of the industry organizations still struggling with achieving big data and AI projects. Many of these like 70 to 80% of them never see the light of production. They just get created in a PLC where the enthusiasm level is very high and they just can't make it into production.
(1:43:23) Why? The same challenges that we have been talking about in this course is actually real world challenges. It has got to do with enterprise scale. Doing a PC or hackathon is is very sexy, very cool, very easy. You feel good. But it's very complex setup and management. um simplified ingestion but nightmarish consumption because of data swamps that are difficult to navigate manage lack of BI support sometimes performance could be an issue to help organizations draw value from their investments so can you imagine especially in a large organization how much money would have been invested in it so Delta Lake was
(1:43:59) created as a need for it that's where uh you know we went from a spark company to a um lake company to a lakehouse company um and Then a data intelligence now because of the rise of gener designed to bring the governance, reliability, performance and structure of warehouses into lakes. So I think we've talked enough about it.
(1:44:20) Um so where when every vendor claims to have a lake or a lakehouse uh what are some of the things that you should check for and these were some of the discussion things that we talked about in class earlier. So first thing you should say is what's your governance strategy? Um what's what's your cataloging uh like? So cataloging is just the metadata but governance is more around access and discoverability and lineage and audit and all of those stuff. And then can I even look at the quality of data and what about data security? I might have some HIPPA data.
(1:44:50) I might have some BCI data. Do you do you have any support for it? Um for data masking and you're talking about rowle and column level uh rowle filters or column level masking encryption. So much to worry about. And if they say yes to all of it, then yes, they are a mature product uh or a mature platform.
(1:45:13) So today um the to-dos are read chapters 4 through six. Six is around your data operations. You will need that for your next assignment. Uh your next assignment will be streaming and it will be on your um incremental injection. So sedd type2 assignment submission 2 is uh coming. So hopefully you guys are doing great there.
(1:45:33) With that, let's go and do some delta stuff. I broke this into a 4 A and uh a 4B. So for 4 A um we'll just go over um Delta Lake uh capabilities because um those are going to be bread and butter and it should become second nature to you. Uh this particular data set uh is from gaggle I think uh lending club data from uh 212 2012 to 2017.
(1:46:08) So each loan um is provided by an applicant. So there's a current status and latest payment and so on so forth. Um typical uh data. So like be like always you first uh make sure that your catalog exists. you use your catalog, create the schema if that doesn't exist and then use that schema.
(1:46:30) Um then actually why don't we attach to some compute and then run. Hopefully I haven't exhausted for today. Let's run this. That should be very familiar to you by now. Um and then the next cell uh we are saying that this is some data from our standard uh data sets uh that is that all of you have access to. This is sample data. It's from the lending club. It's in park format.
(1:47:06) So that's the path and because it's paret so we read it as spark read paret. On that path we should have that uh data and this data is fairly large. So we split it up into um into a smaller part with a seed so that we can we can just uh appreciate some of the nuances of delta rather than grapple with the data set.
(1:47:32) And from there we're going to select the state uh and the loan status uh do a group by. So that means I want to uh count how many uh loans are there by state. Um and then maybe I can create um uh you know a temp view on top of it and so on so forth. So you can see the performances before uh you can see the raw results. So these are all the states and these are the counts.
(1:47:58) If I look here I don't see Iowa at all and it is by design. So you can create a visualization by now you should know how to go to the map chloroplat and then select United States select the state select loan count. I'll show it to you again if you do not know. And here you see there's a hole here and Iowa it says not available because it's not there.
(1:48:22) So again for those of you who want to see how this is done you say you add a plus here say visualization uh choose the type uh and in this case I'm going to choose map chloropl and then um here you have a option to see the world map or the United States. So let's say this the column is going to be the state field and it is the USPS and you want to see the count over here and that's basically what you get and once you save it you'll see that in your visualization. I won't because I already have it.
(1:48:56) It's a heat map and it shows um the counts. So the darker colors obviously uh refer to more data. You can hover over this and see some um some counts as well. Now um if I have a table already if you ran it or whatever partial state drop it if it exists and then create a table um using delta as select star from this.
(1:49:24) So now from a park uh data set we have very easily converted into delta because what did I say that the underlying format is paret. So essentially the data is exactly the same. Now we just give it some some additional bows uh like you know the underscore delta log type of thing to make it into a delta table. So we print the SQL uh created.
(1:49:47) Now if I were to do a describe uh detail on the delta table then it tells me yes the format is delta. Um there's some ID, there's some name, uh description, location, created at um lots of uh things that might be useful for a data steward, the size, there are some properties here, um statistics and so on. So we we have a delta table.
(1:50:17) Now if this was not then the first thing you would be in the format, you would have seen something else. Um, at this point I'm going to just stop the notebook just to show you the the streaming capabilities. Now, this is interesting because you're doing a read stream of a delta table. Yes, that's possible. You can stream data from a delta table.
(1:50:39) And this is not a read, it's a read stream. So, of course, um, you're going to be able to get that. And then you can create a temporary view of it if you uh want to. All of that is fine. Um while that um stream is coming in like maybe you're reading more data what have you you can also do batch operations into the same table because of the reader writer isolation and all all the the um asset transaction stuff.
(1:51:08) You can have these batch operations which are distinct. So I'm doing an insert into IOA. Every time I add 450, I'm in a loop. I'm running this six times. So let's go ahead and run it. It'll take a little bit of time because it sleeps for um 1. So that is 3 4 5 and six.
(1:51:41) So we are hydrating the Iowa cell which was previously empty because we did not have any data in there. So what do you expect when I run this? Um you are going to see the table with the data but the visualization has a dark blue here and you can of course multiply by six to see how it got its data. What it is trying to say is you can have streaming data coming in. You can have batch data coming in. It's the delta table.
(1:52:08) You can read from it as as um a streaming t uh source as well. Um spark read.stream cannot be done on any table. It can be done only on a delta table. Now anita I think Amit has a question. Mhm. Anita quick question. um can we actually feed um data using say Kinesis or Kafka into a delta table and that's how we can ingest into normal manage tables using this? Yeah, absolutely.
(1:52:45) So uh you can ask the assistant to give you an example right here or you can go here and say um data bricks uh delta read from Kafka or Kinesis right you can you can start to see an example of course Google Gemini will want to spoon feed me here but I want to see the real documentation and that's right here it's a stream uh the format is Kafka you will have to give some additional parameters for the servers and what topic you're listening to and so on so forth.
(1:53:14) Thank you. Um now because we are on serverless uh and we do not have like quick access to create a S3 buckets and so on so forth. Um I will not be able to show you that in park this is not possible but in delta it is. So the wow factor is a little low but bear with me. um when you say I want to do fine grained updates and deletes you cannot do that on a delta table this command will fail if this table is just a parket table because park doesn't support it so if I run this of course uh you know we are now deleting from the table where the
(1:53:53) state is Iowa right it said that yes um there was one row which for Iowa that I I got rid of so now if I'm going to run the same query and do a visualization exactly the same way. So if I go back and uh take the plural plot uh look at United States, look at address, look at the abbreviation, look at loans.
(1:54:19) What do you expect? This should be an empty because I just deleted it. So I'm I'm doing the visualizations just to convince you that the data is actually gone. Um if you guys have access to your regular parket tables in some other environment you can see this very very simple so very obvious statement is going to fail.
(1:54:43) You cannot do uh um not this this uh one where we were trying to delete. Uh same thing would be if you were trying to do a simple update like remember the GDPR scenarios that we were talking about. we need to update this person because he doesn't want to be on the list or we have to delete all his data and show proof that we have done it.
(1:55:02) Those are definitely needle in a haststack type of scenarios. You cannot do on um because those uh are file based. They work at the partition level. You'll have to drop the entire partition, make the edit and then recmp compute the data. Too expensive, too long, too too much to do. But but on Indita just to be clear it works in this case because it's a delta table and delta allows the exactly row edits.
(1:55:26) Yes correct. Um so now we have uh done some updates as well. Uh let's look into the merge into syntax. Uh can someone remind me what the merge command is about? Yeah, you can do um an insert as well as an update um using a single statement based on whether or not um records exist in a table or not or perfect. Yes. So it's also called as an upsert um and uh doing it uh earlier.
(1:56:05) This uh graphics just shows you like the seven steps that are necessary to do it traditionally and um um you as you rightly pointed out it can be an atomic operations. So this portion is creating some sample data uh to simulate new data coming in that like the change record. So in here we've got Iowa 10, California and Oregon none.
(1:56:28) So we should see a hole for Oregon. Um once we have this uh data we are going to do a merge into our delta table using this new table. So we call this new table as the merge table uh on a particular key. So in this case it's the state uh when it is matched then update when it is not matched then you insert. You also have variations for delete and so on so forth. So let's go ahead and run this.
(1:57:04) So here now there are three rows that were affected because you are you are touch bringing in these three of which two were updated and um one was inserted and nothing was deleted per se. Uh number of deleted rows is zero. All right. So now if we run it uh we get our data but like before we going to add the visualization so we will convinced ourselves as to what is happening to the data right there.
(1:57:37) So it's a little painful to do it each time but it's all right. Okay let's save it. And according to um according to this what did we uh bring in? Iowa was 10. Uh where is Iowa? Iowa is 10. Um what were the others? California was 2500. California is 2500. But I did not handle the delete case. So um nothing would happen to because there's no delete clause here.
(1:58:24) So even though I had origve uh obvious to me okay schema evolution uh select um address and now I'm going to add a new column here just create random as big count and call it as count uh cast it and blah blah uh and then uh display. So if we run this we are going to have the address and the state and now we have a fictitious amount.
(1:58:59) So that's going to then um we'll try to write this into delta format in append mode. So if we run this, well, this did not fail. Why not? This command fails because the schema for new data does not match the schema for original data. Maybe I've already run it once with the merge schema as true because this did run and it it did not uh fail.
(1:59:30) So it's possible I had run this once before and my merge schema was true. Ideally this should fail with merge schema is equal to true if you run it again then it will be fine and then next when you are going to run from it you will see oh did it silently run one second something is wrong loans display loans is this okay loans write format delta mode is append um but I am not specifying where it is going to write to. So it's not really writing into the same table.
(2:00:17) So I should have this is this is not 100% correct. I need to work on this to show you where it will write and where it will fail. That is why loan by state does not have that additional column. It is still state and count. So bear with me. Sorry. Um, okay. How about this? Okay, now this is much better. So, uh, select the address, select the amount from this, group by this, order by this, and at this point it says this variable with amount cannot be resolved. Did you mean one of this? Because my first thing did not run. And that is why there is no
(2:00:58) amount to look at. Um so let's skip this section al together and let's look at uh uh time travel. So in time travel I can use either a time stamp or a version number. So if I look at um the history of the delta table I should see multiple versions. We started out and when we did a create table then we wrote some data maybe we wrote it six times 1 2 3 4 5 6 then we tried to do a fine grain delete so it will say exactly which state we deleted then we try to do a fine grain update and maybe we did that for uh
(2:01:38) Washington and then we tried to do a merge so obviously I have messed up on those uh uh on those statements I need to look at that. So those are all the operations up to the merge operations we did. The next thing where I was trying to add an additional uh column to fail the pipeline wasn't really writing into the same table which is why uh it didn't show the error.
(2:02:05) But now that we know that these are my versions and these are the timestamps um I can select from the table and say version as of zero. So coil back to this version. If I do that, what do you expect? When we started on version zero, remember Iowa was the one which did not have any data. So we should see a hole in Iowa. Is that fair? So let's see whether we can get Iowa to show a hole.
(2:02:41) Yeah, sure enough. Iowa as a whole. Now we'll do run exactly the same command. Select star from table. Select star from table. The only difference is now I'm going to say version as of 9. So that means after that merge command was done. Uh and in that case we had different values and the easiest way to look at it would be Iowa is populated with that merge value of 10.
(2:03:27) Uh Oregon was deleted and California was 2500. So that proves that it is indeed the version 9 of the table. Instead of that you could use date as well. You don't really need the time stamp. Um you can use um like an the approximate uh date and time. So it will take it from the next version. Um any questions on delta before we move to actually we are over time but I want to show you the workflow very quickly.
(2:03:59) When you go to jobs and pipelines, you can create a job here. So that's the end toend workflow that we were talking about. This ETL pipeline is your DT or LDP pipeline that we are going to talk about. And this injection pipeline is only from for bringing data from um you know popular databases or apps or whatever.
(2:04:25) So for instance you can uh reach out to your workday or your um sales force and so on. Uh now you can see that I was running it right before class. I got the first failure. I can go and look what was the run ID, what was the start time, why did it fail and then I tried it three times with maybe small variations in the code and it succeeded. Uh we can run it again now.
(2:04:49) So I can run once and then I might even try to run it again. So it's going to cue the data. That's why I ran it twice to show you. Um this is the observability field in which uh each of these things run. It tells you what was the start time. Each run is a unique ID. These are all launched manually because I was clicking on it.
(2:05:10) What's the duration status and you can see that this one is running and because I clicked twice and the concurrency of this job is one. The other one is getting cued. Um let's go to the tasks and see what it looks like. There are two simple tasks and they have a dependency. Um you can add a new task by saying do you want to uh get it uh the code from a notebook or a Python script or maybe a SQL query a SQL file SQL there's just a lot of options here even a a jar file or a Python wheel by the way on your um serverless I'm not sure if jars and others will work just stick to basic Python and uh SQL for now
(2:05:48) um and when you're adding let's just add a note book. You can give the task a name and you can say whether it depends on another task or not. I'm not going to add this because I already have two. So let's go here. This is task one. It's running from a notebook. It could come also from git.
(2:06:08) Um it's my notebook is in this workspace itself. And so I give it the path to the notebook. Um it's using serverless compute. And I just put a parameter to show you that yes, you can pass parameters to your workflow. Maybe you'll say your source is this or your environment is this.
(2:06:28) Maybe you're in now end is equal to prod versus n is equal to dev and the code remains the same just those configuration parameters could change. Um and you can have notifications, you can have retries, um you can have uh metrics. So remember that uh concurrency stuff that I was talking about, you could increase that here. So both those jobs could run simultaneously. Um and then um you have schedules here.
(2:06:54) So when we were talking about cron um you can say scheduled and you can specify u maybe you want this to run every day or you want to give it in a cron style um show the cron syntax you can you you can play around with it or maybe you want on every file arrival or maybe you want continuous. So those are different trigger options. Um job parameters are different from task parameters.
(2:07:21) So um here job parameters is is uh JV uh one and task parameters is passed from this task to this task whereas job parameters is passed from your workflow into your job. Right? So the keys I've used is different for the task. It's just param one and for the job is JV uh M1. Now I could go to this notebook uh and see or maybe it's even easier to go to the run. Uh this was uh triggered a little while back.
(2:07:50) So let's just go in here. You can click on this and it says hello. Okay. And uh this happens to be a Python notebook. This is how you are able to use dbuttils widgets get the param and print it. So remember my param one was value one. So that is what is written here.
(2:08:17) Now for the job parameter similarly you will say TV utils widgets get job param one and you can print that. So th those are slightly different concepts. One is passed from a workflow into a job. The other is pass from a task to a task. Um so that's very simple. The second one was just a SQL thing and it was very uh as simple as like say select hello.
(2:08:36) The only reason I did it is to say that each node can be very different. One can be Scala, one can be SQL, one can be dbt, one can be wheel file, a whole uh you can have even a dashboard and a query also another ones and as you build the dependencies your DAG is being uh built out. Um I think that is good for today. Let's stop here.
(2:09:04) Any last burning questions? Uh can you run parallel task in uh sorry yes can you run yes so let's go back there um if I had no dependency so if I go to task two and edit it so let me go back to here to the tasks. So right now task two depends on task one. If I remove it, they are parallel. I'll say save and continue. So they are parallel.
(2:09:36) They will run simultaneously. Got it. Thank you. Mhm. Ma'am, one more question before we end um for tonight. Uh you had spoken about the copy into command earlier. Mhm. Um now this may be a fairly obvious question but uh do we have on error continue so that if there's one or two bad records but there's five plus million records that have succeeded that it can skip past the bad records and load the rest.
(2:10:06) Um so c uh when you say it is a bad records that means you have defined a schema into which you are copying into and the incoming data is um not complying to it like for instance maybe you got a float where you were expecting an int um it will there's absolutely no way it will continue unless you have defined everything as a string and then used a second step to then convert it and then you can define rules as to what to do.
(2:10:41) The constraints and the quality checks that I was talking to you about is only in DT and um I will go over a proper DT workflow with you to kind of show you how you can have the constraints and then specify that if it is violated then should you fail, should you stop or should you just allow to go. Got it. All right. I think I'm a little guilty of going over a little, but we had a great discussion earlier today.
(2:11:11) So, all good. Professor, I have a personal question. Maybe I can ask at the end. Sure. For sure. Go ahead. I don't think anybody else has any questions unless you want others to drop. Yes, please. If I can wait until Eric, can we stop the recording also? Yeah, I'll stop the recording. All right.