103 day 9 - YouTube
https://www.youtube.com/watch?v=TvI3hRCD4Uk

Transcript:
(00:01) Okay. Um good evening and um welcome to lecture nine. Tonight we'll be um discussing the importance of good data to produce um good models. Okay. So um we'll s first go through some announcements uh do a quick review of um last week's lecture and then um have a um discussion about ML ops including model creation and selection registration deployment um discuss the importance of data um in in producing good models.
(00:47) talk talk about um the majority model for MLOps and also go through some uh automation um tools for um managing model deployment and then tonight on will um present the lab which will be a example of using MLOps for a a real real life um use case. So, first um announcements uh case study 1 is in progress and um assignment 4 will be um released um within a day or two in time for um a section on Thursday. We'll we'll review it.
(01:36) Um I think Ron Ron will um do a review of assignment four on Thursday and Thursday's section case study two um will be a group project. It'll be um released later and um just to note on the final project which will also be a group project uh that'll be um that'll be done later. Tonight's lecture will talk a lot about um roles of of different roles involved in uh creating ML pipelines. So, and that'll be um pertinent to the group project.
(02:11) And and also quizzes. So, we've completed our first quiz. Hopefully, that went well for everyone. And um we'll there's still one um a second quiz coming up um that we'll plan to release um closer to the end of November. Any questions about the assignments or so? Yes. Any Sorry, go ahead. No, no, go ahead, please.
(02:45) I was just going to ask any um estimate on the deadlines for assignment for and case study too. I'm asking because I have an upcoming travel um I will be out of country for about 10 days. Well um you should know like within a day or two um the due date for assignment 4. I'm pretty sure that's um we're providing three weeks for that. Okay.
(03:12) And um case study 2 will be um released later. I'm not sure when exactly, but it'll probably you'll probably have three weeks for that as well. So should should have enough time to work around travel. Yeah, thank you. Sure. Uh was there another question? Yeah, my question was about how do we know uh how do we did on the quiz? I mean we have not received the grades yet.
(03:36) Do we have an expectation of when they will be released? Generally, we're trying to return grades within two weeks of the due date. So, um, so I think, um, probably by the end of, uh, next week, the quiz one grade should be done. Okay, great. Thank you so much. Sure. All right. Anything else? I was wondering is the case study 2 group going to be the same that we had in case study one or dynamics.
(04:17) Okay, we'll we'll probably create new um uh groups for that just to help I always believe that it's good to um good opportunity for students to meet other students by uh interacting with um as many as many of your classmates as possible. So we'll we'll um we'll have different groups assigned for um the case study too as well as the final project. Okay. Thank you. You're welcome.
(04:43) Yeah, just a note on that. I I find um the Harvard community is a awesome community to be part of and and um you know this is a opportunity to network and meet meet other um classmates and and um and make long long lasting friendships and um and and build your network out. Often often useful for um finding jobs and things like that too.
(05:17) All right. Okay. Well, um let's move on. So, we'll now we'll review um material from last week. As as usual, I'll just ask some questions and you guys can respond. Um so, model staleness over time is referred to as So what what can happen to a model over time? You lose some drift. Yeah.
(06:05) Outdated. Yeah. Generally we refer to it as um drift. So model or data drift or model model drift is um sometimes referred to. So um the the model the model can the data can drift or sometimes the context of the model as we'll see later tonight the context of the model can change and where um something that made sense the semantics of the business underlying business changes some way to um make the model less effective.
(06:40) MLFlow is an open source framework for model life cycle management. Yes, exactly. Okay. And MLflow tracking server tracks experiments. Yeah. like um all all the all the aspects of um the model experiment. So parameters, metrics, artifacts, code and and data and tags and and other other details of the model.
(07:25) So um you can go back and look look at the um metrics over time. Okay. Oh, and um MLflow registry helps with model you can a registry registry is helps um keep track of um models. So it would help with it's essentially a catalog. of all the versions and yeah that um helps helps enable people to discover it and and also with versioning and and also being able to promote model from one staging area to another and um yeah and much like a a catalog air good and can mlflow work on prim
(08:32) versus versus in the cloud. Yes, it's Yeah. Yeah, that's right. And process of getting data ready for input to a model. Reprocessing. Yeah. Um generally What what does a data scientist create in in prep for um inputting information into a model or data into a model? Extract features. Yeah. Feature feature development.
(09:16) Um taking something like a data birth and computing it into an or converting it into an age that makes more sense for the model. uh different types of model serving and scoring. This this is um so you can do batch or streaming or real time model serving much like um our much like our discussions about data pipelines before. Overfit refers to It's uh it's when a model has way more attributes than than needed. Yeah.
(10:16) Um generally the the model's been trained to fit the data too well and and it it can only work well with that particular data set, not not um like other other input data sets. So it's been overtrained. What aids with future reuse or future reuse? I'm sorry. a feature store which is similar to a feature like a catalog but it actually um stores um the the features and maybe the uh computed features as well.
(11:06) And AutoML refers to that when you pass data and the model is created by an AI or yeah basically a AI assisted model creation. So um and so um so you you and the um thing that we want to remember about that is is that it it helps make it easier for um citizen data scientists but um it generally implies that you have less control over what happens or how how it does it. Okay, very good.
(11:50) All right, so let's um let's continue to our lecture. So uh tonight we're talking about the importance of data and and um generating um accurate models. So um and part of that is is the is not only the data but also the process and the people. Um and there's different um as we'll see there's and as we've already seen there's different personas that participate in this ecosystem of um producing models the including the business stakeholders who are um interested in in generating value for the business through um ML solutions. uh the data engineer that that helps build
(12:37) the data pipelines. the data scientist who um translates um uh business problems and then um produces takes the data and um computes features and then um produces models and then tests the models and um and hands them off to the ML engineer who then u takes the the model from the data scientist and deploys the model in a um reproducible way so that the model can be deployed and used in a production environment and maybe uh retrained over time. And then um data governance is also an important role to make sure that
(13:20) um proper security controls are um placed on on the data and that um the data is properly uh anonymized and um and secure basically um to to um ensure governance and and compliance as well. Okay. So if we look at the um the the the the workflow uh we start with data prep um and exploration and analysis of the data feature engineering uh model training model validation deployment of the model and then finally monitoring of the model.
(14:10) All of this you can think of as a pipeline or workflow and um with overlapping uh roles um from those personas that we just discussed. So the data data governance officer is basically interested in the entire process making sure that um the um compliance and governance of the data is is complied to across the pipeline. Data engineer is primarily involved with a data prep.
(14:39) Um taking data from multiple sources and um cleaning it and homogenizing it and and preparing it for um the data scientists who will then do the uh data exploration and analysis. So it's feature feature engineering, model training and model validation and which also overlaps with a ML engineer who takes it and um deploys it and does the monitoring and then business analysts or stakeholders andor stakeholders um are interested in the validation process as well as the deployment and and using using the the resulting model.
(15:26) So um many different personas involved in the um entire pipeline and and you can think of this as um helping modularize the machine learning um workflow so that um each and with different steps with um different different roles responsible for each of the steps and different roles and and different levels of expertise. piece.
(15:56) So um data or data scientist is really focused on the the feature feature creation and model training and the data engineer is focused on um collecting and preparing the data making it ready and ensuring good data quality and then um and then finally the ML engineer takes those models produced by the data scientists and puts them into production.
(16:24) and continues to um make sure make sure that there's an automated process so the um the model can be retrained um easily with um re refresh data. Okay. So tonight we're going to look at this um process in the in the from the view of um a business context which is customer retention. And so customer retention is about um helping reduce uh customer churn where customers sign up and then they then they um they may use the product for a little bit or the service for a while and then they leave.
(17:05) And so um and then to to to backfill those customers, the business may have to spend a lot of money to find new customers um to to replace the customers that left. So it's much more economical and um cost effective to just keep the customers and keep them from leaving.
(17:30) So one of the um common examples of machine learning is is detecting um when a customer is going to leave before they leave and then doing something to uh help help keep them as a customer. like maybe offering them a discount or or some other incentive to stay on and continue using the product. So um so in this case um marketing analytics team um we have a lot of demographic data and historical service data on customers that have turnurned which has been put into a SQL analytics dashboard.
(18:10) We've um been asked by the stakeholders of the business to um um go further and and be able to predict uh when a customer is about to um leave um before they actually leave. And then um with with this knowledge, the business will be able to take some sort of action to try to prevent the customer from leaving um before they do.
(18:41) And so um and this will be the subject of tonight's lab as well. Anandita will go through a detailed example um using data bricks to to show how this can be done in the um in a note not notebook. So to start with um it's really important to understand the um the problem that we're trying to solve and uh take take those rough um business goals and and convert them into um clear requirements for for the um machine learning um uh goals or mission.
(19:21) So um basically we want to translate the business goals into clear datadriven problem statement and um and so and this is the responsibility of the business analyst to uh like okay churn uh well what exactly is churn? let's make sure that we understand it um in a quantitative way so that we can measure it.
(19:51) And in in this case um a customer is purchased in 90 days or canceled their subscription within a month. And this way we um knowing knowing this description of churn and we can use this to um as a predicate for whether a customer is churned or not. Um identify key objectives. uh what are the decisions that the model will support? Um like what will we do? What will the business do with the results of the model? Uh um potentially retention campaigns um outreach and maybe um adjusting prices um to to help retain customers. And then um next is determine KPIs. How are how are you going to u
(20:39) measure this? Um key performance indicators uh that uh reduction in churn rate uh increase in retention uh return on investment of interventions for example may be measurements that we um would apply to um determine um the the results.
(21:07) And then uh the business analyst is also uh responsible for interacting with the business stakeholders including marketing, sales, product and finance to make sure that um there's alignment with the business in terms of the goals that we're looking to achieve. And then finally once the model's been produced and and um released and and used in production generally it's the um analyst responsibility to uh ensure that the um results are expected that it's actually the model is actually helping predict churn and helping reduce churn as as desired. So I think this is really important um
(21:48) from my own experience um being able to measure results of changes has um and knowing whether or not you're moving the needle in the right direction or maybe the wrong direction is really important. Sometimes you may make a change and that change may seem like it should help but in reality it doesn't.
(22:14) So it's always important to um validate and measure and validate the results. So and finally the deliverables of the analysts are um a problem statement data requirements like what data are we going to uh consume for this pipeline uh evaluation metrics and also um finally the business impact analysis once everything's done and and um deployed like actually um providing some sort of report on the effectiveness of the model and and effect effectiveness in terms of uh how it's helped the business either um succeed or or not.
(23:03) So here's the um here's the workflow. So uh first it starts with data prep and featurization. Uh moves on to um creating a a baseline model setting up web hooks um promoting um best run to to the registry. So uh once the um once once the best model has been uh identified then um it gets promoted goes through some testing.
(23:39) Um it's it goes through um uh staging and um and then finally um we up use it to update the dashboards and maybe um schedule some sort of retraining on some sort of frequency potentially monthly. And and we see the data engineer, data scientist, and ML engineer at the bottom.
(24:07) and they're they're as we mentioned before they're all involved in working together um to create this ML workflow. So the um the role of the the data engineer is um basically to collect collect the data from the sources identified by the business analyst and ensure that the data is clean uh um reliable and well structured um and and and ready to flow into the machine learning environment.
(24:44) So um the data may come from CRM systems, web logs, um billing, customer support. U the um the the data pipelines uh usually include um either ETL or ELT. Um ensuring data quality is important. that that may be um as we've seen already things like handling missing um data values uh duplicate data um inconsistencies in the data and also um validating the schema and um and then making sure that the data is accessible in a a in a um scalable um infrastructure say AWS or Azure and then um also um supporting um data lineage and um versioning of the data as well to um
(25:47) help support reproducibility and and traceability. Finally, the deliverables are like a trusted production grade data set ready for analysis and modeling. So that's that's the role of the data engineer and you can see his his involvement is basically in the data prep um area as well as helping with um staging and and um batch processing as well as inferencing.
(26:23) Next we move on to the data scientist whose role um somewhat overlaps with that of the data engineer. um his his role is transforming data into predictive insights. So um and the deliverables of the data scientists would be features a trained model um insights into which features are most important.
(26:53) um uh some sort of validation report that basically scores the model and also recommendations on maybe um you know whether whether to move forward with the model or not. The um so it um part of the responsibility of the data scientists in terms of um preparing the data is um feature engineering. So um uh first identify the variables that uh influence churn which um maybe um pricing or um the age of the customer the how long the customer has been a customer um and um other things could include usage frequency complaints a last purchase date um and defining some features that help um bring out those those factors and then um the data
(27:50) scientist is also responsible for uh model selection and training. So um experiment with different algorithms uh logistic regression random forest XG boost for example and um and selecting the best model for the um the problem at hand and then using metrics to uh uh evaluate performance is also important and then interpreting results um uh which would like understanding which factors drive churn and and understanding those and and um helping with explaining how the models produce um their results and then um then collaborate with the business um to validate the results and ensure interpretability for um decision
(28:44) makers. So ultimately the model will be used to drive um business decisions by the business and so the it has to be um clear to the business how they're going to be able to use the model to make those um decisions. Okay, I'll just stop there and see if there's any questions about either the role of the data engineer or the data scientist up to this point.
(29:21) Yeah. So and you can see that the um data scientist his role is um spans more more steps in this um workflow. Um preparing the data feature engineering um creating a baseline model and then promoting selecting and promoting the best model to the registry and and also updating the dashboard on on the results of the model.
(29:56) Okay, next move to the um ML engineer. So his primary role his or her primary role is to take the train model from from prototype to production and maintain it. So the model that was produced by the data scientists will be taken and um and that model will be um deployed to a production environment where it can be used for inferencing.
(30:25) Um the um the deliverable will be um uh some sort of um productized churn prediction service. um maybe some monitoring dashboards to see um you know the rise or fall in churn, customer churn and also um helping to support workflows designed to retrain the model with um fresh data.
(30:57) So, um maybe um every every week or every month or potentially uh um more frequently or less frequently um updating the model with um retraining it on on fresh data. So, um to help avoid uh model drift due to um data drift. Okay. So the the um responsibilities of the ML engineer include um deployment of the model, expose exposing the model via some sort of API or um maybe a batch scoring um pipeline.
(31:36) Um also um CI/CD uh uh continuous integration and continuous deployment for um the model. Um automate retraining testing, version control, um monitor performance, uh track drift, data drift, uh model decay and also latency and fairness or bias um of the model. uh um make sure that the inferencing can scale appropriately to support the demand on the um on the model serving.
(32:11) So and this would involve um optim optimizing the serving in some sort of cloud in infrastructure or edge systems and then integrate uh feedback. Um so provide real time u updates uh from new customer data. So again the ML engineer will um provide a churn prediction service um something that can be used um to detect um whether or not a particular customer is likely to um leave or not.
(33:00) And um and this would um provide uh the business an opportunity to reach out and work with the customer to to maintain them before they leave and and thus saving the um business. Uh um lots of um resources in terms of um not having to go find a new customer to replace that customer.
(33:26) And they can um of course they can work on getting new customers instead of backfilling customers that have left. Okay. So um this is a little more detailed diagram that shows the um the model training pipeline, the inferencing pipeline and then the moni model monitoring pipeline. So uh the training pipeline basically takes the the uh raw data and prepares it. Does the feature engineering the features get stored to a feature store.
(34:01) Um then uh we start training uh in evaluating the models. Um select the best model, push it um push it to um a registry and then um deploy that model. The inferencing pipeline basically takes um takes the data um to to use as input to the model and um potentially uh updated customer data to say is this will this customer um is this customer likely to churn or not basically and that will be used um to score the customer.
(34:46) Um so the inferencing pipeline uses the model to predict whether or not a a customer will um will um leave or not. And then the um moni monitoring pipeline uh provides um um checks on on the data um checking for data drift um model drift and also um inter interpretility of the model.
(35:19) Is is the model um producing what we think it should and how we think it should? And um and and if any of these um uh checks fail, then um that would be an indication that we need to uh launch a retrain uh job to retrain the model with um newer data and maybe potentially um new features and and that would generate some sort of alert that will alert the um the um data engineers and the data scientists that uh the the model has um um drifted and and uh it's time to um refresh the model.
(36:05) So in the um looking at more detail in the model training pipeline, uh first we start with data prep which um we um that includes um splitting the data into training data validation and test sets uh that can be used to uh validate the model. uh feature engineering uh basically transforming raw variables into meaningful inputs for the model.
(36:36) Uh like as customer age as an example, the last purchase date, maybe call frequency would be examples for um for churn. uh and then selecting the appropriate model uh or algorithm for um the machine learning process. And then parameter tuning uh optimizing hyperparameters to um improve um accuracy and reduce overfitting of the model.
(37:05) And then evaluation uh um using uh metrics like um accuracy, precision, recall, F1 or um for regression models RMSSE or MAE uh or R squared for um or uh basically measuring the um performance of the model. And then also um part of that model training is for the models that are good we want to persist them as an art artifact so that they can be reused and um ready for deployment.
(37:45) So and the the goal of the training pipeline is to um a well um generalized model that can capture useful patterns but doesn't overfit to the training data. All right. So then um the second um flow for model inferencing uh we want to generate predictions or or decisions based on new and incoming data.
(38:22) Uh and this um this can be this is where we actually use use the um the model to help um guide business decisions. Uh so this can be done in in um a batch using batch inferencing where we uh take a large data set um maybe from from the week before and in the case of churn prediction maybe um uh predict um which customers are likely to churn the following week. And so that that could be used by the business to reach out and um to those customers.
(38:53) And then also real- time inferencing like um potentially um during a interaction on a website or something where the customer is there, you could use the model to um to look at their behavior and um potentially detect um a customer that's likely to churn and or um and and do something like offer make some sort of offer to um to preserve them as a customer.
(39:27) And then um the infrastructure required for the inferencing uh different model serving frameworks uh input output um of the information going into the model and out of the model and then um there's also performance considerations. How quickly does the inferencing need to happen? latency, um the throughput of the model, uh scaling, caching, and also security.
(39:57) And the ultimate goal of the um inferencing pipeline is to um deliver accurate, fast um predictions and that are reliable for the business to use to make um decisions and that can be automated. And then finally the final um pipeline for monitoring. Um we want to continuously track the model's behavior and performance in production to detect issues like data drift, concept drift or model decay and um where data drift is input um data distributions change over time.
(40:39) Um maybe um new new customer demographics or new new product categories could change the data and um cause causing the um causing data drift and um and causing the model to perform um poorly. And then concept drift is where um could also occur where the relationships between uh inputs and target variables change.
(41:08) Um maybe um what actually drives um customer churn could change over time. Um, for example, based on new pricing strategies, um could could change. Maybe the um the the the cost the cost of the um service has gone up and um is is um causing causing um customer behavior to change.
(41:44) And then um another thing to um consider is uh prediction quality where we um compare predicted to actual outcomes when available. Like maybe um the the model said that okay well this customer is not likely to um leave and don't worry about them. But then um the customer ends up leaving. So that would be an indication that the model isn't doing a good job um predicting uh reality.
(42:14) And um so um for for the monitoring pipeline there's uh ML ops um platforms that can use be used to support the pipeline and also alerting and dashboards um that can be used to help um see trends um maybe declining performance and also um generate alerts. And then the pipeline can also um be automatically set up to uh retrain based on triggers.
(42:54) So if a um the model drift um exceeds a particular threshold then that could be um a trigger to automatically retrain the data with uh with the model with fresh data. So any questions about this this flow or these flows? So I'm sorry if I missed this, but how is the data drift and model drift check actually done? Are we checking Is it like based on manual testing or is it some can we have some kind of automated check to see if model has drifted? It can be both ways. Um go ahead.
(43:52) I mean um you have to have baseline characteristics some statistical quality on your data to be able to check both data as well as model drift. In case of model drift there are some additional parameters. But as we discussed in class last time, a model drift usually follows data drift.
(44:10) So both of those things along with of course things like concept drift which are harder. So manual would probably be very hard. So all of these we are showing is uh mostly automated pipeline so that as the drift is detected then it can trigger a retraining of the job. Yeah. And data drift and model drift are different. The data data drift can you can detect that through maybe changes in the data schema that's coming in or maybe changes in the distribution of the data or is the the model drift maybe um maybe um happening for other reasons besides data drift like semantic reasons
(44:50) where um the business context changes somehow. Um maybe um like an example would be suddenly um there's like COVID and um and the um previous assumptions no longer hold and and the model um fails. Maybe the data that maybe the data hasn't necessarily drifted but the the model is no longer accurate. But so for let's for this example of predict having a model that predicts customer churn.
(45:23) Let's say the ML engineer is tasked with monitoring model drift. How could he like what would be the highle solution? Well, if like one obvious thing would be to to um look for um accuracy like if the the model starts predicting that customers that um were not going to leave are going to leave and and then missing customers that are leaving um then um and and and that's stat statistically um uh changing over time for the worse.
(46:02) then that could be an indication of the model is um aging. Okay. So monitor model performance. Yeah. And and just can you think of anything else any well um I don't know onita what do you think? Well, data is the primary concept drift which uh Eric was alluding to is um uh little less common but definitely another thing which cannot be detected by data drift alone.
(46:37) And then um there may be um additional data sets that you could bring in to improve the quality of your model. Um maybe you are your predictions as per your original metric is good but it's still not quite resonating with the ground truth that uh is coming by like people are seeing some recommendations they're reasonably h happy about it I'm giving an example of like say you're recommending some products in this case it is uh predicting churn on a customer so imagine you would be giving discounts and other things to customers and of course they would be happy but they were not the right customers to go after so
(47:14) your model can uh can continuously improve based on some KPIs that business is supplying to you. Uh yes, your um accuracy is good, your F1 score is good, your precision is good, your recall is good, but your business is still losing money.
(47:32) So you still have to go and rethink what your model strategy is and that could also require uh a retraining. Makes sense. Mhm. Thank you very much. Yeah. Yeah. Okay. And then just a summary of the um the data team. It again it's a um combined combined uh in collaboration between the business analyst and um in in terms of developing the pipeline the data engineer the data scientist and the ML engineer and each with different roles and responsibilities that help um do various task throughout the um uh machine um learning um um production.
(48:21) And with that, I'll I'll hand it over to Anadita to to um continue with um ML deployment patterns. Okay. Thanks, Eric. Welcome. I'm not sure why this is not uh loading up. One second. Let's try one more time. Yeah, I'm not sure. We'll just continue in this mode if it's all right. Um when you're going to do the deployment, um you have these uh environments. You have your dev environment, staging environment, prod environments that we were talking about.
(49:31) Um there are two things, two strategies that you can use. Either you develop the model in one environment and you promote the whole model itself. That's the icon that is showed or you develop the logic for it and the data and the model gets the data is used to create the model in each environment because there are certain dos and don'ts about crisscrossing prod data in non-pro environments and so on.
(50:01) So you do your hypothesis testing and everything else with some data. um make sure that it runs it validates it gives you um representative samples because the data that was used was representative maybe not the whole data in staging environment it's a little more pristine and so you allow somebody else to look at your stuff and um uh tested in prod everything is uh redone with produ all of production data and it should ideally be almost identical to staging except that there will be all service principles no users uh no humans allowed
(50:38) unless it's absolutely absolutely necessary and so on. So I'll pause for a second and ask you to consider which one do you think is a better strategy. There are pros and cons to do both. So anybody can uh think about uh why one would use uh deploy models versus deploy code. Deploy code will require retraining.
(51:07) So how much would that cost? So more compute will be used. Yeah. More compute. Mhm. In the other hand, this will be more accurate because you will potentially more has more information, right? Real end data. So Rift is less likely I would say on the second deploy code option. Mhm.
(51:31) So more compute but uh probably more accurate because you're using the right data in dev environment. Let's say you have access to all the prod data. Let's say let's take a hypothetical situation. Then do you see any advantage of building it once and u moving it across? Yeah. You yeah less compute right? You just move faster. Uh because both of them have their pros and cons.
(52:04) as you have rightly pointed out there is no good path but there are some benefits of deploy code because it's more clean from a DevOps perspective right you build it once here we were seeing that this is the training code which produces this model and if we go back to what Eric was showing us from this picture earlier it's basically all this part which is packaged as a pipeline and the model artifact is being created and MLflow ensures that it is reproducible it should be taken to any other environment.
(52:34) Um data access control. So as we were discussing sometimes broad environment needs um read access to the pro training data and that data may or may not be available in the lower environments or if it is available the whole data may not be available just a little bit of sampled uh uh uh you know subset of the data might come.
(52:55) reproducible models engineering control over the training environment which helps simplify the reproducibility. Um so when we looked at MLflow last time we saw all the dependent libraries were packaged. Uh ideally it can be taken into another environment and run as is. So the danger that oh it'll break it can run only in my laptop but if I take it to a cloud environment it might break.
(53:19) That risk is greatly reduced. support for large projects. Sometimes your dev environments have much uh fewer resources than your prod environments and so um maybe even while you take it into prod it might not run perfectly but at least the main uh issues have been vetted and you really tested with large data before you can release it.
(53:44) Um in fact I faced this problem once this is real in uh one of my earlier uh workplaces. Then we were on a Oracle system and we had a staging and a prod environment and uh staging had almost um all the data but not quite I think it was a subset ran beautifully in prod also then you know it wasn't continuous deployment this is a while back so you would have to wait till like 11:00 a maintenance window do the maintenance and test it make sure everything is fine and then go home at around midnight or past midnight
(54:16) and then at 5:00 I get this call that all hell has broken uh loose because my change has uh broken uh the sto like you know the main uh site itself and there was a roll back feature so we rolled back but why did it fail we did all the testing we had a staging environment it failed because of the volume of data uh we had tested it and then the pattern of data so when you work with simulated data you do not have the data skews and the data patterns that real production data happened so just because a million rows or a billion rows were tested on a lower environment it was synthetically
(54:51) generated and not the real data characteristics and so maybe there was one uh petition which was causing a lot of strain and um the code wasn't uh ready to kind of take care of it. So those are uh real examples and this kind of forces you to use the modular code do iterative design but still be ready to scale.
(55:17) Now because we are on spark and distributed compute most of the time these kind of issues are kind of taken by the platform and the engine but still you know there is um um there's something called good code and bad code so that comes in data science familiarity um the data science team must learn to write and hand off modular code to engineering no this is um not really required and engineering setup and maintenance it requires the CI/CD for unit integration testing even for the one-off models that u are built.
(55:46) Uh so these are some of the pros and these are some of the cons but you know none of it can is completely avoidable. I'll try one more time to see if I can actually okay this is much better. Um this is the same diagram that you saw from a previous set of slides but this time we are going to talk in terms of MLflow lingo.
(56:10) uh in MLflow lingo you're using all kinds of structured semiructured unstructured data to do your data prep and your featurization. You might use notebooks, you might use SQL editors. You might be on on your laptop as well trying to connect um uh to the cloud and use compute there.
(56:28) uh your data bricks jobs are going to stitch together the different tasks and usually these are the data scientists who are building the features and the data engineers ensuring that their infra uh is um correct like you know the data engineering pipeline is available for them to create the uh features. This is a little bit of a gray area because sometimes the data engineers are actually responsible for creating the features not the data scientists but they say data scientists because they're little closer to the domain and hence um know how which features are important and which are not like the feature importance and things like that are typically handled by them. Then you do
(57:02) the model development. So we saw how ML tracking uh was very useful in being able to log each of the parameters, the metrics, the metadata and the models. So everything is um is being packaged up very nicely and you can have a plethora of different uh frameworks that you could use to build these models and log them. Everything is recorded.
(57:29) Then the best of this is going to move into the registry and it's in this is going to get tested. This is also going to get tested in different environments. Another thing if you remember Unity catalog is a central entity. You might be in different workspaces. But if your catalog is um available across these different workspaces, so imagine one of them is a dev workspace, another is a stage workspace, yet another is a prod workspace.
(57:54) You can still access uh the model from the registry in each any one of these environments provided you have the access privileges to do so and the model goes through versioning um and it goes through uh environment promotions as well. deployment engineers or ML engineers, they manage CI/CD which promote these into productions.
(58:19) And we said that the same uh frameworks that you have used for development may not be the same ones that uh they are going to be deployed on. It all depends on your consumers and uh their needs. Um all right. So what is MLOps? Um, MLOps is designing, doing the model, uh, training it and running it over and over again. It's at the intersection of DevOps, machine learning and data engineering.
(58:48) So, um, here we have the operation folks. Um, these were the de developers for DevOps. These are the analytics professionals. So, they are data ops. And the data scientist is ML ops. So um these words are like a little bit of a tongue twister because earlier we had just devops, software engineers had DevOps and then when data engineers came by there was data ops and then when data scientists came by and the ML engineers came by there was MLOps and now we have LLN ops.
(59:18) So it's it's just words at that point. Um it's um enough to say that the whole process of ensuring those three pipelines that Eric showed earlier of ensuring that they happen over and over again that continuous process of um making sure that the model gets trained the best model gets selected it's put into production it goes through testing uh and then retires and uh a new challenger model becomes the champion that whole process is ML ops and it moves across environments. It gets promoted across environments.
(59:51) So there's the there's the software engineering aspect to it. There's the data engineering because these are pipelines. There's the DevOps because you're talking about CI/CD and then this is a model. So there is the model training aspects to it.
(1:00:12) uh in between you might have a feature store um and um you might have git where uh all your artifacts are going to get uh stored um not your model artifacts or your data artifacts that's remains in cloud because it's much bigger it's governed by unity catalog so um the um access privileges the lineage information uh the audit information all of that is unity catalog but the code the cluster configuration the YAML files, the pip requirement, some of those things can be checked into uh git repository.
(1:00:47) Um now just a quick check uh when we talk about uh code safety or code um uh repository, we think of git. When we think about a model repository, what comes to mind? well yeah go ahead it's not MLflow yes MLflow one particular component is the registry of MLflow which becomes uh your record keeping for which model has been deployed where we have a lot of metadata associated with the registry which talks about the version the model name uh the tags that it has who created it and from the registry we can actually go back to the actual notebook who created it what who was the user how long did it take all of that stuff so in
(1:01:48) some way the the MLFlow registry is your source of truth for models like if you were to quickly look up um I I pushed something into production but I can't remember what version it was. You can go and say okay let me look at my churn model. Oh it was version 47 and who was who built it and who approved it.
(1:02:08) All of that information is available there. In some companies you might have an artifactory um as opposed to maybe cloud storage where um this might be stored and you can point to that artifactory too. But regardless that is the place that you go to to check your versions and your um uh state of the model. Okay.
(1:02:36) system has code which is what we are talking about with regards to this git repository and all to handle the model aspect which is the algorithm aspect the notebook aspect and then the data which of course cannot fit into git it still remains in cloud storage um but you have to reference which data set you use to train the model because even a simple change in that data is going to change the characteristics of that model uh and that is where you know this whole thing of keeping things together um how did you build it? What patterns did you use?
(1:03:12) Some part of it is very common but then what was the data that you used to do it? So somewhere you should be able to point to the data set and uh think uh like delta for instance has a version number so you can point to not only the table but also the version um that used to train the data. So that makes it all extremely reproducible.
(1:03:34) Otherwise, you might just end up with all of this. But if this was wrong, you can never reproduce the model the same way. Actually, that itself is a misnomer because we're talking about floatingoint arithmetic. So, every time you create it, there's a little variation, but it's so small that we are not able to um catch it when we think it's exactly the same model.
(1:03:54) But behind the scenes, floatingoint arithmetic will never give you exactly the same thing. So um any questions there before we move to how to make models better. So in the beginning we started saying that uh we have moved away from single node uh to multi-node and distributed uh compute and architecture because the volume of data around us is exploding and we all agreed that um having a lot of data is actually a good thing.
(1:04:31) So that is big data but having a lot of big data with of low quality still does not serve us. So we'll have to see how to ensure that you not only have big data but you have big good data as well. And how do you define good data? Uh when you are training you should have if you if it is supervised and you have labels then they should be unambiguous labels because that's your ground truth.
(1:04:56) That is what you are feeding your model to learn so that it learns. It should cover your important use cases. Uh so imagine you're doing a study for all the states and you took only Massachusetts to do your study. That's not very representative. It has to cover um all the important um I would say data sets that are necessary for your um study or your use case to be effective.
(1:05:24) Um then you have you need timely feedback from your production data. Uh so remember the monitoring thing that we had. uh some cases you get the result immediately in some cases you don't whether a customer is going to churn uh it may not be known uh in instantly maybe it'll take a week maybe it'll take a day maybe it'll take a month but there are some lingering telltale signs that uh show that that that customer might join.
(1:05:56) So that is an example where the ground truth is not available immediately. But there are some other cases where uh you are um you're being shown a a product and you're being shown a coupon for it. Whether you you take the coupon immediately and apply to the product is something that can be determined right then and there. So you get your ground truth either instantly or you may get it later in time.
(1:06:20) But it's only against the ground truth will you know whether your your predictions are uh accurate or not. So timely feedback from the production data. So if you get your feedback like 3 months later then maybe for the that period of 3 months you were um using a model that is stale or less performant less efficient and you can't do anything about it.
(1:06:41) So the data pipeline has to be faster to ensure that your uh model needs to get retrained. it needs to be sized appropriately. So again you you'll say I have one data point from each one of the states that should be good enough. Maybe that's not good enough. So again it does not cover everything. Um data is food for AI.
(1:07:03) Uh because we uh we saw that in two lectures back that uh this is a we talk about it as a data science. So we had data engineering then we had data science which kind of feeds into ML. We saw why it's called science because you have a hypothesis. We said where ML fits in. It was in the experiments.
(1:07:21) But AI is fantastic like what's our concept a mental image of an AI. It's nothing but data points fed in to certain algorithm so that the algorithm learns from it and when it sees a new data point, it's able to predict something. Um data is food for AI. Absolutely. And you can do two things to improve your AI.
(1:07:46) You can either improve the quality of your code or you can improve the quality of your data. There are two strategies and they are referred to as model ccentric AI versus datacentric AI. Model centric AI says how can you change your model or code to improve performance and datacentric AI says how can you systematically change your data to improve performance. Now you can take a guess as to which one typically wins.
(1:08:16) Both of them are necessary. You should try both approaches. But which one do you think uh shows more significant results? Data. Data. Great. But why? Because it is the food for AI, right? It it is the baseline or it is the like heart and soul of the um quality of the predictions that that model can make.
(1:08:43) And it's not to say that if you if you've got bad code and good data then your model is going to be good. No, of course your code has to be good. But changing it or spending years and years refining this versus spending that same time refining this, you'll get a better uplift by focusing on improving the quality of your data, making it good data. And we define good data to be all of this.
(1:09:06) Um this was the baseline in a plant where they were trying to do some defect detection in steel in solar panel in surface inspection and they had some model and it was 76 75 85 some baseline they tried the model ccentric approach uh this one was 0% this was 04% 0%. This is uh because the baseline was reasonably okay that was practically no lift that they got. Now with the data centric they got 16% 3% and 4%.
(1:09:45) Now 04 is definitely better than the highest number here which is 04 and again these are directionally correct. So don't be surprised that you get a 1% here and you got a4% here. They're directionally correct but a 16 person lift would have been very difficult to get to squeeze out of a model ccentric approach.
(1:10:04) Is the uh intent clear like what the message uh that we are trying to say like when you're preparing food there is um a data prep or an ingredient prep phase and then a cooking of the meal phase. So you can think of the prep as sourcing and preparing your highquality ingredients and the 20% is cooking the meal and that's your training of the model.
(1:10:34) Obviously going to the grocery stores ensuring that you get the right vegetables, chopping them up and if the quality of the vegetables is not good then your model your your your dish is not going good but you still have to cook it. You have to know the recipe and cook it. There's no doubt there. Now, how can you make it better if this is pretty much um done? Whether you heat something for 15 minutes or you heat something for 16 minutes, it's not going to make that much of a difference.
(1:10:57) But you get a higher quality ingredient, it will. And this is just an analogy. So don't don't look too deep into it. But there is something to be said that a datacentric approach to improving a model quality gives you higher lift than a model ccentric approach. Okay. All right. So, how will you progress? Sometimes what happens is companies will have all the good intentions. They will create a model. They'll improve it a little. They'll be very excited and after that they plateau.
(1:11:29) There is no uh additional improvement they can bring or no additional maturity to the practice that they bring. Here there are several key capabilities. Um Zerk talked about the people and the process apart from the platform and the models and everything else.
(1:11:48) So there is a people people have to have certain level of training and that needs to be continuous. There needs to be a data architecture of how you're bringing in the data how you're doing your featurization. There needs to be a model architecture. There needs to be a process of vetting of quality checks of promoting. There needs to be governance um because almost all industries would uh not want that everything is available to everybody.
(1:12:12) So these are the three different personas that we talk about again and again and they work with models and dashboards and codes and data sets and they have to uh work with each other in some handoff fashion, right? Um so your maturity will start at level one and continue to go on and you have to um make iterative efforts to improve in that game.
(1:12:39) For instance, a beginner will use some familiar tools, improve their productivity and plan for advanced and more complex workloads. So you were building um these type of classical models, now you are going to build deep learning models and you're going to build genai models um and improve your productivity. What used to take you um 10 hours to build maybe now you're familiar, you've got the training, you've got the tools um you can build it perhaps in 1 hour. The intermediate level is all about scaling.
(1:13:05) You have to scale your data and workloads. Doing uh scikit models is very different from being able to do uh multi-GPU model training for instance. You have to automate your work. This manual stuff is good for one or two models but as the number of models grows automation and reproducibility is important. You have to unify your data teams.
(1:13:30) So imagine you have five ML teams or five groups and they all use desper disparate text stacks. There's there's very little collaboration. So the assets they provide uh cannot be reused uh across the organization. So unifying data teams and improving collaboration across all these workloads will at some point provide some acceleration uh faster and then the advance.
(1:13:57) The last tier is uh you are able to do this very fast. So there are organizations where their CI/CD would be uh the first week of every month or the last Thursday of every month or some pattern like that. It's fixed but uh the better organizations like the Facebooks of the world and the Netflixes of the world they will do it at will multiple multiple times a day.
(1:14:24) So these faster production cycles means there are no artificial um blocks in either people or process or technology there. So if there is a problem, if somebody has a better solution, they should be able to put it into production provided it goes through all the tests um and in as automated a fashion because without automation you cannot run fast.
(1:14:42) There should be very well- definfined processes and end to end automation and reproducibility across multiple workloads. Um so um there was um I think it was Netflix which came up with something called as chaos monkey where knowingly they would um they would have a system that would introduce an issue in their production setup.
(1:15:04) Uh and they would test how long it takes for the system to heal recover from it. Um and they their attempt was to make that process as fast as possible because something is going to fail. We are humans. When we make uh uh these um pipelines and these uh products, uh there is going to be bugs. There are going to be issues. Some will be minor, some will be major.
(1:15:30) And uh the trick is uh how mature are you as an organization to be able to recover from it and fix it. Then you are not bothered that yes, there might be an occasional error that is slipped. That doesn't mean you'll not do any testing. It just means there is a way for you to recover fast. Like in the example that I gave of how I broke production, there was a switch which kind of helped roll back to the previous state or the previous code which is why um you know business did not come to an halt. Uh yes there was an outage but it didn't come to an halt. Um so that whole
(1:16:03) roll back mechanism is also very important. So you shouldn't say that whatever I built is like solid will never fail. No, there will be times when it'll fail. So there has to be uh failure and retry and u we'll talk later about disaster recovery of when something happens within a zone or when something happens across a region a whole region goes down and your business comes to a standstill just because you decided to use AWS that's no good right so people have multicloud strategies as well and they have DR strategies to kind of fall back to a different region
(1:16:38) and in detail I was just going to mention just I think it was last week AWS had a major outage on on their um Dynamo DB system in US East one which caused a lot of trouble for people. Yeah, in fact students were not able to submit uh on canvas. So that means canvas behind the scenes somewhere is using AWS.
(1:17:05) Yeah. So it mean may not even be a problem with your your your pipeline. It could just be underlying infrastructure that fails. Yeah. All right. So, uh we looked at two hyperparameter um optimizing uh libraries last time. Can someone remind me what those two libraries were? One starts with an O and the other starts with an H.
(1:17:37) So, what's the question? Uh we discussed two hyperparameter tuning libraries the last time that will help us go through a search space and um automatically pick the best model. It'll go through multiple iterations and combinations and give that to us. So one optuna yes very nice shrea and the other okay I'll give that to you.
(1:18:15) hyper opt and those um are automated man model selection. So random search basian genetic these are ways uh this parameter tuning is actually very critical. You can create a base model using an algorithm but uh you have to tune it just a little bit on your data right otherwise everybody's model will look exactly the same.
(1:18:40) So in different algorithms will have different uh parameters like for instance a tree algorithm might have tree depth and number of trees or or so on but some other model might have some different parameters and the number or the combination is when they are very large you can't really do the manual grid search. So what is manual grid search? um you will have you'll break down your um uh training and validation.
(1:19:06) Uh instead of having them distinct, you kind of have them together. But each pass that you do, you you shift or you change your validation so that your training uh has a chance to um average out any errors here and then whatever is the best you you train it finally with those optimal hyperparameters.
(1:19:31) So it is like um a cartian product of um uh all the various um parameters that your model uses and it it's like a for loop for each one of those parameters. So you can imagine that it gets very expensive very quickly. Uh when you have three uh for loops it's order n cube already right. So it's it does not work well for very large data or for very large uh set of parameters.
(1:19:55) But that's the manual one that we have often used in some um course or other for uh known as grid search which kind of gives you the idea of what a kfold cross validation means. You fold it three times and that's your three passes. You take the average of it so you get it better than just having one training and one validation. It helps to avoid underfit and overfit.
(1:20:23) And um of course um you you have still not seen the test data set, right? The validation data set and the training data set is being used and you finally uh uh use the hold out uh of your test to kind of um uh not cheat and know for sure how how well your model is doing. Your test will come at the very end.
(1:20:41) You just avoid looking into it and using it too early. In the automated one, when you have a lot of parameters or you have lot of data, then random search is actually sometimes better. Bashian search is better. This is like an exponential way in which the um after every search or every combination of hyperparameter it checks and instead of going repeatedly this this this this it'll jump and say maybe this maybe this it'll take a chance and the way it converges to the best model is actually faster. And genetic is also very interesting in which they take um
(1:21:13) uh a good and some bad uh and some normal together and mix it up and that actually produces a a better model. It sounds um un non-intuitive but it actually works in practice for large deep learning models where it's very difficult uh or it takes very long to converge on each of your epochs. um genetic um model selection uh happens.
(1:21:41) Okay, so that's on model selection and again to remind you uh you are you're doing a lot of combinations because you're excited you have so many frameworks and so many tools at your disposal but at the end uh you are tasked with finding out which is the best model and so that uh once you find out okay this is the best model with this set of hyperparameters that is model selection that gets promoted into your registry.
(1:22:08) We are also talking about uh automation here. So um automation is possible um is made easier because there are the concept of web hooks in um as we are transitioning um these models from different environments. Uh you can um request a transition to staging in the registry.
(1:22:36) So that request should automatically kick a web hook which might um do the jobs API um testing phase or it might uh give a slack notification so that somebody knows okay a new model has been pushed into the registry uh to be made aware. So if I if I need the version of a model or or I need that kind of a model I know where to look for.
(1:23:04) Um and by default the model does not have uh any tag in the registry. When it is pushed into staging or production it gets a tag and when it is done when it's retired um because the challenge model has come over then it is archived. These hooks help with automation that's all. And you have APIs in MLflow that you had already uh seen last time.
(1:23:33) those APIs when they are requesting can um be work can work in unison with these web books to make the whole notification process uh easy. So that's where the process and technology is coming together. Now the last slide here is showing you the full picture. There are um different um icons here. So this is a pipeline. Um this there's a task here. Uh so a pipeline has got this this is a little task. The greens are reads, the reds are rights.
(1:24:00) This brain-like thing is a model and the orange or yellow is a model transition. You have repository and then you'll create um branches um as you are building your code. So this is a um a 5,000 ft view of how CI/CD happens uh with proper source control and git um and feature tables and so on. So let's start at the left in the development environment.
(1:24:29) So that's your git repository. You're in dev. You've you've got your dev branch. Um and then you've you've made some iterations of uh training your model. Uh at this point u your data is in some delta tables. You have done your exploratory data analysis. So remember the green is for read and you've also used this data after you've understood this cleansed it um to do your um training.
(1:24:54) There might be some features that uh a data engineer or a data scientist has created for you might use these features as part of your model. Now once you're reasonably happy with it you commit the code. So back to your branch then you say I want a merge request to a staging. So this is happening at the repo level but um you are tied very uh intimately with your um CI/CD environment. Let's say it's a git environment.
(1:25:28) So there might be a CI trigger to say oh the minute it comes into staging I will automatically kick off these unit test case that is the continuous integration part of it. So CI um and then that also is going to look at the feature tables and the data tables that are relevant for that environment. Um and it's going to say okay once the unit tests are passed then I'm going to move it into the integration test. Once that has passed I'm going to merge it back.
(1:25:55) Now that is going to go back to your main branch and then you're going to cut a release branch and that is going to be put into your production environment. uh by then you're going to have again your feature table you're going to retrain your model so this is giving you an example of where the code is promoted across environments uh and this is going to write the um or or rather I should say that's the model transition this is written uh it's done a new feature computation so written the new features done the model training and promoted this into the model registry across these things this was the continuous
(1:26:33) integration ation. Now this is going to be the continuous deployment because it is it's deploying it into the staging environment and the production environment or you know moving the models across. Uh once it's there then there will be inferencing and serving um of the release branch and there is monitoring as well. So we were talking about the data drift, the model drift, the concept drift.
(1:26:59) All of that um is going to get logged here so that we are monitoring this table, monitoring the statistical quality of the data here to be able to have notifications of this alerts of this. Any questions? All right. So now let's move to I want you to go to databicks.com resources demos and library.
(1:27:35) Once you go there you can look up ML ops you will see something like this MLOps end to end pipeline. You can click that. So this is a way by which uh code is fully packaged and um you uh can take any notebook in which you just write these two lines. Um you first say pip install db demos.
(1:28:04) So that's like just like you do pip install a python package and import that package and say install this particular demo mlops end to demo. So now I'll go back to my uh free edition. That's exactly what we have done. PIP install DB demos and then import and install. Now when you install give the name but also give your catalog and your schema and say create schema is equal to true. So you've been using CI103 catalog.
(1:28:33) So the catalog already exists and it's going to create a schema for you called MLOps. This might this will fail because there are certain workflows that it cannot do. But that doesn't matter. It's going to get you the entire code. So in a in a notebook once you have these two commands run then you can go back and see that there would be a directory called MLOps end to end. In there there will be two directories.
(1:29:01) Um and let's go into the first one which is the quick start. This by itself is no quick nothing quick about it but it is u very detailed and end to end and it will help you see the uh flow um let's see again what's ML ops it's the set of standards tools processes methodology that aims to optimize time efficiency quality while ensuring governance in ML projects. That's a lot loaded in.
(1:29:40) So doing it for a hackathon, doing it for a PLC is one thing, but doing it for an enterprise organization where this is going to be in production and the insights or the predictions are going to be uh valuable in improving the bottom line of the business is a different matter. So here it's all the tools and methodology that has helped you build this past not compromising quality nor governance.
(1:30:02) And um we uh were talking about um the churn uh scenario and that is exactly what we are going to do in the context of the the three personas that Eric introduced and the pipeline that you are now all extremely familiar with and the tools that you're now getting familiar with.
(1:30:22) So we said that delta is going to be the protocol on top of your data to make sure that you can um you can manage it. there's um transaction um uh asurances on asset there's time travel all those good things so that's our database now for the model management life cycle management we are using ML flow so the what what is the first thing we have to do we have to do data prep and featurization right so this the first notebook just introduces you through the process the second one is feature injection the third one is the training the fourth one is uh putting the models into UC and then bringing about a
(1:30:59) challenger um the new model that you have created that needs to be validated. it needs to be pitched against the champion and maybe there was no champion so it becomes automatically becomes the champion but the next time the model that comes in uh has to be better than the one in production for it to go there and if such a model doesn't exist there's no need to replace the model and then you're going to do inference uh on that model right so those are the 1 2 3 4 five steps um your feature tables are in unity catalog we're going to use a
(1:31:32) light GPM model we're going to use the tracking uh feature of MLflow to log all our experiments and runs. The challenger model will be created and put into Unity catalog. A new version will be added. If it is approved and it is good, then it becomes the champion will be used for new data and it will score it and that will go back into the inference tables.
(1:31:58) Um so we were talking about like looking at our customer base and saying whether this person is going to churn or not and then if it is rejected and it's just archived and the whole process continues. Um after data preparation we're going to train we going to promote we going to validate and we're going to run inference right the flow should be clear. Okay.
(1:32:24) um some uh installs. So that will be fine. There's a setup. Um we are going to implement a customer churn model. So the marketing team has asked us to create a um a risk score. Um and then our data engineer team has given us the data set. What is the data set? It's called MLOps churn bronze customer. And it will be available in your catalog.
(1:32:48) So you can go into your catalog right here uh by clicking on this um it's probably asking me to connect probably. So let me connect. So now it shows me the yeah it's connected. So at this point I should be able to uh see the catalog. Uh let's see CS CIE. Yes. 103 catalog. And in here uh we said what was the name that we used for installing? We said um the schema is called MLOps.
(1:33:45) So that's the ML ops right here. Inside it we are going to see data as in tables. Now these are tables that you are already um used to but now we are also going to see models here and that is the ML ops churn model that you will eventually see as you are going to run through these uh notebooks. But the first uh uh the first step sorry here was to look at this data and you might do some quick analysis of um um what we call as EDA on this.
(1:34:23) There's a customer ID, gender, whether it's a senior citizen or not. Um a partner, uh how many dependents, uh tenure, how long have they been with the company? Uh do they have a phone service? Do they have multiple lines with you? Do have do they have an internet service? Um online security, online backup, device protection.
(1:34:45) All these are different um things that are available for you. And then there is a label column here for churn um that you're going to use. Um so let's go into the next thing. I will go switch back into my folder view and go into my feature engineering notebook where we are going to just light stuff to show you the art of the possible. This is not very heavy demo.
(1:35:10) Um again uh with this select you might uh want to well this is also interesting when you read a table a lot of you might have used pandas as part of your data science course and might be very familiar and comfortable with it. So it's easy to go from Spark data frame into appendas API.
(1:35:32) So you get a telco df. And if you're familiar with um uh with how to use it, if you're more comfortable with it, that's perfectly fine. You can start to say, "Show me the counts of um internet uh service providers." So there's fiber optic, there's DSL, and there's nothing. Uh you kind of get a quick uh read into it. Um but if you want to do it the Spark way, then you say Spark.
(1:35:57) table the same thing and then display and start to use your inbuilt visualizations as well. You're all familiar with it. Now uh there's always a little bit of data cleansing and featurizing. So here we are going to keep it very simple. We are going to compute the number of optional services they have because more services means less likely to churn.
(1:36:14) Uh we'll provide some meaningful labels and we'll impute some null values. Now if you were to use um pandas on spark API a little while back maybe I would like to say 3 to four years back uh data bricks introduced something called as koalas uh which was basically um on the spark data frame you'll use the koalas API instead of using pd for pandas you'll use kl or or equivalent to use the qualas api but basically up from after that your APIs look like pandas apis uh but since then uh we have integrated it uh I think starting from spark 3.2 to
(1:36:54) uh it is directly uh available. So you don't have to use squalas at all. The pandas API uh can be called. We saw a little bit of it earlier and you're going to see more of it now. Um you have a spark data frame and you say pandas API you get a um a spark pandas data frame and then if you're familiar or you're more comfortable with the pandas apis you can go ahead and do it um that way.
(1:37:19) What are we doing here? We are mapping some senior citizens. we are adding some total charges uh checking on tenure uh filling um null values for these fields and uh and so on. Right? So here we are saying if it is a one that's a yes if it's a zero it's a no.
(1:37:41) It's just a trivial thing but to show you that uh you can add uh um you can change the data type uh to string and u map out the values or maybe make some manipulations like that. um if it is um uh not a float then you might want to strip it and that's going to be your total charges. So when you feed that into your model it's not going to uh bark it at you. Okay. So that was a very simple function to clean the churn features and then you want uh another uh thing which is going to be the number of optional services that the person has and so you've you've kind of counted it you've used these columns online security online backup all of these and then counted how many of if the value is
(1:38:20) yes in any of these columns um you are summing that up right so you added a new column now these um computed features need to be written back uh along with the labels. So your base data set existed. That's your telco data frame. But in addition, you've got this number of optional features and you've cleaned up some of your other uh columns. Okay.
(1:38:46) Uh and you're going to write uh the table for uh training. Here you've done a split um and uh you you're calling it explicitly as train and test. um you overwrite the um data, you overwrite the schema and now you have MLOps churn training.
(1:39:08) You you can also add comments to the table and that will appear in your Unity catalog. And this is very very simple but it shows you the process. This is how you are able to add take the columns, cleanse it and add additional features which are going to be fed into your model. The next one is going to be the actual creation of the model and we are going to use light GBM to do so. So this is the second step now.
(1:39:36) Um here you're going to um give it a name um and your your current path that is where your experiments lie. So in MLflow um you have imported it. You can say get experiment by name. You can get experiment by ID as well. Uh so you get it the name and then you get the ID because this is very unique. Sometimes people might use the same name.
(1:39:58) Um and you print it out. So in my case it is uh this is my user and this is um DB demo MLOps churn demo quick start and this is my unique experiment ID. It's basically a grid data lineage. Uh this is very very important. Uh your you need to capture what data was used. Uh so that sometimes when you do root cause analysis and all it will be important.
(1:40:33) How would you do it? You will say MLflow data load delta and then you give the entire training data set and this was the version uh that you use that is your source data set and you will log it just like you're logging parameters you're logging metrics you're logging figures you're logging um you know some other artifacts and tags you will also log this as an input which is your source data set and that is that was your training training input.
(1:41:04) So later when somebody's looking through the traces and the logs, they will know, oh, this data set was used for training. Remember we talked about the code and the data, the hyperparameters and all of that other stuff. Um, is that making sense that it's important to capture what data was used for the model and so having this MLflow log input of the source data set and how do you get the source data set? MLflow data load delta by giving this was the table that you used remember MLOps churn training table and the latest version of it whatever it was whether it's two or three or one it
(1:41:39) doesn't matter it's going to remember it now well I'll pause here because this is super important not there are so many other MLOps tools uh but they do baby stuff and this is the real deal if you can't capture the data and the code and the parameters and the metrics together you will not be able to retrace but this is creating a copy of the of the data. No no no it's not it's just metadata. Oh okay. Mhm. Which is going to get tag.
(1:42:10) Remember last time we saw that this model used these parameter values. It produced a metric whose RMSC was this value and it logged these graphs. This was the ROC. Um this was the AU. this was the um uh confusion matrix whatever it is all of those stuff like that the data uh set that was used to train it is the lineage of the model so it's the data which produced the model so that's the upstream lineage for it and that needs to be logged on and da I have a question what what about the lineage of the data that was used to produce that input table that is already available as you're
(1:42:50) doing the transformations the lineage of the data is automatically captured. But in MLflow when you're creating the model, you'll have to track the the the data that was used to create it. I see. And then you go to that data set, you will see the lineage of the how that was created. Gotcha. And data lineage is a little more wellnown than model lineage. And this is the beginning of model lineage.
(1:43:18) Um now we um find the data set object from Unity catalog. Um we load it into delta table. We might um filter on uh train. Uh so there's a column called split uh customer and uh split which we don't need anymore. We get this. There might be some pre-processing that we would have to do on this uh data.
(1:43:43) Why? Because we have different types of data. We have boolean data. we have numerical data, we have um categorical data. Each one of them has to be processed or pre-processed differently. So in this uh case, let's look at all the columns which could be boolean which is like gender, phone service, do you have one or not? Number of and do you have um dependence, yes or no. Senior citizen, yes or no.
(1:44:06) All of these are going to have a boolean pipeline which has like a transformer um a column transformer uh and an SK learn one hot encoder. Similarly the numerical columns are different and you create a pipeline for that uh and these are the numerical transformers and remember we have two things we have transformers and estimators.
(1:44:30) Transformers are always taking a data frame, producing a data frame and an estimator takes a data frame and produces a calls a fit on it to produce a model. Uh same for categorical you have these one hot encoder which is going to um encode uh some of these categorical vehicles. Now um you will bundle that into a single pre-processing pipeline.
(1:44:54) So you've got your transformers include your boolean transformers, your numerical transformers and your categorical one hot transformers and your pre-processor is going to be this transformers. Um that um that's the first phase. Then you're going to do the usual stuff of using an sklearn model. Uh your label column is churn. Uh you do your uh split. Um this is all very very um boilerplate code.
(1:45:21) Uh you use an LG BM classifier. Um so now you're going to define a training function because you'll call it again and again with different types of parameters and those will runs within your experiment. So here uh we we have uh the the classifier you built the final pipeline which includes your pre-processor and the classifier and you've used auto log.
(1:45:46) So all the model characteristics it'll silently log everything. You fit the model you say that you need to infer the signature and you log the model. You log the input remember the source data set that the lineage that we were talking about. Um then uh you're going to um uh log the metrics for the training set which is like add this model.
(1:46:12) Um then uh wrap it around a pyunk model so that everybody knows how to call it. Uh and there are two parts that you will evaluate. You'll you'll evaluate once for the training and you'll evaluate once for the validation set. And whatever metrics you get you can you can print that out or you can log it or whatever.
(1:46:31) Uh you'll you'll um uh what is going so you are now going to return what is your loss, what is your metrics, which model was used and what was the run. So this whole thing can be called again and again because you have put it inside a training function and these are what we refer to as the hyperparameters which will change because the model is exactly the same.
(1:46:56) Uh now hyperparameters usually defined as a space. So in this case this is just one uh run that we are saying we gave the run a name. We specified lambda learning rate max bins blah blah. So we call training function with these parameters. What do you expect will happen? A single run of that experiment is going to happen and you can click on each one of them to see additional details. Um so this this is finished.
(1:47:23) uh you print out what was the model loss and you you have actually um got the model itself. So you can reference the model which is basically at that point is just the image and you can see the highle design or the flow. This is your pipeline. You had the pre-processor boolean numerical one hot encoding.
(1:47:46) You had some imputtors in each one of them. You had some one hot encoder and scalers. Once the pre-processing is over, you use the LGBM classifier to figure out how good your model was. Um, then you are going to get the run. You can display something. This is the link to the model uh page. Actually, let's go ahead and open it up in a new window so you can see it.
(1:48:12) That's the light GPM baseline. And these were all the different things that you have. Um that's the estimator HTML. This is the training calibration precision recall curve ROC confusion matrix. Um so it's pretty cool, right? It's able to do everything. And the model itself is here. It's got the packaging.
(1:48:36) It's got the requirements.ext. So these are all the different uh uh pip installs that needed to be done. Uh there is an example of how the serving input is going to be. So remember this is this is an example of how you're going to call it for inferencing. It has to be the same set of features.
(1:48:57) Um then there is the YAML file uh which has some dependencies. Then the model is available as a pickup form. There's an input example JSON. So you can just use this uh to test it. Uh there's a cond environment the model itself uh and it gives you examples of how to validate it, how to make predictions.
(1:49:19) So makes it easy for you to start um using it even after you've done it. So there are about 20 inputs each one what they are that's the model schema and the output column is your churn column right there could be some additional metadata around it you get the idea that's the model run page that single run that you logged um now you can uh make some temporary directory and download the artifact because the same person who created the model may not be the person who's going to use it Right.
(1:49:51) So this is simulating another person who's downloading all the artifacts. Um you give the unique run ID and you give that temporary directory path and that's the evaluation path in which all the um uh all the uh artifacts that we just saw on that other page comes in. So you can now say get me the um confusion matrix PNG and display it. Uh we saw this looked exactly like that in the model itself.
(1:50:18) What is the ROC curve? what is the precision recall and so on. So at this point we finished the second stage which is training of the model where people do spend a good amount of time. Now we are going to see how this is going to be put into uh Unity catalog uh which means we are going to put it into the registry. Um so these are common pip installs.
(1:50:46) Your model has got a name in the same catalog in the same database. is called MLOps churn. Uh you want to find the best run which we did a uh a single run but somebody would do multiple multiple runs. So let's see how you can search among your experiments and then within the experiment how you can search uh for the best model. So your MLflow experiment was set to this path and you're going to you're going to find the best uh um once you've set the experiment you're going to find the best run and then push that model into your registry. Right? So
(1:51:26) MLflow search experiments for a name which looks like this is going to give you the experiment and from there you get the experiment ID and you can print it and gives you this unique ID. Um you're finding the best run from the TB demos uh quick start and pushing the new model. Okay, whatever.
(1:51:53) Now that you have the experiment ID, you're going to use that experiment ID to search all the runs within the experiment and find out the best F1 score and you want only one result back. So obviously you're going to that's going to be your best model and you're printing that out. It says this is the run ID. This was the experiment ID. The status was finished. Here you also had a filter. The status should be finished and the running should have like GBM baseline or anything else you want, right? You might have some other criteria. So it shows you all the details. Uh so that's good. We found our model. Uh we are going to
(1:52:25) register that model. Um uh how will you do that? You'll say mlflow register model. You know what the run ID is, right? It was an skarn model with this run ID and you gave it the model name. So what what do you see? You're registering the model to this catalog MLOps churn which I showed you earlier.
(1:52:49) You successfully registered it and because it's the first time it created version one of that model and again you can click on this to go to the registry. So let's open it in a new window. That's your MLOps model. Um you can see all the metrics your F1 your log your precision your recall all the parameters all that you know all that encoding that we were doing with its numerical boolean parameters this is lineage this is what we were looking for.
(1:53:26) So upstream this was the data set that was used and this was the notebook. So this was the table this was the data and this was the notebook which has the code that built it. So I can click here to take it back here to as to what was the data that was used.
(1:53:46) And if I open this in a new window, it'll take us back into the code that we used to build it. That is what is referred to as traceability. This was the second notebook where we built the model. How cool is that? It's actually a big deal. Most people don't even know what model they are using, what is what they're up to. But in this case we were not we are keeping those breadcrumbs so that at any time in the process we can go forward or go backward.
(1:54:10) Um and then the same artifacts that we had seen is all here as well. Uh give the registered model a description. So again here it's just forcing you to use the API in different ways. From MLflow import the MLflow client. Using the client you update the registered model. you know what the uh model details uh are from here because you you did that.
(1:54:37) So from there get the name and oh by the way put this description as well. So it will say all right I did this and blah blah and you can go into that uh place to look at the description. So back in back here you can go to CSCI catalog um MLOps is the schema and the model is right here called MLOps churn and you can see the description this model predicts uh whether a customer will churn.
(1:55:12) So if I hover over here uh you can go inside it and see it as well but you can read it right here that it was updated uh uh this model predicts whether a customer will churn or not. So this just shows that you can you can interact with it so easily. You can add as much of metadata as will be useful to somebody else who's going to refer to it. Okay.
(1:55:35) Now you get the F1 score, you get the run name, you get this um version description. So you you have a a description for the model in the registry and then every version that it has can have additional metadata also. So in this case you are giving the description for the version itself.
(1:55:57) Again you can model update the model version. So this is the name, this is the version and this is the version description. So we had version one. If we go into version one you'll see these things. Um you can set tags also. So tags will be useful to search for it later when you have a lot of models.
(1:56:15) Uh maybe you can tag by your department name, you can tag by your use case name and so on. Um so here we have said your key is F1 score and that is the value. Okay fine. Uh set the latest model version as the baseline challenger. So whenever a new model is created, it is the challenger model. That's the alias.
(1:56:40) Now let's go to the next um notebook which is going to be validation of the challenger model. So in this validation notebook which is this step uh we are going to do some uh general inferencing and checking. So we'll um we'll uh import this model artifact repository. And this time we are not using runs, we are using models because we're picking it up from the model registry.
(1:57:07) uh we get uh the downloads um artifacts we look at the requirements text you can do whatever you feel like because using APIs you can access it all um now our model alias was the challenger and our model was in this catalog in this data in this schema and its name was MLOps churn so again using MLflow client we'll get the model version by alias so this name this alias is going to get the model details and from the model details you can get the version as well.
(1:57:40) So it says um the ver the model was MLOps churn and the version was one. All right so far so good. Um we might do some checks um like this is simulating the fact that there is some validation and some quality um checks on a new model that has transferred.
(1:58:02) For instance, you might say, "Oh, it should have a description or it should have F1 score greater than some value or um whatever the logic may be using these APIs, using these model details, you can access each of the metadata and check for it. Now, if it everything uh works out well, uh then you'll say yes, this is all good. I say pass, right? This is automated way of uh checking. This is not manual checking.
(1:58:28) " then you're going to promote this only if this is better. So yes, it looks good from a quality perspective. But is it really better than the champion that is sitting in production? Now in our case, we do not have a champion because this is the very first time. That is why there is a try catch block here where you are getting the F1 score of the champion and you have the F1 score of this challenger and you are checking whether this score is greater than the existing champion.
(1:58:55) If it is then this will be true. Now if there was no champion found then also this is true because this is the first time this is going to go into production. So you are going to say no champion found. So just accept this model because it's the first one and um yeah this this is indeed passed. The F1 score is uh better than a non-existent uh champion.
(1:59:18) So now you're ready to push this into production. Um you might do uh you might have a small function to test it uh to predict churn. So you can manually verify some of the parameters and say is this making sense? Is this really good or not? Uh and then this this whole thing is um to get some model value in uh dollar.
(1:59:41) So this is a hand wavy thing in which maybe you have a business use case where you say that this customer has cost us this much and if we lose it then this is the uh for the next 10 years that we would have expected this customer this is the value in dollars lost so that your churn yes no has got a quantifiable business value.
(2:00:09) This is the hardest thing to do but usually most businesses have a certain mapping of um what is the cost of customer churn um minus some discount that you may have given. So you to entice the customer you might give some discount if you give the discount which is greater than the cost then kind of is um uh lost cause right. So how to find the right amount of discount that you need to give.
(2:00:34) So this is where the business aspect not just the tech aspect is coming in play and this is a very silly function of getting the model value in dollars. So if you are able to detect more customers who are going to churn and they indeed churn and you stop them in time. So the model value in dollars of this challenger model is going to be better.
(2:00:53) Um so you do all of that and then your validation results I think uh no champion was found. So just accept the model as its first one. Validation results was good. Um you are ready to promote the challenger to champion and you have given some amount of dollar value also. Uh I forget where that is but it's it's all right. it's not that relevant.
(2:01:15) Uh at this point, you have used your MLFlow client again. Uh set the registered uh uh model alias. You've given some descriptions. Uh and then where did I register the model as champion? Where did I do that? Set the register model alias. Okay, now that earlier it had the challenger alias. Now it has become the champion alias. and it is in the production environment or in the production stage. So, okay, done.
(2:01:47) The model is now existing. So, let's test it. That's the batch inference which is the last stage in which we are going to simulate some data. Where is it? First, we are going to get the get the model. Uh this time it is the champion model that we are getting because you you can have aliases associated with the name. Earlier it was challenger.
(2:02:11) If I go back here, I think we refer to it as the challenger model. But now that we have promoted it into production, uh this is as the champion that is the model. Um we are going to read some uh data. This is simulating some new data using a spark udf. Um we are going to get the uh model say that use this model against uh this data.
(2:02:40) So champion model. So with this data add a prediction column by using this champion model and all your uh metadata. So all your column names are being passed in. Uh you can see here this is the data and the final column is going to be a prediction. This didn't exist earlier.
(2:03:00) you had suppressed the column for prediction. Now you are seeing yes no blah blah. Um do you remember that we had created the training set and we had created the inference set. So this is the hold out set that we are uh getting and we have um gotten our model as the champion model.
(2:03:29) Now we are saying with the inference data add a column called predictions use the champion use all the input schema and the input names and then score it display my uh final data frame with the predictions which is here and that brings us to the end of taking raw data featurizing it training the model validating it promoting it to registry and then inferencing it. We are a little over time, but this has been this is necessary to actually go through the full nine yards to show you any questions.
(2:04:04) All right, with that then let's um close for tonight. Next Thursday, Ram will go over assignment uh four um as to what is required um and then um that will be the last assignment. We'll have one more case study and one more quiz and then of course your final. So we are coming very close to the end of course.
(2:04:28) Good night everyone. Thank you. Thank you. Good night.