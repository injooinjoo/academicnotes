89 day 10 section - YouTube
https://www.youtube.com/watch?v=I7YJ2s3TZNo

Transcript:
(00:01) Hello everyone. Hello Dr. Craft. So today we have uh named entity recognition. Now I know that you're still working on your assignment which is okay. You can finish it. Um and uh I'm going to talk about named entity recognition. Your assignment is number six right now, right? Or seven already. Let me check.
(01:00) I think this one is eight. It is eight or it's okay. So yes, your assignment is number is number eight. Correct. And let me open assignment number nine. It is next one. So it is not due yet. It's not even posted yet. So don't worry. Um and it has three problems. First problem is quite straightforward.
(01:46) You just need to run this type of predetermined rules, right? Prespecified rules to kind of understand if it is about person. We have to have titles professor, Mr. Ms and so on. So it tells you how you can use this regular expressions in order to identify for example people maybe time, day in corporation in this case and so on. So this first problem is quite straightforward. You can easily understand how it works.
(02:14) So you will have to sort of learn how this regular expressions work. You have to understand what is behind why we do it this way. The second problem is about um applying space here right and we have to run this name entity recognition basically whatever we did last time and plus is what you have to do in this case in problem two it is also quite straightforward.
(02:41) Last problem is a little bit more interesting we'll talk about this problem at the end. So last problem asks you to somehow try to use named NC recognitions in order to improve your model. To be honest, I I'm not sure if it's going to be improved or not because data set is very very small. In this case, we have only 124 news articles and we are trying to somehow classify it using using the neural network.
(03:13) It isn't of course must be something like maybe fully connected neural network because you not able to train a recurrent one using only 124 observations in this case we'll use some kind of TF first of all and then we you will split it into into two parts as always it means your training data set will consist of only 80% out of 12 24 articles and then you will try to uh first classify those articles using TF representation as it asks you to do it and then next step would be to try to improve it. We're going to enhance it. Again, I'm not I'm not claiming it's going to be improved.
(03:55) So maybe maybe um I'm giving you some kind of accuracy. So I checked that is achievable. But um the thing is I'm not sure if uh without this name entities it is much worse. So I'm not claiming it is much worse. Maybe some of you can do it without enhancing using entities. Maybe you can improve you can do it even better without entities.
(04:22) But the task is explicitly to try to enhance it using entities. Means you'll have to actually use TFS first only TF to do classification and then you will have to use TFs and you create artificial columns where you're going to provide information about named entities which you're going to recognize using space probably space. So that's what we're going to do.
(04:52) Again later maybe we'll talk about this a little bit more but now at first let's um see uh simple tasks. Let's say first task will be to try to understand um uh I would say try to uh recognize part of speech. POS text so called using NLTK natural language toolkit and uh in this case I'm going to import it and I'm going to look at my uh target tag sets what it means it means just kind of description how NLTK actually specifies what kind of part of speech it is for example conjunction it will specify it CC right if you don't have this kind of reference we don't understand what it means that's why we need to be able to
(05:38) display it so we can understand what it means or JJ what is JJ it is adjective right and uh what is example n it is noun n and s it is plural noun right and we have examples here it also provides examples right away for example if I'm talking about JJ it is for example oiled right oiled it is adjective example then we can look at let's say noun humor is noun and s plural undergraduates you see this plural already so it provides you also examples so we're clear what part of speech it is um yeah That's how it is.
(06:35) Uh to O is simply to what else? Let me see proposition where proposition is. Um in okay in it's like example preposition basically it's going to be proposition in means not in in means like among upon out and whatever basically this preposition beside next and so on. So that's how we kind of going to label them like how package is going to label it for us.
(07:09) So now let's uh see what we can do. We can take text. In my case I just pulled some article. I did it last year. So it is one year old article about Len Mask. Uh it was just from from news from New York Times. And uh I'm going to actually apply my uh tokenization right it is sentence tokenization that means I'm going to break it into sentences.
(07:41) It will for me break my document into distinct sentences. So my sentences will be those sentences. Then I also can actually no I can print it out right will be original sentences. It says original sentence as I will print it out one by one. I will say sentence first, sentence second and so on. It will look this way.
(08:04) Sentence first, sentence second and so on. So it shows me what kind of sentences I have in my document. Then also I will take basically loop over sentences. This is like this list and I have this four loop inside. And for each sentence I will apply a word tokenization. So each sentence will be also later broken into tokens as usually we do it then. Yep.
(08:34) So why we are breaking into sentence? Why can't just we break it to word? Yeah. In this case I just wanted to kind of show demonstrate how we can actually break document text into into into documents. There's always this question how can you break text into into like for example paragraphs or sentences.
(08:56) So I did it this way just for convenience basically. It's not like it is it is necessary to do it right but I wanted to have like multiple kind of sentences in this case. Okay. And I didn't want to do it like manually. So I just show you how to break it how to break. But does it but sorry.
(09:15) So does it make any difference if I just break it into word in terms of performance or accuracy? It should make some difference. Yes, it should make some difference because we're talking about context, right? And and so on. In my case, my sentences are sort of uh different from each other, right? We sort of move to to next observation every time.
(09:34) And if you have like one single observation which consist of huge text, it is for me speaking different. That means if you're talking about like end of sentence for example then that case you sort of when you talk about like name you talk about context your previous sentence already becomes like part of your context and so it means it it should matter. Yes. Okay. Okay. What are you thinking? Yeah.
(10:02) So now let me display my recognize sentences. They will look already this way. Sentence one is right here from electric cars and so on as it was before just talking nice sentences. Then second a nice sentence and so on. Then I will take u my already uh part of speech attacks part of speech recognition is used in this case and I will try to understand for every sentence what kind of part of speech every every token basically is going to be.
(10:40) So I I apply this way for every sentence from my already tokenized sentences I apply part of speech no kind of recognition technique and I will um display them right so already pos sentences will be displayed and already shows me this way uh shows me this way from it is um proposition it's like in in out and so on proposition electric is adjective, right? Cars is noun but in plural form two is just to solar is again adjective. So it's understood. Panels is noun and so on.
(11:21) So it is it shows me what kind of part of speech it is. Pretty cool stuff, right? So it shows me what part of speech it is every every every guy. And now I can uh also try to uh display my uh uh let me say there's a function I define which is extract entities and I will display all those which correspond to the people to persons and u in this case I will do it kind of manually not in the best possible way clearly but I will do it using this kind of keywords like Mr. Mrs.
(12:05) Mrs. doctor, professor. So it is not like completely it is not going to recognize if someone doesn't have those uh titles but this is how you can basically manually do it if you have to. If you have to if you want to you can manually do it this way. Let's say person is going to be the one who has this things in front.
(12:25) No you you better try to understand how how it is structured. So basically we have here as you can see we have here Mr. Mrs. Mrs. Dr. Professor we take one of those right vertical bars means either basically or case. So one of those then we say u uh maybe maybe yes maybe not there is that question mark means maybe yes maybe not. If not it is okay. You don't have don't don't require to have it.
(13:02) So question mark afterwards means maybe yes maybe not. So maybe there is this dot maybe not. No this means actually that so it's understand that I'm talking about specific symbol like that. Then I say s which is space and plus means one or two or three or more. So basically this s plus means at least one space one space or more basically.
(13:31) Then I say in this case it is like last name essentially. So in this case one of the capital letters right? Then I say lowerase letters with plus means one or more lowerase letters essentially and then space and then it tells me question mark which means I may have it I may not have it. essentially I may have it I may not have it and then again this is my name again because I may have first name and last name I may have only last name that's why one of those is optional so it is like first name maybe there maybe not there that's why question mark question mark here means essentially we
(14:17) kind of choose one of those and this question mark means second question mark means it is like optional you may have it you may not have it and then here I also have uh my capital letter at least one and then lowerase letters and then B means boundary of my word boundary means like like empty space like like space between words but it doesn't have to be empty it could be like punctuation comma or maybe end of sentence at the beginning it could be like beginning of sentence and so on so B is like boundary of words boundary is not just space it is more than than space maybe column for example okay and
(15:01) then I I try to match it and basically I see whoever is in my text you can see my text you can see that there is like Mr. Mask should be recognized, right? Wells. So maybe there is someone else in this case Donald Trump. Maybe someone else. I think there was a Lee Chang. Mr. Trump should be recognized, right? Okay. So that's how it is.
(15:35) Again, this is very simple approach. Of course, it will miss for example Donald Trump. As you understand, there is no way to actually understand it is about person. It is very simple approach. Nevertheless, if you think about applications where you want to enhance your model, enhance some kind of performance, it's not like completely useless.
(15:54) Yes, you don't uh maybe catch everyone, but it could be kind of um useful tool to enhance performance of some kind of model. you you build something under the hood you understand that it is doing this not in the best possible way but at the same time you understand that it is just enhancement of your model it means it doesn't have to be like 100% correct so Dr.
(16:16) Karach if um if you were expecting for example let's say you were dealing with uh with with French uh subjects right and there was a possibility that like sometimes like the last name Llas is written as two words but with the first L capital and the P also capital would we have to adjust a regular expression for that or would it catch that as it's written right now? it should not catch it because in that case you understand what you're saying in that case yeah you have to adjust you have to actually it's a very good question you have to that's exactly
(16:45) problem with this regular expressions you have to kind of expand expand expand expand you know this kind of list of rules but you're correct it will not catch it because in my case you see I have one capital letter and the rest will be only lowerase letters it it literally says only one letter plus means one or more but it means Means lowerase letters will be one or more lower case letters but capital one is only one letter any of those but only one there's no plus and since you want to now have like capital and capital again and also there's like apostrophe right plus it has like apostrophe
(17:26) so plus you write it like L apostrophe right plus I don't remember this way. Yeah, you could also do l a p l a c e. Oh, you mean? Yeah, like that. Yeah. So, it it will not catch it. The way I did it, I just wanted to demonstrate how basically this rule-based approach works, it will not catch it.
(17:52) At the same time, if you think about some kind of let's say clarification problem and you say no, maybe I missed something, right? It's not as important maybe, right? you you can kind of try to to work hard to catch all the cases as well, but it may literally have no any impact on your ultimate performance whatsoever. Maybe maybe it will have, maybe not.
(18:14) Right? So if your text is about people where many many people have this type of uh names then it may be a problem but otherwise it may be not as important. uh and uh yes so we can see here like the last one is person Mr. M m mo mask Mr. Trump were added there. So we obtain those as persons.
(18:45) Any questions about this? It is just very very simple straightforward approach right when we try to use matching. So so it missed a Chinese premiere because it didn't say mister right. It said like Lee Chang. Yeah. There was no mister. That's why. Yeah. So there it missed it. Yes. Exactly. Yes. So it may be important, right? Maybe if you try to but the problem is if you try to already generalize your rules you may catch something extra which is not a person right so it becomes problem in order to understand what it is actually you need to have some kind of context around also maybe you have to have like literally
(19:21) vocabulary dictionary of your of of famous people for example of names and so on it is very difficult task of course you understand especially if you're talking about like name and also a name of a company or name of a car. It becomes even more complex because you're talking about context only context can can say can help you basically or save you from mistake.
(19:47) When we read it we understand what it is talking about because of context it means just matching is not going to work. So now next approach is named recognition in space here. So in this case no I clearly imported I take this specific model small SM means small English core small model you you can see some variations right small runs faster is like small model and I take my text then I say NLP remember NLP means a bunch of different different steps a bunch of different steps it is actually pipeline to be precise it means
(20:27) a bunch of different transformations will be applied. One of those is named entity recognition on the way we are going to do that. Right? So now uh we have this results and I can display my uh entity which is mask type as person. Second is bin type is geopolitical entity I think right Chinese type is NR O.
(21:02) So N O means nationals, organizations, religious or political groups, national organizations, religious or political or political groups quite general, right? And so on person, person and so on. You can also nicely uh plot it kind of visualize it this way and it will generalize it this way. In Jupiter notebook you can display this way from electric cars to solar panels. Mr.
(21:36) Mask it tells me person it tells me person Benj Chinese N O N O R P and so on. So this is person. Now you can see it already knows it is a person because it little bit smart than smarter than what we did. Tesla organization. Tesla sells. Okay. This organization not a car but organization vehicle make electric vehicle maker Tesla.
(22:08) So probably organization probably organization probably organization. It is correct. I think in every case even though Tesla could be car right but in this case organization. So any uh questions about this? Okay. Donald J. Trump is person so works. So now last time I told you that you actually can try to also uh tune those models.
(22:36) You can try to uh supply additional training data set and try to basically train a little bit further as a starting point. You are using current weights. Clearly you don't want to train from the start. You're using current train current weights. You add some kind of sample to your training data set.
(22:58) Whatever Python has, whatever library has you will take artificial not artificial I'm sorry you will take manually crafted or whatever whatever how you craft it I don't know but some data set of training data labeled ones and you will add it to existing data set and training data set effectively becomes slightly larger and you can train it a little bit further. That's what happens here. In this case, I save this my small model as before.
(23:23) And train data is going to be cars in China. And let me introduce entity which is going to be vehicle. I'm going to say I want to specifically have a vehicle, right? Cars in China. So let me say sells cars. Cars is not vehicle. It doesn't recognize it. I want to be able to say it is vehicle for example, right? So I will say cars in China is my new observation.
(23:55) Then I say entities and it is going to be called vehicle and it is from 0 to 4. It means 0 1 2 3 from 0 to three is my is my token right is my word and I will say from 0 to 4 which is which is new entity vehicle which I introduce here. So given given can the given entity be multiple words instead of a single word vehicle uh given entity can be multiple words as you can see here right? So it is a person so it can be a mask is a person.
(24:42) So now Tesla has a lot a lot on the line as an electric vehicle maker and I will say uh vehicle vehicle vehicle. So basically I specify location in every case. Um I don't know maybe you know what I just realized that maybe have to be a little bit more careful. Okay let's let's take this data set. Let's assume it is it is okay. Um, my family loves our Honda Civic, right? So, 23 34 0 1 2 3 4 basically 23 to 34. So, I hope I didn't mis miscomputed it.
(25:24) And then this car is the last one is also vehicle specified as well. Now, how are we going to to train it? So, remember NLP NLP is going to be pipeline. That means it has a lots of different uh different models and I will extract from here ner I call n e r this step which is responsible for basically computing my named entities so I will extract it from here explicitly because I want to kind of twist it I want to tune it so it's going to be called tuning I want to tune this model and then I say let me um let me look at my uh NLP. It is pipeline. It has a lots of
(26:10) models and I'm going to take um all of them which are not ne which is not named entity recognition. I will pass it to my disabled pipes and everything else except for neer will be disabled. That means I will not tune those. I will tune only neer others will not be will not not be changed and then I'm ready to essentially uh train it.
(26:46) So I'm going to I'm going to uh say it this way. I'm going to uh create optimizer. I'm going to use um my dropout 0.5. I'm going to take my So it it is important maybe to to start from here. My examples which I'm going to use to kind of train my continue training my model on will consist of two pieces. First piece is original examples.
(27:19) So it will have original examples which were there already and then I will append my examples those examples which I manually crafted right. So my examples um will be coming from train data set and we'll have like um examples which it will continue training training my model on and then I can take my examples here and we'll update my basically pipeline but remember pipeline is not going to be updated from the start I mean not all models will be updated only model which is ne effectively because I disable the other
(28:00) ones. I want to train only only name and recognition and uh I'm going to do it again 20 and then I say let me visualize my results. So there's my results. Let me scroll down and see what happens with those uh uh things. Let me see if I have any any vehicle. I think I I don't see anything right. So let me see.
(28:35) Yes. So it is not not enough yet. So we have to maybe And it also began mclassifying Tesla as a person, right? Before it had Yeah. Yeah. So yeah. So something is definitely happening. Exactly. But this is how we can do it. But yeah, of course it's so little examples.
(28:59) We are not able to um you know maybe I have to clean this uh examples a little bit. Carson China is already vehicle. So it should be kind of be able to understand it is like a vehicle but on the other hand on the other hand it is like not only about cars it is quite complicated model right which tries to identify all the entities.
(29:30) So it means uh we're not only focusing on cars we trying to twist all parameters. That means no guarantee that we will update whatever we want. As you noticed they they basically on the contrary update updated Tesla which is now a person by the way it is everywhere person right which is probably not not what we want at least sometimes it must be company sometimes person maybe in this case there is not no Tesla person so that's how we can do it now of course it is just little demo which shows you how you can do it but on the other And if you have like huge data set and you have a
(30:09) labeled uh named entities you can update it. As you can see it is very difficult and routine work routine task not so straightforward. So now let me open again the assignment uh question three. Let me let me read it. In this problem we will construct new network with news articles and then enhance it using the equation features.
(30:39) So you will use news data CSV data set. Let me see. So this is how it looks. You can see here our headlines, the article itself and also the type. So how many types do we have? 1 2 3 4 5 6 seven types. So we have only 124 observations and seven classes. It is very very difficult problem.
(31:42) So we understand it is very difficult problem right in order to kind of classify it correctly. You have to probably have more observations. But that's what we are going to work with next time in problem three. And let me now see what what we should do in this case. So in this case we're going to load it.
(32:01) We're going to encode our category into numerical values. We combine uh news headline and news article into single column. I will just create single text which will combine two columns together. It will be like one basically text headline and also article together. I'm not saying it is like the only way to do it but I want just to create one single text basically with which we're going to work.
(32:41) Um so what the training what the models will be able to predict it will basically be able to predict slightly better than random guess because random guess if you guess randomly you are going to be correct only in one out of seven cases. Think about this. One out of seven cases is going to be your basically guess, right? Best guess if you do it randomly. Model will be better than that. Definitely.
(33:05) So definitely yes, you can still do better than random random guess. No questions. Uh the thing is I'm not as much concerned about the number like you know like okay we kind of you know correct in 99% cases. It's not about that. It's about techniques. If you understand technique on this small data set, you can always enhance and take different problem where you have millions of observations and apply the very same strategy. I can tell you more maybe from modeling point of view.
(33:36) It's it's also interesting to see how you can try to select important versus not important uh features. Well, if you have large data set, there are different issues. There are issues of computational kind of kind of nature because you have to be able to somehow fit it somewhere somehow. You have to be able to train the model.
(33:59) Maybe it is on just regular laptop clearly, right? You may have to maybe split your data where different servers and so on different different issues. It is also quite interesting problem. But in this case, okay, we have not many observations.
(34:17) It's not like something that you would not want to consider as at least as examples exercise. It is very nice nice problem to consider. Okay, not many articles but we can try our techniques on this problem anyway. Right. So now I'm going to combine these two basically texts together into single one. Then we're going to split our observations.
(34:43) That means um we are going to have only 80% out of 124 for the training. Then we use TF factorizer. Um and we have only 124 observations which means you have to carefully think uh uh what you're going to actually do in this case. What kind of model you want to do in this case somewhat like the current network is not not not not nice approach obviously.
(35:10) Um maybe I don't know probably not. Uh then maybe something like fully connected network is the best approach. I think I tried fully connected that worked u somewhat worked. Let's say there is like goal which I specified which was chosen based on what I obtained and uh also in this case you may want to think carefully what kind of you know features to take.
(35:36) You don't want to take like all tokens from your vocabulary. It would be quite quite expensive. You 120 observations is not enough even more than 124 because you have only 80% of those. Let me say it is assignment number eight right on let's say problem three of assignment number number number number number number number number number number number number number number number number number number number number number number number nine actually so number nine and uh you you understand that we have title we have the article itself
(36:17) let's Okay, title we have article itself now there is this kind of boundary right I'm asking you to break break the boundary I'm going to ask you to combine it together as if it was like single text uh let's say there is something like seven seven What? Uh, okay. Let me take this example. Smith isn't that far away.
(37:02) Smith and so on and so on. And then we say this one X India and so on. Basically we take this two entries title articles there are sort of two different columns and we are going to glue it together as if it was like single text. So this boundary almost like is not there essentially and then it's going to be category and then I say sports for example this case sports this way and so on.
(37:44) So this is my data set effectively not 124 because I get only 80% of one of 124 around 100 100 articles is going to be my text and uh we have to basically be able to perform it. Now when we create TF we should understand that we may get still a lots of different u tokens it means number of entries. You understand that you're going to transform it to tokens, right? You're going to say there is like a first token token, then uh the second token um and so on and so on.
(38:35) So those tokens will be individual columns as typically we do it. But the problem with this is it is like token number one, token number two and so on. So the problem with this is going to be that we are going to actually have a loss of a lots of tokens. You have to think how many how many to take you you you create here.
(39:02) So I have to maybe simply say it is my token which is Smith is my token. Then I create my TF IDFs.9.1 and so on. Those things are TF IDFs essentially and more and more and more and also that way more now you have to think how many to take. You don't want to take like all of them because we have restrictions in this case.
(39:28) You can take like maybe like 20 tokens for example, right? So you have to think how many to take and you have to take the ones which are somehow most important, right? So you can have to think how to choose the ones which are most important ones. Uh and then it's not the end of story yet. It is basically only end of question A.
(39:54) So question A ask you ask you to do that and you will have like 20 tokens and then this is category and then we are going to say um no let's say uh sports and so on those tokens are fixed one as you understand they all fixed right they all unique all fixed first column corresponds to Smith second colum Al X India and so on. We don't want to have so many tokens. That's why we have to somehow restrict number of tokens.
(40:29) Think about ways to do it and then going to build the model and then second step will be to actually try to enhance it somehow. So trying to enhance means we are going to take our original essential data set. In this case let's say I have Smith as my first token, X India as second token and so on. And then at some point you're going to say this is what I chose like 20 tokens for example, right? And then I say let me also try to add some kind of representations of my uh entities which I can identify there. You understand that each line
(41:26) corresponds to document first document second document and so on. So now let me say I'm going to uh use uh space here to in order to understand my entities converted names and I just into binary diamonds indicating the presence of absence of each entity. So in this case um we're going to integrate features with named entity diamonds to create a comprehensive feature matrix.
(42:04) So in this case we are going to um uh do the following. Let's say let's say I'm talking about auto mobile right for example let's say I have some kind of cars and I decide that my my entities is like Mitsubishi for example. So let me say convert names into binary d variables indicating the presence or absence of each entity.
(42:36) Ensure that this ensure that this d variables are correctly aligned with your data set and so on. So now I'm going to say there is let's say no let's say uh let's say Toyota for example Toyota right Toyota and I'm going to say Toyota is like entity I recognize that is entity and first for example data set doesn't have a place zero but later maybe somewhere else it will be about about like um cars or whatever it may have it so it will have one somewhere then zero maybe one again somewhere right and then zero so for Toyota I have it then I'm going to continue and say next kind of entity whatever it is um
(43:20) it's going to be about what what do we have there let me see it's going to be about um no let's take example I'm not sure if there like for example example, right? And again, no, no, no. Somewhere there is Audi and then zero everywhere else maybe somewhere one again then zero.
(43:52) So it will be our basically data set which will be our 20 tokens. Now by the way if you want you can change number of tokens to different number of tokens. If you add entities maybe realize that it is already supposed to be different number of tokens plus some entities some entities this way and I have a number of entities right some entities and of course last column is still going to be no maybe more you understand that maybe more then last column is going to be uh categorous before and I'm going to say sports and so on. So it's going to be like new enhanced data set which you can
(44:38) use to try to improve the model. It's only guarantee that it's going to be like much better because you have more already features. It means if you don't have enough observations, it is more difficult to train. Maybe you want to reduce number of tokens in that case. Maybe you found that 20 tokens is like okay there is no problem.
(45:00) But once you starting adding your damn entities those maybe not already as important more than that it is very possible that those may may completely go away. So you may completely remove tokens and say I want to use zero tokens only my name named entities to try to classify maybe it is even better right you can simply use tokens I'm simply entities entities nothing else it is like one if you want you can use like even counts actually you can do different things here no let's say one if it is there right if is there there at least once in this document I can say
(45:36) one. If you want you can use counts and that's what basically this problem is about. This is more more interesting problem than previous ones. Please try to experiment with this. Any questions about this problem? Yeah, it tells you use binary diamond variables. It means probably if you want to be kind of following the description of the problem, you have to use zeros and ones not counts.
(46:23) Okay, then let's now stop. Will you post a notebook that you were working on, Dr. Crokin, or do you want? Yeah. Yeah, I'll post it. Yes. Yes. Okay. Thank you.