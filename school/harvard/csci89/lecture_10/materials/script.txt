89 day 10 - YouTube
https://www.youtube.com/watch?v=9PDB0a4qGPM

Transcript:
(00:01) Hello everyone. Good evening Dr. Croken. Hello. How are you? Hello. Welcome to lecture 10. with nine. So first question um what is fundamental assumption behind nave various specifier we remember that in that case basically features like axis individual axis for example if you use like term frequency it means those ter frequency IDFs they should be independent of of each other conditional on the class they will belong to. I guess it was assumption if you care for read about this.
(01:02) It says every feature is conditional independent of every other feature given the class label. This is correct. The the other ones are not correct. You can see features are equally important. All features are modeled as gausian distribution. It is not correct. It doesn't have to be the case.
(01:21) Right? We can use some kind of multinnomormal distribution for example and the class I assume to have equal right probabilities. They could be but it's not really required. We can do whatever we want with prior. Let me look at our class n base classifier. So this is the model and we assume independence which means this is basically how joint distribution looks.
(01:48) So if you say I'm talking about let's say people and uh they have age maybe right education let's say experience I basically say uh education experience um uh age they don't relate they they completely independent given person of particular type for example someone who works in IT industry will have unrelated experience and education this is not really typically correct of course it is naive classifier but still it sometimes performs nicely as we saw last time maybe it was the worst model but still it did some job.
(02:24) So this is how it is designed. Those individual axis this ais x bolt means vector of those axis and we are saying that individual axis are unconditionally independent. Basically conditional means if you condition on the type of on the class of observations let's say classes people who work in IT industry for example then those features of those people like education experience will be independent of each other for given person for given class we are talking about independence of those features it's not like people are independent is different question they should be they better be independent but we talking
(03:00) about the assumption that those features different characteristics basically independent of each other given particular class. In our case, given particular type of documents, TFS for different um tokens will be independent of each other which is not not correct either probably because we understand that words correlate clearly.
(03:25) It means if one occurs then similar words in this context may occur as well. It means it is not always correct. That's why it is called naive base classifier. So next question about k nearest neighbors. In this case we say um that we have for example uh two features in this case height and also weight. They are slightly on different scale kind of right? Oh maybe.
(03:56) So I mean height is in cm around 100 maybe 100 maybe 70 or so and second case it is around like maybe uh 70 it means on different maybe twice twice more one is twice more than second one. It is not maybe so critical in this case. But typically in such cases if you use KN can neighbors.
(04:23) Let me say if you use K nearest neighbors, we actually typically use some kind of um some kind of loss or some kind of distance, some kind of metric. And let me say as an example, that is going to be example. We say distance between two vectors. Let me call it X vector also X vector prime is defined.
(04:51) Now you remember it is like x first I'm talking about ukingian distance in this case x first min - x first prime squar plus x 2 entry minus x second entry of prime vector squar maybe more features maybe right in our example we don't have more but generally speak we may have more features it means our vectors live in multi-dimensional space we may have many many many features one vector is x vector and second vector is x prime vector.
(05:27) So there is some kind of distance which we define this way using ukidian basically metric. Now the question is what if in my example I have for example first one to be for example height let's say 160 170 is the difference like around 10 difference in my case for example and second one is is weight in this case it will be somewhat similar maybe there's not as critical but sometimes it is much much more important right let me Say in my example in this example I'm going to use something like express on tones for example let me say8 minus
(06:14) 85 for example right and you can see what happens if basically I'm talking about this type of distance then second term under the root is going to be quite small it is like.5.05 05 squar it is like a very very small comparable to the first one which is 100 10 square root is 100 it means effectively the second one will not be used that's why if data on will be on different scale it is much better it is important actually to to try to first rescale it that's exactly what B is saying less normalized features so they have the same scale so we don't want to
(06:53) have 160 minus 170 versus 05 five especially with this square it becomes even even more more distinct from each other that's why we do it this way so next one question three logistic regression what role does maximation of likelihood play in this case now in this case we understand that we are talking about maximization of likelihood right because logistic regression means we have to fit parameters using maximization of likelihood so in this case We find parameters by maximizing the likelihood function given observations. Let me
(07:34) maybe consider one example to make it more more transparent. I'm not sure how many of you know about maximation of likelihood. Last time we discussed let me give you one more example. So it is completely transparent what's happening when we fit logic regression. Basically we maximize likelihood.
(07:56) Let's see example uh I'm going to use very simple example even though logic regression is somewhat uh simple as well because it is basically bol variable it is 01 nothing else boli type no in that case probability of success depends on features which makes it more complicated than just bernoli but nevertheless model itself is not not so difficult probability of success is a function of our features by sigmoid it basically So we take transformation y sigmoid right take linear combination of features apply sigmoid and we get probability of
(08:28) success that's how we model it let me say I'm going to consider basically binomial distribution I'm going to toss a coin 10 times so let's say I toss a coin 10 times right and let me say I observe uh for example seven heads and I observe three tails three tails this way. Now let me say parameter which I'm trying to estimate here P denotes probability of observing head on every coin flip. So this is parameter to estimate we are trying to estimate this parameter.
(09:19) So in this case we say we need to construct a likelihood a likelihood which is function of our parameter. Now it is understood this way function of our parameter given my observations given data. So what is data data in my case is exactly what I observe. So basically this is what I call my data. Whatever I observe essentially is my data in this case three 7 H's and three tails which is by definition.
(09:56) So likelihood by definition means probability to observe my data given I formally plug my parameter P. This is definition of likelihood. Likelihood is probability to observe my data. parameters whatever this P but it is being considered as a function of parameter kind of explicitly because we because we can maximize respect to parameter and if you recall uh probability theory so basically if you don't know it don't worry much about this you're talking about seven heads it means probability of getting head P * probability of getting head of getting head 7 * it means 2^ 7 * 1 - Three we
(10:34) are going to get three times it means three times here there is also coefficient in front seven choose three right I'm sorry not seven choose three term sorry about this 10 coin flips choose seven 10 choose seven coefficient in front because we are we may be getting this data in no kind of different order right if I'm talking about particular order then maybe there's no even coefficient but when maximize likelihood coefficient is not important at all it's not going to change where maximum occurs now maximation of likelihood is going to tell me approach tells me let's now plot
(11:17) function L as a function of parameter no given date of course will be likelihood which means probability basically if probability is like very very very small getting seven heads is impossible almost impossible almost impossible. Impossible but very unlikely. That's why it is small.
(11:41) If probability of getting head is like almost one, it means essentially let me be even precise. If probability of getting head is like zero uh if P of getting head is zero and probability to observe our data will be zero. If probability of observing head is one by assumption that probability to get seven heads is also zero.
(12:04) So we have these two points and there is somewhere in between our basically maximum occur that's how probability looks. You can plot this function. Essentially maximation of likelihood tells me find the maximum and deduct from here what probability P is going to be. In my case maximum occurs obviously at 7. It means it is skewed that way clearly right and I maximize it. So this is maximum.
(12:32) This is where maximum occurs and my P no called P hat MLE is what maximize likelihood. Typically in such cases we actually say let's take a log likelihood because in such cases we are talking about usually product of things. product is is possible but it is not most efficient way of doing this from mathematical point of view and also from computational point of view is not most efficient way to do it.
(13:06) Winter is log likelihood as a function of P given data as well which is by definition going to be logarithm or natural or not natural whatever you like is going to be logarithm of capital L and if I continue it's going to be in my case logarithm of this first term product becomes plus because of property of logarithms logarithm of P to power 7 is 7 * ln of p and next term is + 3 * ln of 1 - p.
(13:46) This is logarithm just basically stretch it stretch it in a sort of um um uh way that maximum doesn't shift anywhere. So it is a function itself is monotonic monotonically it means my location of maximum doesn't change. Now if I plot log likelihood it's going to maximum is going to occur at the very same location and then I say let me simply compute derivative of my log like likelihood with respect to p is going to be zero from the first term it is a constant from second term 7 / p then second term is 1 / 1 - p * 3 in front and because of chain rule minus gives me minus Then I
(14:31) set it equal to zero. And if you solve from here, you can obtain solution which we call PMLE exactly solution is going to be precisely 7 which makes sense because we get seven heads out of 10. That means of course probability of head based on this data better be 7. That's how it works.
(14:56) So in case of for example regression similarly we can write down likelihood we can maximize it right maximization is already kind of invol involved it means we have to use numerical algorithms typically to maximize it we can maximize it especially if you want to introduce some kind ofizations becomes even more pro problematic. So now this how it it works.
(15:18) Um let me see uh question. Yeah. Uh so the line where you write likelihood equal LP given uh data and then equal the definition equal P data given P I I'm lost here in this in this one line. So um if you know probability of head right suppose I know it probability of head is 0.
(15:52) 5 for example I know that it is unbiased coin what is probability of getting whatever we what whatever we see what do we see we see seven head out of 10 right seven head out of 10 probability of such event is given by this formula where p is going to be.5 five I can compute it. It is called I was asking about the notation right before it. Yeah.
(16:17) Yeah. Yeah. So it is probability of observing our data given P is whatever whatever I plug in. So this function is considered as a function of P. So now I say since I don't know what P is I flip things around and I say it is being considered as a function of P. Where data is fixed, it is whatever it is observed.
(16:42) It is not like if you think in terms of probability when we cannot flip probability right probability of event A given B is the same as probability of event B given A. Clearly it is not probability it is pro is just likelihood. So it is not like probability of such event. It is function of L which I call capital L is exactly probability of this type which is considered as a function of unknown parameter P.
(17:06) So P here could be considered like a generative model we want P is considered as as independent variable. Typically in probability theory we say P is.5 let's compute it. Okay P is.7 let's comput it. Okay, we can compute for every single P. We can compute probability of such event.
(17:30) Now we say since we don't know what P is let's sort of scan through different piece through different parameters P and see what probability of event whatever we observe is going to be in every case and probability which maximizes out probability of our specific outcome is going to be called PMLE right okay and function this function as a function of P is going to be called likelihood so basically if you concerned about notation You can simply say it is function.
(18:04) So consider this as function of P. Sure. No, it means scan through different piece. That's what it means. For convenience, we call it capital L of P. That's all. But but why do we flip why do we flip uh the the we said the likelihood of P given data and then that's by definition equal the probability uh of data given P why do we flip P and D because P and data it is different philosophy in probability theory we say we know probability we take a coin coin is unbiased P is equal to.5 we ask a question what is probability of getting
(18:44) such and such event for example seven seven heads out of 10 it becomes probability of data right which we won't observe we didn't observe it yet we only ask hypothetical question what is probability of observing a particular outcome P is fixed we know it under after line means it is given so basically this line sort of tells you given P given P I want to find probability of such event it is kind of one question different question would be what if I already have data my data is already available able then I can ask a question what probability should I
(19:22) choose P to kind of maximize this probability of output given data so data is fixed whatever comes after line tells me so given data right so it is like here given data basically that's why you write it this way to sort of remind us whatever comes after a line is already given given means exactly this particular outcome Um in probability theory we typically say given P right in our in our case P is not given we have no idea what it is that's why we sort of rewrite it to remind ourself that date is already given and P is to be chosen does it make sense
(20:06) okay yes very clearly thank you yeah that's that's notations this is nice just you know kind of excurs to inference But when you run this models, keep in mind that all it does analy it will analytically compute probability consider such probability as a function of parameters and maximize it. No, especially like in case of STM structural topic modeling model will be quite complex.
(20:33) still have a loss of loss of parameters. What it does it literally write writes down likelihood with respect to as a function of all the parameters whatever it was right and we'll maximize respect to those parameters problem becomes quite complex because we have multi-dimensional space in this case only one P2 scan in that case lots of lots of different parameters and we have to find optimal parameters by maximizing likelihood numerically but techn technicality is exactly name.
(21:04) Okay, thank you so much. Yeah. So, next one support vector machine. So, let me actually check something very quickly. Yes. So, this is uh how we should understand uh logistic regression. We take data, right? And we fit this kind of curve basically which is like neural network which has sigma activation function uh output will be 0 and one but with the output output uh I mean actual output is number 0 and one but model network will output number from 0 to one which is probability because we use sigmoid function and uh this how it looks essentially have some kind of inputs
(21:53) let's say x let's say only single x then sigmoid function applied to linear combination is going to produce some kind of probability. That's exactly what we assume is probability of output, right? And we maximize likelihood by using this assumption.
(22:12) It is almost like in my case, probability of success or probability of getting ahead as P. In this case, probability of success is denote is also a function of some features. Generally speaking, we may have multiple features. It will be like multivaried regression here. Know many numbers, X's, many X's. So I'm not sure if they Oh yeah, right here. So probability will look this way. Essentially that's how logistic regression looks.
(22:37) By the way, this logarithm of P / 1 minus P is going to be exactly linear combination, right? Just to keep in mind what meaning of this so-called odds ratio for of ratio. Uh, so the quid is actually graded. Yeah, I didn't post grades maybe yet. Let me do it right now. It takes like two seconds.
(23:08) Uh, so I hope you don't see my second screen where all grades are. No, we we do not see it. Should we post it in two seconds now? Support vector machine. support vector machine and we ask a question and support vectors are the only data points that influence position of decision boundary. Yes, by definition support vectors are those points which influence position of decision boundary.
(23:49) It means the statement is indeed correct. Basically by definition support vectors are those points which influence decision of decision boundary right. So let me uh let me say uh support vectors right. So support vectors vectors in support vector in uh SVM. Support vectors in SVM. Um now let me sketch something here. Let's say I have one point this way. Maybe I have cloud of points.
(24:36) Cloud of points here. And then I say there is second cloud of points. I'm going to sketch it over here. Second cloud of points this way. Now if I split it with a line like margin like we discussed last time it will want to place it this way right. So this margin this way kind of this way. But now what if it happens that I also have one extra point somewhere here. Maybe I just happen to have it.
(25:14) Then my maron is supposed to look differently because there is no really way to already place it that way. I'm forced now to place it differently. I will have to place it this way. It is not like something which intuitively seems to be correct maybe because only one point to this point may actually impact position of this margin.
(25:39) My margin becomes sensitive to a single point. It is known as overfeitting. One single point influences it is like example. Let me just linear regression right. Y depends on X and we say let's fit linear regression. Now you know how it's going to look. But what if there is outlier then line will look this way.
(26:04) So we understand that this is outlier or we understand that this is kind of overfeitting. Basically one point impacts my position. It is not nice case. One single point impacts position. In this case this point impacts position. It is not nice case. Uh no I mean formally speaking that's what we get. This is support vector.
(26:29) This is support vector and that one is also support vector. You understand that? Because they all impact position of my lines. As a result they impact position of my decision boundary which is in the middle. But what about this point? If I decide to shift it to maybe that way or that way a little bit.
(26:49) If I try to shift it then my output my result my decision boundary is not going to be impacted anyhow by this single point. The single point in a sense doesn't matter. Of course if you pull it too far away it's it starts to be important but not not yet. So basically this is not the boundary. There is some kind of flexibility in you know changing this point without any impact on the result.
(27:10) So support vectors are the only points which will impact position of my boundary. Right? By definition basically whatever impact position of boundary is called support vectors and this is like first example which doesn't have any allowance no allowance it is called allowance sometimes called how else maybe allowance no allowance no allowance allowance no allowance um now let's assume that we have second scenario I mean we have same data set actually but let's assume that we have a allowance and let's see what happens data set basically same this is my point my point and we say now let us introduce some allowance. Let's
(28:13) say that it means basically some of the points could be inside of my margin that is allowed. In this case, it does not allow it. They should be either on the boundary or outside. First of all, it's not always possible because not always my points could be on different side. They could be mixed. Secondary, even if they are not mixed like in this case, I don't want to be so much sensitive.
(28:39) You know, this single point should not impact my line. maybe that much. So in this case with allowance and we say if I allow some points to be inside my margin could be slightly slightly wider it is okay now points could be actually inside could be even on wrong side it is also allow it formally speaking it means if my data set is not like clearly separable with allowance I don't have any issues in this case if my data set is not not separable. You try to run algorithm will not converge.
(29:15) You will have convergence issues it is like overfeitting in this case if the data points are mixed the will not even converge. So in this case I have this one as support vector. This one is support vector support vector. Support vector. Four of them from that side also one two on that side. this way. That's how we do it, right? Now, of course, we understand that boundary in first case is going to be in the middle of this margin and second case in the middle of this margin.
(29:57) Ultimately, this boundary is all we kind of care about, right? So, this is decision boundary. decision boundary in both cases and the question is what is better how to choose allowance question to you how to choose allowance if I have no idea should it be that way or should it be this way what's your suggestion um I I think we should allow allowance because um so many data sets are not linear seable and we will have some mclassified points no matter what to do. Now allowance remember constant C should
(30:38) it be 0.1 should it be 0 2 should it be 0.5 what should I choose for allowance I I think it should be suppmental we don't how much I if it is large it becomes wider wider and wider right and also rotates clearly if there's no allowance no it will just fit the best possible kind of way in order to split it if it is possible if splitting is not possible where it will not converge you will have conversations like warnings or errors even.
(31:08) So the question is how okay you said that allowance seems to be better but okay but what is what constant C should we choose remember like plus constant C experimental we have to try based on the data using a validation set or something like that exactly exactly yeah so we simply optimize some kind of test metric right exactly yeah that's how we do it intuitively we understand why we should choose the best kind of second case right but in reality We don't even maybe see what's happening. We cannot plot even maybe. So in reality we just look at test metric and we scan through different
(31:46) parameters. Now let's say in first case C is basically is equal to zero. Remember plus constant C. In second case I chose C to be 9 for example right let's say one let's say.9. So it is clear is not discrete. uh and uh there was this support vector motion there was constant C right and those size which kind of measure how far we are inside in this case no one is inside so all epsilons are zeros basically all those size I'm sorry those size are zero because no one is inside if someone is inside now like
(32:26) this guy for example halfway through no it will be like point maybe four.5 5 4 this guy uh this guy is close to that side it means it will be 0.1 maybe 2 maybe 2 maybe that's how it works and we basically sum together this violations measures violations and we see we control how much we want to allow those violations no if you don't allow violations then it means we So I'm sorry I'm not correct actually uh so it is important uh it is u there is like different parameter which is like one c so formally speaking in first case when there is no allowance
(33:17) basically I'm saying that it is infinity I hope it does make sense why it is so right if c is infinity so basically those size better be all zero it is kind of extreme case no allowance means constant C is equal to infinity now because if C is equal to zero what it means? It means you are welcome to violate right.
(33:44) So C is equal to Z is is on the contrary it is like huge allowance we don't care typically well at least in R 1 / C is kind of constant to be selected that's why I got confused one over C is like zero in this case one over C is like let's say around one and second case that's how it works I think in Python they used constant C if I'm not mistaken so maybe I Double check.
(34:14) Do you remember how what what constant C they use in Python? You can you kind of can clearly see it. If you display your margin, you can see what happens if you start you know increasing C and if your margin is going to decrease. It means the way it is written on the screen.
(34:35) If if you see when you when constant is going to be in the contra increased if it is increased but your margin is going to be increased as well. That means basically C is going to be one / C but typically you just scan through different CS and see which optimal C to choose. So now um let me move to next question. Question five. What is one of the primary advantage of using a forest over a single decision tree? So question B is correct.
(35:08) in the forest is capable of reducing variance by averaging over multiple decision trees. So yeah, this is indeed the case. This is advantage of random forest. We can reduce uh uh variance improving generalization. So any questions about anything? No, we will not talk about ren forest. I think we already discussed.
(35:35) So the forest is clear. Could I ask you a question about question one? Yep. Sam is option D also true or is it uh or is it not true? No, it is not generally speaking. It is a good question. It is your assumption. Basically in this case we are saying that there is some kind of prior distribution right if you have no any information like whatsoever you don't know any you don't have any information you can choose like uniform distribution in this case and say prior is like 50/50 or whatever like how many clauses you have but what if you do have
(36:18) some information right in that or maybe from previous experiments. So basically this result can be used as prior input to the next experiment. Maybe you already did the experiment last year. you have kind of idea that maybe uh for example proportion of IT engineers is not like 50 like you're talking about IT engineers and you know chemists it's not like 50/50 right for example you know that no you understand the idea right so it doesn't have to be 50/50 if you have like some kind of assumption which is a
(36:57) priority assumption which you believe is correct you can use it first of all the secondary um you can actually use your previous experiments that means your previous P of C given previous data if actually becomes new P of C becomes your prior assumption for current experiment that's how we can do it and also there is another thing which is kind of interesting like from practical point of view actually this PFC could be used as a regularization basically so what what does it mean it means if if I try to do
(37:31) some kind of let's a classification task, right? One thing is to just blindly pluck here whatever you believe is correct, your prior assumption. Maybe it came from previous experiments, whatever. Another thing is you can actually try all different kinds of P of C which is like prior distribution and then just as we do it like in case of neural networks with hyperparameters.
(37:57) So it becomes like hyperparameter basically you can plug all kinds of PFC and look at your results. Your results will definitely depend on your assumption. But then you say let me look at some kind of validation set and see how it performs. And basically PFC in practice often plays role of regularization technique of hyperparameter basically. So you can do it even that way. It is like how they do it in practice like in B statistics for example.
(38:26) This typical approach does make sense. Yeah. Yes, it does. So there's no reason to assume that. No reason to assume. No. Only if you kind of completely, you know, um unaware of distribution of those classes. It also seems like like we need to I don't know need but it see it seems like we would definitely benefit from becoming well informed on probability and statistics so we understand what these all these approaches and functions are doing under the hood.
(38:58) Right? because it seems like you you talk about um prior probabilities and likelihoods and it looks like there's a lot of underlying terms that I may not know well it is okay we're not going to go deeper into that I just wanted to give you some kind of you know ideas but if you have to learn you have to learn it further right so yeah but um it is better to kind of know of course probability theory Definitely.
(39:31) Um, is it like prerequisite theory? I'm just trying to check now. I think this for not not even prerequisite, right? Probability theory. Yeah, I don't I don't remember, but it certainly you certainly give me the impression that there's a lot to know about um about these tools. So rather than just you know saying oh I'm going to use this approach it would be nice to know you know what there are different type of people some people just use it happily and actually very successful and they get experience and they know how to
(40:09) apply it in practice and there's nothing wrong with this because there are so many different models you may not be able to kind of know all of them right I personally like to kind of kind of dive deeper and see how they work right but there is also kind of cost if you try understand the math behind you may not get so much time for experimentation and I don't know what is more important so if you just want to kind of get your hands on some kind kind of problems know approximately how it works and you kind of understand the assumptions and idea that's maybe all you need to know and
(40:42) you may get very very kind of professional uh with those models uh that's why I give you kind of you know brief overview but it's not like really required for you to know exactly what's what's going on right just only kind on high level. Yeah, of course it is nice to kind of dive even deeper and try to introduce theory here and so on.
(41:09) No kind of uh it is no not not always as important actually because many people can apply all kinds of models and they are so well informed about kind of availability of different kinds of models and also get have experience and they can easily choose the best model without even maybe knowing much like what is behind again I I believe it is better to dive deeper and understand how it works but uh that's why I give you kind of a review But if you don't understand like all of it don't worry about this. It doesn't mean you cannot apply it in
(41:41) practice. Right? So in this case u all I want to say is distribution of those classes which I call prior distribution without any data. I say what is probability that it is spam or it is like ham 50/50. Okay, you can use 50/50 even though we understand that probably not not going to get like 50/50 spam and ham in our mailbox, right? It means maybe 50/50 is not the best approach.
(42:14) But there is another kind of alternative approach where you say let me treat it as a hyperparameter. Basically, I understand that probability of first class plus probability of second class must be one. But other than that, I don't worry what it is. I just plug different kinds of distributions basically and see how it will classify my text or whatever then validate validation data set and choose the best distribution in order to achieve the best performance.
(42:37) That's how people use in practice like like in real applications where they study for example apply to medit for example. Does it make sense? Yes. Thank you. So now uh we have lecture 10. So today is a I would say relatively easy class actually. It is about uh named tensity recognition. Uh and uh in this case we are talking about uh I would say models which tell you which identify if you have in our text we have like names maybe cities um maybe date email address and so on that's what's called named entity.
(43:33) Let me open this slide and say uh I will you so in this case named entity recognition is a process of of process of location classified named entities. So that means you want to find in a text like person names. Why is it important? If you do for example let's say translation it is quite important to understand that this is a name of a person right when you translate a different language if you don't if you understand it is a name you can translate you can simply you know without much thinking translate a name
(44:05) you just have to spell it different in different language that's all but what computer should do if you try to translate and this is a name of course you have to tell it you have to say this is actually name it means translation is simple no kind of kind of spelling in different language. That's all.
(44:22) That's why that's quite important to identify those entities. Same about organizations like companies and so on. If if they occur in the text, we have to be able to identify those in our text. Also, geopolitical entities like countries, cities. For example, you can see that America is often considered as geopolitical entity.
(44:47) America is like continent, right? basically but often it is associated with United States it means it is geopolitical entity if you take Africa Africa is most likely going to be identified as continent example so this kind of idea uh date and time expressions if you have if you see time like for example November 4th 2025 how to translate it of course it is better to know it is date then translation is going to be much much easier even if it is not translation even If it is like some kind of classification task, maybe we are trying to maybe uh let's say do
(45:22) sentiment analysis. It is sometimes important to understand that this is not just um some kind of word but it is named entity of some kind. Then translation becomes e translation or even classification becomes easier because we can basically like literally kind of uh concatenate. We can add those specifications to the input and say we have so many names in this text. We have so many uh uh cities in this text.
(45:54) We can even say this particular city this particular uh token is actually city not just word. So it will help um to translate to classify and so on. That's how we do with event names, product names. So or brands. So the all all of these are examples of named entities.
(46:19) So application as we discussed we can do translation, we can do uh classification, right? Question answering system. For example, if you want to ask answer questions, of course, it is important to understand that we are talking about like a person maybe not just some kind of word which describe object or whatever generally speaking, but this is like person name of a person semantic search data annotation machine learning.
(46:44) And here is example I took it last year in 2024. It was actually November 14. I think we were a little bit a little bit uh ahead of this year because it was a holiday of some kind. I think it was like Mondays last year and I took this article from November 14 last year about Donald Trump who won elections. So this is my text.
(47:09) Now if I want to do something with this text, let's say I want to translate it, I want to do whatever, it is quite important for me to understand what is name, what is company name and so on. If you apply name entity recognition basically this is what is going to to be happening. We will see how to do it later but basically we're going to say Donald Trump.
(47:34) Okay, this is a person so it will understand that more than 50 that is some kind of cardinal basically number. So it will tell me it is a number this year. This year rel is related to date. So related to date this year is not like this year. This year means just the phrase itself. This year it describes date then also number first ordinal. Tesla in this case Tesla's organization.
(48:02) You may know that Tesla is actually not only organization. Tesla is a last name. Actually there was a physicist. His last name was Tesla, right? But in this case, it tells me this organization, which is actually correct because from the context, we can see it is indeed organization, not not not physicist. Then um dollars one trillion of dollars is money 2022 is date over 40% is percent. Bitcoin is a person.
(48:32) You see it believes that bitcoin is actually last name uh cryptocurrencies it believes it is organization a week is date March is date September is date federal reserve is organization bank of ang organization and so on so this how it will it will basically look after we apply name entity recognition so now uh we will talk how how how it does it actually.
(49:04) So basically by default at least if you use space by default that we will use convolutional network we'll see how how it does it. Uh but there are ways to essentially when we do it typically we just apply it pre-trained one and we kind of you know classify it in order to obtain this result. It's like two lines of code or maybe five lines of code to get this result.
(49:28) Um but um if you actually want to tune your model, no it means you want to take particular data set of your interest related to your particular uh field then you you can actually twist a little bit parameters. You can tune it you tune a little bit this model then it may improve. If you don't do it you just can apply default pre-trained model and you get this results.
(49:55) So now let me um briefly talk about applications of name named entity recognition. Um it can enhance text classification. Now if you have text you want to classify what what type of text it is. We can first extract uh name entities. Then we can there are kind of there is a kind of you know room for improvisation.
(50:23) But we can for example add uh maybe counts of of people in this text like literally concatenate. You have a vector of some kind which represents your text. Then you can concatenate and say there are five people in this text, right? two organizations um maybe a couple of geopolitical entities and so on. So that's how we can we can enhance it. Maybe for your model there is no really way to kind of see it right away.
(50:48) What is name? What is what is address? What is email address and so on. But it could be quite useful even for text classification. That's how we can do it. You can actually do it differently. You can maybe keep the name of the person. You don't have to say only like five people and without saying who it is.
(51:06) You can actually keep the name of the person. Right? It will be like already uh information about this being a person and specific name is being supplied as input. It may also useful to enhance your performance, enhance your features. Uh it can be used for simply dimensionality reduction.
(51:29) That means basically we can focus on entities and we can say if I want to understand what this text is about. Maybe it is enough for me to keep Donald Trump Tesla cryptocurrencies it is already loss of information. Maybe I can keep entities and maybe I can disregard like the text for example itself then it will be the reduction. So I can do it this way. uh we can try to understand relationships in the text right we can basically see when those things coe together or when they on the contrary don't coe together Apple and Tim Cook they can co together if you realize that or if you understand that those are named entities we can try
(52:13) to kind of focus on those and try to study relationships no kind of correlation basically between those. So that's what we can do. Um we can actually uh it is quite related to what I already discussed like feature enhancement. We can actually use our entities to kind of add like concatenate basically to our data set those uh let's say frequencies of like counts of people counts of organizations which occurs not will be like metadata metadata of some kind structured structured uh metadata which is supply in addition to text itself is like unstructured right but if
(53:00) you create this data set Like in the case of social topic modeling remember there were was like metadata metadata is is what is like data about data right so data about your text data about your text is what's called metadata metadata is like you have data in this case text and you supply data about your data like data about your text in this context and will be called metadata you supply data which contains information about uh who who it is about what companies it is about and so on. It could be quite quite useful quite useful because it also
(53:36) enhances performance. Uh last part is also quite similar to what we already discussed. We can actually use this name NC recognition to tag content just like we did up here with tag content and it will enhance for example categorization right we can enhance performance of some models uh uh if you have for example system where you have to answer questions you have to try to match what is being asked to what is being kind of answered For example, right? It means you have to try to match u know there are kind of examples of
(54:22) questions and answers and you have to match u if there is like a person in the question and also a person in the answer you have to find those and match kind of construct link link between those it means this step of named entity recognition is quite important in order to to kind of estimate this link that there is this link between question and answer uh semantic analysis is a text key and it is for analysis.
(54:56) Now it is also um it is also quite similar. Basically in this case we say what if my text itself is not like purely positive purely negative. It could be the case that some of the entities are sort of associated with some associated with something positive.
(55:15) some of the entities will be associated with something negative in order to kind of you know nicely represent this information. We first have to as the first step recognize those entities uh you can think about examples like well this person is blah blah blah the company is you know kind of is growing for example. So this is partially negative statement maybe about the person like about CEO for example and company is mentioned in a positive way then you can sort of create a training data set where you will say this person this entity is mentioned in negative in negative way and second uh
(55:54) part where we talk about second entity which is like company is mentioned in positive way we have to combine kind estimate link between those entities first of all and secondary we have to assign different sort of semantics right different kind of characteristics to first entity the second entity during the training that will help help um help will enhance this type of models and of course we can use as always hirit approaches right so it means that we can create a rule based approach where we create a let's say you see author in
(56:32) your text you say Dr. dot and name you can understand that this is probably about a person you can say inc something something something ink incorporated okay it means it is organization we can definitely apply rule based based filters it is not the best approach clearly because sometimes we don't have those ink just says Tesla it doesn't say doesn't say Tesla ink right just says Tesla example it means um it is not always the best approach but we kind of like to combine different techniques rule based approach if you see INC
(57:08) incorporated most likely it is organization and also we can actually use machine learning algorithms to understand to enhance performance so those things combined together any questions about that so let us make a break after the break we'll continue and ultimately we'll see some examples we'll see more examples on Friday but today we'll see simple examples So now there are two main techniques one is called rule based approach right now
(1:09:36) it means basically we create some kind of rules let's say if you see if you see u let's say uh inc incorporated that means we understand that is most likely company it will be rule rule based approach and second type of statistical methods basically statistical method methods means not only statistical models but also neural networks which are sort of considered statistical models as well.
(1:10:01) So statistical methods will be second approach which is going to use some kind of pre-labeled data set which is used to train the model so we can later use it to identify name and recognitions. So let's see first one. So first one is about uh rule based approach.
(1:10:24) So we understand that it means that we have to sort of have hardcoded rules. It is quite problematic because everything has to be specified first of all. Secondary typically we don't look at any context of any kind. If you just look at some keywords basically some kind of way the data looks for example if we're talking about date we understand how it is supposed to look July then you know for example November 4, 2025 from the way it looks and from the months which we can see there we can say it is date. So this is rule based approach. Now maybe for for date it is
(1:11:00) okay. It is probably going to be correct but the other cases are quite quite complicated to to make sure that they work always correctly. So rule-based approach is not so easy to maintain. We always have to make sure that we update it properly because we as we move to the next data set I mean as we move over time things may change and we may have to update it also and so on.
(1:11:26) So that's what rule based approach is about right expressions is like typical approach first of all it means we see some kind of specific way the data data looks like piece of text look looks and we can say it is named entity recognition. We can also use some kind of grammar rules like from linguistics. We can use prespecify list of noun entities.
(1:11:51) Obviously why not we can use it right? If we have some famous uh person name we can use it. We should use it definitely and so on. this kind of ideas behind rule based approach and uh sometimes it it is actually quite accurate because if you use names for example from predetermined list then of course it may may be quite accurate.
(1:12:21) Uh it is interpretable understandably because we have list of those entities and we don't need to use large label data set to train it on. We just need to keep kind of data set where we store information about what is what who is who basically uh it is not so flexible.
(1:12:41) It is difficult to adapt to new languages obviously because it is is going to require a first of all formatting. Even if you move from kind of United States to Europe, it is already different because date could be differently in United States. First one is months then then day and then then year. In Europe it is not always this way. It is actually day then month then year and so on.
(1:13:01) Right? So even that it needs to be adjusted. But more than that you have to also keep track of know basically people companies which occur over time. So this is limitations uh maintains burden if you have to maintain this huge data set and also not to mention a huge huge kind of um set of rules which we use you have to actually maintain it and always update it and it becomes quite complex scalability not scalable right because if it becomes uh complex becomes complex as you have more and more things to keep track And uh here is a very simple but sort of illustrative example I created for you to kind of show how it looks. It is very
(1:13:45) simple example doesn't use like linguistics of any kind. We just use like pattern matching basically. And in this case uh we say I want to have I will define here entities which will be dates, emails, uh times, titles, organizations and I will specify how exactly they they're supposed to look in my text. So I can identify them as being date for example.
(1:14:13) Now let's say date pattern I use regular expressions, right? Then B means like boundary of words. Then I say D1 to it means like digits one or two digits then slash one or two digits slash four digits and that's it. That means I'm talking about basically date in this form or it could be expression which is one of those January, February, March and so on.
(1:14:47) one of the months then I say maybe next to it maybe because of question mark maybe next to it some kind of date which consists of one or two digits comma and here I have a number of uh strings and I have spaces I'm sorry spaces maybe there is space maybe no space and then I have this digits for fourdigit number which is year basically it will capture something like November 18, 2024 or November 2019. It will capture this way if I specify that way.
(1:15:23) This how we can identify date pattern. Now it is I mean it is of course supposed to be much more complicated because we have to have uh also uh names of well-known people, well-known companies, cities, countries and so on. But in this case I just want to kind of demonstrate how it works in the simplest case. Then let's say email pattern.
(1:15:47) No that's how it it is going to be. Whatever basically you have here at whatever you have a dot right this way. Maybe you can also see like title pattern if someone has doctor or maybe Mr. Ms and so on. You can also keep track of those that and then some kind of name whatever it is. Then we identify it as some sort of person also organizations we'll have inc Ltd corporation or whatever this means either of those basically incorporated and then we simply apply to our text right we have our text right here and we're going to simply extract those um places where it matches and we'll print it out. So
(1:16:37) dates. So my dates will consist of all things which contain my uh dates which match this regular expression. It happens to be November 18, 2024, November 2019 with no any comma and no any date itself. Then email from here was extracted. Right? Then also time was extracted.
(1:17:09) So time is also going to have a regular expression here am PM is like key feature is quite important in this case two digits here or one or two digits before column space maybe no space people use it differently right so now uh actually this is like at least one space to be precise one or more spaces now this this means space one space or two spaces or more spaces is basically miss at least one space this way.
(1:17:42) Uh now so what's next? Yeah, this is how we get it Apple ink incorporated and so on. This is kind of idea behind u behind rulebased approach. Of course, we understand that the best practice would be to combine different algorithms together. But this is kind of almost like almost like uh you're trying to uh recognize pattern, right? Try to use regular expressions to recognize pattern. It is not the full story.
(1:18:11) Of course, we have to keep track of well-known people, known companies, of cities, of countries, of cont continents and so on. But this is like a starting point uh rule-based approach. Now next one it is already statistical methods. In this case we are going to use some kind of models. We can use so-called hidden mark of models.
(1:18:34) At some point later we'll talk about hidden mark of models a little bit more. If you know about uh mark of models, a mark model means basically the state is jumping from kind of current state is jumping from one state to different state like forward backwards forward backwards. We may have multiple states and in mark mark of models uh we assume that probability of transition doesn't depend on past.
(1:19:05) It is only a function of current state and hidden mark of models or assume that there is also some kind of hidden state which you will never observe. But basically we assume that there is some kind of hidden latent state which we will never observe and then we model it we have to assume that there is one right and then we also maximize likelihood basically uh given observations and we can estimate what parameters of mark of hidden mark of model is it is one approach conditional random fields is also something we are going to discuss later and also neural networks we can use convolutional neural networks Or
(1:19:39) maybe we can use recurrent networks for beard right this way. So now u uh let me say that clearly if we have this type of models advantage would be obvious we can actually uh we don't have to manually uh craft those labels right now maybe again if you combine this rule based approach it will be even better but if you use purely kind of statistical approach and neural network you don't have to manually craft it so this is clearly advantage.
(1:20:21) It becomes datadriven approach, right? Domain flexibility means it is adaptable to different domains, right? With different training data set. If you supply different training data set or if you try to kind of tune your model a little bit, it means you supply it with more training data set. You start with parameters which you already have. Then you supply your specific data set which is used to tune your model.
(1:20:47) you can tune a little bit and you can improve performance maybe potentially if you have specific kind of domain if where you work. Uh challenges are also quite obvious. We have to have a lots of data right if it network for example which is a labeled data set we have to know what actual labels are in this case. So basically to put it differently if you go back to this example if I consider this as being kind of train train data set it means I have to already know in advance that Donald Trump as a person right this uh for example 2.4% for percent is percent. I have to have this labels up front available. When I train my model,
(1:21:29) everything has to be available and then I train my model on this data set. It means I have to have a lots of a lots of data with sort of labeled named entities. It is very difficult is not just saying that this sentences belongs to you know this entire document belongs to to for example politics right to economics.
(1:21:56) It is actually very specific labels about every single entity the bank of England it is organization for example and so on. So it means of course it is quite quite challenging if you have to train such models. You have to have a large first of all and also well well labeled data sets and labels not just one label per document. It has to has a lots of lots of labels for every entity basically and of course computation it could be quite quite demanding.
(1:22:28) Right now let's see example in space how we can how we can do it. I have text here. Let me say I have spacey and I specify NLP spacey.load and I specify the type. So what is this NLP? So NLP is actually a pipeline to be precise. Remember last time on Friday we discussed pipeline. So pipeline means we have a set of different kind of you know techniques which we apply.
(1:22:52) In this case pipeline is going to consist of things like tokenization, limitization. Then we use name energy recognition right so this is what NLP is going to be NLP from space is going to my my what I call NLP from space is going to do exactly that it's going to be like pipeline of different um different transformations uh and one of those is going to be named entity recognition within this pipeline or one will be named named entity recognition first of all you can actually I can tell you you can actually use it as is And we are going to do it exactly this way on the next slide. Or if you want you can actually try to tune
(1:23:33) it. There is a way to do it. Maybe on Friday we'll consider example how to tune it. But you can actually try to supply your specific data set which is kind of labeled one and try to sort of tune this name and recognition which is part of this pipeline pipeline NLP. NLP again you see I didn't do any any kind of you know preprocessing. I didn't didn't do the organization.
(1:23:57) I didn't do limitization simply because it is already part of this pipeline. So my pipeline is going to include everything already and part of it also named energy recognition and if I apply to my my text. So my text doc as result and I can display it display is going to display it the way which I demonstrated before is going to show it this way. It's going to highlight basically label every name name and named entity it found.
(1:24:31) So any questions about this? So now let me talk a little bit about this model because it it is like mystery maybe at this point how does it do it right? Let me try to give you example to kind of show what exactly is happening under hood. It is another way to display it. I can just extract from here entities and I can display them labels. Uh right. So I can show location.
(1:25:00) So in my text I found Donald Trump. It is a person where it was found. more than 50 cardinal where it was found and so on. You can keep it this way for convenience just to see how it is or you can extract those from my object and see what it is about. It is about Donald Trump which is a person and so on.
(1:25:26) So now let me try to talk about the model itself how how how it does it. As you see, we didn't have to train anything. Of course, it was pre-trained. We simply supply text and we basically obtain output. As simple as that. But if you want to train it with someone, if you're talking about how someone trained it.
(1:26:00) So what happened actually? Let's say named entity recognition. Now let me say why neural network right let me be even specific why convolutional neural network. So basically by default if I don't change anything is going to happen by convolution convolutional neural network. You may be surprised but what what you see on the screen is done by convolutional neural network.
(1:26:24) So let me let me see let me let me show what's happening. it is input. As an example, I'm going to say that I have a sentence of some kind. Let's say let's say something like the cat set on Mat. Now first of all I can tell you that actually we are going to do tokenization because it is again pipeline which includes tokenization within it.
(1:27:08) Then we're going to do limitization and we also may drop something here. Let me assume I'm not going to drop anything. I just say this is my sentence and then I say let me represent my sentence somehow. Let me use our typical approach which is embedding right. So embedding embedding is typical approach. So embedding is going to happen there is first word let's say 8.
(1:27:41) 1 3 7 for example four dimensional embedding I don't don't want to write more then I say cat is also some kind of word it is embedding so don't even try to understand what numbers represent vector in this multi-dimensional space we understand There are some kind of relations between these representations but they're not so obvious clearly right so five 7 it is not probabilities that is just vector of my embeddings as we at some point discussed so then it's going to be another vector is another vector And finally math is another vector that you could be like punctuation like
(1:28:40) period at the end. So we can also use representation of of punctuation as well if you don't drop it. Let me say I don't have it. So my document doesn't have any any dot any period at the end. So basically my document is going to look this way.
(1:29:00) If you look at this and if you kind of think that maybe it is good idea to try to also uh during this process try to look at the neighboring words right cat I don't want to just use cat I want to use cat and also some kind of you know uh neighboring words around cat to capture kind of semantic basically right then it becomes maybe obvious that it is good idea to use like convolutional networks right so That's why we actually might want to use filter here which we are trying to slide over this representation.
(1:29:35) This is my basically filter. If I use commercial network which is what what is being used on the screen then I have to basically use filter. And what is output? Now I will tell you that that there is a convolutional neural network right as always and then there is output output and output is going to look this way. Let me say uh person organization and so on.
(1:30:19) The whole thing what I have here is basically my output. Now it means in first case there is not person not organization uh it is zero right 0 0 in every case it is zero basically right 0 0 0 so um oh yeah so that's how it is maybe should change something um the cat okay let's keep it zero so you understand so this is my output so basically if I'm talking about this specific data set I have to have this labels and my labels will have dimensions which correspond to number of types of entities I have in this case if I have like five different types of entities will be five dimensional vector right depends on dimensions of my
(1:31:28) number of my entities in this case and basically that's that's all now it becomes quite clear what's happening my output for every single uh for every single uh document my input is like image basically right an output is going to be this type of array which during training will have to have zeros and so on and then during during my uh predictions it will be a bunch of probabilities here right bunch of probabilities so maybe by the way I should say it is not like true labels it is maybe output from my model right so
(1:32:10) they is 0 0 then cat is maybe it is a person maybe not 1% 1% 10% chance it is a person right my model will tell this way so basically I will have to apply this type of uh softmax and for every row I will have to have probabilities which add up to one there belongs to one of those classes one of those types of entities cat 10% chance it is a person maybe it isn't a person will be basically if it is small will be not identified identified as a person on uh Matt maybe Matt is like organization or whatever. So basically this is my output if I run it. No, this is like
(1:33:04) what's what I call predict predicted values. During training, I have to have correct labels as always correct labels here, correct numbers, zeros and ones. That's what I use to sort of uh train my model in this case neural network. Any questions about this? Very very quick question just summarize my understanding.
(1:33:31) So the input is actually is is a is a phrase representing an entity not not a whole document right and the output is is more of like a multi- label or multiclass uh output well in this case actually um one one single observation is is my document if I have only single document it will be not like phrase not like sentence it will be entire document entire document is represented this way but if if it isn't entire document.
(1:34:03) Um how do we know for example let's say that Tesla was was mentioned three times um so so the document so the output will say okay we have we have an entity here uh representing a company I'm just not not sure um so if let me tell you Tesla here is organization if Tesla occur second time it is completely Okay to say it is last name. It is a person. It is not only the word itself.
(1:34:36) It is actually also location where it occurs because first time Tesla could be organization. Second time Tesla already could be last time could be person. It is totally okay. In first case it will be identified as organization. Second time you see Tesla according to this probabilities that it may be identified as person is this your question right u I'm just trying to understand the the high level so the input is going to be a complete document and the output is going to be multiclass probabilities output will be so for every in this case first of all I have to break the tokens
(1:35:23) right So this my tokens and then I will have to have u if you like during like training phase I have to have correct labels here. I have to say the corresponds to no any kind of name tenders and so on. If this was like a person I would place one and the first location zero everywhere else and that's how I train it.
(1:35:48) Every row is sum of probabilities. When I know what it is, it will be like zero and single one. If I don't know what it is, it will be sum of probabilities because I use something like soft max. I use soft max in this case. For every for every entity, we will have a row. For every entity, we will have row.
(1:36:05) Exactly. Exactly. Correct. Okay. And that's why for for different locations if if test like yours here it may be organization test like yours later on maybe in the context they discuss like talk about physics and it is obvious for the model it is already more about person it may be identified as a person very same kind of you know uh entity Tesla can be identified different in different places no for this there's no miracle right for is to work we have to have correct labels. Yes. Because first time it would be first
(1:36:45) time Tesla would be organization and second time Tesla in the context of like discussions of physics and history and development and so on it will be labeled as person. That's why next time when similar situation occurs my network will be able to identify that first Tesla is organization second Tesla is person.
(1:37:06) That's why we use this conversion network to see like context around basically does make sense now. Yes. Yeah. So that's how it works. Now typically when it comes to this type of models we we can make some kind of adjustments, right? But we already have much less capabilities to have huge impact on this results. Typically we just use it as is.
(1:37:34) We may use different techniques, different models, but typically we use pre-trained models. So we don't really actually even try to kind of overestimate I mean we don't have try to outperform whatever is available unless you have like maybe you have if you have like huge data set it is specific to your industry you may try to tune it but it's not going to be as much useful maybe maybe it is in some cases so but nevertheless it is a good idea to try to understand what's happening under hood uh so this type of models is going to be was basically estimated and then we use it. Uh can I ask one question?
(1:38:13) Yeah. Yeah. Yeah. For uh for the named entity with multiple words for example uh the bank of England on this page of your PBT, right? So basically there are four words. So uh do we have to uh I mean uh do the four words have to uh fall into the same category like the organization or just uh maybe uh two of them like a bank England the of that that uh they don't have to fall into same group.
(1:38:47) So in this case I believe that um uh some key so it is already adjustment to tokenization probably right. So in this case we have to if if I correctly talking that everything is going to be kind of u nice right the black cat set on the mat black cat should be like single talking so how is it happening is basically we have to use this type of predetermined already kind of vocabulary where we have Donald Trump and so on that that's how it should is how it should be happening but it's a very good point if you have like Donald Trump you
(1:39:24) probably saying why Donald is not one So we can trump as second talking and how do we do it right? So now we can say that Donald is a person, Trump as a person but it will be like two different like labels two different uh labels person but we have single label for entire entire Donald Trump.
(1:39:46) It means of course in this case uh clearly that we have to uh tokenize it differently basically. Uh yes that's a good question but we we don't really have much control of it probably we can try to change something but it is how it is happening under hood that's a very good question so Dr. Karachkin does would proximity um of the same word then it cause even more confusion.
(1:40:15) For example, if I said uh if I had written I was writing an article on on the Tesla car and I said uh Tesla would be rolling in his grave if he knew that the new Tesla um engine only had such weak uh Tesla measurements in its magnetic fields. Yeah, I think that since we use this convolutional network, it kind of filters and combine things together from the neighborhood, I'm sure that it will it will have impact.
(1:40:39) So it will be a little bit more confused if it is like close to each other. Right? If if uh one test like yours like at the beginning of your document and second test at the end and surroundings are different, problem is easier obviously. But as as you put it, if you have it this way, then it is already more difficult of course. Yeah. Yeah, but nevertheless, you know what? Maybe we understand from the context that first one Tesla is like a person, right? And second Tesla is about like company or about car.
(1:41:16) So maybe um it is enough to have this few words around your current Tesla to understand if it is person or it is company. So it is more difficult but maybe not as much difficult as as you may think. Because we understand if you understand that probably for new network it is also possible to understand like the sentence you kind of gave as an example for me it's understandable that first Tesla is a person second Tesla is like vehicle right so I how did I understand from the context so same here maybe it is already enough sort of separation right my filter is probably not huge maybe it is
(1:41:51) quite small quite you know quite small and captures only neighboring ing words not entire sentence this interesting example. Yeah. Yeah. But I think it it is uh still possible to understand. Yeah, context is important. Definitely context is important. You're right. Uh but company cannot drive anywhere for example. Right. That's why it is already maybe sufficient.
(1:42:28) So any questions about anything? There will be assignment on this next time. We'll we'll post it next weekend. Um now let me let me see what else we have. So you know that there is a There is a schedule on the canvas and uh you can see that on November 9 project selection begins. It is like Sunday basically. So on Sunday, so pay attention.
(1:43:28) I will post basically description of your project. No kind of list of possible projects where you can choose from. You can suggest your own project is not a problem. And then you will start thinking what you want to work on. Thinking means not just thinking like oh I want to do that. You have to gather gather data right.
(1:43:47) You have to make sure that it is doable. You have to you have to see where to get data, what you're going to do, what kind of network you're going to use. Right? Typically, I ask questions about what kind of inputs you're going to have, what outputs are going to be.
(1:44:05) If you build some kind of model, I want you to understand what inputs are, what outputs are. So, you understand what you are doing, right? I will post a also maybe example of some kind how it it is supposed to look at the end and uh no maybe I can I can see I remember one example from let me let me check maybe this example is from different class but I think I'll I'll show you it anyway.
(1:44:33) So then you will have to have a presentation. You will have you have to have presentation which is like kind of video presentation. You'll have to have document which desri describes what you did and also you'll have to prepare a slides which you use to to create your video where you present your results and also as always as always you have to supply your working demo like code so we can see how you did it and we can run it if you want.
(1:45:02) So now let me uh see some examples. So professor y can it be a group project or it has to be individually done? Actually it is designed as individual project if you really want to do it as a as a group project please let us know we'll consider. So it is not a problem as long it is a little bit more more difficult. Right. So okay. Yeah.
(1:45:29) Sure. So this is actually for deep learning class but nevertheless let me just show this is some kind of I know this is assignment I'm What? So this is final project it is for maybe different or maybe not maybe not for
(1:46:35) different class I don't remember but it says in Russian national language processing so this is oh so this description this is just description I'll post this file no one like around weekend right so it tells you that You need to basically think about your project. It tells you that you have to define problem. Uh you find data.
(1:47:00) You have to understand what your inputs, what your outputs are. You have to develop code. You have to also present your demo slides, PowerPoint slides of of some kind of PDF whatever you like. Then report itself and also you will create video presentation. It is like YouTube video maybe kind of kind of video presentation you don't have to use YouTube if you don't like it you can use something else but it must be video where you can watch it where you explain what you did and this is uh this is points two points for the project selection which is part of final exam
(1:47:39) final product basically then slides presentation video and so on this whole 15 points will be this is about the project and this is a template which you can use to no I will change it to current year which you can use to create your slides which we will demonstrate during video presentation and during video presentation it's not interesting to just look at your what you have to extract something useful something uh representative so we can see what you did so this is about uh about this project again I will post it later you will find it line
(1:48:19) and by the way it is going to be due let me see it is going to be due on November 25 right and final project itself is going to be due on December 16. So keep in mind this dates November 25 you submit your projects uh which you want to work on it means I encourage you to already start thinking about project you want to by November 25 you want to have everything available like data set everything should be already available you don't want to kind of postpone it until the end and uh also I will post some examples
(1:49:12) Let me show you one example. It is only a document only not presentation itself not quote but it is like a document which describes what you did and it's someone did it in 2020. It was actually deep learning class but we have same structure and they say abstract then they say content they describe what they working on they present data understanding data then they some kind of you know diagram which shows then they talk about architecture of neural network modeling and then it is like already network itself, right? the loss function. Whatever they optimize performance metrics, they compute some kind of
(1:50:06) scores and they show some performance and then at the end they show some uh discussions additional models which we can use and then some discussions inception block and then result the discuss results conclusion lessons learned and then references and then appendix which includes some parts of code, right? But quote needs to be supplied anyway.
(1:50:40) So it is not only what you have to supply code, it is only maybe important um snapshots which you can refer to in the text but code itself should be also presented and also some figures which you can refer to. That's how your final product is going to look approximately. So any questions about this? So on November Nine instructions will be posted on November 25. You have to submit your project selection and on December 16, you have to submit the final project project itself.
(1:51:16) Any questions about this? So on November 25 when you say uh project selection does it include like writing proposals or just say which it is going to be like I be like n grader it is like one page basically a proposal which you you going to type on canvas it's going to be like quiz essentially and there in a window you're going to uh type it.
(1:51:45) So I think I I will ask you to do it simply this way to just write it in in in a window on canvas. It's going to be like quiz basically but um open question essentially where you type your answer and you will have to describe what you want to work on, what your data is, where you got it. No maybe question about why you chosen and also you're going to uh uh say what inputs what outputs are what kind of the network you you're going to use to solve this problem.
(1:52:17) Does it make sense? Yeah. Yeah it makes sense. Uh quick question professor. So if we need to train the neural loop network uh and uh we um do does it have to be like uh created from the scratch or can we use like open source uh model and then use it to you can use not only open source you actually can load already pre-trained network so this is typically how it is done right nobody nowadays will train I mean most of the time nobody will train from scratch which we want going to use something pre-trained it is totally okay
(1:52:54) okay but but you always have to specify what you what you are doing where you got it and so on even if you try to correct your code using a uh chpt for example you always have to refer if you don't do it basically it is violation so but this tools which we're using it is already nowadays it is basically what people use I mean I talked to different people from industries they said that it is already routine right so this how how it is done maybe many of you know you know yourself you can tell by the way maybe you can tell maybe some of you can tell what's happening in in workplace nowadays I
(1:53:32) talk to people they tell me that yes nowadays we always use basically just GPT to write code to correct code and so on so this is how it is happening so people become more efficient by using by using artificial intelligence that is already kind of inevitable and by the way it is also related to layoffs maybe recent layoffs also were kind of driven by use of because people became more efficient kind of and as a result yeah we use we use CL cloud code GitHub copilot very extensively in workplace but so it is already no kind of restrictions people encourage you to use it right
(1:54:10) correct yeah yeah so anyone else you experience but in our class you should understand that we not only solve problem we have to learn it means if you just use charge GPT to produce code without thinking much and if you submit it and especially if you don't even site that you use use CHP of of of some kind in that case you are sort of in violation so keep in mind if you're doing this it is not correct you are in violation of kind is actually se violation if you report it which you may easily report. Uh then there will be investigation
(1:54:53) and they will talk to you. They probably they will meet you like in person over Zoom or whatever. There is special committee who will try to understand it and uh it it may may impact your ability to enroll in future to courses at Harvard Extension School. So basically there is special special committee who is doing exactly that trying to understand if there is plarism or not.
(1:55:17) I'm saying you can use artificial intelligence but you always have to site it and you always have to actually make sure that you understand what's happening. It is not just you have to learn first of all. Yes. Question. I just use uh AI generative tools to give them an instruction about what I want and whatever the they generated I always site it. I believe this is this is legitimate. It is. Yes.
(1:55:43) If you site it, if you if you especially if you I would say this way especially if you use like a kind of approach where you write code yourself but you ask to improve it or you ask to correct it or something like this then maybe add some comments useful comments right it is actually I'm curious anyone else can you tell me what's happening nowadays in in in workplace like if you work for some company it whatever data science what's happening there.
(1:56:17) So in my company um the enterprise uh modeling tool is data bricks and uh it has uh this AI assistant so it's very well it becoming easier for debugging um in some cases um so if there is a error some somewhere um it will be identified pretty easily and then there will be recommend recommendations that the data bricks AI assistant could could uh provide and then you can take a look if you want to accept it or you want to reject and try something else.
(1:56:54) So yeah the the environment is is pretty nice. I like that feature. Typically my understand my understanding is typically people use these tools sort of internal tools. Now it is let's say like charge apt basically but it is sort of specially pretrained to you know write code and typically it is like internally used so there is no leak of information if there is a concern about no kind of uh you know uh secrets like company secrets right and yeah and they use not like open source clearly but something within the
(1:57:29) company and uh my understanding is from like talking to people that people nowadays widely use it And uh I'm not sure you said debugging. Debugging is is is is quite quite nice. I think that people like literally write instructions what they want and then generate code which they can later kind of try to digest and change if need change change it if needed. Right.
(1:57:56) Um yeah I I think for us within also government like over the last two months um we used to have there used to be this separation between our ability to actually use it on the low side versus on the high side. But I'll say that over the last two three months they have deployed with on the high side.
(1:58:14) So you know you can now even work with it to the extent of you know even generating like dummy data for testing and uh about 2 weeks ago um Amazon Quicksite also actually deployed um an assistant within quick suite which you well they now call it quick suite instead of quicksite and you can literally I guess you know generate like a whole dashboard in a couple of minutes by essentially just kind of giving it instructions And same goes for essentially like a whole pipeline within using kind of like the data that we could not move to the law side a couple of months ago. So it's definitely getting expanded within the industry even government.
(1:58:55) Yeah. Yes. So that's yeah that's what I hear from people. Anyone else anyone's anyone has experience? Uh professor actually work in the language industry uh in translation for languages and this has been happening for quite quite some time. um many of the work that uh previously went to us through human translation has has been reduced by a factor of probably twothirds.
(1:59:20) But what it opened up was uh the uh domain of speech to text where in a lot of uh um demand has opened up because of the volumes and obviously costs involved. Now the the main problem now is the um minor languages major languages the edit distance is is already there. uh it's uh approaching a near human um aptitude or quality but the minor languages are the ones are they're really both a um problem at the same time a business opportunity for us.
(1:59:47) So yeah, we liken it to be like the advent of the sewing machine and that we we used to be a human uh you know we used to use thread and needle but now with the sewing machines we we are able to make 10 dresses instead of one dress a day but we still we are still needed as as humans for for the quality stuff.
(2:00:06) What what couples what languages do do you use for translation? I mean not you personally but your company uh we handle usually Asian languages um Southeast Asian I handle myself a Filipino uh Tagal and it's one of the minor languages so it's it's pretty interesting how it's evolved over the decade or so.
(2:00:25) Yeah those are particularly difficult languages right to translate. Yeah especially the nonlat non-Latin ones are really difficult. Yeah. Uh anyone else? Um I have something to share professor. So um I cannot say clearly the name of the companies that I I had experience with but I I just want to share my own experience.
(2:00:52) It can be true for me but not for everyone else. So um it's different companies. Uh for one um they have deployed the um um QD or sort of within um the cloud database only. So it cannot it cannot work for any other uh environment. It can work it can understand to some extent the architecture and the relationships of the databases that we have on the cloud.
(2:01:25) But uh back then it was still has a lot of limitation because um the relationships or any kinds of diagram or graph um um database they they are kind of complicated. Um so it's better to ask the data engineers to be honest in order to know what should be correctly connected with each other. Um but to some extent it can work for basic table connections in SQL queries. Yeah.
(2:02:01) Yeah. Sure. Yeah. Another company um the uh the QD is not the one that we have with um the big names but um it's kind of deployed across the whole world or or the whole company. So any departments in a company can use but the thing is that for low tech or non tech uh departments they can access it and and support their work quite efficiently uh because it can generate insights or even consolidate the writings that they need to submit something but uh you already saw uh the cases that the deoy company earlier um that they they leaked the no they submitted their work to the client and the client find out that it's cheaply writing. So it was a scandal for deoid.
(2:02:46) Um but in my company um it's different because um we have to check everything that we need to submit uh even if it is generated by the um GBD or something. Um but the thing is that from my own experience because I have to develop the code from scratch pretty much uh from low level to high level as well.
(2:03:11) uh the harder the level of the coding or the complexity of the coding or any techniques that we can study from the data science major um the GP that will deploy in the company across the company it is not that intelligent or smart it can exhaust very quickly and the way um I I wouldn't say that is a trade-off between the imagination and hallucination but um it can exhaust itself So the uh the solution of the codes that it provide might not be efficient enough.
(2:03:47) So if you suppose that we um we have our personal paid version of chat activity, it can work even better. But the thing is that at the end of the day um it's not a plan, it's a pilot. So um so for example uh the computer vision course I took from the Harvard extension school that helped me study a lot of the techniques and uh initiatives that I I can translate and help the AI system to support my idea but the thing is that it's still my my whole ideas in order to move forward with the projects because even chubity they sounds very creative but they cannot
(2:04:27) uh efficient and are precise enough. Yeah, that's what from from from my experience with the company GD versus the personal GD. Yeah, thank you for sharing this. So, I'm not sure if anyone else wants to share this story. If anyone else Okay, if not then we can stop now. Thank you. You have a good night. Yeah. Thank you. Good night.