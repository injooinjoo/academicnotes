(2) 89 day6 - YouTube
https://www.youtube.com/watch?v=agx37_XDNok

Transcript:
(00:01) Hello everyone. Good evening Dr. Kkin. Hello. You should see my uh slides right here. Yes, we can see them. So now we have uh quiz number number five. Let's do it first. This is probably not very difficult. Nonetheless, let's like talk through.
(00:32) First question is about which stat which statistical mirror considers document specific vector frequency and also adjust it based on how often the D appears across the entire corpus. In this case we understand that we use exactly TF ID of turn frequency inverse document frequency one hot coding doesn't adjust by word doesn't adjust term frequency uses only frequency within current document uh which means ABC is not correct but D is correct TF second question which of the following best describes how TF basically modifies the weight of words that frequently appear across many documents. And in this case, we specifically said that we actually decrease the weight
(01:15) uh reducing the overall influence, right? So we kind of see how often this word occurs in our documents and this is metric. This is kind of additional measure, additional weight which we incorporate into this TF metric. So B is correct. If you have any questions, you can interrupt. We can talk more. But I I hope everything is clear. Next one.
(01:37) In this case, we simply need to compute it. Now, let's comput it. Formula is given. By the way, TF is defined some sometimes differently. There are some kind of adjustments and so on. I gave you formula. So, we kind of all on the same page. Even though we should understand that sometimes people define TF differently.
(01:57) Even default value could be different from this formula. This is like starting point basically. But sometimes people add like plus one for example in order to avoid division by zero and so on. Even though division by zero is not usually issue because if you're talking about terance which came from from the corpus we know that at least one time it was observed nevertheless people do it this way. So let's compute TF in this case.
(02:23) So question three tells me that I have specific document which is document number one apple banana apple and I'm talking about term apple in this document as you can see tf is referring to particular document and also particular term inverse document frequency doesn't refer to particular document it looks over all basic documents but tf refers to all documents Now let's say in this case turn frequency first of all turn frequency is how often this particular term occurs in my case apple in specific document which is document number one so document number one and
(03:08) you can see the formula it tells me number of times it occurs in my text over number of terms in my text and by text I mean document right so in first document I have three terms it means out of The total out of three tokens firmly speaking and number of times apple occurs is two. It means 2/3 it is my TF term frequency.
(03:33) What about inverse document frequency? Inverse document frequency doesn't care about my document. It actually looks at particular term. It means apple in my case. And we say let's apply logarithma in this case. Let's apply to this ratio total number of documents which in my case is three and then number of times this particular term occurs in my documents that means number of documents containing my term apple in the first one apple is not in the second one apple in the third one it means out of two and we get this result.
(04:12) So what that means? It means we can easily compute now term frequency inverse document frequency. It is going to be clearly specific to my term in this case apple and also will be specific to my document which is document one and it will be as you can see first one 2/3 times logarithm of 3 /2 and I'm going to get 2703 2703 that's how we do it but is nice to kind of know how computers.
(04:50) So we have like intuition what's what's happening here? Why multiply by the cell and so on. So next one quick question for you. So um I was I was reading a lot of articles online and they seem to be suggesting that it was more more common place now to use log base 10 instead of a natural log.
(05:14) Do you know if that's true? You know what it is completely um you know completely uh equivalent right because you know that because you know that uh uh let's say logarithm of b with base a is basically ln of b over ln of a right what we are talking about we're talking about multiplier we just multiply it by constant like consistently everywhere multiply by constant So it means if you are talking about like logarithma with base 10 of number b it will be essentially ln of b over ln of 10.
(05:56) To put it differently it is 1 / ln of 10 times this matrix. So this metric which is log with base 10 is completely proportional since we multiply it means every term frequency do frequency in our case will be multiplied. Maybe you are correct, right? But what I'm saying is it is not really important. It is just multiplication. But maybe yeah, maybe packages just use base 10.
(06:23) But I I remember recently I read about some kind of metric. They used to look with base 10. What was it? It was it wasn't frequency frequency. It was something a different metric. They also mentioned that base is 10. So maybe maybe base is 10 in this case. Maybe you're correct.
(06:44) But it's not really important because it is just multiply right algorithm of 10 is around like what like 2.7 power three maybe around three it's like one third in front maybe and that's it that's only difference but you're correct it is possible yeah I don't I don't really remember let me let me check it but you see my point right yeah you could basically use any base and it would just be just multiplication uniform will be multiplied everywhere.
(07:14) So it is not really big deal especially if you afterwards normalize it which we often do. It will not have any impact whatsoever on the result. We often take a vector of IDF TFS and normalize it length will be one anyway. It means this base completely irrelevant. I mean it's not important what it is. Result will be same. Does make sense? Perfect. Thank you.
(07:40) I tried to validate that one with the um TF um vectorzer in Python um and I got different numbers. Is there parameters that you have to put in to I guess the match? Yeah. Yeah, different numbers. It is absolutely understandable because there are different variations. U remember last time we discussed you can you can use different variations of term frequency.
(08:09) First of all, this is what we use just frequency but people use sometimes even len here right and use different also double normalization as you can see they use maximum number of times it occurs and so on. So there are variations also invers frequency may be modified as well. All right. So I think that this is default in Python.
(08:35) So you you kind of saying that it is logarithm base 10. Maybe you're correct. Maybe I wasn't wasn't correct. I have to double check. But this must be default in Python. You can see + one + one as I mentioned. That means will be like 4 over 3. Basically also they add + one at the end.
(09:02) No, it happens if it happens that your algorithm is equal to zero then it's better be uh if it is like one basically right if logarithm if if ratio is one it means your word occurs everywhere like literally then your logarithm will be zero because of one is zero and people sometimes say zero is not nice let's make it plus one okay you can make it plus one I don't think it's going to be much much different to be honest but No, maybe slightly right because we afterwards normalize it that's may impact if there is some kind of term which is very very common basically which is everywhere we say that signal better be one rather than zero
(09:39) if term is completely missing that is different maybe it is zero so we want to differentiate it in cases completely missing term or completely like you know term which is everywhere that's why + one so this must be by default And as for term frequency I believe this must be by def Oh yeah it says it must be by default.
(10:01) So it means this is modification + one + one plus one at the end. If you check it maybe it will be that but also as I mentioned last time by default you do normalization. It means you got your vector and you have to normalize it in order to no make sure that length is one basically right.
(10:24) So now if you try to read documentation I'm talking about like logarithm u so I don't see it right away what they use have to double check maybe somewhere else. I don't see it right here. So answering a question which uh algorithm they use I don't see it right away but since they by default normalize the result it's not going to matter at all. So it's going to be completely equivalent.
(11:07) Well actually maybe I'm not correct. If there is plus one it may it may matter. So if there is plus one it may matter because plus one already means different. That means multiply by something like 1/3 and plus one to the result makes it already different which means yeah it may have impact on ultimately yeah it was pretty different it was 089 I think is what I got for 89 first one which is oh you mean you mean you use this oh I see you use this example using like tf factorizer so you got you got how much uh 089 so but yeah I saw that you had
(11:46) formula there. So I just went with that one.9 but keep in mind that vector is also typically normalized right gotcha. Yeah. So that's why you have to compute all of them then normalize and see if it is same. Um yes I'm not sure if I we discussed normalizations right yeah often employed normalization right so now uh yeah so it is I mean if you want to get exactly same result you have to basically compute all of them for specific document normalize it and you probably will will be getting same result if you add one one one so I assume what will be the case? I'm not sure if I I'm not sure
(12:33) when I checked it last time. I have to check it maybe to to be convinced kind of documentation doesn't really tell it explicitly. So I I cannot confirm right now. You can you can you know you can basically check it and let everyone know maybe it's it's interesting.
(12:54) I'll check it is very easy task right just compute like for all of them right basically in your first document and also normalize manually and see if it is same no we can do it right now I just don't want to spend time so now um next one we are talking about uh issues which arise with static word embeddings. It means we basically take a corpus we compute this word embeddings using particular corpus and then maybe 10 years later semantic slightly shifts people use words slightly differently they mean something different right it always happens especially when we move
(13:39) from generation to generation people start using words differently it is normal uh and in this case if you sort of use static word embeddings of course we have issue related to this change of the use of the words change of Semantics, semantic drift. That's exactly what we have, right? Semantic drift. Now, next one. Um, yeah.
(14:05) So, uh, context insensitivity would only be the different meanings right now and semantic drift would be different meanings as time goes by. Is that the difference? In this case, uh, we we are talking about so basically we say static word embedding, right? uh will have issues when language evolves and is it called how is it called uh or semantic drift right so I'm not sure if I understood your question I'm sorry I asking what it just the um the definitions for context and sensitivity and semantic drift were very similar so I wasn't sure if semantic drift is only um you know a
(14:48) generational change like it's it's a different meaning 10 years from now and context of insensitivity is only like the different meanings right now in in this uh current period of time. So I'm not sure if they overlap or if they have similar meanings. Basically uh in this case we have a specifically issue that the use of the sword changes right uh in sensitivity it is completely different story.
(15:17) It can be not even related to the shift. It can be I mean this is not about the change of the use of the word to this different conversation and we are talking about basically when language evolves over time so it's it says it right evolves over time it means that we are referring to semantic drift specifically I mean I'm not saying that there could could not be other issues there could be also a graffiti issue but it is not related to the language being evolved over time it is different issue arises by different reason and so on and if we are referring to the issue related to language being evolved it means we
(15:56) talking about semantic drift right perfect thank you yeah I'm not saying that there are no other issues overfeitting is also issue right and also could maybe right potentially I don't know if language changes uh probably not probably fitting will not be due to language changes so Now next question last question is about uh about um similarity right similarity last time we discussed similarity.
(16:33) So basically in this case or to be specific when we train this embeddings actually they use specific function which sort of uses that product essentially right as a result similarity is defined in this case exactly as cosine of angle between two embeddings because of laws which they use when they obtain embeddings is related to basically that product right that's why naturally similar Similarity is sort of identified or I'll say similarity is related to the angle between two words because of loss which they minimize during embeddings.
(17:09) In this case, question five, let me say question five asks me which word is closest to cat, right? Which is closest to cat and uh cat in my case is equal to two and three. Then dog is equal to five and seven. And then mouse is equal to 1 and two. We can even try to sketch it. So maybe it will not be obvious from the sketch. That's why we'll have to compute cosine ultimately.
(17:51) But it is interesting to sketch it kind of 1 2 3 4 5 in this case. 1 2 3 4 5 6 7 and we say cat is 2 and three. It is right here is cat. Five and seven is that one right? It is dog. It is far away but angle as you can see is almost same. This is cat and first last one is one and two. It means right here is mouse. Again from the sketch I would not really be able to guess maybe which one is closest to me that's why we have to compute I'm not sure the way I sketch it looks like dog and cat is closest right dog is closest to cat even though it is much longer it is further away in this dimensional space but if you're talking
(18:48) about angle the dog seems to be closest now let's let's compute it let's first of all recall let's recall uh if I take two vectors a * b using dot product right length of a * length of b time cosine between those two vectors it is regular times and this is dot product because it is vector multiplication of vectors in a dot product sense it means from here cosine of alpha we assume that a b will be not zero lens cosine of alpha uh will be that product over a length of a also length of b. Now let's say assuming
(19:39) that length of a and also length of b they are strictly positive numbers. So we don't divide by zero. In this case we have to assume because if vector of l we don't really know what angle is it is not defined. You can kind of look at zero vector any way you like. That's why A and B should be strictly positive.
(20:00) If you want to define angle. Now in this case we say first of all let's choose distance between mouse and cat. Let's say it says dog and cat. Okay. Let's say for sign of animal between let me say this way cat. you will understand cat and dog cosine doesn't really have like you know two arguments but it's like in quotes cosign of angle between those two so you understand it means I have to take first of all cat which is 2 and 3 multiplying that product sense by dog which is 5 and 7 and then divided by square root of 2 + 3
(20:44) 2 it is a length cat over a length of dog this way and we are going to get some result. Very similarly, if I want to cast between cat and mouse, I need to take cat, which is 2 3 multiplied by mouse, which is 1 2 and then square root of 2 + 3 squar as before is my length. Second one of 1 2 + 2 is my length. Let me compute these two things.
(21:27) I will use some tools to compute it. U so let me see uh edit preferences. So let me say that pet is simply vector which consist of numbers two and three. Then dog is a vector which consist of numbers five and seven and finally mouse is a vector of one and two one and two so in this case um first of all I can compute maybe similarity between cat and cat right so it will be a * a this way a * a if You say a * a it means component wise multiplication.
(22:26) It means actually it is a object a sorry a not a but cat time it means like vector if I want to compute that product I also have to take sum of that sum of that. So sum will be like that product. Well, maybe there is another way to compute that product. But you can also I believe it should work this way if you want. Yeah, should work this way.
(22:55) Let me say sum over then I say square root of essentially my uh uh get square root right also component wise. Then I take sum it means I'm going to get exactly what I want. I'm going to get sum of my squared components and I take square root I get exactly what I want over square root of sum of my squared components which is cat again square root and I'm getting one.
(23:29) So basically similarity of cat and cat is equal to one because ang go zero it means cosine of zero is one. So clearly so formula works as you can see. Now, if I want a cat and dog, I simply change one of those to dog and second one in this case to dog as well. And I'm going to get this value which is 99 48 this way. And second is cat and mouse. I change dog to mouse.
(24:11) And I get already 999 two two and last one is let me say eight you know if I round it. So it means even though the difference is not as like huge but I can see that cats and dog they are closer right just as I sketched it. So this is better. This is closer to one. It means angle between cat and dog is is smaller.
(24:43) It means dog is more similar to cat than mouse even though it is far away. But we understand that in this case we are talking about angle basically that's why it is this way. Any questions about that? So again why similarity is angle? Basically this related to the loss function which we use in this case when we do this type of embedding.
(25:06) We didn't really cover it because you will not be able I mean you will not have to do it yourself typically with just run this but if you try to kind of understand what's happening you can check what kind of loss function is used loss function is minimizing specifically this type of that product not kind of we apply sigmoid function to it then I believe some kind of algorithm and so on.
(25:26) So basically we minimize this type of function as a result ultimately we are supposed to be getting similarity is given by smallest angle as a result. So that's what happens that's why similarity sort of nicely given by by cosine between this representation of angle between this representations.
(25:51) Any questions? So now we have today we are going to talk about character embeddings. Before we discussed like two gs word embedding now we talking about character embedding. So we basically uh move to at the character level every token will be character and we can do embeddings as well. Then second part is out encoders.
(26:24) It is very interesting actually discussion about ways to nicely I mean efficiently represent your signal whatever it is that it could be could be could be sentence it could be image or whatever you can efficiently represent your signal using out encoders the result it can be used for multiple tasks including even uh so-called denoising out encoding then I will talk about sparse out encoders why we need those and last part is ational encoders.
(26:50) We may not have time. It's not really part of this class. If we have time, we'll discuss a little bit, but maybe not really, right? So, let's see how it goes. Uh, any questions so far. Let's talk about character embedding. So, you understand that character embedding means we break our text into characters. Token will be individual character.
(27:17) Uh and then we say um it is especially useful if you for example want to maybe uh uh correct some kind of errors right in your sentence. If you use word embedding error means essentially completely new word it is already a problem for you. It is already out of dictionary. What to do with that? How to kind of you know build some kind of a spell checker? In this case, if you already at the character level, it becomes much much more natural to kind of build this kind of spell checkers, right? Correcting this errors, alphagraphic errors. So that's how we
(27:56) can use it. Now uh we can actually we should understand that this character embedding is much more useful in some cases because you may have variations of words especially if you take different languages let's say Turkish I'm not sure if any of you know not know Turkish but from what I know Turkish may have up to like easily up to 15 different u different different variations of word in our case running runner ran not so many still many in Turkish we may have much more because prepositions will become like part of word on on on the table maybe single word basically
(28:41) the result in that case again tokenization at the word level maybe not the best idea that's why this could be a little bit better right is actually not the only way to do it we can also break words into sub words this is probably the best approach actually run then in and so on. You could break words into sub words. Most likely it will be the best approach.
(29:05) Nevertheless, let's talk about character also embedding and uh in such cases where you have like words which essentially consist of different different parts especially if there are variations of like those forms. character meaning could perform much much better. Right? So if you have out of vocabulary words then in that case it is already much better to represent such things especially if you have some kind of misspellings or whatever right some kind of error in your spelling of the word which you're using. Uh that's why it is quite um
(29:43) quite popular as well and handles of course prefixes suffixes and so on because it is at the character level. That means we already much more comfortable with those. Before I would say we have just do stemming, we have to throw away those prefixes, suffixes not the best approach.
(30:01) Maybe especially if you want to produce something meaningful. In that case of course you don't want to just throw away things if you want to do translation. Stemming is not not approach you would want to use because stemming means whatever you produce is not readable, right? It may may make some sense actually but it may not readable. We don't speak this way.
(30:20) That's why uh character embedding would be more appropriate. Not to be precise. Actually, if you use some kind of sub words, it would be even better. So now um we understand that especially different languages as I mentioned like Finnish or Turkish may benefit from character embedding because they have a lots of different variations, right? They have a lots of different um different forms. One word may have like easily 15 15 15 grammatical cases.
(30:53) No 15 different form basically. Uh also it is useful in name named entity recognition when we when we try to extract those from the text right um because name entity recognition may involve recognizing words which were not seen during training. That's why in this cases maybe again tagization is not the best approach just to keep in mind.
(31:18) So now um what we can do with those uh character embeddings of course we uh can use whatever we discussed before. Now it will be like every character will be essentially vector embedding vector. It means your text will be sequence of characters. It means sequence of sequence of your vectors.
(31:40) That's why we can use basically same techniques as before. We can use recurrent networks. Right? Even we can use convolutional networks as we discussed at some point. Uh yeah. So this is what we should understand that if we decide to be at character level basically the mechanics after this uh representation doesn't change much. It's going to be very similar. Results could be quite different.
(32:06) Results could could produce completely different uh different performance. Right. But mechanics of that is basically same as before and uh you I can also uh uh mention hybrid approaches. Sometimes people try to use basically character level and also at the same time word level. They combine those things together essentially.
(32:33) You can try to represent word by sequence of vectors because every character is a vector. Then you can somehow represent sequence of vector y single vector and combine it like literally like concatenate representation by word embing one vector will be representation of sequence of characters ultimately and second vector will be just representation of your word at the word level and you kind of concatenate and you get some kind of result.
(33:00) So hybrid approach is also quite popular techniques. So just to keep in mind right we understand that embedding layer will be implemented very similarly we move from tokens to characters mechanics will be quite quite similar. So now u here is example how we can do it. I took a text which consist of three documents hello world machine learning deep learning and I say first one is one second is zero last one is zero right not is like labels which I'm trying to sort of predict in this case of course prediction maybe maybe not so kind of uh interesting because only three three documents which I'm using to train my neural network
(33:43) nevertheless this is what as an example I want to demonstrate then I can say let me use character level tokenization just as before my tokenizer right uh from tensorflow in this case and I say character level is true it means I'm going to apply character level tokenization I apply it to my corpus to my text and I get sequences ultimately of my tokens right now in this case I say let me create vocabulary size basically my vocabulary will consist now of what not of words every entry in the vocabulary is going to be character clearly. So every every element of my vocabulary every kind of word and quotes
(34:27) in quotes is going to be character. Now I say plus one because in this case we typically try to make sure that every document consist of same number of characters and the result we implement padding. Remember like a zero padding in case of convolutional networks. In this case, same idea.
(34:48) We have padding equals post. That means we're going to add zeros at the end. Basically, now we take machine learning is longest one. Hello world is shorter. It means at the end we just add 0 0 as many as needed in order to make sure that everyone has same length. That's what I say by pinging a post to post.
(35:06) I just add zeros at the end. And because I have the zeros which is not really character it is just kind of kind of representation of my uh of my padding right it means I have to create special character for this padding that's why I say plus one plus one means essentially zero zero zero index will corresponds to padding if I add zero in my vocabulary zero will mean padding just so we kind of have it as well and that's it I can represent and my pred sequences as I use array in this case and then I can build my neural network. Now embedding dimensions in this case is equal to eight. Now it is quite interesting why why eight remember
(35:49) we would say embedding could consist of like two maybe 100 um dimensions maybe 300 dimensions but this is when we talk about like tokens such as words or even like two gs we have many many different words in our corpus we have many many different toss in our corus it means it means we may have easily like 300 dimensions in order to get nice performance because in that case um uh we have too many different things in our vocabulary.
(36:24) In this case, how many things do we have in our vocabulary? Well, in case of English language, it is just characters. 26 letters and that's it. 26 letters plus some kind of you know special symbols. Okay. plus 10 symbols. Maybe 26 + 10 will be like 36 characters. 36 enters in our vocabulary. It means of course we don't need like 200 dimensions anymore. It could be much less. Could be as little as eight.
(36:52) No, maybe if you want if it performs better you can add a little bit more like 15 20. So you can add more but it should not be like 200 anymore. That's why I say embedding dimension is eight because number of different basically characters is around like 30 something right 30 plus then I say let me build my my model embedding layer as we discussed in this case then lstm and then I say dense one to produce my output which is going to be classification remember there were like ones and zeros so this is basically how we typically use it very simple but illustrative example which shows how we can use it right in
(37:32) practice. Embedding will be trained on the fly during this optimization problem during classification and then we say let's compile it binary cross entropy because we do classification metric which you look at is accuracy and optimizer is Adam and then we can feed it on those ped sequences.
(37:59) But sequences means basically when we take character character character character at the end as many zeros as we have to have in order to make sure that every document has same length that's why I say sequences as 10 pox mini B sizes too and I get my result I I didn't plot accuracy I think it's not really interesting because that is only three documents but this is like sort of a skeleton for the for the for the um uh classif ifier right which we can use using character embedding and then I display my uh vocabulary in this case E for example right so this is character index E is one L is 2 N is three so basically it is my vocabulary and every number is
(38:44) index which corresponds to my uh character E is first L is second any ideas why E happens happens to be first. L happens to be second. N happens to be third. E L N Let's look at the document. This is my purpose. Why E is first? Any ideas? Is it alphabetical? Not quite. Is it frequency? Frequency correctly. Yes.
(39:11) So E is most frequent. So it makes it first just for convenience. If you later decide to maybe trunate something or reduce dimensionality so you kind of conveniently can do it. So basically it's not as important to to be precise right because when we build neural network we don't really care about the order of those representations especially if you apply embeddings it doesn't really care in which order you you place those but that's how it does it by default just to keep in mind is most frequent first L is second most frequent and so on learning learning so
(39:48) hello world so that's how it does So any questions about character embedding? It's not going to be much different from what you did before, but I mean mechanically, right? But results could be quite different. So now uh okay, let's see uh what time we have. It is 8:52. So we have some time. We can start talking about auto encoders.
(40:13) Out encoders u uh in this case I gave you example just to sort of motivate. If I ask you which sequence is easy to remember, is easier to remember? Which one requires less for example memory? Which can you memorize better? If I ask you which one can you memorize? First one.
(40:38) First one. Well, I'm not sure if I can do it, but you it is great if you can do it actually. I mean, some people can do it. Yeah, people usually can do it up to six easily and then more than that is difficult. The second one is decrement. Yeah. Second one is minus 2 - 2 every time. It is much easier. Right. So what what did you do? We we did some kind of encoding.
(41:03) In this case basically we just basically realized that there is some kind of interesting structure in the sequence which we can use in order to you know nicely represented in our in someone cannot do any of those. No we can do second one right? just need to remember 70 minus 2 and how many and that's it.
(41:25) So basically it is the idea behind behind auto encoding we we should u try to understand the structure of the data set right and then we can represent it in more efficient way it could be useful not just because you you can represent it more efficiently. It's not like about like storing this data on your hard drive but is it is about representing your basically data in more efficient way in order to use it as input to network to do some kind of task like classification or whatever that's why it is quite quite interesting and important task actually oh this is yeah auto out encoders called out encoding so now there is like this kind of you
(42:03) know image which kind of suggests that when chess players for example uh look at this you know board they can easily also remember they don't like remember every single piece it is difficult but they may remember some kind of game which had similar similar similar kind of combination right on the board that's what they basically do when they kind of for example remember easily all of those they remember something different and if you sort of translate it to autoenccoders you can say let's say threedimensional vector is input. How can we try to compress it? All we
(42:43) can do I mean in order to compress it I mean not all we can but we can do as one of the options we can create bottleneck let's say consisting of two neurons right here and then try to recover three dimensional vector again if you try to actually place input and output to be the same it means we try to reconstruct the input bottleneck we'll be forced to remember in most efficient way.
(43:19) There is no way around it that we'll have to remember it, right? Because you you try to sort of tell it you have to reconstruct it. It means you have to remember most important things. This example is especially nice because it gives you idea how how encoders work. Let me sketch something to kind of indicate what we are doing. Here we have neural network. This is out encoder. One, two, three inputs.
(43:55) X first, X second, X third. Three inputs. Then I say let me create bottleneck to two neurons in my case. and then three neurons for the output. I'm going to have x first prime as it says right let me use heads we use this notations head for consistency I will say head and also head this way then I create my full network this way maybe one is my bias also one is my bias and I do it this way as usual seems to be quite strange procedure.
(44:41) You can say what to do with this. But actually it turns out that these two numbers which are called encodings, right? Encodings. These two numbers like C1 and C2 vector which are called encodings will be nice representation let's say best possible representation of your threedimensional threedimensional signal.
(45:12) In this case we want to minimize so reconstruction loss. It means basically we say we are going to try to match output to the input itself. So basically input itself is going to be also playing role of output. It means x so let's say k min - x k hat squar if I use l to norm for example over k from 1 through 3 if I minimize it with respect to parameters this what I'm going to basically solve this type of problem if you sketch it you can imagine what's going to happen it is almost like soal pca principal component analysis you may not know know what it is but now you will see what it is. It is my data set right
(46:00) in threedimensional space all my observations we essentially say let's try to find position of plane in a way that it will capture most of the variance most of the variance this guy can be a little bit off that one can be a little bit off but it is okay even if it happens I still can capture most of variability in my data right because I basically place this plane in a way that it is going to uh go along highest resolution sort of along lowest variance.
(46:38) If in this case I use activation functions which are linear functions it will be quite similar to principal component analysis. It will be literally plane of this type and we are talking about some kind of you know coordinates over there. Now let me call them C1 C2 C1 C2 coordinates and basically every number will be sort of coordinate of the projection onto this plane.
(47:05) Then the tricky part is we have to force neural network to choose plain most optimally. That's what happens if you create part bottleneck. It will you will have to do it because we minimize cost function which this way which is called reconstruction loss. It has a name and it is called reconstruction loss because we try to reconstruct.
(47:32) Reconstruction was let me show next. So outputs themselves are called reconstructions. Xhat is reconstruction. Cost function is called reconstruction loss. Dimensionality of internal representation typically is lower because we want to can reduce dimensionality. In that case we call out encoder under complete.
(47:55) If dimensionality is lower typically we want to get under complete out encoder. It's not like impossible to use different types. So it is impossible with specific regularizations which we discussed later but uh no basically we don't have to use like all basically we don't have to use all neurons every time we can switch off part of neurons it means it is possible to use like over complete autoenccoder but typically we aim at getting undercomplete so now let's think about this if I by some reason don't like the dimensional vector it is too much for me because I know that uh maybe two dimensions is
(48:36) enough to capture most of the you know variability in this data set. What I can do I can train my auto encoder on the data set which I have this one and then I say let me erase second part. It is like uh first part is called first part is called encoder encoder second part is called decoder. What I can do? I can say let me first train the whole thing out encoder and then after some time once it is train trained I throw throw away my decoder and encoder will be like tool for me to sort of compress data and I essentially use it for literally for compression of
(49:29) my data which is going to be like my encodings will be used as input to to network. So typically we erase this part and build second part of this network. After that well we typically freeze coefficients of enorder. Not necessary. You can twist it on the way. You can tune it if you want.
(49:47) But typically we just freeze it and say it is already nice out in order. It is it compares information in in the best sort of possible way. You can say what is advantage? Advantage is that you may later have to use a lot of parameters. Right? If your input is highly dimensional, it is kind of problem, right? So it works kind of similar to embeddings. Kind of similar to embeddings.
(50:13) Even though in case of embeddings, we don't try to reconstruct the very same signal. We typically map word to context around this word, right? We map word to this embedding layer and then to classification. Right? In this case, we say let's map this guy to itself. Let's take one sentence and map it to itself. Let's take image map to itself.
(50:39) In case of image you have to build not fully connected one you have to build like convolutional outers and convolutional layers image to itself. Now in case of sentence you have to map sentence to itself. Maybe you can use like recurrent type of neural network for example it will be autoenccoder it will ultimately be able to represent your sentence by some kind of representation by vector maybe. So any questions about autoenccoders? So now uh there is a question that Yeah.
(51:09) Yeah. Which one? It was addressed to me. Can we have more than one intermediate layer in auto encoder? Absolutely. We can have many many layers inside. We're going to talk about this a little bit more. So it doesn't have to be shallow autoenccoder. It can be basically deep out encoder and there is a beauty even though you you build deep out encoder you will have you will never have to train if you don't want you will never have to train deep out encoder you can always train out and shall encoder at a time we'll talk about this a little bit later but yes you can make many layers if you want typically it is going to be
(51:47) the is going to be less less less less less less and in a symmetric way is going to be more more more more until we recover completely what we had before. By the way, use of such autoenccoders is not only reductional dimensionality in order to pass this information further.
(52:06) We can also build so-called denoising out encoder. In that case, I take my vector let's say image example I disturb it disturb it artificially and map it to correct one. It means my autoenccoder will be forced to sort of ignore useless information. That means it will it will be forced. It will learn how to ignore scratches on my image.
(52:31) If you take sentence, you take sentence sentence but during training you kind of spoil your sentence a little bit. You introduce some kind of u you know some kind of errors. Essentially your autoenccoder will have to learn how to remove those errors because they're not so much useful. You want to reproduce sentence which didn't have any errors. You take nice sentence you spoil it artificially.
(52:56) spoiled one as input, correct one as output. Now beauty of this type of approach is you don't have to have any kind of labeled data set. You can take like a bunch of images like literally millions of images and train your autoenccoder on millions of images or maybe sentences.
(53:19) Yeah, you can take sentence to sentence, sentence to sentence and build your auto encoder which allows you to basically nicely efficiently represent your input whatever it is sentence or or image. You don't have to have any labeled data set. That's why it is quite interesting idea because later on you can remove decoder and your representation is what you want to use as input to the next part of your network. You can even freeze coefficients of of encoder.
(53:47) So any further questions? Um okay this is actually yeah yeah yeah just ju just one related to that den noising thing is there a similar deal for text d noising text in the sense of like people like they write badly you know type yes definitely in order to to do it you have to take sentence which is correct one you have to artificially sort of spoil it right do some kind of you know random kind of disturbances supply your sentence which is disturbed one but output must be correct one and it will have to learn how to correct your errors. It means whenever it sees completely new sentence but not correct
(54:28) one it will basically know what to do with this how to correct it. Does make sense? So that's how we can do it. Yes. Now stacked out encoder basically answers your question. Can we do more than one hidden layer? Yes, we definitely can do it. And let's see how we can do it and why it is useful. So it's not like we have to use only one hidden layer. We can use more than one hidden layer.
(55:27) Let's say I have some kind of vector 784. Now this example represents image. Basically they say it is image which they sort of reshape. They create a vector 784 numbers. Then we say let's try to reduce dimensionality first 200 because maybe 30 is is too much too ambitious right away. Let's say 100 and then 30 then we recover it in a symmetric fashion 100 and then we say let's do reconstruction of 784 numbers.
(56:06) This is my network reconstruction on the right input on the left clearly right 30 in codings and people also sometimes call it codings as you can see on the slide coding is a little bit a less official name this is from Jeron they call it codings but encodings is sort of more official and widely used name in codings right so now uh 30 numbers uh Now the question is how to train it.
(56:40) What if I have like many many many hidden layers and deep network means I'm going to encounter issues with training because deep network usually means all this type of issues when remember we discussed gradient becomes um essential derivatives become on different scale if you move from layer to layer you can think about this type of loss function which is highly stretched along of one of the double W's let's say first W and this is like uh 47 W.
(57:13) Then it means since they belong to different layers very possible that my levels of cost function this is where my J is constant. It is very possible that my cost function looks this way. The result if I say I want to move that way is not towards minimal is completely off. Okay I can go back back and then there is a problem.
(57:39) what to do next because it is very possible that if my alpha is small my learning rate is small it will take forever if I start increasing my alpha I can even go out I can diverge so this is quite quite an issue as a result we if it is possible we never want to train very deep encoder nowadays actually people do it they introduced interestic techniques such as shortcut connections and stuff but if it's possible whenever possible you want avoid this type of you know very deep uh neural networks.
(58:10) Uh in the case of auto encoders especially it is possible to actually reduce uh the work I would say it is possible to break it into parts. Let me say remark on training stacked out encoder. Let me say on training stacked all the encoders. Let's see how we can do it without the need to try and deep network.
(58:44) So I think I didn't show it here. Let me just just tell you. Let's say phase one. phase one. First we say it looks like we are sort of kind of okay with 100 numbers to represent input because that's what effectively we are doing. We say 794 will represent 100 at first. That's why we say let's first aim at getting 100 numbers which represent my input and uh that's it is going to be my out encoder.
(59:20) still shallow. It means no such issues at least not much. We can easily train it quite nice. We can train it and phase one will train this autoenccoder and we are going to get 100 numbers which represent my 784 numbers. So we train it once it is finished we have already nice basically representation.
(59:45) What about second phase? As you may guess, second phase tells you now my my signal is not 794 anymore. It is nice 100 numbers efficiently represented by 100 numbers. It means I can easily say let me represented by 100 first 100 numbers first this way whatever it was and I don't even have to train this piece.
(1:00:13) I I can simply use whatever it was already from there. I can freeze this coefficients. So basically this going to be borrowed from there. We just simply borrow it from there. Freeze coefficients and we already know how to represent my image or whatever it is by lower dimensional representation. Maybe I'm not happy about 100 numbers. Then at this point I can say let's actually create bottleneck 30. Let's recover 200.
(1:00:42) Let's have 784. Where to get this 100? Now, basically, you can think that we first sort of pre-process date and you can say we already know how to create 100 out of 70 784. But people don't even do that. They just do it on the fly. They say let us essentially take this piece and also freeze coefficients, right? And this piece comes from my decoder. First piece is my original encoder.
(1:01:13) Second piece is my original decoder this way. It comes from there this way. And I have this coefficients. I don't have to train during second phase. All I need to do is I need to train my in intermediate part during second phase. So train train only this part. And I can get already representation by 30 numbers. You see I never have to train deep encoder.
(1:01:47) It is always shallow effectively because I freeze things always shallow which means this issue is sort of mitigated. Now I I can I cannot say it is like widely used nowadays. people find different ways to do it short uh shortcut connections and stuff but it could be actually quite useful right if you perform particular task and you realize that it is not really drainable because of this issues you can do it they improve the significantly optimizers like Adam optimizer which sort of takes care of let's say local cur kind of right it even rotates your
(1:02:24) vector a little bit it literally rotates your vector they improved architecture. Sometimes you can use shortcut connections and stuff. That's why it is already not as much important maybe. But if you want you can train it this way. You can train it by step this way. Any questions? So uh the autoenccodor network have to be uh symmetric right? Yes it is sort of sort of has to be symmetric.
(1:02:57) The problem is you cannot really do it uh you can't really do it like um as inverse operation. If we lose, we lose. We cannot really invert those by activation function, right? We still have some activation functions. You cannot really inverse sigmo applied to recombination. It is not invertible operation.
(1:03:20) It means it is symmetric like um from the kind of the dimensional point of view. But it is not by no means is inverse. Same with convolutions. If you build like neural network, you cannot possibly invert convolution there is no like unique like there's no universe right but we still try to do it in a symmetric fashion. So we can recover at least dimensions and also because we can use this trick as well.
(1:03:45) We can say if I want I can by this phases that's why 100 is very useful. That's why typically it is symmetric. It's not like it is impossible to do it not not in a symmetric fashion. You can do it probably but typically people do it in symmetric way. So okay let's uh let's make a break. now let's see it as some example that is quite simple example but it's
(1:10:19) interesting in this case we say let us take images of this type it's going to be uh 28x 28 pixels 28x 28 pixels it means 784 numbers we are going to essentially reshape it. We are going to simply flatten right away. It means we're going to reshape and take 784 numbers as input like in this case image as input. We flatten that.
(1:10:51) Then we create 100 uh neurons for the next layer 30 neurons for the next layer. And that's what we call stacked encoder. First part is my encoder. Right? Then we take 30 numbers as input. Here 100 numbers will be number of neurons in the next layer. Then sorry then we take and then we take output to be 28 by 28 which means 794 and we create stacked decoder this way.
(1:11:24) Two pieces of this network stack decoder stack decoder. We combine it together and we get stacked out in order this way. So what's next? Next we say oh let us train it. X train is input X train is output as well because it is out and we're trying to reconstruct very same images number of a boxes 10 validation set and then we do it right.
(1:11:51) Then we say how to what what you want to show. Essentially we want to show the original image itself extra and also we want to show those reconstructions. It means you want to show reconstruction that means outputs from this network. Keep in mind it means essentially we are going to pass the whole image through 30 numbers as a bottleneck and then we're going to reconstruct.
(1:12:18) That means we effectively at some point we're able to represent entire image by only 30 numbers 30 encodings or 30 codings and that's what we present here. We say original images as first row second row is output from the network. It means effectively at some point it was represented by 30 numbers which is quite impressive. You still can recognize that short of some kind shoe like and so on.
(1:12:45) bag is not clear if there's a bag. Maybe it was back. Now it doesn't. It's not clear. So any questions about this example? A very simple but also illustrative example which shows how these things work. Uh now there is a a tool so called TSN right it is T distributed stoastic neighbor embeding TSN.
(1:13:15) Let me say a couple of words about this. Now I I can I I should tell you that TSN is not really uh dimensionality reduction which you use in order to kind of produce nice you know kind of input to your network and produce nice classification or whatever. It is used purely for visualization. It's not like something which you kind of use in practice in order to reduce dimensionality right and then reduce dimensionality vector will be used as input to network. No, it's not the case. It's not part of network.
(1:13:51) Typically it is purely to kind of look at your results. Let me say um distributed stoastic neighbor embedding essentially is doing the following. Stoastic neighbor embedding is doing the following. You can read about this if you want. It's not really as important for us. Not just kind of visualization tool.
(1:14:19) It computes some kind of probabilities, right? Related to uh related to some kind of dependencies between points. Let me simply say this way. Let's say I have a 30dimensional space. 30 dimensional space. It means my encodings basically space of my encodings. Then I say there is some kind of cluster. Okay, cluster. Then there is second cluster over there.
(1:14:46) In order to represent what we typically do, we try to kind of rotate it. We use like principal component analysis which means we find a plane in a way that resolution is is greatest and we can see if there are clusters on all clusters. Now there is a problem. What if one cluster is behind this cluster is in front that cluster is behind that cluster is over there. It blocks the view. It is over here. It is blocking. This is blocking.
(1:15:10) It means no matter how you rotate. It is very possible that there is no really way to see those gaps. Even though there are gaps in this small multi-dimensional space, there are gaps between clusters but we don't see them. And what TS and E effectively is doing this how my T SN is doing this. It is not as stic.
(1:15:36) It means it is not deterministic procedure. It is purely for vis visualization. It maps it to two dimensional space and it says let me take first cluster and somehow you know plot it. Now next one is going to be next to it in this small dimensional space and I know that there is a gap gap if there is a gap in mult dimensional space I will have to place it like next to it this way and I try to sort of preserve visually my gap if it exists in multi-dimensional space next cluster okay if there was a gap I place it next to it and I try to preserve space and so on and basically all my clusters this
(1:16:15) way will be visible As you can see there is no like unique solution to this problem. You can intuitively probably guess no solution to this problem right because in threedimensional space we can have like three balls close to each other right in two dimensional space is already more difficult. We can even have like four balls close to each other easily.
(1:16:39) The four into dimensional space it is already more difficult to preserve what it was originally. But we can kind of visually try to preserve gaps. If gaps existed, we can try to preserve them. This is widely used for kind of visualization. It's not like input to network, not input to network. But with a visualization, it is kind of nice tool.
(1:16:58) That's what we're going to use here. We say let us use TSN essentially and take this compressed representations which means codings and try to plot those codings from three dimensional space onto plane. We take only two of those right we use this TS and E to present in two dimensional on a plane essentially and that's what we are going to have in this specific case we are going to have this kind of clusters this is already result of TSN and we see clusters what happens is this cloud corresponds to shoes this cloud corresponds to pens and so on and this
(1:17:38) kind of explains why autoenccoders are so so powerful because my 30 dimensional space actually nicely it has to do it this way in order to be able to reconstruct nicely nicely um captures most important information somehow about the input also you can think easily that if I want to pick someone from this cluster since this is too t short most likely all of them are t-shirts I can even do like classification and say this is too short next one is too short next one is t short typically people don't do this way
(1:18:12) because this way means This approach would mean that you have to do clustering also on the top of this. But if your data set is like partially sort of you know trained now you can sort of easily guess that you can label everything else based on to which cluster it belongs. We don't do it that way. We do it different.
(1:18:36) We say in order to obtain all the points all I need to do I need to use my images. I didn't have to use my labels. I didn't have to use them at all because I reconstruct my image. As a result, uh once my representation, my 3D numbers is ready, I can pass it to some kind of classifier and that classifier already may be trained only on the smaller set of images which have labels which has labels and then once I train my second part, I throw away my decoder. throw it away. Build classifier 30 inputs.
(1:19:11) It means it is not so difficult to classify. It means that a set which is required to train it could be much much smaller like only few images would be sufficient and then you will be able to basically build your classifier and of course it will nicely classify everything. Everything from this for example cloud will be t short as we can see it here.
(1:19:35) That's explains why it works so efficiently in this case. uh it's exactly what what I told you. It shows how to do it in practice. We first phase two for phase one build out encoder right we build it then we throw away upper part we throw away decoder we keep encoder copy parameters typically freeze parameters and those three numbers will be input to our kind of classifier whatever it is and then we train this second um already second neural network well typically freezing coefficients of the encoder. We train only last part essentially and it will be trained on
(1:20:18) smaller data set. It doesn't require you already to have so many observations anymore. If you didn't have this option, it means you would have to use as input to this classifier 784 numbers. It means we would have to use much more labeled data.
(1:20:38) We often don't have it so many, right? That's why this approach is quite quite impressive. So any questions? Yeah. So I it's not it's not very clear to me that what's on this slide the second part the right part. So my understanding is I understand the autoenccoder but I don't understand that where we get the labels for the soft max are we going to select like a few samples that we really know.
(1:21:02) So yeah in order to build out encoder no labels are needed right? Yes. We don't need it. We can build it. If you have data set of any kind text books, we have books. We can build it. So now next I want to build actually already classifier. For classifier I have to have some kind of data set which has labels. It doesn't have to be so large anymore because what I do I borrow my encoder.
(1:21:29) You see copy parameters. I borrow it and I freeze coefficients. It means input to my effective network to part of my network which will do the classification will be already not 784 will be only 30 numbers. It means number of observations which have labels could be much much less already because number of number of parameters of my remaining network is not so large.
(1:21:56) So the the rational of this approach why it should work uh very well is it's some sort of a pre-training some sort of pre-training of part of your classifier. Exactly. Exactly. You can put it this way. This is your classifier and you pre-train this this part which could be quite quite expensive part because your input is highly dimensional. Right. And you print this part using no single label.
(1:22:21) That's the beauty of this. Yes. Got it. Got it. Yes, thank you so much. Yeah, you can view it this way. We build classifier. Now, if you have like labels, we can just train. That's not a problem. If you don't have many labels, we can pretrain this part which is effectively encoder using this type of network which doesn't require any labels.
(1:22:45) We pre-train it and then we basically throw away this upper part and add classifier. And why it works? Because this kind of you know representation of encodings kind of nicely represent this data in third dimensional space kind of cluster this data let's say this way that's why when we build classification of such points it is always not a problem it is not like point is not like classification of images it is simply classification of point and threedimensional space which naturally you know have clusters very easy problem actually
(1:23:20) that's why it works my concern is I think it will work very well for points around the center of the cluster but for for border for border points it may struggle a bit because having seen a lot of examples yeah yeah it it will mclassify of course yeah in this case we will use this you know metrics such as sensitivity specificity sensitivity is also called recall right and we have to get a particular problem decide what metric is most more important for us if you want to mclassif ify one type or mclassify more on second type depends on application but yeah you're right you will always have will
(1:23:58) always be be mclassifi mclassify mclassifying something but this is okay because it is like nature of this type of you know uh data even doctors cannot always say right yes I think it's a very good solution if we don't have enough label data it's a very good yeah if you don't have enough label data you can think about like for example Even translation of sentence from one language to second language.
(1:24:25) One thing to have cal another thing to take image like sentence in one language and try to map it to something in the same language. You sort of can pretrain part of your network. Typically nowadays don't they don't do it this way but it kind of used to be at some point right but now they just try to find couples which correspond to it which is indeed translation.
(1:24:51) But if you don't have many kind of couples, you can pretrain this out encoder and try to kind of shrink the mality of your input. Thank you. So now uh this is what we already discussed. Uh so what's next? Uh ting out encoders. Right now we already kind of visually uh visually discussed what it does. We are talking about um uh uh training one sort of you know um shell encoder at a time and then trying to st additional layer in between and continue training only training the middle part we already discussed. So this is already clear. So any questions about our encoders?
(1:25:42) So now there is one issue with this. Everything seems to be nice. As long as our data set is sort of homogeneous, kind of consist of similar things. Let's say hundreds and digits. Okay, we can do it. We have like some kind of fashion or something you know we can do it. But what if data set which we use consist of all kinds of things, cars, bridges, people, dogs, cats.
(1:26:07) It may be not so easy to just represent image via 13 numbers. Right. Maybe not easy. Let's talk about this. If you have like 10 digits, it is understandable that you can have only 30 maybe codings even less maybe 10 10 15 encodings and represent your written digits digits.
(1:26:40) But if you set that consist of different kind of images, maybe 15 or even 30 encodings is not enough. And that's why we introduce notion of sparse out encoders. Let me say sparse out encoders. I will say what it means. Sparse out encoders. Let me sort of discuss the issue itself. Right? Well, let me say first case one. Case one just as we discussed we have as we discussed we have let's say only digits handwritten digits for example right let's say digits only only digits and this will be 784 maybe uh pixels 784 pixels We can represent it via 100 numbers then via 30 numbers then v 100 numbers and reconstruct v 784. So please remember we are sort of aiming
(1:27:57) at getting bottleneck here. It is important because we want to first of all reduce dimensionality. Secondary if it is if it is some kind of denoising auto encoder we can deliberately say we want to throw away everything which is not important. So bottleneck is quite important here. Bottleneck is used let's say to reduce dimensionality right and to effectively represent whatever we have as input.
(1:28:22) That's why 30 numbers for digits for hundreds and digits seems to be kind of reasonable solution. You can do it yourself. So you will see that 30 numbers 30 in is more than more than enough to capture everything in your handwritten digit. You don't have to have 784 pixels. It's too much. But there is second case to consider.
(1:28:41) Second case let's assume that my data set consist of let's say digits right digits. It's my it is my data set. digits, digits, digits. Then maybe something else. Maybe it consist of let's say animals, right? Animals, something else maybe cars and so on. It may have different types of things.
(1:29:15) You can imagine that maybe 30 numbers is already not sufficient because different types of you know features we have to represent. It means if I build similarly my out encoder I have to keep in mind that let's say 100 is my first layer then 100 is my layer if I skip one output is 784 and the question is how many do I have to have in between so it is very possible that because I have different types of images already or different types of data whatever it is 30 is not sufficient to nicely represent nicely recover. Let me say 90 is is needed 90 know kind of intuitively we understand
(1:29:58) that there are different types of you know images that's why 90 maybe three types of images I have not so many different types only three types of images it means maybe 90 is what I want to have in this case now there is a problem if I have 90 it means effectively I don't have any bottleneck if I supply digit it will activate all of those encoding ings all of those codings will be basically activated right.
(1:30:32) So those are my those 90 is my encodings which is like C1 C2 and so on C90 my numbers let me say CN N is 90 in my case so if I supply digit before it was like 30 numbers and that's it now I cannot hope to get 30 numbers and zeros everywhere else I want to get 30 numbers and zeros everywhere else but your network will not do it because it will try to recover in the best possible way. It means it will not try to shrink this information into 30 numbers.
(1:31:08) It will use every every single encoding available. Same for animals everything will be used. Same for cars everything will be used. It means effectively if my data set is now larger I cannot compress it as before anymore. You can say what's the problem? Built one out encoder for digits, second out encoder for animals, next for cars.
(1:31:33) What's the problem? Why? Why it's on the solution? Can you say the question one more time? Sorry. You can say let's build first autoenccoder with 30 codings for digits of the same type of autoenccoder separately for animals. Third that work separately for cars. We have different kind of encoders for different type of images.
(1:32:06) What's the problem with this approach? Computational costs. No, no, no. It's you you have to know what kind of image you have. You have to know what kind of image you have exactly. You kind of take your you know iPhone and you take image and you have to say now it's going to be animal.
(1:32:24) You have to press a button and say is animal right? It will not know how to work with your image until you tell. Secondary your data set itself which is used for trained maybe not labeled. The whole point of using encoders is to avoid labels. We don't have any labels. That's why building this type of different outers is not a solution. It has to be like one for all basically.
(1:32:46) Now the question is if it is one for all how to make first three numbers working for digits, second numbers working for animals, next three numbers working for cars. And that's how that's exactly idea behind so-called uh uh so-called um activity regularization. Right? So we are going to do the following.
(1:33:09) We are going to say okay I don't have it on the slides I'm going to do on the phone I'm going to say uh I want to sort of punish if my codings my encodings are on I want to punish it right I want to basically introduce term added to my loss function which is L1 activity regularization activity regularization And I'm going to say my loss as before it is some kind of you know loss categorical cross end. Well in this case it is it is out encoder.
(1:33:52) It means some kind of you know L2 norm regular loss plus my sort of punishment term. In this case, L1 means lambda time absolute value of C1 plus absolute value of C2 and so on plus absolute value of CN. What what it means? It means if I train this type of alter encoders, my C will tend to be smaller. Some of them will be even switched off.
(1:34:20) If I do it this way, if network doesn't doesn't have to have all 90 activated, it will maybe switch off some of those will make them zero. That's how we do it. Any questions about that? Yeah, I have a question. So, in the phase one training, do we need to apply similar regularization in the first case? Yeah, in the phase one phase one training, we have phase one phase two training, right? when we get the 100. Uh oh.
(1:34:49) When when we get 100 um No, in this case, not not actually not we don't have to do it that way. It is okay to use all 100. Yeah. Yeah. But it's not the thing is we don't we don't have to do it in trainision phases, right? You're talking about like shod with no 90, right? It's not like it's not like completely necessary to do it because nowadays we have optimizers which can handle deep networks at least if they are not very deep this type of network is not a problem you don't have to do it using phases but if you have to use phases I don't see any reason why you want to apply this type of penalty
(1:35:30) for the for the 100 yeah because sometime we just worry uh 100 may be a too big number Maybe not but but we will create bottleneck later, right? So anyway, okay, we need to create one bottleneck. It is like 794. Okay. So we don't apply any kind of you know we don't suppress signal from 794 just pass it.
(1:36:01) Okay, we just pass it and then we suppress it here and to create bottleneck. We don't have to create bottleneck for no maybe you're right actually I don't know maybe it's interesting I have to think about this if you you use phases typically people don't do it this way right people just create this loss for one of the layers the smallest one and they uh create bottleneck you are saying is it useful to also create bottleneck before uh it's interesting question actually I don't know I I I I I I I have never seen people if you know using this but why not right maybe you're
(1:36:40) right maybe it is useful to try to represent where 100 but not all 100 will be will be on so it feels like it is sufficient to have only at one location this suppression of signal but if you kind of concerned about those um especially if you train during phases maybe why not why not to try to suppress it right away. Yeah, maybe it helps later on.
(1:37:08) So maybe uh even though probably it is sufficient. So now let's see an example. In this case we have basically our encoder as before and we use activity regularization lambda is chosen to be 10 -3. It means we going to literally take our already 300. You see 300 encodings but we apply regularization that means not all of them will be switched on some of them will be suppressed some of them will be smaller and then we apply it yes so that's how we do it no results but that's how we do it we just get activity regalization
(1:37:51) there is um another way to do it another way to solve this problem sort of let me try to u uh talk about is it is called Calvic lever divergence. Calvic lever divergence. So now let me uh uh erase my maybe first case and say what's happening here. So now the issue let me first tell the issue.
(1:38:25) The issue is if you do it that way it's not like necessarily the case that some of the inquiries will be switched off. They could be all small of all of them could be small because it is like maybe quadratic this is like L1 norm it is possible that they will be switched off just like in case of L1 regularization this possible but it's not guaranteed that's why there is a second approach which is called le divergence let me talk about this case first of all let's recall binary cross entropy binary cross entropy binary cross entropy remember we introduced binary cross entropy to measure sort of distance from a label 01
(1:39:13) to probabilities which we get let me say I'm going to duce notation capital H p and p hat this way and it was minus So phat is let me think phat denotes like output from a network maybe let me let me kind of be specific what I mean by phat and by p because in this case it is kind of kind of confusing right so in um let me say that p hat is uh kind of my So P head and P will refer to proportion of my codings being off and on.
(1:40:03) One P will be what I observe kind of a second P will be what I set as a goal set as a target. If it is like P with no head so probably I want to denote by P what I will use as target right target. So basically I say I want to set my P my target as a specific ratio. Let's say P is going to be like 0.1.
(1:40:36) It means only 10% of my neurons here will be on 90% must be off. It is my sort of target which I set up front. Then phat is going to be essentially what I sort of predict, right? what I observe whatever you want to call it. So phat will be like uh actual right actual let's say actual actual proportion of neurons which are on if I want I can say target is like 0.
(1:41:10) 1 example it means I want to get 10% to be on 90% to be off what I actually observe is denoted by phat when I train it I will see that some of them on some of them off okay 15% for example 15 is my p hat is actual Then I introduce just as before using cross entropy basically minus p * ln of p hat minus 1 - p * ln of 1 - p hat this way. So that's how we essentially define cross entropy. In this case we use the same formula using the same formula.
(1:41:41) And now we say this formula is nice but we can slightly modify it to get calic lab divergence. Let me say calic lab divergence will be defined as follows. It is not not not any kind of distance right we don't say distance we don't say uh it's not distance not metric that's why it is called divergence is what we minimize so this is capital D call the cl divergence it's going to be like function of p and also p hat now in this case we use this double bar because we sort of measure distance between divergence between one distribution and second
(1:42:33) distribution you can view P and P hat as a sort of two distributions in this case the uh B distributions and that's why they use this double bar to indicate that we are talking about distance between not just diver between numbers but between distributions then I say by definition I can take let's say by definition I can take my cross entropy at point P and P at and minus I'm going to take h at point p and p.
(1:43:07) So what is h at point p and p? What do you think h at point p? It is not a function of my uh observations not my actual observations that is simply a function of my target. If I say target is 0.1 or h of 0.1 and 0.1 will be some kind of number we subtracted just to improve convergence.
(1:43:30) So by by changing sort of shape of the distribution of this divergence function we improve a little bit convers because it becomes steeper. We do it this way. It is called baseline. Basically baseline baseline it is widely used in in different fields as well. This type of approach when we subtract baseline in reinforcement learning for example they use it as well.
(1:43:55) Right? And what does it mean? What do we get? Let's see what we got. If we do it this way kind of kind kind of cross entropy minus cross entropy between P and P itself seems to be strange kind of idea right but we do it in order to get function which has a little bit better shape to get improvement for convergence.
(1:44:13) So in this case P and P hat is my function P and P is my function P hat is P. It means this one minus this one is PN minus minus means P ln of P minus P ln of P hat that means I get exactly P * ln of P / P hat first term plus 1 - P * ln of 1 - P / 1 - P hat and this is my calic diver So what do we do next? We say calic labor divergence is going to be used this way.
(1:45:00) loss is whatever it was before some kind of alonor maybe plus specifically calic labor diverence which we introduced this way which is sort of diverse between P which is our target and P hat. So again P means P means what we set it is our target we simply set target and a target better be like 10% okay.
(1:45:35) 1 is target just specific number and that one maybe I have to be careful maybe plus lambda as well right so an NP head will be what we observe it means actual what we observe what our network produces sort of right how many co in coins will be on how many off behead mean proportion of those which are on and that's how we do it. So now this sort of graph represents that copic level divergence works a little bit better because of it is steeper and the result in order to get target sparity we actually probably prefer calic level diversions feel it is steeper that means
(1:46:12) calmers is faster toward this target if I say my target is 10%.1 that means I aim at getting 90% of inquiries being off and I I I I get it by essentially comparing actual with my target using this divergence within those two distributions. And here is some example which sort of shows how it works. I mean introduce lab divergence. So in this case uh target is 0.
(1:46:45) 1 by default right a weight is going to be basically lambda. A weight is nothing but lambda. Then calvic divergence one minus target it is like no essentially essentially this is like p times ln of uh ln of this u p hat plus second term and we get this formula Weight is lambda target is simply what we want. Mean activities is proportion of those which is going to be indeed on.
(1:47:52) So main activities is P head and then we say level divergence target P head one minus target one minus P head and then we apply this type of regularization in this case weight is chosen to be 05 lambda basically target is 0.1 which is P my P and then I add activity realization which is my specific Calvicia divergence and as a result my target is set at basically 10% right even though I have like 300 encodings but because of 10% that means effectively only 30 will be working every time because target is 10% will be plus minus of course if it is completely necessary for network to use
(1:48:39) more it will use more because it tries to minimize first term and also tries to minimize Second term if it turns out that second term is large it will be switching on more neuron I mean if first term is large it has to switch on more neurons more encodings it is okay it will do it secondary will be slightly greater but first one will be minimized but for the most part on average and on average basically 10% will be on and 90% will be off and that's what we kind of have activations which is actual observations right activations questions.
(1:49:14) So now any any questions about that? Uh it's it's more I'm sorry maybe I'm too tired tonight so I keep asking simple questions. I'm really sorry but yeah it's okay it's okay. What what is what is the idea of uh the main idea of using killback label uh diversion I understand this is this is statistic that compares two distributions. Exactly. Yeah.
(1:49:36) But how do you use it for regularization? I know that you explained it in details but but can you just say it like maybe in one or two simple yeah so let's let's forget about the maybe form of this function let's say I introduce 19 90 19 codings in my example 300 encodings 300 over here 300 right 300 it means uh I don't want to use all of them and I say let me aim at getting only 10% on at a time it is my goal how to achieve 10% at the time.
(1:50:11) Then I have to look at actual how many on how many off and my target which is 10%. And I say if my target is 10% and actual number of codings on is also 10%. Kic lab is minimal in that case. So it will aim at getting my actual proportion of codings being equal to my target. That's that's how it is realization. First term minimizes reconstruction loss and second term minimizes proportion.
(1:50:46) So would it be accurate to say that we we're going to create a synthetic distribution of our target let's say 10% and then we're going to compare the actual weights. Exactly. Yeah. Distribution and if we are if we are there we're happy. If we're not we're going to push them the optimizer our goal. Okay. Okay. Yeah.
(1:51:06) And uh sometimes it is possible that first term is if the network is not able to represent Y 30 by 10%. It is possible that if first term is large, right? It may deviate slightly, right? Okay. So it it it may want to remove decrease first term at the cost of increasing second term. That is okay. Sometimes it may happen. Sometimes small codings will be required, right? It is okay.
(1:51:33) But yeah, but if if it is capable of using only this 30, it will use only this 30 will produce reconstruction and minimize second term. If it is not capable for some particular image to use only 30, it is okay. Maybe it will use like 50 in order to minimize second term minimize first term and second term will be slightly greater but it is okay because they're competing sort of.
(1:51:59) Okay. Thank you so much. Yeah. So now uh we have five more minutes. Uh let me note I will not maybe sketch anything. I will just say what var encoder is doing. Uh in this case in case of autoenccoder turns out that um this space of of our codings is unstructured.
(1:52:30) So what it means if I take my codings like 30 numbers let's say if I move to my space of codings in this case and I say let me teeny tiny move away from my my you know t-shirt not like far away just teeny tiny slightly side from my t-shirt what's going to happen the neighboring point which is very very close maybe potentially has nothing to do with t-shirt Because we when we build this type of autoenccoder we said this is too short next is too short next is too short in between in those you know gaps in those holes those holes do not have to correspond to any kind of image which which makes sense any kind of t-shirt
(1:53:13) because when we built it we didn't care about you know trying to structure space somehow and the resulter is first attempt to solve this problem very interesting idea. They say during training of training of our our outen encoder, let's actually randomly disturb the signal which means uh like let me show you maybe right here which means when we build this 30 numbers remember 30 numbers which are codings let every time add some noise some kind of gausian noise. Gausian means normal unarable right some
(1:53:53) kind of gausian noise we add some kind of gausian noise a little bit here and pass information further what it means every time we make update of parameters we add some noise as a result neighboring points of our current codings sort of start making sense because they also correspond to kind of you know out being also too short for example and it means whenever you build this type of already varenoder.
(1:54:25) If I move away from current coding it this current place also makes some kind of sense because during training we were trying to testing everything around current point. The result this space becomes as I say structured space of codings. It means we can take this little point and start moving a space of codings.
(1:54:46) If I move it away from t-shirt to you know to pants it continuously will transform to pants like literally you will observe that it becomes more and more like pants. If you move it back it becomes again t-shirt that's why we introduce this type of vational out encoders in order to make sure that so those gaps are sort of filled in so that it also makes sense.
(1:55:10) How do we do during training? We take whatever coding is and we add artificially noise and whatever goes next is not our codings is going to be slightly disturbed signal and then we pass it further in order to reconstruct whatever we want to reconstruct. So it turns out that this little twist allows us to actually already structure space of codings and now we can already walk on space of codings and try to do like image editing.
(1:55:36) Basically we can create new images. we can already generate something which is intermediate between t-shirt and you know and short for example uh tall shoe and uh short shoe. Any questions about this idea? There is one challenge. If you do this way, we typically say there is a parameter sigma, right? Parameter sigma which controls how much variability you want to add, how much disturbance you want to add, plus minus how much 100, plus - million, plus -0, how much do you want to add? If you train this type of original encoder without any
(1:56:20) restrictions on sigma, what do you think is going to happen? So I I'm going to say let me every time add some noise. Sigma is trainable parameter. How much sigma is going to be if I tell you that work do it you know the way which minimize a loss function. How much sigma is supposed to be chosen? Do you have any intuition about this? Okay let me ask a different question.
(1:56:56) If I disturb, is it harder to reproduce nice nice nice image or nice signal or it is same or it is more difficult if I disturb or it is easier with the same if I kind of disturb out of nowhere I say plus some kind of noise is it going to be more difficult to reproduce signal or not I would have thought it would be more difficult more difficult okay now if I tell your network and you decide what sigma to use which sigma will it Would the network be smart enough to know to use a very non-disturbing sigma? Yes, non-disturbing sigma. Exactly.
(1:57:29) It will it will naturally uh shrink sigma to zero. It will say I don't want any disturbance. So it means we have to introduce some kind of regularization which doesn't allow sigma to shrink to zero. Right? And that's what the what what next next step is about.
(1:57:52) We basically introduce this type of cic diver which is let me say simply we add this type of term in order to make sure that sigma doesn't shrink to zero. I will not justify it but you can read about this why it is so it is basically called this expression is exactly divergence as diverse between standard normal distribution and whatever we use.
(1:58:18) So we basically say disturbance which we have here better be something close to standard normal. It means sigma better be close to one. We artificially say it not any sigma you want anymore but sigma which is close to one. If I compute the step of kic divergence in case of normal distributions because p and p hat become normal distributions. It means summation becomes like integral.
(1:58:44) I can compute it and I get exactly this expression which is used to sort of uh make sure that sigma doesn't shrink to zero and more than that in order to kind of you know cluster all the points together for convenience all this all the points they all over the place right now I want to cluster them together I will have some variability but I want to cluster all together so I can walk on this space of codings I also say effectively that mu better be close to zero.
(1:59:13) So every distribution will be will be will be kind of unique right but mu will tend to be close to zero sigma will tend to be close to one and that's how we do it that's that's why this type of additional caliber diversions any questions okay let's now stop could we just ask um there's been a question open on patia for the past six days if we could have the TFS take a look to answer that'd be great.
(1:59:46) Okay, I'll check. Yeah, thank you. Thank you. Thank you, professor. Good night. Right. Thank you. Yeah. Thank you.