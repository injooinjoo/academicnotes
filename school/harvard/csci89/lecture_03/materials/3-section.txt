(2) 89 day3 section - YouTube
https://www.youtube.com/watch?v=Uy9nJT78OmY

Transcript:
(00:01) Hello everyone. Hello Dr. Kroken. Could I ask you a quick file question before you begin? Yep, please. Um, there's a unanswered posting in PATs, I think it's question 28, that the intro NLP notebooks that you're that you're going through are not in the course. Instead, there are introd uh notebooks.
(00:27) And I was wondering when the intro and LP notebooks are going to be posted cuz none of them are on the on Canvas site. So you're talking about like section files uh for your section. Yeah. Like this one that you have like right here intro NLP section 3. We'll never see that on the Canvas site. We're instead seeing introd or section two. And it doesn't seem to pertain to what you're going over.
(00:52) Yeah, let me double check. So yeah, I think uh you are correct probably. Yeah. So now let me update it right now. And it's the same for section two and section one. Yeah, you're right. It is from different course. Let me update it right now. How how is everything? Uh things are going well. No complaints.
(01:49) Okay, that's good. Very good. Just give me one second. Uh uh sir, how do you pronounce your last name? Is it ch or or or it is like like ch is like ch Okay. Yeah, I uh I uh I started a bit don't worry about this. It is Yeah. No, no, it's okay. I'm I'm very curious. I I did I did live in Poland for a year.
(02:22) So I um I I I'm just curious if the ch is pronounced like the Polish way or or like the English way. Somehow my Yeah, just give me one second. I'll try to share this files with you. I didn't realize that I pulled files from from deep learning. They're probably quite different, right? Yeah. that that may be one of the reasons why there's homework confusion.
(02:51) Yeah. So I don't know what's happening. Um becomes not respond responsive. So today we are going to cover tokenization right we are going to do tokenization and also we are going to look at limitization tokenization limitization and stemming we're going to apply this techniques and then we build
(04:04) some kind of neural networks to make pred predictions yeah let me finish this task So last time we discussed three important uh techniques tokenization that means we split text into units of informations information called they called tokens. We can use for example words as tokens. We can also if it works we can also apply some kind of transformations to words in order to reduce dimensionality.
(04:43) We can reduce it to root form for example or to dictionary form. So now I placed first, second, third sess section files. They all must be available now. You can take it right now. Yes, I can see them now. Thank you so much. I will remove deep learning files. I think you don't need them right now. So okay.
(05:39) Yes. So this file uh is shared on canvas under files. So last time we discussed organization right and also stemming which means we just kind of trunate some trunate some kind of um literally like strip uh prefixes and suffixes that's what we do in case of it is kind of root form right ideally Ideally we try to recover root form in reality is not going to be like root it's it will be a little bit different sometimes because we have specific like rules how to do it like in is always to be thrown away for the most part right also n is
(06:31) maybe supposed to be thrown away in this case and s at the end is supposed to be thrown away sometimes it's not going to produce a root form but often it often does ultimately we're trying to reduce dimensionality of problems. Limitization is a little bit more interesting but more expensive at the same time.
(06:54) We trying to reduce it to the dictionary form. Cars will be car driving will be drive. Right? You can see comparison example driving in case of limitization will be drive. In case of stemming will be just some kind of not even dictionary word it represents something. It means if we create already kind of dictionary for based on those words if we create it ourself it will be probably okay there's of performance.
(07:23) So now today let's look at some examples. In this case we take uh uh data set which consists of uh forums 20 different forms. You can read about this uh on the documentation web page. So fetch 20 news groups, right? It consist of forums. Basically, they dedicated to different subjects. Now I picked like two of them in order to do classification. Even though you can pick much more if you want.
(08:03) If you want to do classification, you can also try to classify into different like into 20 classes if you want. Now I picked two just to demonstrate how it works. If I want to classify it into like zero and one types and they have a list of different uh uh topics. Well, let me simply say I'm going to um import it from here. So, it is available now and I'm going to look at what it has available.
(08:33) I will look at target names. It will tell me what type of forms we have. For example, motorcycles, guns, music. I'm sorry, not music, music, politics talk, right? Then we have a a hockey somewhere. Hockey right here. And we have also uh forum where people sell and buy stuff for sale. This one for sale.
(09:09) I chose on one to be a hockey second one to be for sale and basically I'm trying to to say can I based on only content of this um on of this post tell if it is about hy or if it is about sale right well clearly we we able to understand the difference we can clearly and identify if it was about sale or about hook it means work must be able to do it as well let's see how we can do it so again I chose those two types of uh forms about who can for sale it is my categories.
(09:43) Then I say from here I want to take train data set and I want to specifically take only hockey and only for sale nothing else. Similarly I want to create test data set. I take a subset equals to test and also only two types of classes hockey and for sale.
(10:06) Now it means I'm going to have like as always like for the four data sets my train data my labels it is called target in this case. So my labels train texts and also train labels. Second one test texts and also labels which correspond to the test data set. So this is like that target is my labels but it comes from test data set. If I look at brand text 53rd one, I'm going to see that it talks about uh about Mintosh.
(10:43) We understand it. It means it comes from from sales, right? If you look at the label, it tells me zero. It means zero corresponds clearly to sales. Zero label in this case means it is for sale. It comes from forum from post for sales. And similarly if I take example for example 65 it has label one it means it comes from hockey.
(11:08) You can see they talk about like uh octopus were thrown somewhere right. This one they they talk about hockey the game where octopus thrown on the ice fish and so on. basically about hockey. Now the question is how to try to how to build a model which will differentiate one type of of of of post from different type of post.
(11:38) So again we have uh basically we have basic um preprocessing which is which is tokenization. We'll break this into basically doodle words and secondary we can work with words as is or we can apply stemming or we can apply limitization or we can directly work with with words. If you work with words a little bit more difficult because it means years will be different from year maybe it is not as important right that's why maybe stemming is useful even limitization could be useful in practice limitization is a little bit more expensive but probably not going to give you much if you don't not concerned
(12:14) about words being represented in the dictionary in actual dictionary right English dictionary you can simply use stemming it's not a problem I mean if you don't care really much about about uh how it looks. You can use stemming or maybe sometimes stemming doesn't produce same result because run ran stemming will probably not know what to do. It will be different words.
(12:41) It means in the case of stemming most likely your dictionary will be ultimately slightly bigger but only slightly. So it is not a problem really. So now let's build a neural network. This is my neural network. You can see it uses actually first of all maximum number of features plus one. Maximum number of features is actually number of unique words which I'm going to use to represent my text.
(13:01) What is this? This is basically no kind of parameter which I predefined and I say let me use only 10,000 most important I would say most frequent words maximum number of features is like 10,000 most important words whatever is is is like less important.
(13:22) So according to frequency basically I'm going to sort of throw away now throw away what does it mean throw away if I say some word is not in my dictionary what should I do with that no sometimes um even if you keep like all unique words into your dictionary sometimes you still when you look at the new data set like test data set you still may be getting something which is not in your dictionary the question becomes what to do about those new ones any ideas what to do with new ones let's say I have 10,000 words and I create my sort of basis my dictionary which consist of 10,000 words what if something is not in my dictionary what should I do like there are different
(13:59) options actually so there's my text let's assume that uh something like something like uh and your bus is not in my dictionary what should I do if it is not in my dictionary So basically you can kind of ignore it maybe right you can just say it is not there it is like if you just throw this away from the text or there is another option which is probably a little bit better option you can say let me create some kind of out of vocabulary token and say if it is out of vocabulary it will basically referring to that particular out of vocabulary word in my dictionary
(14:43) and I will say this corresponds to the one which is not in my dictionary. So basically I create some kind of specific uh specific u word and my dictionary I will say whatever is not uh actual word it will be referring to that one and that's why I have plus one plus one because I'm going to create specifically so called out of v vocabulary.
(15:07) So I will create this type of u index which will correspond to whatever is not among those 10,000 most frequent ones. So this is my my neural network. Now let's look at this new network and talk about this a little bit. You can see that I'm going to use recurrent network. Every recurrent layer is basically LSTM, right? LSTM.
(15:33) So I'm going to use LSTM initially. It means initially I'm going to pass sort of sequence of vectors. Every vector will be of length 10,0001. Not because one is the one which corresponds to out of vocabulary. So it means every vector at every point in time is I don't know almost 10,000 dimensional to be precise 10,0001 dimensional and then I say uh I say it is input then I will use dropout and then I will use my LSTM which consist of 128 um hidden sort of states.
(16:13) Remember two memories short one long one each memory will be 128 dimensional then I will again use dropouts dense layer in this case I have sigmoid simply sigmoid because I have basically two classes that means it is zero and one sigmoid is sufficient you can use like two neurons and soft mark so you can use sigmoid with one with one neuron to this is totally fine optimizer is optimizer and loss will be binary cross entropy metric which I want to display just for myself so I can look at it and sort of judge how it performs is going to be accuracy. Accuracy this
(16:48) metric is not going to be optimized. So keep in mind loss is minimized during training. Metric is just for me to take to look at. That's the only reason we have it here. So metric is only for me to sort of display and take a look and sort of see what's happening. It will not be optimized.
(17:11) Let me say um just to be kind of specific let me say that in this case we have sigmoid activation function we let me say let me say recall what it means sigmoid activation function right it means the function which is basically used in case of a logistic regression it is called sigmoid activation function sigmoid activation function sigma z is equal to 1 / 1 plus e to the z.
(17:40) If I pluck here a linear combination, now let me say note if I pluck your linear combination, it becomes exactly logistic regression. So if I say yhat which is like my kind of sigma function apply to a linear combination it means minus beta 0 plus beta 1 x first plus so on beta let's say kx k parameters k inputs.
(18:15) So know this is a logistic regression logistic regression. So basically this function corresponds to logistic regression. You can even think about like uh logic regression being neural network if you want which is indeed the case. Essentially logistic regression is a kind of neural network. Now if you look at this logic variation it is a function which ranges from 0 to one which is quite important because we want to produce probability from 0 to one.
(18:43) Is a probability that I basically predict first class. What was first class? First class corresponds to one which corresponds to uh octopus to the ice to the hockey. So one will be hockey. It means I'm trying to predict that it is hokey or not hockey essentially. And my output logic regression if I take like zero it means 12 right plus infinity 1 minus infinity negative 0 it means I'm going to get that one which ranges from 0 to one which is quite appropriate if I want to produce output which is basically probability so my sigma of Z look this way and then the second question is how to design a loss function remember in such cases we
(19:25) discussed that the loss must be some kind of cross entropy. In this case, it is specifically binary cross entropy because we are talking about only two classes. We have to specify like binary cross entropy because we use only only single output which is basically probability of success probability of of of one.
(19:45) It means my loss will be that is negative y * ln of yhat right ln of yhat minus 1 - y * ln of 1 - yhat that's how we define loss in this case it must be like y first y first y first head y second head but second now is simply 1 - y because I have only single output as my output. So remember mean square error is not appropriate in this case that's why I minimize cross entropy.
(20:24) So recall that it is specifically it is specifically not mean square error mean square error will have very very very bad properties right if output is like probability and label itself like zero and one it will have very bad property that's why we don't want to use mean square error we have to use some kind of cross entropy in this case binary cross entropy so now any questions about the problem itself So now my vector at every point in time will have 10,000 one dimensions. There is also time right.
(21:01) I have different time steps because my sentence becomes essentially my text becomes sequence of different words. First word, second word, next word, next word. Each word is essentially going to be 1,1 dimensional because I'm going to use 1,1 dimensions as a sort of basis in the space. Every word will be this type of vector.
(21:27) Now there is this interesting interesting uh step which is called embedding. Let me talk about embedding a little bit. We actually going to spend some time I believe later again. But let me talk about this embedding so it is clear what's happening. We didn't discuss embedding last time, right? Do you remember embeddings? I think we didn't discuss embeddings. I don't recall discussing it.
(21:52) Yeah. So, we didn't. Yeah. So, let's let's talk about embeddings. It is actually quite quite easy stuff. Later we'll talk maybe more about this in detail, but on a high level you will understand like in 7 minutes now. So, it is it is quite easy stuff. Um so let me say uh embeding um is what I want to discuss today and basically I have u in this case let me say right here say that I'm talking about maximum number of features plus one it means 10,000 1 0 I'm sorry 0 0 0 1 this way comma then I say 128 that's what we have as part of my
(22:51) network in this case what what actually is happening here so basically we are saying that at every point in time I have a vector xt which is vector which lives in 10,1 one dimensions. No, it is literally like a vector of observations, right? It is like um it is it is essentially it is going to be one hot and coordinate which look this way and so on and so on zero vector of zeros and ones it lives in 10,0001 dimensions.
(23:30) What I'm saying is embedding is a kind of mapping which allows me to reduce dimensionality which will be now vector which lives in 128 dimensions. This way 128 dimensions. Now we can look at some examples to see how we can do it. So the first example typically we like also like um in linear regressions maybe if you if you know about this about this like econometrics we say if there are two classes like male female for example we can label those using zero and one right so if there are two classes essentially no it means let me say space in which I live is two dimensional no let me say let me
(24:18) say I live And I start with two dimensional space. My input is two dimensional, right? It is example. I will simply sketch it. Example, I have two points one and zero and also zero and one. uh this one for example will correspond to female and second one will correspond to male.
(24:56) So basically in this case my XT lives in two dimensions. No very typical situation. You have sort of variable you have two classes and you can say I can say that first one is 1 Z second one is 01. But you know that typically we don't do it that way.
(25:16) Typically we say how what do we say typically? Typically we say in such cases that let us call female to be one and let us call male to be zero. Like typical approach would be to say that one is essentially female right and zero is going to be male this way. So now if you think about this why is it possible? First of all, turns out that if you do it that way, when you look around in regression for example and your input variable is not like two dimensional but literally scalar, turns out that this way is totally okay because moving from 0 to one, moving from no let's say from male to female means kind of nicely switching on your coefficient. Switch off, switch on.
(26:02) That's why when you run some kind of linear regression beta in front of this variable which is essentially now x still the reser right which lives in one dimensional space. So it means corresponding parameter like a linear regression will be switched on. If you take one switched off if you take zero it means cofficient itself play a role of of sort of effect when you move from female to to male from male to female.
(26:34) So this is exactly what I'm talking about as this type of mapping. So this is already embedding. We probably know this type of examples. I'm not sure if you know but sometimes when we want to see some kind of effect and the variable is binary two classes we kind of without thinking say zero and one. So we can definitely do it.
(26:53) And if you think about this it also makes sense because you have like two dimensional space but only two places are occupied. Everyone is either at this location or at that location. So it seems like there is space for everyone on the line because only two points. In case of like economic matrix like linear regressions, it is also important to interpret coefficients.
(27:15) Turns out that in this specific case, it is not a problem. We can easily interpret coefficients. What about like neural networks? In neural networks case, we can do it more aggressively. We can say let's say example we can say let us assume that I have three dimensional space three types of uh three classes right let's say threedimensional space that means my input is three dimensional I have one point which is 1 zero second point which is z I'm sorry 1 0 0 right is three dimensional second point 0 1 0 next point is already 0 0 1 now it
(28:06) means my x vector lives in three dimensional space now how can we reduce dimensionality of this space basically I can tell you that we can also take w1 w2 w3 we can take W first, W second, W third. Actually, if you do it this way in classical kind of applications like econometrics, it is not very good idea because why signal from one from female for example must be W1 from male must be W2 and why it must be like twice stronger for example.
(28:51) Right? In this case it is like off on it is understandable interpretation is clear in this case but if you just take like any W is like 5 755 it doesn't really kind of make much sense why we do it that way because it means that signal from the first second type of data will be like a few times like stronger right and how do we do we define how much stronger it needs to be that's why we often prefer to keep like neural network where we have input which is like one hot encoding of this type But sometimes turns out that maybe reducing dimensionality is useful. It is often actually the case. If you have sparse data, it means you have a lots of
(29:27) zeros. Reducing dimensionality is is useful. In that case, you can say if I actually don't specify what those W's are, but I make them trainable, then everything becomes much already better. So it means my vector which I have here will live in again one dimensional space. But parameters which I choose will be trainable.
(29:51) In that case if I choose parameters which will be trainable. Let me say trainable parameters. It means I don't really have as much restriction as I would have if I placed like 5 715. If I place 5 715 it is kind of a problem right because we sort of may lose something. Uh now if you take extreme case 5 10 and you know 5 10 and 10.
(30:17) 1 it means 10 and 10.1 are very close to each other. Why should they be right? Same idea if you take five 7 and you know 10 for example still seven and 10 close to each other maybe seven and 10 close to each other maybe there's not the best solution.
(30:35) That's why when we do this type of things we actually have to be careful and say in case of neural networks let us simply assign parameters and those parameters will essentially be trained during optimization of cost function. That's exactly what's called embedding. We are going to do this type of mapping but the way we do it is going to be obtained by coefficients. That's all we say. Any questions about that? No, of course you.
(31:05) Yeah, I have a question about the type of the network or the layer that learns embedding. Yeah. So, this layer. Yeah. Yeah. Yeah. So, this is embedding layer. So essentially in this case if I have um let's say 10,000 inputs right 1 2 and so on will be my inputs x first x uh it is like to be precise xt first xt 2 second and then xt 10,0001 so you're asking how to produce 128 so I can take first second and so on.
(32:00) 128 which is Xild the T first Xa T second Xild the T 128. So basically what I'm going to do in this case I'm going to say let me take a linear combination of inputs. Now it means first tilder will be sketched this way. Second till they will be sketched that way. Last till they will still they will be sketched this way.
(32:26) So basically my embedding layer is simply fully connected will not work. But it is important that no not important but it is done this way that my activation function. If I say a linear combination of inputs what it means my activation function is a linear basically right now there is there are few kind of details first of all we don't take biases no bias at all we take linear combination of inputs it means some kind of w place right x * w plus second x plot * w and so on and we get result secondation function is linear it means we don't apply anything my x still is literally linear
(33:08) combination. You understand what linear combination means, right? It means like premultiplier time number plus multip times number and that's it. I'm just curious uh if if if we use bias or if we use nonlinear activation would that uh make a difference? So that it will make some difference because embedding is not just a full network to to be precise.
(33:33) I mean stly speaking it is but we we kind of know that in that case when we do embedding we basically talk about this type of uh inputs. It means whenever we compute this transformation, we don't have to compute those linear combinations I discussed.
(33:56) We can simply take a look and say okay it is related to the first to the third to to this third um to the third input. It means we can sort of efficiently do this transformation. there will be some specific matrix matrix which transforms input to the output and based on this one I will know which kind of line to grab from the from my matrix and I can do it efficiently using basically index rather than using linear linear transformation formally if you do just fully network you're supposed to be getting same but it will be more computationally expensive that's why we
(34:30) do it by embasing layer mathematically it is equivalent as can network with linuxation function and no biases but practically computationally embedding is much much more efficient. Yes sir. One final question I I I did read one time that embedding layers are non- differentiable so I can't map back from the embedding to the original one hot encoding but not not invertible you mean right? Yes.
(34:59) Yes. Something so I can move from the the embedding space to the one hot um vector vector but but I don't see a reason for that. I I think um there must be another I I can just add another layer uh to take this linear combination and map it back to the one hut. Is there something I'm missing in my understanding? Well, if you reduce dimensionality then speak and we kind of cannot invert it, right? But of course in this case for example you clearly know that one is female, zero is male similar in this case you clearly know who is who is who.
(35:31) So I would say that um if you talk about like transformations formally speaking that of course it is not invertible right but uh if you have case where you have for example this type of sparse data then probably you should be able to um invert it but uh denot it is related to basically your data. If your data is sparse like in my case then you kind of know which one is which like in this examples right when you say invert it will basically say that I want to recover my result yes you can recover your result clearly in this case and in second case if my D is don't correspond to each other if they different then you
(36:17) can recover but generally speaking if you have like input which is not sparse like mine but it is everywhere and you try to reduce dimensionality you're talking about some kind of projection it means you definitely lose some dimensions it means you definitely cannot recover it back it depends on your inputs essentially now it is like you know it is like like example like very simple example if I do if I take a line if I say one two three points and I do projection what do I get I got three points right?
(36:59) So first one, second one, third one, everything is okay. Now if my points are such right but what if I take different example and my points take no let's say not even this way but what if I don't have three points but I have some kind of object in this space right no it is like two dimensional space in my example two dimensional space of course projection means I'm going to get this segment and nothing else this segment now it means of course I lose I cannot recover recover it.
(37:36) But if initially I had some kind of you know points like that then I should be able to recover. Even though it is clear that it is only if my W's are not same. No it is it means if they are not on the same line basically perpendicular to my space onto which I project same here if you have like more like dense dense points and the space of course projection means you will you're going to lose.
(38:04) So it it means like basically to application if you have some kind of sparse representation you're supposed to be able to recover it actually no unless my wideite uh so now we have this u uh model we have embedding so we understand what that means we sort of shrink 10,000 dimensions into 128 in this case I shrink from 2D to 1D from 3D it to 1D but I can also shrink to let's say 128 dimensional space I can also do it clearly. So now let's talk about dropouts.
(38:41) This is dropout which is not um uh drop out which would um destroy your uh destroy your connections inside of your uh inside of your LSTM. It is dropout which actually only impacts your input. Let me talk about dropouts as well. So this drop out which is basically going to input connections from your input uh to your LSTM layer itself.
(39:14) Let's see let's see what I mean by this on drop out let me say now drop out 02 from from my example. So basically I have a sequence of vectors right now let me say x t vector is going to be maybe not even t let me say first one is going to be some kind of vector in this case it is 128 dimensional vector right it belongs to already 128 dimensions because I already applied my embedding now it will be some kind of vector 1.
(39:55) 7 and so on.9 some kind of vector. It doesn't have to be sparse anymore. It may have actually easily more than one number different from zero. And then second vector belongs to also 128 dimensional space 9 1.3 and so on 7 and I continue until the last time step which is r 128 dimensional vector as well and I get something like 2.9.
(40:36) 0 one. So if I say drop out 2, what it means? It means every guy has a chance of being dropped 20%. Let's say first I'm going to drop it. So basically I will replace it with zero. Right? So make make this one zero. Make it zero. In second case, no, I decided not to drop it. drop or not to drop probability of dropping is 20%. Okay, don't drop it.
(41:10) Maybe you drop something there right inside but we don't drop 7 either. So this how it's going to look in this case we don't drop drop something inside then drop last one for example. So this is make it equals to zero make this one equals to zero. So basically number of things which we make equal to zero fluctuates from time to time because we are saying let us uh drop this probability 20% probability point to each of those that means sometimes I drop more sometimes less but it is like binomial distribution basically it means it's going to look this way and it means that we are going
(41:48) to essentially drop uh entries from my input that's how dropout in this case works there are also recurrent dropouts you can see it is different one. No, it means I'm going to kind of drop some kind of recurrent connections which we discussed last time. Those recurrent connections will basically will be dropped. Those hidden states will be somehow impacted.
(42:10) It is different dropout recurrent dropout. But this dropout simply going to uh make some of the entries zero. Then again drop out which also going to make some outputs already zero or entries of outputs to be zero. Then I will connect it to dense layer with C of symbol neuron and also sigmoid function and that's it and then I compile it add them binary cross entropy metric is going to be displayed and that's it I'm going to have this model this is my network any questions about this design very simple single LSTM layer right
(42:46) embedding before drop out LSTM drop out and then that's it single LSTM very straightforward Uh in this case uh dropouts are useful in order to prevent overfeitting. It means if you train longer long and longer we probably will not overfeit anyway. So we kind of safe we can do it. Uh any questions about this network okay now let's uh process the data itself. Again data will look like sequence of vectors.
(43:18) Everyone is 128 dimensional. I mean the one which enters my network is actually 10,0001 dimensional but I'm talking about dropout before LSTM right let me say drop out which is before LSTM it means after my embing so this is my uh section three file the one which comes before LSTM that's why 128 dimensions the input of the network itself is actually maximum number of features plus one that is my dimensions of the input of the network okay now let's do tokenization last time we already discussed how to do it let's say maximum number of features will be
(44:10) 10,000 it means I'm going to only use 10,000 most frequent words maximum length of my text is going to be 500. It is a good idea if you have some kind of text of variable length to maybe uh make it constant right in this case maximum 500 essentially means capital t number of time steps ultimately right so this how we're going to do it and we're going to say let's create a dictionary we're going to store our index and also frequency and now I'm going to take my text train text first imply tokenization for every
(44:50) for every basically text for every post for every post from this text I'm going to apply tokenization I'm going to get my tokens and I I'm going to apply I'm going to count how many times particular token occurs so I can compute word frequency so basically word frequency will reflect how many times I observe my token in this text and that's how we compute frequency. Then second I'm going to simply sort it.
(45:25) I'm going to say let me look at my uh word frequency. Then xx x itself means key. No, it means we are going to have like x0 x1 in the dictionary. X0 means my key. X1 means value. Right? I store my frequency. And this way it means I'm going to store according to my frequency which I stored.
(45:50) Then I say reverse equals to true. It means from highest to the lowest. And now my words will be stored this nice way from highest frequency to the lowest frequency. And I'm going to simply truncate it. Essentially I'm going to say index which correspond to particular word will be I + one. So I just move forward and I store those into my dictionary word index and I'm going to take only 10,000 of them essentially.
(46:23) So that's what's going to happen plus one because actually the very first one which corresponds to zero index is reserved for out of vocabulary. I will say key is this ov. It means out of vocabulary and index which corresponds to out of vocabulary is actually going to be zero. Out of vocabulary will be the first one in my dictionary essentially I mean index index is going to be zero and whatever is going to occur later from for example in the test data set or maybe in train data set but it's not among my uh words in my dictionary I will call it out of vocabulary I will store it over there. Right? So now let me print print out
(47:04) some examples. Print text 65 and then let me see what I get after tokenization. Uh and you can see what happened. This is my original text. It has some email right some text eyes a few years ago and so on. Now organization will look this way. First one is uh from next is column. Next one is email address at and so on and so forth.
(47:37) And you can see what happens next like years is next you go period I and so on octopus for example. So this is simple tokenization. So basically if I do it this way if my data set is not huge I can easily use it already. I can create my dictionary of words of tokens based on this. It's not a problem. Actually for this exercise it is not a problem and let's see if I can do it.
(48:05) And by the way also display my corresponding indexes. Example um this one philip ws is not really frequent word because it is some kind of email it occurs in particular maybe post right maybe in two posts not not often that's why it becomes effectively out of vocabulary you see it is zero what zero means first second third zero means uh it wasn't wasn't often there it means it didn't really fit it didn't really get into is most frequent 10,000.
(48:44) No, it means it became eventually out of vocabulary which corresponds to index zero. That's why it is index zero here. Then at is 11 it is quite frequent because everyone probably is using email address right the next one is spk.hp.com it is not frequent so it became zero and so on. So basically I keep 10,000 most frequent words, most frequent tokens. The ones which are not most frequent will be will correspond to word to this uh key out of vocabulary and index will be zero and that's it essentially.
(49:25) By the way index means um no index means uh index means uh which one it is in the dictionary, right? So that's how we have it. Um during indexes example uh of indexes with words let me see I just um take some examples and I display them you can see 1 800 worth is during fifth is that is quite frequent Detroit is less frequent five is quite frequent 22 is is quite freent That does make sense, right? The first one is not quite frequent.
(50:10) Uh 233 is used not as intermediately frequent and so on. So that's how we organize this tokens and we also created indexes. So basically indexes is what we can pass further to the network. We can sort of create this one encodings and then pass it to this embedding layer. That's how we can do it. It's not the only case to do it.
(50:29) Later I will tell you how to do it more efficiently. Basically we can directly pass this indexes to my embedding layer and create essentially 128 numbers directly from the index. We can skip the step where we create a vector of one hot encodings. So vector of those zeros and ones is not not really important.
(50:55) we can use basically one-dimensional output which has indexes is essentially right and transform it to 128 dimensional vector but um let's first think that we are going to create one hot encoding and then we'll pass it through this embedding which has 10,000 one in dimensional input and 128 dimensional output. So now we can uh create this function which will convert my text into indexes.
(51:26) So I will say my text over which I'm going to loop right and I say let's tokenize text. Then I say my sequence is going to be um is my sequence is going to be this list of my uh indexes which correspond to particular token from this text. If it is not there, we're going to basically use zero index from this OV and we'll look at overall tokens then we'll append it and returns this sequence sequence of this particular lists right so we're going to have list of lists essentially and that's what we can do now if I apply it to my text train texts
(52:16) and also um I use my word index which I sort have created before based on my uh vocabulary right before I can create this type of list of lists. First one is basically first document it has word which is not frequent not frequent and so on and then it has n word 9 58 and so on.
(52:43) So first one is my first first post transformed to two indexes as second list as my essentially second post and so on. And I have a number of Os right here stored in this extra tokenized. No, it is essentially going to be sequence of vectors, right? Sequence of this vectors. Um actually to be precise um so let me be clear about this.
(53:06) The first line which I highlighted is essentially first post right but first it's like one data point but for first post post itself is a sequence of vectors. So essentially this zero itself is a kind of vector. No it is zero. You understand? But if I use one hot encoding it will be one and 0 0 0 right. Second the same. This one will be will have like 0 0 0 eight times.
(53:31) Um actually nine times because it starts from zero in the ninth locations will have one and then 0 0 going forward. So basically the very first post is first line which I highlighted every number here is kind of ultimately going to be vector.
(53:50) It is still a number because of this index but it will be kind of vector ultimately and uh vector which are going to sort of pass to my uh embedding layer. Now let me say I'm ready to train my uh network. So number of epox is 10. I'm going to build my model. Remember I created function. So function is is built model. So I I'm going to create my model. Then I'm going to feed it using my uh train data set. My labels.
(54:21) So it corresponds to train data set. Number of epox is going to be 10. Mini batch size is 32. And validation kind of split which I usually respond to. I can do it. So I split into two pieces that the train and test, right? And then I say let me uh evaluate my model on the test data set and print my accuracy result.
(54:48) So that's it. That's how we can do it. And then I plot my plot my results for the accuracy. I train it using tennox and this is my results. So after 10 epox validation accuracy became above.9 something like 93 or something you can see here 935. So my test test accuracy became that that much and train accuracy even even got better right not train accuracy is not representative because it means somewhat fitting maybe is happening there clearly but test accuracy is quite representative so it means in 93 almost
(55:26) in 94% cases I'm able to classify correctly which post it is of course my model is not not difficult just one single layer but still it does a job now second step is quite similar but I want to actually on the top of my tokenization apply steaming that's only difference so there is a step when I create my dictionary of words I want to first uh actually truncate those words I want to kind of take out suffixes prefixes when I create my diction it will not have those it will be slightly smaller basically so I'm going to use stemer
(56:02) which is order stemer as we last time discussed right one one of um basically widely used ones. Then I say let me write down this function which does the stemming. I use NLTK word tokenization first as before I need to do it. Then I say let me apply stemming for every token in in my tokens and then I will return this stem tokens.
(56:35) So this is for stemming apply steming to text. So now I apply this function steming to my text and I have as an example I have again the fifth one which correspond to my I think octopus and uh the stemmed one the first one is original right and stemmed one is the one which is obtained after this tokenization and also on the top of on the top of tokenization stemming.
(57:01) So basically stemming means first organization and then stemming. Now it is clearly important to first create tokens. So this is my original text again. Uh my eyes then throw fish uh octopus what it became after after this applying this steming. You can see that octopus become octop right it is not even in the dictionary not in English dictionary but that's how we did it clearly ears became in this case year right and so on you can see what happened it is still recognizable but some of them are not really in the dictionary but for network it is probably not as important so now
(57:47) next we're going to essentially do exactly same as before it is not different. All right, we're going to do exactly the same. But my uh text is now not from tokens. It will be from kind of stamped tokens, right? A stem text which consist of already those um uh we first apply stinging basically and then we do the same as before and then we get this results which are quite similar. What is this.
(58:18) 9? You see already a little bit better. N 68 almost.97 like 97%. Yeah. So this could be like flutation or could be a result of doing this a little bit more um efficiently because we don't create like huge dictionary it becomes now slightly smaller because you do stemming. No it is in order to get convinced you have to play more and see if it is like consistently better or not.
(58:46) it is uh okay it is never above 0.95 and this one is already above 0 95 so maybe it is somewhat some kind of result right I mean some kind of improvement finally limitization it is very similar idea but we need to instead of instead of stemming we need to apply limitization that means we use it to basically dictionary form right words which are recognizable so in this case I also I'm going to use NLTK Okay, similarly to stemming and I apply my eleatization here.
(59:21) So every token I apply my limitization here and then after that I apply basically very same uh the very same uh procedure but I use my text which is already based on limit sort of liatized results and then I plot my I fit my model and plot my results. In this case, performance is slightly worse but quite comparable as well. So this is how we can do it.
(59:44) If you want to apply essentially this type of procedure and transform text into this uh numbers. Any questions? Uh in in the three cases we use the same exact number of the top most representative words. Same number. Yes. Okay. Yeah. I had a question about um the use of zero. So just to make sure I understand this.
(1:00:13) So if a word is appearing too many times, that's when you've decided to make it zero. Could you say that one more time? So So zero corresponds to out of vocabulary. So this is ov. It means if word is not if word is not in our created dictionary, not among those 10,000 most frequent words. Okay. So that would be like if the validation data or maybe the test data if they have something new yes it will be it will have zero index correct okay so all right all right all right all right all right all right all right all right all right all right all right all right yeah no it is I mean you can there there
(1:00:44) is a question how to do it you just throw them away and it will have nothing right or we know that there was some kind of word but it was out of the dictionary and we use zero vector to represent it so what is better basically you can just throw them away completely and sort glue you know pieces of your of your sentence you say this word is not there basically it is something I don't understand let me just glue pieces together second approach would be to create out of vocabulary index out of vocabulary key in my dictionary and say whatever is is not known will have this
(1:01:17) index zero oh would it be so would it be data spillage just to make your dictionary include the vocabulary of the test data so I'm sorry. Say it again. What did you say? So would it be like spillage or something to uh just have the test data vocabulary in the entire index for the dictionary? So cuz cuz one thing you could do is you could just take all the data that you have instead of just the training or just the validation.
(1:01:45) Yes, you can take all the data. First of all, it could be quite you know hu huge and expensive to do it this way. Secondary. Okay. Also importantly in anyway in the new new text something is new going to appear and what to do in that case? I I guess maybe you could upload a literal Oxford dictionary and then have the indent. Yes. But it is it is good idea.
(1:02:10) We can definitely do it. But the problem Yes. So yes, we definitely can do it. But the problem is first of all you have particular task. You may not want to really refer to the dictionary. you want to I mean based on particular kind of you know corpus of text you may actually be more concerned about your particular text and also if you notice we keep extra 10,000 most frequent words we don't keep all of them if you keep all of them it becomes quite difficult problem if you use a lots of texts then you can use kind of as you said hawks for dictionary but there's also problem what about you know things which could
(1:02:48) be like slang or something maybe talking about forms some people use particular you know expressions which which they use to to express whatever they what whatever they mean and if we try to use old kind of side dictionary like English dictionary Oxford dictionary then it may not have it simply so it is better to use actually whatever you have to you'd have to combine them both combine them both maybe yes but again the problem is whatever is in your Oxford dictionary maybe not in your train data set and the question again
(1:03:18) why do you need it actually right if it is not there I mean if it is not in your training data set you you kind of don't need it you understand what what what you're saying if you get like new text data like test data set you want to actually be able to recognize those words but it's not going to be useful anyhow why so because my my neural network never saw those words on my training data set yes it is in your kind of dictionary it is in your vocabulary But your network will not know what to do with those because your training data set doesn't have it. It means it doesn't
(1:03:54) have it. It means it never saw it. So basically what never saw it even though you kind of know it is a kind of English word. But the data set which was used to train your neural network didn't have what it means. Network doesn't understand it. That's very insightful. Thank you.
(1:04:12) It is kind of useless to keep it basically to put it different. It is this if it is not in the training data set it is kind of useless to keep it even though you know it in future it will occur somewhere right but during training it it doesn't see that means it doesn't know what to do with this yeah that's a really cool way to look at it yeah thank you yeah yeah that's why we don't don't really need this outsource dictionary in some sense it must come from the training data set yeah okay okay let's now yeah there's some chat Yeah.
(1:04:52) So there is only a thousand or so basically is a concern. What people say is this exactly correct. That's why we apply this type of um dropouts. Drop out means literally drop outs means we spoil basically this uh connections multiple times. You see what happens every time. So let me talk about dropouts.
(1:05:18) What happens is I take my mini batch it consist of like um consist of like 32 32 observations, right? And I say every time I move to the next mini batch, I'm going to randomly spill my inputs this way. Not only inputs, I'm going to spill my inputs. I'm going to spill some connections inside of LSTM. I'm going to spill some outputs from this LSTM.
(1:05:43) So it means over fitting is not really a concern anymore. So you may be concerned about not achieving high accuracy. Yes, but overfeitting itself is already not so possible. simply because I every time when I move to new mini batch I spell it differently. When I move to next update of W's I spell it again differently.
(1:06:04) Every time I make update of WS I make those zeros randomly basically which means everything is not really almost impossible basically when we use this type of dropouts. It's not yes. So that's what I wanted to mention. Um so okay now let's stop. Thank you.