(2) 89 day3 - YouTube
https://www.youtube.com/watch?v=okSKQ0qtwhw

Transcript:
(00:01) Hello everyone. Hello Dr. Kroskin. Hello. Hello. Yeah, I have uh shared my screen. I I think you should be able to see my slides, right? Yes. Yes. So, how how was your first assignment? Bit time consuming. Time consuming. Well, and um I felt like I found myself um going into like chapters further into the book. No, you know I expected.
(00:37) Yeah, I see. So time consuming because of like uh the trials and errors or because of you had to kind of study in order to to solve this assignment. I I think both. I think um trial and error and also it took some time to get um more background to understand more what you're doing right yeah so basically when you deal with this type of models you will have to spend time doing experiments so this is sort of inevitable understanding is one thing right and second thing is trying to do experiments in order to achieve best performance this is sort of
(01:16) inevitable understanding the concept concept and translating the concept into some type of solution. It took some time and also a lot of experimental approach like it's not like you know one attempt you go and write the program you're done with it. It's a lot of experimental approach.
(01:35) So you put the problem in problem in such a way that like you know it's all interconnected. Okay. Nice. Yeah. Yeah. So now today we have uh first of all as we are going to always do we're going to solve quiz questions. Let me see uh let me just give me one second. I think I I didn't open my uh quiz. Second quiz. So it is correct. Let's discuss this questions.
(02:58) First of all, we have recurrent neural network right now. In this case, we have a layer of recurrent neurons. We have two neurons specifically because it's a two. two means we have two neurons and this layer of recurrent recurrent neurons maybe you remember last time we discussed how it is designed this time I'm going to just briefly kind of refresh what what we discussed last time I'm not going to sketch network like in detail I will just remind you that when we say simple recurrent simple RNN let me say simple RNN
(03:36) then we Say two in this case. No, it means two hidden number. Remember there were two hidden numbers. E equivalently there were two essentially two neurons in this layer. Then we say activation whatever it is not important but input shape is kind of important if you want to sketch it and compute number of parameters.
(03:56) Input shape is going to be 20 and four. 20 and four this way. Okay. So now um 120 is basically number of time steps. I don't even have to maybe keep it. Let me just say dot dot dot. This is like number of time steps. Strictly speaking. You don't even have to specify it. We just can type none.
(04:22) It doesn't really matter how many time steps you have during training. Your network will know how many time steps you have on the fly. It will kind of basically understand and train train it properly. When we sort of paint neural network, we don't have to specify how many times test we have.
(04:39) That's why in order to compute basically the number of parameters all we need to know is this number and also number four. Remember there there was a layer of recurrent neurons. In this case we have two neurons specifically because of this two and this is basically dimensionality of my y vector and y vector consist of yt first and also yt second specifically because I say two two neurons now you understand that yt at time one is out yt I'm sorry yt at time t first component is out yt at time t second component is out and then what is what is going to enter this layer we going to have fourdimensional
(05:27) vector at every at every at every time it means x vector t is basically every time at every t is four dimensional t first t second t3 t fourth this way or vector vector at a time. Remember there was like sequence of vectors. You may remember it was like x first which is t t is equal to in this case t = 1 will be vector x first first and so on.
(06:06) x first at t = 2. It's going to be x second vector which is x 2 first and so on. X second force and we continue this way. It is my kind of time series but a vector value time series at every point in time. I'm going to basically have a vector of observations. You can think about the sentence. For example, sentence consist of different words.
(06:36) Every word can be represented by a vector of some kind in some space. Right? It could be literally just just simply reference to the dictionary would be maybe one hot encoding or could be more complicated representation some kind of less sparse representation. Sparse means we have a lots of zeros. One hot encoding is not really nice because it has a lots of zeros. We sort of waste resources.
(06:59) We have to compute a lots of zeros which doesn't really add much to the accuracy ultimately. But computations are still there but they don't really produce much. That's why we have to some somehow compress representation. We'll talk about this so-called embedding later. We'll talk about this. So sequence of vectors in application to for example finance.
(07:20) You can think about like portfolio of some kind. You have returns from different stocks. You're trying to make predictions of something. Maybe um how does time impact RNN? By time you probably mean you not sure what you mean by time probably you mean number of times steps right so the I would say uh this is just sketch of my current recurrent network in reality it will have capital T in this case 20 20 time steps it means my network will be basically consist consisting of 20 sort of steps 20 layers I don't know how to call it it is called layers But in
(08:01) reality inside of it inside of it it has like 20 layers itself because of 20 time steps it means my time steps will directly impact ultimately architecture of manual network but as for specification of what it is at this point as for specification of number of parameters we have to have I don't care about 20 during training when we want to estimate parameters 20 will impact if you change 20 to 100 it will be bas Basically, formally speaking, completely different neural network because these two neurons will be represented 20 times or alternatively if you use
(08:42) like 100 time steps will repres will be represented 100 times. So during training of course everything will be different not to mention it means you have to basically supply different type of sequences as right capital T is equal to 20 in one case capital T is equal to 100 in second case so data is different formally speaking if you have sometimes if you have like a variable length they may add zeros at the end to sort of fill it in they can trunate if it is longer than recurren network can accept So they kind of have to be made of same lens basically. So now we have two
(09:22) neurons and we have Yeah. So when it has a 20 layer uh you said 20 layers each layer will have these four vectors x1 x2 x3. Uh yeah it is basically so this network remember last time we said basically it means it means uh two neurons this way at first time step when t is equal to one I sort of continue then when t is equal to two it is again two neurons this way and then I continue so basically when I sketch it this way in reality what I what I have to sketch what I have to sketch is two neurons at first time step two neurons and second time step then x capital t in my case
(10:08) capital t is equal to force then no this is like capital t I'm sorry in my my case it is 20 right 20 20 is equal is vector of type x capital t first x capital t second and so on x capital t last force it is sequence of vectors so it means I will have to have two outputs from there. This is what I call y vector.
(10:42) No, maybe not even vector because I I sketch two of them. Let me say this one is y first first y first time step second entry y second time step first y second time step second entry y capital t first y capital t second entry every time what I'm saying that my vector let me sketch it as maybe no maybe okay I can do it I can do it this Okay. First, second, third, fourth. Four entries.
(11:23) First, second, third, fourth and so on. First, second, third, fourth. So, basically this is like X first, X first, 2 second, X first, third, X first, fourth. First vector is in x 2 1 first x 2 x 2 3r x 2 force is in x capital t first x capital t second x capital t3 x capital t force is in.
(11:59) So what I'm saying is every signal will enter every neuron this way for this particular time step then it will enter this way then it will enter that way then this will enter that way it is first time step I I I would not call it like layers maybe I didn't say it like quite correctly no if it is like classical network you may think that is like a layer because of the copies of each other people say time steps it has capital T time steps um every signal enters every neuron also every time there is a bias which enters every neuron we don't sketch it in this case as well
(12:39) so this is kind of what we have to sketch now what about signal how how it propagates over time y itself will enter first neuron and also will enter second neuron what about second signal Y one one it will enter first neuron it will enter second neuron that's how we design it then this enters no whatever comes next right whatever comes next enters whatever comes next whatever comes next this one enters enters and same here one signal from Y first second signal from Y And we obtain this type of network. So this is basically exactly representation of this network in this case network I
(13:33) mean this layer which consist of two neurons in this case recurren a layer of recurren recurren a layer of two two recurrent neurons that's how it is designed specifically in this case now it is not nice way to sketch it this way it becomes quite crowded right that's why people simply say no people sketch it kind of that way 1 2 3 4 x XT first, XT2nd, XT3, XT4. That signal goes everywhere.
(14:07) That signal goes everywhere, everywhere, everywhere. And we have this type of layer of recurrent neurons where a signal kind of comes back, kind of comes back, comes back, comes back. And this information flows through the network like capital T times enter comes back enter comes back enter comes back like capital T times like it's sketched over there.
(14:29) It is completely good in basic network but people sketch it that way. And now it is easy to compute number of parameters. You can say number of parameters. This exercise is especially nice if you want to fully understand how it is designed. You have cars. You can type summary. You can see how many parameters you have.
(14:54) You kind of have almost like hint here. Basically, since you you allow it to use errors, you can just basically type it and see how many parameters you have. But I always encourage you to kind of think like this. So in this case, number of parameters will be not every neuron basically will have as input four access. That's why four it comes directly from my axis.
(15:19) First X, second X, next X, next X plus two numbers which come to first neuron from Y to come from Y. Let me say yt minus one because it is from previous time step plus also what I didn't sketch but there is bias which enters first neuron it means also bias and I computed number of parameters which correspond to first neuron 4 2 + 1 7 parameters but in reality there is also second neuron it means I have to multiply by two this comes already from the output kind of formally it comes from number of parameters which we have in this case I'm sorry number of hidden hidden hidden hidden states we
(16:06) have in this case from this two because we have two neurons now it means 14 is correct answer any questions about this u network I can tell you one thing people actually sketch it also equivalently kind of they sketch it this way kind of little box which sort of we understand has two neurons inside.
(16:31) Now you can think that there are two neurons. If you want could be generalized could could be more than two neurons could be LSTM which means some kind of subn networks. Then we say not YT out my T first YT out but YT vector is out. We sketch it that way. Then input will be XT vector is in and then signal comes back as vector YT minus one.
(17:01) So basically you can see this is detailed representation less detailed but still quite detailed and this is just so-called memory cell we can sketch it even that way. No as long as we understand what's happening it means basically there are 20 time steps. Uh but we have to understand that it means in in reality during training it will kind of have to think that there is there are 20 time steps it means this type of network is being considered.
(17:26) No, it means if you maximize if you minimize your loss function, right? It means you have to essentially understand how compute derivatives it comes from this representation. That means when you comput derivatives, you have to use full representation and differentiate the function.
(17:50) So any questions about that? A layer of recurrent neurons has input XT and also output YT and therefore recurrent neural network can only map sequence of vectors to sequence of vectors. This is not correct. In this case it seems like output is sequence of vectors wise but it doesn't mean we have to use only sequence of vectors as output. We can ignore everything else.
(18:17) It is widely widely used. We can use only last basically vector of capital T's and we can ignore everything else. In case there is option you can say return underscore sequences equals true or equals false right you can you can specify it means you don't have to use sequence of vectors as output now maybe I can open previous uh slides I think we have some kind of uh yeah some kind of slide on that there are different types of architectures first of all we can ignore all this wise we don't need them sometimes times we need only one vector for example not by many reasons we may
(19:00) we may want to have it that way right for example you want to make predictions of only one specific observation maybe observation of time series like which is next one you may want to do it that way for example maybe it is like a scale or maybe it is vector time series you want you may want to only make predictions of next observation of your time series for example you take a sentence and you try to produce some some kind of summary or some some kind of you know even number you want to say it is positive sent kind of sent sentiment negative
(19:32) people feel positive about this movie for example negative about the movie you don't need like an entire sequence you need only like a single number at the end it means you can ignore everything so this is widely used if you want to do some kind of classification of text for example maybe predictions based on text you don't need to use those uh this is also used by the way so You can use it this way especially if you want to build multiple layers of recurrent type. Why not to use this wise vector of sequence
(20:04) of y as input to next layer. You can do it as well. Not a problem. So now next this one you may think why would you want to do it that way? You enter x but you output vector of y. There there are also applications of this type of design. Uh so encod the question is if encoder decoder is is a type of RNN.
(20:31) Encoder decoder is basically I would say auto encoder which consists of encoder decoder like in this example is more general type of neural networks. It can be it can be used with convolutional networks with recurrent neural networks and whatever you want.
(20:51) Basically for recurren networks it uses this type of ID that you can take your input in this case in case of recurrent networks it will be vector sequence of vectors right and then you take output but on the way you create specifically this type of bottleneck in order to compress information you can think about this for example you take a sentence and the sentence is represented by a sequence of vectors no it is nice actually we don't lose like time component in this case.
(21:15) But what if I want to do something smarter than that? What if I say there is a bottleneck which is just single vector? No, basically I'm talking about Y capital T, Y capital T vector. This vector will represent my entire sentence. You say a sentence, it will be compressed not into sequence of vectors. No, which is okay. It is informative.
(21:39) But it it implies that you have to you have to use many parameters going forward. But what if you compress it into a single vector which represents your sentence? If you do it in a smart way, especially in case of auto encoders, we cannot do it in a smart way. It means we take sequence of some kind and map it to itself. Basically in case of encoder, the goal is to map data to itself.
(22:02) It seems like strange procedure why you want to take sentence and map it to itself. But if you create on the way some kind of bottleneck this bottleneck this representation this kind of representation where a vector must be done in the most efficient way. It means it must be representing very important I would say the most important uh properties of your input.
(22:28) Now in case of sentence it means it it will have to represent your sentence in most u most informative way given this restrictions given it is a vector but nevertheless vector could live in like hundreds 100 thousand dimensions dimensional space it means you are trying to think about this you're trying to map uh your sentence to a point in thousand dimensional space it's not like there is no space for everyone no there is space for everyone for every single sentence That's why it basically works. We live like in three-dimensional space, right? No,
(23:01) maybe if you take time and time into account, we live in four dimensional space and feel quite comfortable, right? In four dimensional space. Why wouldn't sentences feel comfortable in 100 dimensional space or thousand dimensional space? So there's no surprise why we can basically compress sentence and make them live in thousand dimensional space.
(23:24) Let's say 100 dimensional space. Oh yeah, we can do it. That's why autoenccoders actually quite quite interesting tools to compress information and represent everything by a vector in this case by sequence of vectors but we can do it by vector. So we don't need don't don't need to use time anymore. Later you can say once this out and code restraint input of my sentences will be mapped to the very same sentence via some kind of representation.
(23:58) It means I found a way to encode my information. So this piece, this part is already pre-trained so-called encoder. It will transform in a deterministic way. Now basically I stop my training in a deterministic way. I pass a sentence which is vector sequence of vectors and I compress it into one vector.
(24:19) I can erase decoder and I have this nice tool to represent my sentence by a vector. You can do whatever you want with this later on. You can do classification, right? Sentiment analysis. You can do even translations formally speaking. You can try to take this vector and you can try free your encoder. But decoder will be replaced with something different because you have to now not just recover to the same sentence in the same language. You have to translate to say French.
(24:50) It means your decoder will have to be mapped to French sentence from different language. But the first step which allows you to sort of compress could be quite quite important because it saves you parameters. It turns out that for translation this is not the best approach. Even at the time when people used recurrent neural networks, it was around like 2014 through 2017 like around 3 years maybe 3 and 1/2 years people used recurrent type of neural networks for translation. It was like dominant technique. Basically even at that time people not would use like out
(25:25) encoders not maybe not always use out encoders like rarely they would actually build this type of maybe model for translation but they would not pretrain it on the same language. Most likely they would still have to take calls from one language and second language because there are some some kind of details some nuances related to this translation which is better probably be which means it is better probably to train this type of translator using even if you have a bottleneck it is better to train on
(25:55) couples one sentence to different sentence but the idea is actually quite amazing. It could be applied to many different um problems. For example, it could be applied to problems where you want to remove noise, right? Your sentence, let's let's say your image has some kind of scratches, some kind of noise.
(26:18) And uh if you create data set of images which destroy it, let's say original images which are not destroyed and you artificially spoil them. You kind of destroy them and you train your network to sort of recover original image. Then when you supply completely new image which network didn't see but which is partially destroyed it will be able to kind of recover it.
(26:42) It is done it is done this way because this bottle specifically because of this type of bottleneck bottleneck basically is used to compress information in case of so-called denoising out encoders. It is used to throw away not important information. Think this way if you throw away not important information it could be the noise which is not important we don't need it right on the training data set we say noisy image is mapped to nice image it means noise is not important and since there is a bottleneck my network will have will have to learn how to throw not important things such as
(27:19) noise for example so this is widely used for such problems for example now again for translations it could be used but it is a little bit more difficult Nowadays of course people use transformers not recurrent network that works. So now back to question which I I started so this was answer to your question which I started talking about this case why this design is also possible you can do like um you can do like um you know uh caption you can take image for example and try to describe what you see in your image. Describe image means we have to generate text. You have to generate
(27:58) let's say sentence. Input is image. Input image doesn't mean we have any kind of sequence of vectors. Now we have some kind of vector. You can think that image is represented by single vector. You may say how is it vector is like a matrix. Now first of all you can basically reshape your image will be vector.
(28:21) Secondary uh which is probably better approach you can apply convolutional network few layers and then after this you will be getting some kind of convolutional layer and you reshape it so-called flattening and you get vector so basically there is no surprise that image can be ultimately represented by a vector you're not going to discuss commercial neural networks I think no maybe not sure probably not uh but you can Imagine that image can be represented ultimately via a vector not by a sequence of vectors. There is no real time component.
(28:55) If you have just single image that's why when you try to build this type of network which will produce description of your image what can you use image which is vector then you have fake zeros afterwards. Yeah. You have question you can ask question. Uh yes. So it's very interesting. So if if we give an image and then we can generate a caption. I'm just wondering how RNN was successful in the opposite.
(29:22) If I give a caption, it will generate an image. I don't think it's going to is going to be a good idea to be honest. I don't know about this type of uh results, but I don't think RNN is appropriate to generate images. there should be some kind of um some kind of convolutional layers involved, right? Well, well well maybe if you combine with convolutional layers to be precise deconvolutional layers maybe you can do it but um uh if you work with images there should be uh let me think. So you take like a
(30:04) sentence and you take Ricardo not. Yeah. So Sora model is basically generates the images uh from the sentences. Right. Right. Yeah. But u is it like based on recurrent network? I don't think so. It's probably transform. Yeah. I think so. Yeah. So that's what I'm saying.
(30:31) Probably recurren network for gened images is not the best idea, right? The the opposite problem is kind of simpler. At least my intuition tells me but if you find some you know results it's interesting to read but should be recurren because the the sentence is a time series is a time series. Yes you can formally speaking you can do it but you you ask how successful was it? I think this type of problem is quite complicated.
(30:54) You take a sentence you don't want to just you know say some is it positive or negative? Is it talking about you know cats or dogs? Simple problem but you are saying let us take a sentence that let us pass it through recurrent network and then try to generate image. No maybe you can do it if you there is also second component which is like convolutional network.
(31:14) It generates image ultimately. No it means some kind of deconvolutional layers basically. Uh you can do it I mean you definitely can do it but the question is about accuracy. You asked about accuracy. I don't think this will be accurate model but yeah but for speaking yes you can do it you take like sequence of basically your vectors pass it through recurrent layers you produce some kind of output and this will be some kind of seed to your second part second part which is like soal devotional layers which ultimately will produce you image but this generation is
(31:53) quite um quite difficult because you also have to add some kind of randomness to it, right? It means you have to make sure that your image is not done in deterministic way. It must be done in some kind of quasic way as well. It means there is also some kind of random disturbance should be involved.
(32:15) And then there is another another question how to make sure that this disturbance is not completely destroying your image. It means you have to think how to incorporate some kind of constraints so that what you generate makes sense. I'm not saying it is impossible.
(32:33) I'm just saying that I don't remember about this type of uh you know results which would would be kind of um kind of reasonable. Maybe I'm not correct. Maybe maybe it is possible right I mean this is done by transformers. The question is about recurren networks. Yeah. I was wondering what would happen or like what happened before transformer came to the spotlight.
(32:58) Um so RNN as you said they were they were the they were the def facto uh layers uh for like few years ago before transformers. Yeah. Quite sure people try to generate images using maybe them or or other Yeah. So no basically. Yeah. So basically it would be probably the best option at that point. Uh the question is how successful was it? I don't remember like you know impressive results when it would generate like images which you can you can see are kind of ideal right and you can say that wow that works I just take a sentence and it will generate image you you can do it and it was probably best option available at that
(33:36) time but it wasn't like something you would say now it now now it does it so probably it was uh quite only quite in developing phase and then people switch to transformers. That's what I'm saying. Yeah. So I making like captions is is a little bit easier problem I believe than taking sentence and from sentence generating image. It is more difficult problem.
(34:09) In that case attention mechanism like what transformers do is is needed. So now in this case we take image which is ultimately going to be vector right then and we say we are going to generate sentence or kind of kind of capturing of some kind using recurrent type of network. So yes we can do it formally speaking in that case question is what to do about other inputs.
(34:31) Remember we have other inputs as well at different time steps. In that case, people will just basically supply fake zero vectors just to fill it in kind of. So this is idea behind this approach. So it means it's not only many to many. This is called many to many. This is called one to many, many to one. No, many to one. Many to many.
(34:57) But why this type of bottleneck so called out encoder? No, it means answering question quiz question. We can say false. It is not only many to many different architectures could be used. So now next one LSTM long shortterm memory cell. So in this case what happens if I say that um I take vector C and vector H. Vector C right here and vector H.
(35:30) Now you can see that there is like summation. Summation means addition one vector plus second vector. If I add two vectors, it means basically I don't change dimensions. It means output. No, I mean let's see this vector and that vector comes from the same source. This vector and that vector is same. Then this is just tangent applied component wise. It means we get the same vector.
(35:55) Then we have element wise multiplication. It means this multiplication doesn't change dimensions. It means starting from this point H has some kind of dimension unchanged whatever it was at the start and vector C will have the very same dimension as well. It means C and H will have same dimensions. The answer is true.
(36:18) They will have same dimension. Just from this kind of representation you can see it. You can look at the formulas as well. It will be more obvious. You can look at the slides and say C and H and you can find that it is exactly equivalent. If you like uh want to look at math representations you can look at it and get convinced that C and H will have same dimensions as well.
(36:50) Right? So if you want no in this case C then tangent applied to C component wise then component wise multiplication means H will have same dimensions as C. From here you can see it right away right from this line you can see it. Now of course uh if you look at the uh at this u uh at this representation visual representation you can see the same result.
(37:17) This one is C and that one is tangent. You apply it to your C then some kind of element wise multiplication you get H. It means the same. So basically two memories I just take away from here two memories we'll have same dimension. If I decide that I want to have like five hidden states I will have five for longterm five for shortterm.
(37:36) How is it done in this case? Basically every sub network it is FC means fully connected fully connected fully connected fully connected fully connected. Every sub network will have five neurons. Five neurons. It means output is five dimensional. Five dimensional.
(37:56) And when I say five, if I specify five, if I say LSTM5, it means five dimensional, right? Every sub network will have literally have five neurons. It is simple layer of you know a simple dense layer. Here FC means simple dense layer. Let's say five five neurons. Five neurons. Five neurons. Five neurons. And we get it. So next one. Dr.
(38:24) Kian, could I ask you a quick question? Um, so the um the long-term memory uh vector has hits a forget gate. Um does the amount of data that's forgotten always correspond to make sure that the um that whatever is left of the um of the long-term memory vector is the same size as the uh short-term memory vector. So by design we have same size for long memory and short memory.
(38:49) And your question is I'm sorry your question is when the when the uh when the long-term memory vector hits the forget gate it never gets um out of shape to not correspond not interact with the uh with the short-term memory gate. So forget gate means element wise multiplication.
(39:12) It means signal which comes from this network will be multiplied element wise multiplication means we take every every component and multiply to corresponding component right it means this forget gate will have some impact on on vector C will be modified somewhat modified because of this multiplication the signal from F clearly and then there will be second modification which is addition with vector which comes out from these two networks they multiply together and we get some modification this way and that's it.
(39:45) we get C after that H is slightly different. It passes through more modifications. First of all, function is applied. Secondary, it is also multiplied component twice with signal which comes from this network. That's why more modifications for H. That's how it is designed and I'm not sure if it answers your question, but this is how it is designed. Yes, I see.
(40:10) So the so they would they would always stay the same same dimensions. Okay. The dimensions will be always same because element wise multiplication means you take like one vector time different vector of same length first component time first component is your first entry second component time second component is your second entry dimension doesn't change no plus is just plus we know vector plus vector is same vector right I mean vector of same dimensions it means times and plus they don't change dimensions dimensions are not going to be different right it means c and h will be always of
(40:41) same dimensions. Perfect. Thank you. So is the short-term memory and the long-term memory uh would be same not on the dimension but on the values at any point in time. So they will be of same dimensions at any point in time. Yes. Okay. Just like in that case my y is always two dimensional two dimensional two dimensional.
(41:12) Basically uh this type of short a long short-term memory cell is designed in a way that we have kind of two memories right basically which you pass next it is like generalization inside not two neurons but sub networks outside we have one signal out like y out then h out which is same as y and also c out h and c are pushed back as input it is called memory So because inside not neurons but just sub networks.
(41:42) So basically LSTM is going to look it is nice to think about LSTM this way. LSTM is already modification I will say generaliz kind of kind of more complex right. Memory cell it is called it is also called memory cell by the way. So now we have input which is my signal XT some networks inside I'm not even going to sketch it then output is YT very same YT is basically goes out now even though I call it H right it is YT minus one if you will but it is same as H we kind of call it H convention and maybe I don't
(42:30) have to sketch it this way. Let me say let me say I also have two signals which go back. One is what's called uh Ct minus one kind of goes back and then ht minus one is what goes back. In reality h and y is same in this case in this design. As you can see it means basically we have Y and HD to be same. That's how we should understand LSTM. Okay.
(43:09) Thanks for Yeah. Very quick question. So the the the two uh the two memory will also feed back and they will also um uh connect to every other neuron. Is that is that correct? uh they will uh they will come back and enter all of this neurons in this sub I mean the other neurons in the same exact layer. So let's say that we have a layer that has four neurons the one that we you sketch on the board.
(43:39) So every every um every neuron outputs H and C and Y. So these will also connect to the other neurons in the in the uh in the layer. So in this case it is different because we we so basically you're asking about LSTM right you probably I'm not sure if you understand what what is meant by the visual representation on the screen let me let me sketch it is not like like you say neuron will output there is no like just neuron there are entire subn networks it is more complicated now right it is if we treat it as box as you draw it here. So this box three things will come
(44:22) out of this box Y and C and H. Correct? Yes. And those those three things let's say that I have four neurons. I have three other neurons in the layer. Um I have other three more boxes. Okay. Let me say when you say three more neurons probably you mean time step is equal to three, right? No no I I actually I actually meant physically like other neuron.
(44:51) is a layer like maybe like 10 neurons not um which I'm sorry which other neurons so all neurons are inside in those sub networks which other neurons okay so there's only you mean like for different time steps yeah so this box represent the layer not not one not not one memory cell is right this box repres represents entire memory cell which we see on the screen. Yeah.
(45:23) I think again the question is is this single unit in a network? Can we treat this as a single unit and can we have multiple units like this? Yes, we can treat a single unit. When we move to like next layer, we place another LSTM on the top of this but we kind of know that inside there are hidden subn networks but we typically just call it one layer even though we know inside there are sub networks but those sub networks are not deep they just have single layer that's why it is almost same as just saying it is one layer but what I what I want to maybe emphasize I'm not sure if this one
(45:55) is maybe not clear if I have like a number of time steps. So essentially you should think that it looks this way right? Every time I have some kind of subn network my signal X first is in signal X second is in signal X capital T is in this way. Then output yt which is y first during first time step y second and so on will be out. I can again use them. I can pass them further to next layer.
(46:33) I can ignore them. I can use on the last one and so on. But there are two memories which kind of go that way. Two memories. Two memories go that way. That's how we design it. And two memories enter the last on the last time step. That's what we have.
(46:53) And then what is inside? We have some kind of subn networks inside subn network like four of them, right? I will not sketch it here. I don't have much space. But inside we have sub networks. It is not just like two neurons. It will have like a bunch of neurons, another bunch of neurons because four sub networks. Does it is it clear now? Um so when I say I I I wanted to create LSTM and I provide uh number of neurons to be 10.
(47:25) Uh do I get 10 10 cells of whatever is drawn on the board here? I mean I mean the the slide. Ah I see I see I see. So yeah. So if I say if I say um if I say LSTM uh 2, right? What's going to happen is every sub network will have two neurons. First one, two, second one, two, next one, two, next one, two. So effectively inside I will have eight neurons in this case.
(47:56) That's what it means, right? One subnet network second next next. So two will result in eight neurons. Okay? because every subnet network. So basically two means think about this two means dimensions of my y of my h of my c they all have same dimensions. Okay in this case uh in this example it also means that my y will be y first y first second.
(48:34) So basically vector in two dimensional space two neurons in every sub network will correspond to two dimensional Y h and c all of them will be two dimensional in this example and and the recurrent the recurrent connection will be processed according to the diagram here I see so it's uh so whatever whatever we pass Y c and h it will be processed according to the gates in the inside this LTM cell Right? So when we take this information as you can see on the screen it will enter the cell X itself will enter every neuron. Every single neuron will accept Y.
(49:15) Yes. Also H will be accepted by by every neuron. Okay. and and C will not be accepted by every neuron because C kind of goes straight through my memory cell, right? So C will go this way not through network. It will goes kind of around networks one through network.
(49:46) Second one is not it is like if I sketch it here one two one two one two one two. So basically one like X signal will go to every neuron this way literally. Then one of the memories will also go to every neuron this way but this memory go around then there is like forget gate and stuff like that right so this information and so on that's how it is designed does it make sense yeah I think I got the idea thank you so much yes so basically sub networks will be exactly as it is on the screen you just need need to keep in mind This sub network simply means uh two
(50:31) neurons in our example that's all which means F is two dimensional G is two dimensional I all all of them are two dimensional yeah this way so now problem five in this case we have LSTM which has 6,8 A32 parameters and we say what if I want to use uh maybe previous question built on reverse sequence.
(51:07) In this case we remember there was a like idea that we can use birectional LSTM for example birectional RNN. The question is what does it mean? Why do we want to use like birectional? Because sometimes sequence as is is better because maybe there are no kind of correlations between your X capital T and whatever you want to for example predict right some strong correlations maybe your X capital T must be close to close to the output.
(51:42) Sometimes on on the contrary maybe the start of your time series is more important then in that case you want to maybe reverse your sequence but sometimes it is possible that maybe both end is more important also beginning is more important especially if you think like about language sometimes the start is important end is also important.
(52:01) If you think of like like German language they say they some often I would say basically classically right they use negation at the end like at the end right so uh which means maybe end of sentence is important in order to understand if it is like negative statement of positive statement and also the beginning could be important it means there was idea that we can actually incorporate so-called birectional design And birectional design means we can use A B CDE E as is.
(52:37) And then ABCDE E in reverse order E D C B A. We use two type of recurrent networks kind of layers of recurrent neurons maybe LSTM whatever you want and then at the end we sort of concatenate which means we just merge those vectors right if our vector is like two dimensional from second piece it is also two dimensional every hidden state will be already fourdimensional we can catenate and we will do whatever we want next with this we want to do some kind of predictions or whatever so this is design in case when you in case when your data is such that
(53:19) reversed or original order is not the best approach. You have to basically experiment. Original order, reverse order, birectional order, what whatever is going to be best is basically your model for your task. Well, this idea is is widely used. That means of course false.
(53:38) No, we can actually combine it together and say in this case actually build on reverse sequence is is is always less secret. No, it's not. It may be the case that on the on the contrary basically your your start of sentence is more important maybe then you can revert it right. So start is kind of far away from the output.
(54:04) It means probably you kind of capture correlations between X capital T and Y capital T. But if you take converse you capture correlation between X first and output Y capital T but in that case basically sometimes you might want to do it because sometimes maybe your data is such that those correlations some more informative. So that's idea and finally the last question is about birectional ass.
(54:30) As we already discussed we design a type of networks in that case we simply need to double number of parameters. Exactly. Because of this design we say we have one layer second layer if you have to use both of them that means this specific layer will have to have twice more parameters. later it is not the case later when you sort of use like fully for fully connected layer for example like dense layer in that case we don't really double because we have to add only one bias for everyone it means it is slightly less than less than double we can see here example my LSTM has 8320 by directional will have exactly double which does make sense if you look at this representation of course it is
(55:12) twice more there's no way around. But what about next layer? Dance layer will have 33 parameters. If I use birectional, it is not 33. It will be actually you see this 32 plus bias. It means 32 + 32 plus bias. It must be 65. Okay, it is indeed 65. That's how it is done. But we are talking about specifically by directional layer. It will have to have twice more parameters.
(55:42) So this is the case. It means the answer is yes. It will have to have twice more parameters 13,664. Any questions? Okay. Are you saying that the dimension of the output will also double or only the it actually it is going to be defined. You see it is going to be defined u it is not probably the case. It is going to be defined by this by by this specification.
(56:13) If you say one in first case, I mean output from I mean yeah I have to take it back. You you probably mean output from the layer itself, right? Yes. From the layer itself, it will be doubled. Exactly. Because of concatenation later on we we'll add something else like one neuron for example. It means the final output will be exactly same.
(56:36) That's why we can use it for the very same task. But you're right. If you're talking only about current layer like your LSTM layer and compare it with birectional layer, then your y's will be doubled because we concatenate. We take y from this piece and similar y from second piece and stack it here. Y and y from second piece.
(57:02) It means it will be doubled. Exactly. But uh there's only one bias term right only one bias term when we move to the next layer to the next layer but here we have bias for first subn network bias for second subn network it means bias will be doubled as well remember we have one network and second network independent of each other it means they don't share bias yeah I know I mean if but next next part will share bias Yes. Yeah.
(57:34) Right. Yeah. I just mean the output. Yeah. Output. Yes. So kind of bias which corresponds to next layer already which accepts this concatenated vectors. We'll have single bias. That's why that's exactly why 33 will not be doubled. It will be 65 because only one bias is needed. Okay. Any questions? So now um let me you know what we have some stuff.
(58:10) Let me think now let me you know let us make a break. Okay, let's make a break. So now today we are going to talk about no kind of general overview. First of all what is natural language processing
(1:05:34) and then we talk about important uh steps. Uh no by the way they important not always but often important tokenization stem limitization if you want to do translation maybe those steps are not actually actually needed but if you do some kind of classification of text right for example maybe they are needed.
(1:05:57) So let me think first of all maybe before I even start talking about this I want to uh make one comment which is kind of related to your next assignment. Let me just briefly mention how we can handle text. We'll talk more about this like in detail.
(1:06:25) We'll talk about embeddings and stuff but let me as a starting point talk about one example it will be kind of helpful helpful for your next assignment basically right so let's say I talk about example where I have a sentence some kind of text let's say cat set on when I'm at this type of check text I have this text now the question is how to use it as input to the neural network there are many ways to do it by the way but what what we can think of we can think of some kind of dictionary we can say okay there is a dictionary which consist of unique words found in our corpus and this case only like five
(1:07:19) words. Let's assume we have more words. So it means we can first create create a dictionary and we we ultimately want to be able to represent everything from from the corpus everything from the text available text we have called corpus. Let's say the dictionary has word cat over there a dog over there.
(1:07:48) What's next? A B C D let's say M let's say alpha basically in reality when we create dictionary it will just take first from the first document second from the second document and so on it will sort of move through your text and will not even have to sort it out let me just uh do it alphabetically right uh let's say uh on let's A table for example let's say there get a set also maybe set first this way.
(1:08:38) So will be will be my dictionary it means like basically dictionary of unique unique words found in our in our text. There are many questions which you want to ask. We'll talk we'll be talking about this exactly today. What is word? Should we kind of reduce it to like root form or should we just keep it as is? Should run be same as ran running or not? Should it be kind of you know reduced to the same same form? It is like questions which we are going to discuss today.
(1:09:07) Now let me say my dictionary basically means first word, second word is dog, third word is met, horse is on, fifth is set, six is stable, seventh is the now cat is like first word it means I can say first word is is cat next word is fifth one it is five next one is on fourth there is Seven M is third that will be like like sequence of integers.
(1:09:39) Those integers basically represent indexes to refer to the dictionary. Now it is okay. We can kind of use it somehow as input to network. But we have to be quite careful here. We have to do some kind of transformations first soal embedding, right? In order to kind of achieve better performance. We don't want to just take indexes. We want to do some some kind of embeddings.
(1:10:03) Now let me say this is basically sequence of indexes. This is just index. But the problem is it's not really going to be used like input to the kind of neural network. It can be used as input to neural network which has so-called embeddings. Let me say but but not directly used not directly used as inputs to neural networks right oh again it should be some kind of transformation first such as one hot encodings for example we can do one hot encodings now let me say basically index means I know that this is first word It means I can
(1:10:47) create sequence of vectors. Sequence of just indexes is not good idea clearly but sequence sequence of vectors is already nice idea. One is here. So because it is like first 0 0 0 is my first vector at time t is equal to one fifth word. Now it means number five five will be literally translated to 0 0 0 1 0 0.
(1:11:23) Number four will be translated to 0 0 0 1 0 0 0. Number seven is the last one. It means will be like a vector of 0 0 0 1 and third one means 0 0 1 0 0 0. So basically not indexes is what I want to use but maybe this type of vectors as input to my network.
(1:11:57) Again I can use indexes to sort of represent those vectors basically right in efficient way. But I cannot use indexes directly as input to network. It will not perform nicely. So now this is my sequence of vectors. Can I use it? Yes, I can use it. It has time component, right? So it has time component. It has time component. So everything seems nice. We still kind of preserve time. Not a problem. But this vector is sparse.
(1:12:24) So, but it is sparse. Sparse means we have a lots of zeros. It requires a lots of memory. If your dictionary has like thousands of words, every vector is thousands thou thousand dimensional already. So, it becomes quite complicated in practice. You need a lot of memory and so on.
(1:12:50) And those computations will be almost like fake computations for the most part because multiplying by zeros it's not going to you know give you much right so it means uh it means we have to have neural network which handles those vectors which will have a lots of parameters right let me say it means it means neural network has many parameters many parameters it is a problem clearly.
(1:13:21) Then there is alternative which is kind of not maybe appropriate to not appropriate to cases where you want to do maybe translation and stuff like that but it is actually nice approach for classification problems. Let me say xt is going to be I'm sorry no t anymore x vector which is just single vector which is going to represent my entire sentence is going to be so-called bag of words right is going to be simply saying there is my dictionary it is right here so it means I'm going to sketch corresponding vector to my dictionary first location corresponds to cat second corresponds to dog next one to Matt next
(1:14:03) one to on to set to table to the I'm going to say okay cat is represented in my presence in my sentence I say one dog is not I say zero met it is there so I say one on is there set is there table is not the is there so it is much much better if you kind of if you kind of know if you kind of will observe ultimately that time is not important Thisation is much better in terms of kind of you know uh parameter efficiency. You will have to have much less parameters ultimately. So this is already soalled bag of words
(1:14:48) right for problems such as classifications maybe maybe this is what you what you want to use right. Of course, time here is lost, but time t is lost. In some cases, you not necessarily want to use like vector representations. It is not most efficient. You may want to use some kind of bag of words.
(1:15:14) And your second problem is basically going to ask you to do exactly that. Problem number two on your assignment is going to do that. So, it is like problem number do on your next assignment which is assignment what assignment second right oh maybe I'm sorry let me open it man. so in this case in second assignment first question is about making predictions basically it's going to be quite similar to what I did last time last time I generated time series I would do predictions in your case you need to load data from from the file and you're going to construct some kind of recurrent network to make predictions.
(1:16:24) It is quite similar to what I did last time, but you're going to use data from the file. Second question. In second question, you're going to use movie reviews. It has uh it has movie reviews and it tells you change most frequently used words to 200 to kind of reduce dimensionality. It tells you plot results for triangulation accuracy report test accuracy explain what it does. So in that case problem two basically is going to use bag of words.
(1:17:05) If you open this file you will see that it is exactly going to use bag of words. So this is assignment number two as well. Second problem in last problem I'm I'm I'm going to ask you to do basically similar task I'm asking you to do classification but I'm I'm saying let's try to change architecture to using recurrent at work I can tell you that maybe it is not as important in this case since we are going to use classification uh but I'm going to tell you nevertheless let's try let's try to use Ricardo network to do classifications in
(1:17:42) this case essentially I'm going to have 200 dimensional inputs at a time because I have 200 most frequent words. That means my dictionary will consist of 200 most frequent words, right? It means one hot encoding will be 200 dimensional first thing. Secondary I'm going to use um uh 25,000 uh documents right this way and uh 500 uh 500 uh time steps I believe that's what we use it means basically in this case we are going to uh use this approach it is problem number problem number three already so I just want you to try this approach in case
(1:18:34) when we are talking about text how to apply recurrent networks to text so that's about your assignment right you kind of um free to choose whatever you And there is accuracy. I can tell you that um it will take some time to achieve this accuracy. At the same time, it is not the best accuracy. There is some kind kind of you know um room for even for further improvement.
(1:18:59) So you will be you will be able to achieve it. Um you will try. So that's what we're going to do. Now let me go back to to the slides and uh let me start talking about u natural language processing. So what do we what what do you mean by natural language processing? So basically we are talking about some kind of techniques in order to analyze text using computers right I mean this is some kind of computational problem clearly we need to make sure that computers should be able to understand text ideally they should be able to even generate text answer questions and so on basically there are many types of problems whatever we can
(1:19:43) do with text ourself we hope to kind of train computers to do nowadays you all know like chat GPS right I example transformers. So this kind those predin transformers it means you understand what we are talking about. So how to basically work with text in a way that people do.
(1:20:07) Now what kind of algorithms we can use in order to do it? Let me say there there is a kind of visual representation of what kind of models we have. First of all artificial intelligence. What is artificial intelligence? It is quite kind of broad class of models. And uh so now um broad class of models there is a question about some kind of submission of assignments.
(1:20:34) I will not read it and comment on that. You can later reach out. Uh so now um I want to say that uh artificial intelligence is quite broad class of functions. First of all, it could be quite sophisticated models such as transformers clearly. At the same time, it could be things such as like doctors will specify what to do in every single case will almost like write down you know prescriptions, store it on computer, then nurse will come open this you know this prescriptions and uh tell what to do and or will basically decide what to do based on those stored prescriptions. That means artificial intelligence could
(1:21:17) be basically kind of hardcoded you know solutions in some sense. At some point people even believe that if you want to develop the system which plays chess for example you have to hardcode kind of store hardcoded solutions for every move for every combination. So as is quite quite you know broad class of models.
(1:21:40) I kind of like um one of the representations in order to better understand what's the difference. Let me say how machine algorithms are different from general artificial intelligence. Let me say say this way machine learning algorithms is one type of models and more broadly broadly artificial intelligence.
(1:22:09) Now first of all artificial intellis is basically everything including neural networks including machine learning algorithms and so on. But what are machine learning algorithms? So basically this is kind of model which accepts data and also accepts all kind of results right results labels outputs whatever like results generally speaking the results will be also supplied as input and machine algorithms will produce rules to basically handle this data to maybe make predictions.
(1:22:43) So rules will be outputs from here. Now you can think about very simple model such as linear regression. If I have data set and I say I want to fit linear regression Y is here X is over there. Let's say I want to do some kind of predictions based on very simple model linear regression. Then I fit it. I say y hat is equal to beta 0 hat plus bet one hat * x and this type of model can be basically estimated from data without our involvement right we don't have to hardcode anything here we can easily estimate those parameters betters so those better are sort of what we mean by rules rules in this context means better
(1:23:32) you can essentially program your algorithm to estimate these parameters every morning and you can make some kind of predictions for every day, right? So it is like machine formally speaking already machine learning algorithm and you make predictions you can say what is my output okay you supply data and you make prediction over here this is already yhat which you produce as a result of this machine algorithm artificial intelligence first of all is also machine algorithms but broadly speaking it may look somewhat different
(1:24:09) also in addition to this type of algorithms It it may look somewhat different. You supply data and you also supply already rules. You kind of hard supply harded rules for example and you make your know for example predictions right as an example you can say here I have some kind of date or whatever right basically I have my model which is exactly my rules and my model tells me y head must be 2 + 5 * x so basically I say 2 and five are sort of hardcoded somehow I know it right somehow I know with maybe doctor for example knows some kind of stuff from
(1:24:53) from experience we'll store this rules on computer and we can use computers you know you type some kind of parameters like fever temperature blood pressure and you see what to do basically it is different from kind of how we understand machine learning today but it is what's called artificial intelligence it is already like more broader right notion and it is quite important in case of So in case of um natural language processing because in our case we are talking about very difficult problems where sometimes we have to use hardcoded rules. We cannot just tell neural
(1:25:30) network to do the job right. We may have to do some kind of um some kind of um uh borrow some kind of rules from linguistics. So we can for example try to reduce uh words to root form. For example, we can say run ran running. They all the same. They basically represent a word run.
(1:26:00) It means it is already kind of hardcoded in some sense rule some kind of some kind of you know kind of library which we have to use which means part of kind of uh algorithms or ideas which is outside of machine learning should be used as well. That's why natural language processing first of all includes machine learning. It is clearly deep networks.
(1:26:20) It is clearly but sometimes it is also outside. So it doesn't have to be exactly like only part of deep learning. That's how we understand it. So now again this is like natural language processing. It is multi disciplinary field. It includes linguistics and also artificial intelligence. Right? No all this kind of algorithms which we which we discussed machine algorithms and also those which incorporate some kind of hardcoded rules.
(1:26:52) basically right so linguistics no we understand it is kind of study which which uh which field which studies how language is structured syntaxes right syntax we talking about how sentences is structured morphology we're talking about like how words are structured semantics which means we're talking about like meaning of words and context and so on fanatics is also part of linguistics so linguistic is basically field which studies natural language and The section of this linguistic and also artificial intelligence is what we call natural language processing.
(1:27:22) So ultimately the goal will be to train computers to understand language to maybe generate language and so on and so forth. That is quite clear, right? So now challenges if you look at this examples you can see what challenges are. It is not so easy to understand this type of sentences. No, at least um maybe now Judge GPT will understand but it is not so easy if you basically don't have uh some kind of background right court to try shooting defendant. What is really happening here? Court is trying to shoot defendant or what what is happening? We have to
(1:28:00) have sort of kind of um knowledge background outside of given text as well. Basically that's a challenge right? So it is not so just easy to understand this this sentence if you don't have specific background this this is the idea that's why we uh uh have to keep in mind that is not so straightforward as trying to just make predictions in case of linear regressions for example.
(1:28:28) So now uh now we can use we can look at different applications of natural language processing. First of all text classification no quite obvious application we already did it right. So spam detection for example like topic characterization sentiment classification is review positive negative or what is that? What is that for this type of problems? In that case we often don't even need to use like u recurrent networks. we can use different techniques.
(1:28:55) So basically representation of language doesn't have to be like sequence of vectors would be much easier often or maybe not to be precise and not always right sometimes performance may be better why different representations when we don't lose time component no because again because of because of those challenges which we just discussed.
(1:29:15) So now um next one is so-called name entity recognition. It is quite important sub fields quite important application in this case we have to say is this particular word a name of some kind is it a city country right or just some person what is this you may say why is it important it is quite important if you do for example translation when you translate a sentence you don't want to how to translate translate like name first name for example now you basically have to know how to translate in order to Oh, first we need to recognize it in the text. It is called name entity
(1:29:54) recognition. We extract names of people, organizations and so on, locations, right? It is important application. Next one, sentiment analysis. No, it is like similar to classification, but it could be a little bit more general. Clearly, we can also kind of understand if if it is like very positive or slightly positive, not just like 01, but it could be also some kind of predictions.
(1:30:24) In this case we are trying to understand if if the statement itself is positive or not. It is as you understand quite difficult actually task. When you read a you know message from someone they may not you know say bad words to you but you see that attitude is quite negative. You can understand it from the text.
(1:30:41) We can understand it from the text that attitude is negative and what what is this some kind of sarcasm? What what is happening? And this is kind of problems which are we are talking about right. So it is quite also difficult task actually speaking. Now of course when we use like important words I'm happy I didn't like it you know and so on it is clear what's happening most likely but generally speaking is much more difficult problem because again we human humans can understand if it is negative or positive how to train computers to do the same. So that's that's the task information
(1:31:11) retrieval. It is almost like almost like you know some kind of search engines right we try to extract information relevant information from from the text. So techniques which which are used could be such as TF we we we perform particular tasks where we try to find relevant information from the text. It doesn't have to be like very complicated.
(1:31:38) Often TF turn frequency versus document frequency could be quite quite appropriate. BM25 is some kind of modification of TF more difficult uh kind of more more advanced uh more advanced uh advanced uh more advanced uh representation models we kind of classify which is more relevant which is less relevant. This is application. So next one is optical character recognition.
(1:32:08) We supply for example image which represents text and we want to be able to recognize what kind of you know text is applied as image actually quite important application clearly we scan document and we want to transfer document into the text. So this is application also of course it should use most likely it should basically it uses conventional networks as part of this models.
(1:32:35) Next one is machine translation not quite obvious. We take sentence in one language and translate it to sentence in different language. As we discussed, we need to use some kind of maybe some kind of models similar to auto encoders. But nowadays people use already or maybe car networks which maybe even going to have some kind of bottleneck.
(1:32:58) But nowadays people use attention mechanism such as transformers. So this is what what what another application is. Next one the text summarization. We just um I want to basically uh produce uh some kind of aggregation of text that is interesting application. We want to understand what is most important in the text.
(1:33:26) We throw away the rest and we produce some kind of summary. So this is text summarization. Speech recognition. In this case it is probably once one input is going to be basically sound right. We have to somehow break this sound into pieces somehow represented via something similar to FIA transform basically slightly different but similar to free transform and we then try to supply it as input to the network like chunk of you know chunk chunks of this of this sound.
(1:33:58) We uh prep-process it somehow supply it as input and try to understand what was what was what was said. Speech recognition is application question answering systems. Now it is when you ask a question and you get a result right you can say what is like capital of you know France for example and then you will get like answer to this type of systems. Next one is chat bots.
(1:34:21) No, it is like chat GPT basically for example when you ask a questions could be not like like like from encyclopedia for example but it could be how are you doing for example then your chat will answer okay I'm doing great then you ask next question to notion of continuation of conversation basically and so on and so forth it is somewhat different from question answering systems but idea is quite similar but we sort of continue conversation it is another application. Next one topic modeling. In this case, we use text and
(1:34:57) we try to sort of classif kind of classified into different topics. It is some kind of unsupervised learn algorithms which will say my text consist of such and such topics. One topics is related to this idea. Second topic is related to that idea. And we can identify what each document consists of.
(1:35:25) We kind of create like a basis of different topics and we say this particular text consist of such and such topics. Second text, second document consist of different topics kind of you say how much each topic contributes to every document. Next one clinical applications in this case it is basically when you when you work is like medical literature for example when you can different applications are possible but when you for example tell like symptoms and you want to know what the problem potentially is with the health what is health issue in this case and so on.
(1:35:59) basically how humans like how doctors work we supply some kind of information they want to get answers so it is related to clinical applications so next one language generations how to generate language itself so it seems to seems to be like seems to be similar to human human uh humans right whatever was generated by humans so it is another applications so this is kind a real of applications and now let's talk about techniques which can be used to quantify the text.
(1:36:38) So three important techniques tokenization stemming leatization sometimes when we perform tasks such as specification for example we don't want to just take every individual word into the dictionary sometimes you want to reduce it to particular form as I said run ran running they all maybe should be kind of similar to each other right maybe one single word this idea behind steming and limitization what about tokenization tokenization is basically um uh I would say extracting units of information from the text but typically we say let's take like individual words we have a sentence let's take first word second word next
(1:37:20) word every word will be talking sometimes you can say I don't want a words I want two grams that means two consequ consecutive words two words two words two words two words will be also talking will be two gram or Three words will be your unit of information or maybe entire sentence. First to talk in a sentence, second to talk in a sentence and so on. You can do it in the case of for example two grams.
(1:37:47) You can for example choose to use overlapping gs not overlapping to gs that is also decision which we should make. Basically we have to ultimately uh look at the test performance. Right? So that's what we do. Now other techniques will be stemming and limitization which means we kind of try to reduce uh dimensionality of the problem by trying to reduce words to kind of you know short form is called root form right.
(1:38:19) So let me now talk about first tokenization. Tokenization is process of segmenting text into individual units called tokens. So basically units of information will be extracted from text. As I said, typically it is like words, sometimes two grams, sometimes three grams, sometimes even letters, I mean like characters, sometimes sentence as unit of information. Token could be whatever you want.
(1:38:44) Basically, you say my token is sentence. Okay, your unit of information will be sentence. My token it means document will be sequence of sentences, right? You can say my token is going to be character. Okay, your text is sequence of characters or maybe sequence of words or maybe sub words sometimes uh you have like um uh like uh a words which have like hyphen which means you have to decide if it is like one word or it is two different words.
(1:39:18) You can use sub words if you want. Right, that's what what we can do in this case. And uh now we have to understand what to do with such things as for example uh so-called stock words when you say there should we keep it or should we remove it. Sometimes it is better to remove it because it maybe not so important if you do some kind of classification for example it may not as important you know it is not something which is going to be helpful in in differentiating your text.
(1:39:52) That's why there is like list of um uh stop words, dictionaries of stock words basically which often it is useful often to remove before you even try to create your dictionary. You don't want to have like the in your dictionary, right? That's what happens. And uh after we do we do tokenization, we have to apply some kind of technique such as stingatization. Let's see what it is.
(1:40:22) So in first case I have tokenization. In first case I have a word tokenization. I have Carl Benz in invented the first car. Okay. I can apply to organization. It means my sentence will be basically list of individual words also kind of period here as is as as a word. Right.
(1:40:46) Now in second case it is sentence tokenization. two sentences. If I apply it, I will have list of two different sentences. Oh, makes perfect sense. First one and then second entry to the list will be second sentence. Maybe sub word organization. I have word which consist of two subwords. I may not like this as a unit of information. I may say no, I don't want to have this kind of word in into my in my dictionary.
(1:41:12) Let me break it into pieces. In that case, we can use subword as a token. So totally okay as well. So next one stemming or after you completed um uh tokenization you can say let me try to decrease dimensions of my dictionary and we apply stemming. What is stemming? Basically we say let us try to kind of throw away parts of our words. If it is running let let's keep run jumps. Let let's keep jump.
(1:41:44) So basically we literally uh throw away like uh kind of you know parts of the words right prefixes suffixes we just throw them away essentially it becomes um important if you want to reduce the dimensionality. It may be not useful for translation because if you do it it means um for example when you try to generate text you definitely want to keep it but even as a input you may want to keep it right.
(1:42:14) It means it is not always useful but for problems where you don't try to translate maybe it is often okay to reduce the malarity this way you just throw away parts of your words so it is called stemming stemming is you know kind of quite quite cheap I would say operation you just uh using specific rules you just throw away parts of your words sometimes it uses u maybe some kind of um not not even not it's not even using some kind of you know uh libraries it just has specific rules how to do it essentially like in this case electric will become elector it is not even word right basically that's what
(1:42:54) stemming is doing it just tries to kind of reduce dimensions and tries to make sure that different different variations of the same kind of similar words will be reduced to the same form that's all what we it's all we need ultimately there is famous porter stemer built in 1979. It will just use some kind of rules to strip suffixes, right? And here is example what it's going to do. Transportation becomes transport which is a word which is okay.
(1:43:26) Electric electric becomes electric which is not even a word. So it happens sometimes. It means if you want to ultimately understand what it was it may be kind of difficult, right? But if you do some kind of uh classification or maybe semantic analysis, we don't even really care much how neural network saw the sentence, how it show the text apply this type of kind of truncated variations of words to the neural network and all we need we need nice performance.
(1:43:58) we need some kind of predictions or maybe some kind of you know classification results which we can easily get even we don't if even if we don't supply original words to the neural networks that's why it is going to be quite in practice important to do it because we may eventually increase performance network may not have to know like original word it doesn't have to differentiate between different variations of word second stem soal mobile.
(1:44:30) It is also developed by the same person. It is sometimes called portrait tool I believe and uh it is going to be supported in multiple languages also improves accuracy and it is more complex. It is also using some kind of uh maybe uh kind of dictionaries right to understand how to better do it. And next one is Lancaster Stmer by Chris Pace.
(1:45:04) It is already much more aggressive than Porter it itself. It means it will cut more from the words. Now it will try to kind of you know reduce it to do the in a way so different variations will look the same. So it is it is done this way in order to even further reduce dimensionality of our dictionary. So it is this one and uh let me say that steming is not trying to represent your words in a way that is recognizable.
(1:45:34) I mean in a way this is word from the dictionary from the actual dictionary. It just literally cut parts of your words and say okay now it it looks a little bit you know more kind of unified across your text. It means dimensionality will be reduced. But if you really want to reduce your words to the in a way so they represent actual words limitization is the way to go.
(1:46:00) It is more expensive clearly because it needs to look at some specific libraries. In that case limitization means as this examples show cars will be cares limitization and and stemming will look the same way but in case of driving stemming as I just mentioned will just remove in which means the driving will be drift and that's it but limitization will result in drive so it is already some kind of rule which is stored somewhere it remembers that driving corresponds to drive it means It is more expensive to run this the limitization obviously but it keeps your words to to to look um
(1:46:40) something which you can recognize. If it is important for you to be able to recognize after this limitization after this uh preprocessing then maybe limitization is what you want to use. If not stemming is actually quite quite sufficient for for many problems and uh let's say inflated limitization will tell you it is inflate stemming will reduce it to this form in full and that's it.
(1:47:12) So engines limitization means engine stemming means engine right which is not quite correct. So it means limitization provides more accurate uh results will be basically actual words for the most part that's how we do it. Now we can use different libraries and um in first case we use word net limit limitizer.
(1:47:50) In this case, we actually need to first of all kind of identify what kind of uh part uh part of speech this word is. Is it like noun or not? Is it like verb or what is that? It means it is also quite more expensive because you need to first identify it before you can run it. But this algorithm is widely used in academia among researchers. So it requires you to actually first identify uh what part of speech it is.
(1:48:11) That means it is also more expensive and it is also um not really it cannot really work with new new words. If there are some kind of new words like slang for example, it will be kind of struggling trying to identify what it is. So it is like limitation of this approach. And also second limitation is that this um uh alimatizer is only kind of available in English basically, right? It means if you want to work with different languages maybe word net is not the best approach. It is again it is popular in researchers but not in in industry. If
(1:48:51) you want to use liatizer maybe space is the way to go right. It is like more kind of industry or oriented. it is much faster or sometimes it may be a little bit less accurate kind of right but it is typically okay so spacey library is probably what you want to use if you want to put it in production for example so there are libraries which are available to do this kind of techniques now let us look at some examples first of allization using so-called natural language toolkit NLTK natural language toolkit Second one will be stemming. I'm going to use also NLTK. And then finally using
(1:49:39) space three examples. Let's look at theization using NLTK. I'm going to take my NLTK. Then I'm going to say let me create a text Henry for innovation and so on. Then I say let me apply tokenization to this text. So you can see what happens. Henry is my first token because I say words tokenization.
(1:50:09) That's why it will split into into words. Every piece of my text, every unit of information will be word because I use word tokenization. Then I say first one is Henry, second one is fourth, then S is next one, innovation is next one, comma is next one and so on. So again this is tokenization. It's a little bit probably better than what you would get if you programmed it yourself, right? It is just going to split it using some kind of rules. It is not just space.
(1:50:44) It is more than that because if you say like can't for example, can't will be just one unit of information. can it can keep it together. If you split whenever you see like punctuation, if you split whenever you see space or apostrophe then you will have to break can't into pieces organization will not do it.
(1:51:04) If you if you observe like can't it will actually keep it as talking. That's why using libraries is more kind of kind of is safer kind of I recommend you to use libraries when you do this type of tokenization. Of course, we all can easily program how to do tokenization, but it is probably better to use libraries because it will try to at least uh not break things like can't for example into pieces.
(1:51:28) But in some cases when we say fors it will maybe take away s. Now next is uh already uh tagization into sentences. It means unit of information now is going to be sentence. We are going to say let's apply to this text send tokenize this one right and it will be basically creating a list of individual sentences out of my text. It will know how to do it.
(1:52:00) Basically period is going to be used will break it. I'm going to obtain list of my sentences. Every sentence is talking now it comes from NLTK library. Now on Friday we'll consider some examples. We we even will create some kind of networks. We first will take a nice and we do some kind of limitization or stemming. We'll transfer those things into maybe vectors of maybe bags of words for example and we'll do some kind of classifications or something similar. So we can apply it in practice.
(1:52:37) Now it is just you know for reference how it works, how it looks. Now next one stemming with NLTK how do we do it? In this case I have text which I import portra snowball stemer as we discussed right two types of stemmers and I say this is my text which consist of one sentence. I want to first tokenize it.
(1:53:00) No because I apply stemming to my my tokens. What is stemming again? Stemming means I will kind of uh truncate my word. I will kind of stream away the ending of the of the word. That means first will be application of my tokenization. That means it is required step clearly. So I willize it. So I will tokenize it. So my words will look uh this way as in that case right.
(1:53:26) I will be tokenization. Then as next step I'm going to I'm going to apply my uh stemming from porter stemmer to the swords. The way it works it actually supposed to be applied to every individual token. It means of course I have to do it in a loop here. Right? For every token in my list of individual tokens I will apply my steam and then I get list of my already stemmed words.
(1:54:00) So it means everyone will be processed this way and I can plot results and finally I can apply my snowball stemer very similarly snowball. stem apply to every individual word in my list of tokens. Words consist of list of tokens and I get my results. So let's see the results.
(1:54:26) So in this case my tokens look this way. Henry Ford apostrophe s innovation and so on. If I apply order I get Henry Ford as in comma then so on. If I apply snowball timer, I get quite similar results. You can see some differences as for example profoundly in first case. If I use porter and profound in second case, if I use snowball, you can see the difference, right? Slight slight difference because different rules how we do it. Again, we presumably believe it is not so important.
(1:55:04) We just kind of reduce it to particular form. We make sure that they kind of similar to each other. So we want to make sure that if they are kind of related to each other we get same word. But if you do stemmer remember if you do stemmer running and ran will not be reduced to the same word. They will be different. Limatization will make them same. Stemer is just going to cut something out from the word.
(1:55:28) It means it's not going to be same. Yeah. Question. I'm just curious why did it change Henry with Y to Henry with I or is that a typo? I don't think it is a type actually. It's interesting. Let me let me uh you know what I will double check. So I did it I did it in Python. It's not like I just typed it here. So um maybe it is a typo.
(1:56:04) Maybe maybe it considers um Henry with an I as a as a stem for Henry and Henrietta. So it becomes like a a beginning of both both the sexes. Yeah, I think so. Yeah, it is it is so you see it is it is interesting what you noted. So it is it is doing some kind of things uh right which tries to unify basically my tokens in order to use dimensionality. As you noticed it would replace Y with with with I right.
(1:56:30) It is actually same in case of profoundly. It is not just a type actually. Oh, you know what? Let me see. Um also for industry. Yeah. Yeah. Yeah. Yeah. You're right. So let me um let me see what examples I have here. Uh maybe we'll discuss it uh in detail on Friday. But let me see uh if I have some examples here stemming. Okay. Stemming.
(1:57:13) I'm not sure if I displayed anything but uh yeah. Okay. Uh you can okay you can check it later. Maybe I'll check it as well. But it is not a typer because I did it in Python. I believe it is just the way it is. Right? So in order to prepare this slide, I took this example and I run it in Python to see how it is going to do it. So it is not a type.
(1:57:35) It is doing it exactly this way. Yeah. So now uh finally finally elementization using space here. In this case we have space here and we say I want to apply limitization. First of all, tokenization is the first step. It is important, right? Because I want to produce tokens first.
(1:58:02) Then I will say let me take my word netatizer and let me also apply it to my word every individual token in my list of tokens I apply it and I get results. And uh let's now see what results I get. So this is my word net and also spacey polyatization and we get Henry Henry Ford for innovation innovation profoundly profoundly profoundly right dynamics in first case dynamics in second case dynamic you can see you can try to understand but to be honest the difference is negligible and most likely it's not going to impact much your performance so it's not like you not tremendously
(1:58:51) important. If you kind of see the difference, it is good because you can choose the best one. But in reality, it is not as much much important. Technically speaking, it means we just choose one of those. We use it consistently and probably it's going to be sufficient uh as a sort of step to reduce dimensionality. So any questions? So just like more of a summary.
(1:59:15) So limit limitization is just aggressive stemming. No. Um um well limitization is more interesting limitization when you say run let me go back as a summary this one if I say for example driving limitization will produce drive nice recognizable dictionary word that's limitization stemming will just cut something just stripe you know the ending and that's it doesn't like doesn't like in it will say drift and that's all stem is quite cheap but it's not actual word.
(1:59:52) It's not really a word which you can find in dictionary. Limitization is more expensive but it produces word which is in the dictionary. That's a difference. Yes. Thank you. Yeah. So limitization will reduce to kind of a dictionary form and stemming will just you know reduce to root form will just take something out which it which it believes is not important.
(2:00:18) So um it is now 10 10:13. So yeah, let's now stop. Let's now stop.