(2) 89 day2 section - YouTube
https://www.youtube.com/watch?v=ddof5WAAaaI

Transcript:
(00:01) Hello everyone. Hello every so I'm going to share my screen with you. It says right now section two. I hope you can see it right. Let's first go back to section one file and talk about this. So in this case I uh try to basically classify a particular documents based on content of those documents.
(00:44) Uh and I'm using so-called TF term frequency inverse document frequency. Later we are going to talk about this in more detail. Turn frequency inverse document frequency. So basically we're going to somehow quantify text and TF is one of the kind of ids which is widely used in such cases right. So let's see what we have here.
(01:09) In this case as an example I have like text extra consist of three documents the cat set on the mat and so on. I want to compute my um TF IDF quantities. So in this case I say let me essentially apply TF IDF vectorizer to my train data set to sort of you know train those transformations which produce those quantities TF frequencies and then I can apply it to my train data set to my test data set and get result which is representation of my text by those by those uh frequencies.
(01:51) Let's look at the example how we can do it. Let's say this is my first sent my my my first my first u document the cat set on the mat and so on. Let me say TF TF IDF. Let's just see how we can do do this transformation. By the way, there are many types of many variations of this transformation. Now let's see what the idea is.
(02:21) So first document the cat set on the mat this way. Second document. The cat did something again. So let's say the cat again did something and we got second document. Next document doesn't contain word word cat. So it's just some sentence but there is no word cat. Let's now try to focus on particular document. Let's say document number one.
(02:59) Document number one. And I want to compute my uh TF IDF then frequency inverse document frequency. Let's say I'm going to focus on uh word cat and also I'm going to talk about document number one. Document number one I want to quantify basically word cat in first document right. So now turn of frequency how do we compute it? Turn of frequency if I'm talking about the very first document means how often my word cat occurs 1 2 3 4 5 6 total words only one time I see cat. So it is 1 / six it is 10 frequency in particular document.
(03:49) Now inverse document frequency what is that? Let me say you could do it maybe this way. You can say in my case I have three documents and I have two of them containing word cat. It means two out of three but inverse means I have to flip it. So three out of two kind of two out of three reciprocal. Two documents out of three documents.
(04:14) That's why two out of three then it is inverse means I have to flip it. Sometimes people do something like plus one for example which is to make sure that we don't divide by zero. Right? So that there is some kind of modification. So not to divide by zero. Obviously this is not the only way to do it. You can do differently. Do it differently.
(04:41) For example, you can say algorithm of this ratio. So people apply some kind of algorithm of ratio, right? Do something like that. Um for example and so on. You can say + one maybe as well. Right? So there are v variations. By default packages will use some kind of parameters to ensure that we don't divide by zero. Let's say we use len of 3 / 2 + 1 for example.
(05:08) So then how do we get the frequency inverse document frequency? It is simply multiplication of those two and we get 1 / 6 * let's say ln of 3 / 2 + 1. L is often used. So now this is my result. So basically I'm going to say cat in first document corresponds to this kind of frequency. Right? That's what we see here. First document get some kind of frequency.
(05:37) No it may be not exactly that because because some default parameters could be slightly different. And also second thing by default they will try to normalize it. That means if you get some kind of frequencies, we will get length of this vector and divide everything by length of this vector not to make sure that we sort of within specific boundaries, right? To make sure that we don't get like crazy numbers that's why it is also different.
(06:03) So they somehow normalize it as well often. Now there is also one one interesting question how to compute how to train first of all my my vectorzer on training data set and how to translate information to test data set. So basically term frequency is always computed based on given document.
(06:26) If it is like test observation already I get term frequency I don't transfer knowledge from trained data set I simply if it was like test data set ter frequency would be exactly same. What about inverse document frequency? Turns out that inverse document frequency actually uses only train data set. That means basically I'm going to look at my word cat and see how often it occurs in my documents in my train documents.
(06:54) It means TF in test data set will be computed like newly and document frequency will be simply translated transferred from the train data set. That's how we can do it for test observations. Right? That's why when I apply it here we say let's apply to test observations TF will be newly computed IDF will be transformed from my train data set and I get this vectorization this type of vector so this becomes some kind of quantification of my text widely used for problems such as classification if you don't care about like like uh the order of my words because we basically
(07:35) lose order of my words is nice approach to quantify it. So in this case I present one example where we basically try to classify u patent data. So what is patent data? Patent means like a publication right? So let me find something. It is a Google uh let me take any of those. Let's say first one uh second one whatever.
(08:09) So I can look at my uh patents patents.google.com for example. I can type it and we have this publications. It is just like a publication has images sometimes it it has title it has u description right title description and also body of this text. So the question is one of the kind of uh questions which I remember this type of projects was performed in Harvard Business School.
(08:43) They try to understand how innovations basically drive companies, how companies drive innovations. It is quite difficult problem clearly because we're talking about causality and one of the tasks we need to kind of classify if it is for example part of particular industry or not. this particular patent. There are some classifications by the way already readily available but first of all they don't go back to like uh let's say 19th century they go to 70s maybe a secondary they classify by industry and by industry doesn't always mean they classify by type of innovation right in this case people would um study no I would help with this actually people
(09:22) would study uh people would study uh innovations related to automo industry and we had this kind of question how to uh classify patents and see if they belong to automo industry or not and this was basically kind of task which I want to demonstrate we load my data this data comes from a CSV data file basically it means we don't take it from Google even though it is exactly same document we don't take it from Google we take it from particular data set in that case it was this library nowadays Libraries not only contain
(09:59) books, they also contain data sets. In that case, there was a data set consisting of patent data and uh we were interested interested in somewhat around 1895. I believe this was threshold right and until 1935. So this was uh this was basically time frame when most auto auto innovation secured. Let me see if I have this lo somewhere here.
(10:31) Yeah, it is right here. So this is like number of firms which would basically produce something related to automobiles. They would be entering would then exit and number of firms basically would decrease over time. Not because number of cars would be produced less uh but because uh some of them will dominant dominate the market clearly right so some will become almost like monopolist in some sense but this chart essentially highly correlates or at least maybe there is a delay but somewhat correlates with innovations in
(11:07) autoal industry and this is what we actually observe if you try to classify patent by titles using this approach using TF F approach they increase and then start decreasing over time but not as much because number of firms may be low but firms become big and they still produce they still invest they still produce innovations that's why it doesn't have to go down obviously but it would increase and then would go down maybe would stay stable whatever right then second approach uses classification by description remember I told you there is a title there is a description Then you can combine all things together
(11:48) and classify by title and description this way. Now let's see what exactly we did here. So if I scroll back you can see that uh we have data set which consist of consist of patent data. It is publication number, title, description itself. The description means like the entire publication.
(12:23) Basically again if I refer to my uh patient it has a title but also it has title but also title is somewhere right here right title and description means the entire body basically of this document is description title that one truly harp and then description The question is how can we classify it? Again we can classify it using uh this approach frequency ID and document frequency. Now then there is a question how to get train data set.
(13:02) So in this case we took um publication of all companies available at that time between 1895 until 1935 from a book called wheels on wheels. wheels with within wheels and then we matched by company name by by address and so on. Basically by company we matched by company and we we labeled those innovations which were produced by companies involved in producing automoils.
(13:40) Now it's not the full list of relevant patents clearly relevant innovations because maybe someone would also work on innovations in this field but would not be officially company which produces cars. It could be independent researchers right it could be those who work for universities and so on.
(14:00) So it means of course number of uh innovations in the field is much much more than whatever was produced by those companies. In that case we simply say let us label innovations which we believe rel are related to automobile uh innovations and then we take from entire body of all innovations we take a sample and uh basically assign zeros it's not exactly correct to be precise because maybe we get a few which are not I mean by zero I mean like label one label means related to automobile industry. Zero label means not related to automobile industry. One comes from
(14:41) companies which do work which did produce automobiles and which most likely were involved in innovations like the automobile industry and those which are labeled zero basically basically currently just unknown right we still don't know but what we say we say let's take similar number of patents and uh assuming that most of them are not related to automo industry because many at the many many many huge like thousands of patents were produced every year. It means uh most of them would likely be indeed not related to auto
(15:18) industry which means we could label them zero. It is just not 100% correct but most of most of the time it is correct. That's exactly what we did here. So this is my this is my data set. Then we say creating training data set if it matches those companies label is one. Then we take sample of similar size and we label them zero.
(15:45) So we believe that if they are outside of basically this specific companies which were working on this industry they most likely were not related to automo industry and we label them zero. So this becomes training data set this way. one zero one and so on. Any questions on how we create a training data set.
(16:10) Okay, then we can use some kind of preliminary classification. In this case, I say let me use TF. We run it on my training data set and then we run logistic regression. So we can run logistic regression and uh use titles and we get this results right. So now I want to I want to mention one thing actually turns out that we because we have uh a lots of lots of different words it becomes quite uh quite difficult to train models because we have a lots of a lots of inputs.
(16:54) No maybe if you use like network words it is not a problem but if you use like classical models it becomes huge problem because every word basically every unique word will produce corresponding TF IF it means we have to think how to maybe reduce dimensionality of this problem so what we have we are going to have a data set which consist of two types of observations let me maybe sketch We are going to reduce the nationality based on t tests. Let me explain what it means.
(17:46) So in this case I say let me take a particular term let's say engine particular term and we are going to compute corresponding frequencies right TF 7.1 and so on five it is like first type of observations and then also have those which are not related to automobile industry maybe 0.1 2 and so on 15 so what do I mean by this I mean that we have like labels one is related we know is related we know is related we know it is not related not related not related it will be ultimately output from the model and this is formally speaking exactly my column which is
(18:39) called wheels with within wheels wheels within a wheels this way. So what do we say? We actually run t test. We are going to say let me focus on particular term like engine. Let me look at this specific sample which belongs to the first type of patent. Second sample is belongs to second type of patents and we run t test.
(19:13) So we basically run classical we run classical t test and then based on p values we are going to take most important ones right I mean if n engine for example has p value which is 7 you probably don't want to take engine in this case engine most likely is going to be significant one most likely engine will have t test which is large it means p value which is which is small maybe in this case there is significant difference those which are related to automobiles will have a large TF ID of those which are not we
(19:50) will have small term frequency or document frequency it isn't the case we observe it so if there is a difference which corresponds to P value like 0.1 okay we're going to keep it we're going to keep this term for the second term we may not want to want to keep it if P value is also not small that's exactly how we decide it's exactly how we reduce dimensionality of this problem you And now look at now you can see that we run here we present like this terms and and so on.
(20:21) Do you want to use and for classification? Maybe not. Maybe it is not for so interesting right? What's the difference between I mean why should it be different differently occurring in first type of data and second type of data? Probably not. That's why we probably don't want to keep it. The question is how to remove it. That's exactly what we do.
(20:42) We run t test and say uh my uh t test which I present over here my t test it will look this way p value and count if p value is small it means you want to keep it then I say let me take uh threshold in this case I say I want to keep maximum like 300 most important terms according to p value you can see what I do I take p values I sort them and I take smallest P values.
(21:13) Smallest 300 turns will be used as my as my features which I use to to build my models. They come again they come from titles at this point. Then I run logistic regression. I do classification and basically based on title I can classify it. No using logistic regression. So I hope ID is transparent what what I did here. Right. Questions? I just have a basic question.
(21:37) I I have heard many times about the t test but can you just go over um the fundamental idea of it of the test please. So basically we assume that there are two populations. So we run in this case t test for the difference between for the difference between between two population means there is one distribution around mu1 there is second distribution around some kind of mu2 and we ask a question is mu1 different from mu2 or not how can we get it how can we answer this question we can take a sample One sample will consist of these numbers
(22:29) from the first population and second sample will consist of numbers from the second population this way. this way right? Okay. We observe from sample that there is a difference some kind of difference between averages average here average over there they are different.
(22:54) There is sufficient evidence to claim that my expect my expectations basically my means are different. It depends on t test. It depends on how significant this difference is. How do we construct t test? We simply take let me call uh let me call uh this sample x1 bar is like average of my observations right x1 bar denotes average xar 2 denotes average of my observations from second sample I'm going to construct my t test now which is simply x1 bar minus x2 bar minus let me just say that's it just a difference between averages.
(23:35) No difference between averages is always there. Difference between averages will always be there because we by random chance shouldn't be getting same results. Right? Even if mu1 and mu2 are exactly same. You take one sample I take second sample we are going to have different averages. It doesn't mean that populations are different.
(23:55) It means we have to actually compute the difference and we have to normalize it by standard deviation. Now in this case if we assume that my sample sign dependent we can get sample variance in first case over number of observations plus sample variance in second case over number of observations will be estimator of my standard deviation of this quantity.
(24:20) No it is basically standard error. It means the statistic is nothing but difference between averages measured in standard deviations. And we say it is let's say example example it is 2.9. So what 2.9 means? 2.9 means difference between averages is basically 2.9 of standard deviations. It is huge.
(24:48) It means uh it is probably it is probably not by random chance. So this probably indication that there is difference between mu1 and mu2. People construct this you know so-called normal curve or whatever t curve it depends on what assumptions you use.
(25:07) Let's say this is like because of this t test let's say t distribution you place here 2.9 and you compute p value. So basically people instead of looking at t test which is already informative they look at so-called p value which is area over there and also area from negative infinity to9 over here this area together is what we call p value right the p value is supposed to be small in my case why small because test itself is large and far away it means those tails are small let's say I 07 small.
(25:48) If P value is small, let me say P value is small. The smaller the better. The smaller, the more evidence that we have difference. Then we say that evidence that mu1 is not equal to mu2 because difference between averages is large. Difference me and standard deviations is large. It means we can conclude that means are different. If this difference is just.1.
(26:22) 1 of standard deviations we can say okay this just fluke fluctuations right. It doesn't indicate that mu1 and mu are different. That's why it is interesting if you want to indeed take those terms which correspond to different different uh frequencies you better look at those which have smallest p values. That's why we do it that way.
(26:46) We sort it and take smallest p values. It means the cases where differences basically smallest obser difference is largest smallest difference is largest. the difference as long as measured as deviations. Any further questions? No, thank you. That's very clear. Yeah.
(27:06) So that's how we run used to test and now we can based on title using this case logistic regression, right? We can assign this labels. Now we can plot basically number of firms over time or I would say number of patents over time which are related to automobile industry and you can see that also grows growth growth and then it got got stable right it is somewhat correlated with this chart then you can study things like how firms which enter this market impact number of patents how number of patents impact firms difficult question because causality is always much more difficult question. We can see clearly some correlations. Now uh we can do
(27:46) similar stuff using descriptions. Description is more informative. Title is clearly less informative but description is more informative. We can use description using exactly same idea as before. We can choose terms which are more important. We can say now we look at descriptions.
(28:09) These are my terms attent and so on. Off. Should we use off? Maybe not. The test will tell us should we use off or not as a term? Probably not. Then we use the same idea. We sort P values. the IP values and we basically keep in this case 600 of most important terms which are found in descriptions and uh we get this uh labels based on descriptions.
(28:43) Based on descriptions we say it is automo related auto industry based on title it is not. Second one truly harp. Yes. Yes. B. Yes. Yes. And so on. That's how we can quantify. Now you can ask a question. Why not to combine all these things together? Yes, we indeed can combine. We can concatenate.
(29:08) Basically when we run logistic regression for example why not to concatenate and say we have terms from from title terms from from documents. We can concatenate together. This is just chart. We canite together based on both titles and descriptions. In this case, I simply say let me uh concatenate titles, descriptions, titles, descriptions together. So, I got them together.
(29:42) Right? It means I'm going to have 900 terms now. 300 from titles. Now I use words like engine and so on. 300 most important ones from descriptions. 600 most important ones. Then I run my logic logistic regression in this case. And I get label based on both one one one and so on.
(30:08) This is probably better better model and I get my results this way now. So any questions about that? Yeah, I have a a a question. Sorry, I I think I just a bit lost here. What are we trying to to find though? Because I mean, if we go back to the data frame, we have based on title. What's based on title? Or I guess I'm just so output output which are trying to label which we're trying to assign is indication if this belongs to automo industry or doesn't. Okay.
(30:43) Okay. So, we have a pile of patents and we're trying to figure out which ones belong to a particular company. Yes, we have uh in this case um um 141,000 right? 142,000 patents and we want to classify them which one belongs which one doesn't. Okay, thank you. Yeah, we create this type of train data set once where is signed based on companies which are definitely related to this industry and zeros.
(31:13) It is extra tricky part zeros is signed sort of artificially. Now we take a random sample of around 2,00,000 per patterns and we say random sample most likely since we have so many different patterns right most likely will not contain many mislabeled many patents related to auto industry which means we can safely make it zero now of course later you have to manually also look all kind of check them check them as well and remove those which are not related so basically it is important to kind of remove it in this case you just Take as you saw random
(31:48) sample it means we don't really remove anything but in practice it is better to also do manual work and make sure that you don't keep something which is definitely related and this way we create training data set 200 plus 2,000 and we use it to classify 1,42,000 patents for the training data set did we consider the title and description or just the title for the training data that when we so when we create data set we simply take everything a variable right and we assign label one or zero. When you say when we
(32:27) fit a model we have three types of three types of models based on based on title. It is one model 300 inputs most important ones which found in titles. Second one based on descriptions 600 most important terms found in descriptions and also third model based on title and description together remember we concatenate 300 terms from title 600 terms from description we have 900 terms 900 input to logistic regression and we run the model so different different models uh sorry about that I see so for the training it was the entire patent that
(33:12) we looked at to label Yeah, we just label patent. Yeah, but when we when we talk about uh when we talk already about features which we use, it depends on the model. If you use first model only titles, second model only description, third model everything. But when we label, yes, we just label entire patent. Yeah. Yeah. So it is okay. Related.
(33:40) That's what we do, right? So now uh now this is what we get. So now model selection. So this is something which we probably have to discuss right. Uh we kind of uh so we kind of have to understand that we have to look at such things as test performance. Right? Let me see if I have some nice table where I have different models.
(34:24) Let me let me just show you results. Um we have logistic regression and we talk about performance in this case uh and different models. So this is my uh performance which is accuracy score. Okay, right here accuracy score. I sort of pull it from here from this um accuracy underscore score and store my accuracy based on test observations into this performance then I display it.
(35:03) So basically if I I'm talking about a regression I get 72% run for 76% and so on for the for the accuracy that's what I get as my result for the test accuracy so that's how we get it in this case new network is not exactly the best one just some kind of new network let's see what we can do else so basically we can say let us try to I did some kind of uh random forest.
(35:37) Let me let me jump to neural network. So in this case I say let me try to now build some sort of custom neural network. I'm going to use 16 neurons on the first layer, 16 neurons the next layer, 16 neurons the next layer and then two neurons neurons for the output. In this case, I use soft max that's why two neurons you could use. You could use um sigmoid one neuron would be sufficient for the output.
(36:10) This is my neural network. You can see how many parameters I have in this case, right? So total number of parameters is almost 15,000. Then we run RMS prop. It is my optimizer RMS prop. Uh loss is categorical cross entropy as we discussed metric which I display every time at the end is accuracy train number of epoxes 200 mini batch size and so on. Now what is RMS prop? Let me see if I have something to demonstrate.
(36:45) So in this case you can look at optimizers. Remember how we introduced graded descent and also min graded descent. We just say w minus alpha times derivative. But there are some kind of modifications like RMS prop root mean square propagation.
(37:05) What do they do? They sort of if you divide by something, right? So basically what happens in this case they try to also twist the vector slightly. They try to somewhat rotate the vector because now my alpha becomes specific to the entry. You see sub index it is specific to the entry. Let let me say this way. If I'm talking about particular W, it means um I will adjust alpha in a very specific day specific way to this particular W. If I move to different W, I will adjust it differently.
(37:37) It means speaking RMS will kind of rotate my my vector, right? And uh that's what we get as RMS some kind of modification. This is one of the optimizers, right? We can also look how Adam looks. For example, Adam will also twist something else. Again, I will not spend too much time talking about different optimizers even though I really like this stuff, but we don't have much time. You can you can look at this yourself.
(38:09) So basically some kind of adjustments which are related to ID that you can slightly rotate your vector along which you move which is what's happening here also you can collect u you can collect information from past transitions it is called momentum you get like gradient gradient graded gradient then you take all past gradients and take some kind of average average weighted sum of those gradients which you can use to move to to make the next move.
(38:42) Why not to kind of why to throw away previous gradients? You can take them as well and and make your transitions smoother. So now I take Ramas prop again I ask on the assignment I ask you to experiment with this experiment with parameters. I'm not expecting that you should understand all this uh optimizers. I will not actually cover it in this class in detail.
(39:08) You will have to read about those if you want to understand better. But at least for the practical kind of applications, you should be able to to apply those. So now I train it and I get my results. Training accuracy goes up almost forever. Validation accuracy becomes stable. No, maybe um after like 60 ps I should stop, right? And that's my accuracy. It has 16 16.
(39:38) It means like three hidden layers. Each of the each of them will have 16 neurons. And I will add it here. So this is already better performance is 8.81. So that's how we can use this type of models to do classifications. Uh yeah. So this is application. Any questions about this? So um at some point I'm not sure if it is.
(40:07) So I saw that you did um use the sigmo the the soft max with two neurons and it could be done using sigmoid in in practice. Is there a difference? Well um if you're if you're concerned there should should not be much difference. If you're concerned about like computations and so on, maybe sigmoid is slightly better because you have less parameters, but it's not like an issue in this case.
(40:33) If you have like two classes, maybe sigmoid is is is sufficient, right? They they work pretty similar to be honest. So it's not as important. I just like soft max because it is like generalizable. If you have like more classes, you can do it. Sigmoid is not generalizable. But sigmoid is perfectly fine just like logistic regression.
(41:00) Basically if you think about this logistic regression is what is neural network which has two input like a number of inputs and only one neuron and then sigmoid activation function. So logistic regression is exactly neural network based on sigmoid. So right so what is logistic logistic regression? I'm not sure if you know if if you know this type of model but essentially it is like x first and so on x n then bias one and output right away if I sketch it that way I say output is yhat.
(41:42) So basically logistic is exactly this type of function which uses sigmoid as actuational function. The simplest basically neural network which you can imagine is logistic regression. There is a difference when we when we optimize parameters of logistic regression we use maximization of likelihood in case of neural networks we use this optimizers to to minimize cost functions.
(42:02) In practice results are quite similar. So basically no difference. So regression uses sigmoid. If you want you can use it even for more complicated neural networks even for deep ones. But if you want you can use soft max. Typically if you like if you have more than two classes you have to use uh soft max for two classes it's not as important you can use one one neuron or two neurons for one neuron you have to use sigmoid for two neurons soft max sigmoid is okay actually I mean somehow I used like soft max but sigmoid is Okay.
(42:45) So now uh let me so about test error. Does anyone have any questions about test versus train error? Why do we look at test error not at the train error? Why do you have like this? I think I think I have a question. This kind of relates to the the homework as well. So when we um when we want to choose which validation accuracy I mean instinctively I know that we might think oh we should just pick the whatever has the highest validation accuracy that should just be our model but I I believe that's wrong right because as as soon as it flatlines that um I think you said
(43:28) that means it could be overfitting. I was wondering if you could Yeah. Yeah. So, so in in this case, so statement maximum validation accuracy is best result is correct. You just maximize validation accuracy and you are fine. What is validation accuracy? It is performance on completely new data set which neural network didn't see during training.
(43:58) So the logic is if your valation accuracy is highest, it means on the completely new data set it will also be basically highest. That's why maximization of accuracy is good idea. So that's what we do. Now you say we don't want to take maximum and soon when we talk about like training accuracy, we don't want to take we don't want to take maximum of training accuracy. You don't want to take this point because valation may go down as well later on.
(44:21) But um but what I'm asking though is let's say that the validation accuracy curve is what we're looking at. But let's say at like 175 for some reason it's just a little bit higher. Like it just kind of spiked a little bit at one at 175. I don't know. Um cuz cuz maybe um cuz I don't know may maybe at like in between 125 and 150 it's highest. You could use pandas to find what whatever the maximum validation.
(44:46) Yes, you can you can just find find the maximum. No issues. No, if you I mean even if this like slightly higher of course it is most likely just some kind of fluke. Yeah. So it is not guaranteed that next time you run it like on some kind of data it will be highest. Okay.
(45:04) But but why not to take the highest which you see? So you can take highest. Okay. I also had a question too that that I'm glad I I get to ask which is I actually don't understand um epochs. So is there any equivalent to an epoch in just normal machine learning like a polomial regression? Why are we getting different validation accuracies per epoch? Like like what's going on? So do you understand what epoch itself means? I I thought it meant like retraining or something but to be honest no I I I don't. Yeah. Yeah. So so so yeah. So epoch
(45:41) one epoch means we use data set once. One epoch means data set was used once. Two epoxes means data set was used twice. Let me say x1, x2 and so on, x n then output y and we have like data whatever it is 72.18.5 some kind of output let me say labels then and let me take more and that's it. So basically in my case I have six observations.
(46:47) six observations, right? And let's assume that mini batch size is equal to two this way. So what it means first of all how many times how often will I update parameters these two observations because mini batch of size two means this observations will be used to update my parameters. After the update I will get W first.
(47:16) First means like time time is equal to one right time is equal no like time is equal to one remember wt there is sub index then I use this two to get wc second update of w then I use those two to get w3 so after three updates I get w3 so w3 is what we get after so called one epoch. One epoch means I use my data set once.
(47:55) The question is how many updates do I make to W? It depends on my mini batch size. If mini batch size is two in this case, three updates will be happening. So are you saying that W1 and W2 somehow were used to determine W3? Is that what you're saying? What I'm saying is that there was initial initial choice initial choice of w 0. Uhhuh. Then when t is equal to 1, I would get w first as wus alpha * my gradient.
(48:31) Mhm. Gradient at point w to be precise is a vector. Let me emphasize is a vector. It is bunch of w's, right? So this way this way this way next t second will result and w2 which is w first when you say somehow we're used that's exactly how it was used right not to be precise um well gradient of j is actually is is actually just exact gradient let me say I have to be maybe more careful let me write it explicitly it is Average of gradient of loss function based on first observation of W0 plus gradient of loss function
(49:24) based on second observation at point W is how we updated. So you may remember that in case of mini batch gradient of J gradient of cost is estimated by average of gradients of losses. It means loss based on first point, loss based on second point based on first point based on second point and we get W first.
(49:48) Similarly, I take average of gradient of loss based on third point over here. This is third point W1 already plus gradient of loss based on fourth point. So when you say somehow updated this is exactly how it was updated, right? So we know how it works. Now of course if there are some kind of modification to modifications to optimizer there could be adjustments of alpha. It could be even specific to the to the parameters to the entry of W.
(50:20) So it means could be rotation as well. So w minus alpha and then gradient of my loss based on.5 WC plus gradient of loss based on 6 and essentially my let's say first a book is finished that's what I mean by first a book Okay. Okay. And then that makes me just dying of curiosity then.
(50:59) Well, how did how would a second epoch work? I mean, would it just be different? Well, we start over from the beginning. But wouldn't you just get the same results if you start over? No, no, no, no. First of all, current W is current W is current W is uh different, right? So, you start with W3 on the next. Yes. So, okay. Time four means W4.
(51:26) It is already second box starts w3 minus alpha * 12 again gradient of loss based on point first w already three plus gradient of l based on point second w3 and so on right and we continue this way after t = to 6 second epoch finish finishes. Now there is one one of course one one thing we usually don't use same points and same order when we start new book we shuffle first observations it means this first means maybe different data point we shuffle by default we shuffle we can switch it off if you want but typically we shuffle when I move to next first I shuffle observation that means first
(52:14) effectively will be already first in my kind of shuffled data set but it is not same first so It means first is maybe different and W is definitely different. All right, cool. Thank you. Yeah. And then what we do number of updates within one epoch could be huge, right? Could be huge. That's why what we do, we say let me compute J.
(52:41) Let's say of test, let's say J. Let's say simply J. I compute J as a function of uh my uh parameters right as a function of a box essentially. So what we do we sort of want to save computational resources and we say we actually plot only J for W3. It is what we call first epoch. First epoch finishes, we compute J for W3.
(53:22) Second epoch finishes, we compute J for W6 because two epox means six updates of W's. Third one, J W uh 9 and so on. In between we also have up updates of W's, but we don't compute those because we want to save resources. We know that one we know that intermediate result is also not so interesting.
(53:51) we kind of finish a poke and then we evaluate performance right so this J it depends on what J it is let's say it is some kind of train one for example train one similar you can compute test one if you compute your J based on test observations for the same W's you get test performance So now this is what it is. Again I will I assume that you understand what train performance test performance means.
(54:28) We sort of split observations into two two pieces. Train one test one or validation one. We compute all the things using train train data and then later we compute basically at the end at the end of every report we compute train performance that means we refer to train data data data set and also validation performance it means we refer to test data set how it does on train is not always informative because it may be a fitting case that's why we often refer to actually test performance when we decide which set of parameters to choose. So now second u second second u second
(55:15) uh section file in this case we talk about recardial networks we have example n features is equal to two it means basically we are talking about vectors which have two components number of time steps is 200 it means capital t which we discussed last time is 200.
(55:46) So there's like two numbers at t = to 1, two numbers at t = 2, two numbers at t is equal to 3 up to like 200 which is capital t is equal to 200 sequence of vectors. In every case we have two numbers and we build recurrent uh neuron one single recurrent neuron. This way we specify how many features we have. It means vectors of two two components every time.
(56:11) Then I say simple recurrent simpler than one in this case it means we are going to have single hidden state y itself is one dimensional I use really and then I take this output and I apply it I I pass it further to my my dense layer to my fored part this is what I get I get four parameters y4 because two dimensional right 2x's plus by 3 + 1 Y which comes back as input four that's my four parameters here we discussed last time how to get it now two recurrent neurons I specify two here specify to here it means it means effectively I'm going to have two hidden numbers Y and Y two hidden numbers if I
(56:56) want to compute number of parameters of this um uh layer of recurrent neurons which consist of two neurons if Actually it's going to be 10 again last time we discussed right y so every x has two right every neuron has to two plus bias and also those things come back that's why we have 10 so 300 neurons you can specify three you can get convinced why it is 18 so next one shortterm memory so remember we would say every kind of memory memory cell or I will say layer of recurren neurons is a kind of kind of kind of
(57:40) memory cell you can view it that way but inside of that memory cell you can erase neurons and you can place even sub networks that's how long shortter memory cell is designed in this case we specify LSTM 16 means every subn network inside of LSTM will have 16 neurons you may remember that LSTM consists of four sub networks every sub network will have 16 neurons No, it means effectively my memory memory itself is 16 dimensional in every case and I have two memories.
(58:16) It will result in 1,216 parameters. You can easily verify why it is. So if you look at the architecture of LSTM four sub networks, every sub network has 16 neurons. You can see that every neuron will accept two signals plus bias, right? And so on. And you can compute and also memory itself 16 and also plus another 16 will come back as input.
(58:42) It means you can also compute how many parameters are needed to handle those inputs from my memories will be exactly 1216 get recurrent unit let 16 and we get this one. It has slightly less parameters because we essentially don't have two memories. we have only one memory which is 16 dimensional but only one memory we combine two memories into one vector as a result we sort of save parameters and then we can use it to predict time series so in this case I say I'm going to define function which will generate time series in some specific specific way it is somewhat related to garch
(59:23) basically this function is very similar to what's known as uh uh garch right generalize out hersadastic general g is generalized out recessive hyroscodastic garch in this case only difference is I added this term artificially some kind of interaction term where I multiply things out it is typically not I mean this is not part of garch if you remove this term it becomes exactly gar if you're kind of curious how garch is that's exactly how garch is you can use my function to generate garch process If you add this term, it becomes kind of crazy. If you plot it becomes this way,
(1:00:05) it looks much worse than Garch because of nonlinearities, right? But neural network still can handle this type of uh time series because it doesn't assume like linear dependence. Garch in some sense assumes also linear dependence. Basically it is linear model.
(1:00:28) Uh even though there is dependence of my variance on the previous observations it is still kind of linear model but I take much more complicated case by the traditional interactions and then I say this is my time series. Now question is how to make predictions. So we can make predictions using recurrent network. In this case I say let me essentially create my data set this way. Remember I told you we can break into chunks.
(1:00:52) Uh first observation second third third is output. Second observation third forces output and so on. Basically basically I create my data set this way. I have two time steps output two time steps output. I can do it differently. Basically using same idea. In this case I did it manually.
(1:01:16) In this case I did it slightly better. I say let me use 14 time steps. I use this approach to generate my my my uh uh chunks of time series. Every chunk is of length 14 and output is one dimensional. So in this case you can notice one you can you may wonder why one why did I reshape it.
(1:01:45) I said let me reshape and sort of introduce one over here. You may say why is it so? Why do I need it if it is one-dimensional? So basically recurrent neural network will expect that you supply sequence of vectors. If every vector is one dimensional you still have to indicate that you can say it is one dimensional.
(1:02:02) It means you have to reshape and say there is this dimension specifically one dimension is required because again your recurren network expects a sequence of vectors which means every guy should be vector of a number of components maybe 10 maybe 100 right maybe one even if it is one you have to explicitly indicate this dimension that's why I reshape it explicitly to have it reshape I add this dimension and that's it once we have it I can break my time series into parts train test 80% of observations from the start will be trained remaining
(1:02:38) remaining 20% of observations will be test and that's it I train my time series let's say recurrent neural network with two recurrent layers so my number of features is one in this case because I have every vector of one dimensions not a scalar basically then I say number of time steps was like routine and I used dropouts for realization.
(1:03:03) It is to prevent everything fitting. We kind of get it. What it does on the flight will basically during training drop some of the neurons and train work on the next updates of parameters will drop neurons randomly update parameters. On the next update of parameters will drop neurons update parameters.
(1:03:23) It is done in order to prevent a refitting sort of intuitively. It means we not going to like nicely perfectly fit the model because we don't really want to feed the model on my train set. That's why we sort of kind of spoil this process on on purpose. 10% means probability of dropping particular neuron is basically 10%. We are going to drop to be specific in case of recurrent neurons we don't drop neurons we drop connections.
(1:03:54) Remember those connections which go back right we randomly drop connections 10% chance of dropping connections is what I define and the result when we train it during this updates on every update I will drop something on next update I will basically drop something differently because every time I drop something during one I poke I will drop things differently the result every everything is is already quite unlikely anymore more this is called dropout approach that's what we do here return sequences is true because I want to use my sequence of vectors as input to next neuron then return sequences false
(1:04:32) because I want to use only a last last hidden state which I push further to my single neuron because I want to produce output of my next observation and that's it this is my new network optimizer is Adam loses mort error because I'm doing predictions then I say batch size I specify validation data because I want to display my performance it gets smaller smaller smaller and I plot it so my train and test MSE now we can similarly do LSTM the only change is I use LSTM in this case I say two previously I would say somewhat like
(1:05:16) also two Okay. Also two and six in this case I say two and four. Now I chose I chose slightly less because the list itself has like inside of it sub networks that's why I chose somewhat less and uh also training and we get this results uh then I plotted red plot my predictions one step ahead predictions versus whatever was actually observed and that's how it looks.
(1:05:59) Any questions? Okay, let's now stop. Is the ideal number of epochs just the smallest MSE? Say it again. Is the ideal number of epochs just the smallest MSE? Yes. In this case, ideal number of epochs is smallest test MSE. Test one. Red one. Test one. All right. Thank you.