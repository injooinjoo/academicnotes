%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Harvard CSCI E-89B: Introduction to Natural Language Processing
% Lecture 02: TF-IDF and Recurrent Neural Networks
% English Version
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}

%========================================================================================
% Basic Packages
%========================================================================================

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% --- Page Layout ---
\usepackage[top=20mm, bottom=20mm, left=20mm, right=18mm]{geometry}
\usepackage{setspace}
\onehalfspacing
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% --- Table Related ---
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\usepackage{longtable}
\renewcommand{\arraystretch}{1.2}

%========================================================================================
% Header and Footer
%========================================================================================

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textit{CSCI E-89B: Introduction to Natural Language Processing}}
\fancyhead[R]{\small\textit{Lecture 02}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.3pt}

\fancypagestyle{firstpage}{
    \fancyhf{}
    \fancyfoot[C]{\thepage}
    \renewcommand{\headrulewidth}{0pt}
}

%========================================================================================
% Color Definitions
%========================================================================================

\usepackage[dvipsnames]{xcolor}

\definecolor{lightblue}{RGB}{220, 235, 255}
\definecolor{lightgreen}{RGB}{220, 255, 235}
\definecolor{lightyellow}{RGB}{255, 250, 220}
\definecolor{lightpurple}{RGB}{240, 230, 255}
\definecolor{lightgray}{gray}{0.95}
\definecolor{lightpink}{RGB}{255, 235, 245}
\definecolor{boxgray}{gray}{0.95}
\definecolor{boxblue}{rgb}{0.9, 0.95, 1.0}
\definecolor{boxred}{rgb}{1.0, 0.95, 0.95}

\definecolor{darkblue}{RGB}{50, 80, 150}
\definecolor{darkgreen}{RGB}{40, 120, 70}
\definecolor{darkorange}{RGB}{200, 100, 30}
\definecolor{darkpurple}{RGB}{100, 60, 150}

%========================================================================================
% Box Environments (tcolorbox)
%========================================================================================

\usepackage[most]{tcolorbox}
\tcbuselibrary{skins, breakable}

\newtcolorbox{overviewbox}[1][]{
    enhanced,
    colback=lightpurple,
    colframe=darkpurple,
    fonttitle=\bfseries\large,
    title=Lecture Overview,
    arc=3mm,
    boxrule=1pt,
    left=8pt, right=8pt, top=8pt, bottom=8pt,
    breakable,
    #1
}

\newtcolorbox{summarybox}[1][]{
    enhanced,
    colback=lightblue,
    colframe=darkblue,
    fonttitle=\bfseries,
    title=Key Summary,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
    #1
}

\newtcolorbox{infobox}[1][]{
    enhanced,
    colback=lightgreen,
    colframe=darkgreen,
    fonttitle=\bfseries,
    title=Key Information,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
    #1
}

\newtcolorbox{warningbox}[1][]{
    enhanced,
    colback=lightyellow,
    colframe=darkorange,
    fonttitle=\bfseries,
    title=Warning,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
    #1
}

\newtcolorbox{examplebox}[1][]{
    enhanced,
    colback=lightgray,
    colframe=black!60,
    fonttitle=\bfseries,
    title=Example: #1,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
}

\newtcolorbox{definitionbox}[1][]{
    enhanced,
    colback=lightpink,
    colframe=purple!70!black,
    fonttitle=\bfseries,
    title=Definition: #1,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
}

\newtcolorbox{importantbox}[1][]{
    enhanced,
    colback=boxred,
    colframe=red!70!black,
    fonttitle=\bfseries,
    title=Important: #1,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
}

\let\cautionbox\warningbox
\let\endcautionbox\endwarningbox

%========================================================================================
% Code Block Settings
%========================================================================================

\usepackage{listings}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstset{
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{lightgray},
    keywordstyle=\color{darkblue}\bfseries,
    commentstyle=\color{darkgreen}\itshape,
    stringstyle=\color{purple!80!black},
    numberstyle=\tiny\color{black!60},
    numbers=left,
    numbersep=8pt,
    breaklines=true,
    breakatwhitespace=false,
    frame=single,
    frameround=tttt,
    rulecolor=\color{black!30},
    captionpos=b,
    showstringspaces=false,
    tabsize=2,
    xleftmargin=15pt,
    xrightmargin=5pt,
    escapeinside={\%*}{*)}
}

\lstdefinestyle{pythonstyle}{
    language=Python,
    morekeywords={self, True, False, None},
}

%========================================================================================
% Table of Contents Styling
%========================================================================================

\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\setlength{\cftbeforesecskip}{0.4em}
\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsubsecfont}{\normalfont}

%========================================================================================
% Graphics and Tables
%========================================================================================

\usepackage{graphicx}
\usepackage{adjustbox}

\usepackage{caption}
\captionsetup[table]{labelfont=bf, textfont=it, skip=5pt}
\captionsetup[figure]{labelfont=bf, textfont=it, skip=5pt}

%========================================================================================
% Mathematics
%========================================================================================

\usepackage{amsmath, amssymb, amsthm}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

%========================================================================================
% Hyperlinks
%========================================================================================

\usepackage[
    colorlinks=true,
    linkcolor=blue!80!black,
    urlcolor=blue!80!black,
    citecolor=green!60!black,
    bookmarks=true,
    bookmarksnumbered=true,
    pdfborder={0 0 0}
]{hyperref}

\hypersetup{
    pdftitle={CSCI E-89B: Introduction to NLP - Lecture 02},
    pdfauthor={Lecture Notes},
    pdfsubject={TF-IDF and Recurrent Neural Networks}
}

%========================================================================================
% Other Useful Packages
%========================================================================================

\usepackage{enumitem}
\setlist{nosep, leftmargin=*, itemsep=0.3em}

\usepackage{microtype}
\usepackage{footnote}
\usepackage{url}
\urlstyle{same}

%========================================================================================
% Custom Commands
%========================================================================================

\newcommand{\important}[1]{\textbf{\textcolor{red!70!black}{#1}}}
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\term}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\defterm}[2]{\textbf{#1}\footnote{#2}}
\newcommand{\newsection}[1]{\newpage\section{#1}}

%========================================================================================
% Title Style
%========================================================================================

\usepackage{titling}
\pretitle{\begin{center}\LARGE\bfseries}
\posttitle{\par\end{center}\vskip 0.5em}
\preauthor{\begin{center}\large}
\postauthor{\end{center}}
\predate{\begin{center}\large}
\postdate{\par\end{center}}

%========================================================================================
% Section Title Spacing
%========================================================================================

\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1.5em}{0.8em}
\titlespacing*{\subsection}{0pt}{1.2em}{0.6em}
\titlespacing*{\subsubsection}{0pt}{1em}{0.5em}

%========================================================================================
% Meta Information Box Command
%========================================================================================

\newcommand{\metainfo}[4]{
\begin{tcolorbox}[
    colback=lightpurple,
    colframe=darkpurple,
    boxrule=1pt,
    arc=2mm,
    left=10pt, right=10pt, top=8pt, bottom=8pt
]
\begin{tabular}{@{}rl@{}}
$\blacksquare$ \textbf{Course:} & #1 \\[0.3em]
$\blacksquare$ \textbf{Lecture:} & #2 \\[0.3em]
$\blacksquare$ \textbf{Instructor:} & #3 \\[0.3em]
$\blacksquare$ \textbf{Objective:} & \begin{minipage}[t]{0.70\textwidth}#4\end{minipage}
\end{tabular}
\end{tcolorbox}
}

%========================================================================================
\begin{document}
%========================================================================================

\thispagestyle{firstpage}

\metainfo{CSCI E-89B: Introduction to Natural Language Processing}
    {Lecture 02 -- TF-IDF and Recurrent Neural Networks}
    {Dmitry Kurochkin}
    {Master TF-IDF for text quantification, understand RNN architecture for sequential data, and learn about LSTM/GRU for overcoming vanishing gradient problems}

\tableofcontents

\newpage

%========================================================================================
\section{Introduction: From Text to Numbers}
%========================================================================================

\begin{overviewbox}
The fundamental challenge in Natural Language Processing is converting human language into numerical representations that machines can process. This lecture covers two major topics:

\textbf{Part 1: TF-IDF}
\begin{itemize}
    \item How to quantify the importance of words in documents
    \item Using statistical tests (t-test) for feature selection
    \item Application: Patent classification
\end{itemize}

\textbf{Part 2: Recurrent Neural Networks}
\begin{itemize}
    \item Processing sequential data (time series, text)
    \item Understanding weight sharing in RNNs
    \item The vanishing gradient problem and its solutions
    \item LSTM and GRU architectures
    \item Bidirectional RNNs
\end{itemize}
\end{overviewbox}

%========================================================================================
\newsection{TF-IDF: Term Frequency-Inverse Document Frequency}
%========================================================================================

\begin{definitionbox}{TF-IDF}
\textbf{TF-IDF} (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document within a collection (corpus). It combines two metrics:
\begin{itemize}
    \item \textbf{TF (Term Frequency)}: How often a word appears in a specific document
    \item \textbf{IDF (Inverse Document Frequency)}: How rare or common a word is across all documents
\end{itemize}
\end{definitionbox}

\subsection{The Intuition Behind TF-IDF}

\begin{summarybox}
\textbf{Core Idea:} A word is important for a document if:
\begin{enumerate}
    \item It appears frequently \textit{within} that document (high TF)
    \item It appears rarely \textit{across} all documents (high IDF)
\end{enumerate}

Words like ``the,'' ``is,'' ``and'' have high TF but low IDF (they appear everywhere), so their TF-IDF is low. Domain-specific terms like ``transformer'' in ML papers have high TF-IDF.
\end{summarybox}

\subsection{Computing TF-IDF Step by Step}

\begin{examplebox}{TF-IDF Calculation}
Consider three documents:
\begin{enumerate}
    \item ``the cat sat on the mat'' (6 words)
    \item ``the cat did something again'' (5 words)
    \item ``some sentence but no cat'' (5 words)
\end{enumerate}

\textbf{Step 1: Calculate TF for ``cat'' in Document 1}
\begin{equation}
\text{TF}(\text{``cat''}, \text{Doc 1}) = \frac{\text{Count of ``cat'' in Doc 1}}{\text{Total words in Doc 1}} = \frac{1}{6}
\end{equation}

\textbf{Step 2: Calculate IDF for ``cat''}
\begin{equation}
\text{IDF}(\text{``cat''}) = \log\left(\frac{\text{Total documents}}{\text{Documents containing ``cat''} + 1}\right) = \log\left(\frac{3}{3+1}\right) = \log(0.75)
\end{equation}

Note: The $+1$ in the denominator is \textbf{smoothing} to prevent division by zero.

\textbf{Step 3: Calculate TF-IDF}
\begin{equation}
\text{TF-IDF}(\text{``cat''}, \text{Doc 1}) = \text{TF} \times \text{IDF} = \frac{1}{6} \times \log(0.75)
\end{equation}
\end{examplebox}

\subsection{IDF Variants}

Different libraries use slightly different IDF formulas:

\begin{table}[h!]
\centering
\begin{adjustbox}{width=0.9\textwidth}
\begin{tabular}{p{3cm}p{9cm}}
\toprule
\textbf{Variant} & \textbf{Formula} \\
\midrule
Standard & $\log\left(\frac{N}{df_t}\right)$ \\
Smoothed & $\log\left(\frac{N}{df_t + 1}\right) + 1$ \\
Probabilistic & $\log\left(\frac{N - df_t}{df_t}\right)$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{IDF Formula Variants ($N$ = total docs, $df_t$ = docs containing term $t$)}
\end{table}

\subsection{Normalization}

\begin{warningbox}
\textbf{Always Normalize!}

After computing TF-IDF vectors, normalize them to unit length:
\begin{equation}
\text{normalized}(\vec{v}) = \frac{\vec{v}}{||\vec{v}||}
\end{equation}

This ensures:
\begin{itemize}
    \item Document length doesn't bias the representation
    \item Cosine similarity calculations are meaningful
    \item Values are bounded and comparable
\end{itemize}
\end{warningbox}

\subsection{Train/Test Split Considerations}

\begin{importantbox}{Critical: IDF Uses Only Training Data!}
When applying TF-IDF to test data:
\begin{itemize}
    \item \textbf{TF}: Computed fresh for each test document
    \item \textbf{IDF}: Transferred from training data (never recomputed on test!)
\end{itemize}

\textbf{Why?} Computing IDF on test data would leak information from future data into your model---a form of \textit{data leakage}.

\begin{lstlisting}[style=pythonstyle]
from sklearn.feature_extraction.text import TfidfVectorizer

# Fit on training data ONLY
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(train_documents)

# Transform test data using trained IDF values
X_test = vectorizer.transform(test_documents)  # NOT fit_transform!
\end{lstlisting}
\end{importantbox}

%========================================================================================
\newsection{Feature Selection with t-Tests}
%========================================================================================

\subsection{The Dimensionality Problem}

When using TF-IDF, you create one feature per unique word in your vocabulary. This can result in:
\begin{itemize}
    \item Tens of thousands of features
    \item Computational expense
    \item The \textbf{curse of dimensionality}
    \item Overfitting risk
\end{itemize}

\textbf{Solution:} Select only the most discriminative words using statistical tests.

\subsection{t-Test for Feature Selection}

\begin{definitionbox}{Two-Sample t-Test}
The \textbf{t-test} determines if there's a statistically significant difference between the means of two groups.

For a word $w$:
\begin{itemize}
    \item Group 1: TF-IDF scores in documents with label 1
    \item Group 2: TF-IDF scores in documents with label 0
\end{itemize}

\textbf{Test Statistic:}
\begin{equation}
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}}
\end{equation}

Where $\bar{x}_i$ = sample mean, $s_i^2$ = sample variance, $n_i$ = sample size.
\end{definitionbox}

\subsection{Interpreting p-Values}

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{3cm}p{5cm}p{6cm}}
\toprule
\textbf{p-value} & \textbf{Meaning} & \textbf{Action} \\
\midrule
Very small (e.g., 0.001) & Strong evidence that the word distinguishes classes & Keep this feature \\
Small (e.g., 0.05) & Moderate evidence of difference & Likely keep \\
Large (e.g., 0.5) & No significant difference between classes & Remove this feature \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{p-Value Interpretation for Feature Selection}
\end{table}

\subsection{Feature Selection Pipeline}

\begin{lstlisting}[style=pythonstyle, caption={t-Test Feature Selection}]
from scipy import stats
import numpy as np

def select_features_by_ttest(X_train, y_train, feature_names, top_k=300):
    """
    Select top_k features with smallest p-values (most discriminative)
    """
    p_values = []

    for i in range(X_train.shape[1]):
        # Split by class
        class_0 = X_train[y_train == 0, i]
        class_1 = X_train[y_train == 1, i]

        # Compute t-test
        _, p_value = stats.ttest_ind(class_0, class_1)
        p_values.append(p_value)

    # Select features with smallest p-values
    p_values = np.array(p_values)
    selected_indices = np.argsort(p_values)[:top_k]

    return selected_indices, feature_names[selected_indices]
\end{lstlisting}

\begin{examplebox}{Patent Classification with t-Test Feature Selection}
\textbf{Task:} Classify patents as automobile-related or not (1895-1935 US patents)

\textbf{Process:}
\begin{enumerate}
    \item Extract TF-IDF features from patent titles/descriptions
    \item Run t-test for each word comparing:
    \begin{itemize}
        \item Group 1: Patents from known auto companies (label = 1)
        \item Group 2: Random sample of patents (label = 0)
    \end{itemize}
    \item Keep top 300 words with smallest p-values
    \item Train classifier (logistic regression, neural network)
\end{enumerate}

\textbf{Result:} Words like ``engine,'' ``wheel,'' ``chassis'' have tiny p-values and are selected. Words like ``and,'' ``the,'' ``of'' have large p-values and are removed.
\end{examplebox}

%========================================================================================
\newsection{Recurrent Neural Networks (RNN)}
%========================================================================================

\subsection{Why RNNs for Sequential Data?}

\begin{infobox}
\textbf{The Problem with Feedforward Networks for Sequences}

A standard feedforward network:
\begin{itemize}
    \item Treats each input as independent
    \item Has a fixed input size
    \item Cannot capture temporal/sequential dependencies
\end{itemize}

For language: ``The cat sat on the mat'' requires understanding word \textit{order} and \textit{context}.

\textbf{The RNN Solution:}
\begin{itemize}
    \item Maintains a ``hidden state'' that acts as memory
    \item Processes sequences one element at a time
    \item Shares weights across time steps
\end{itemize}
\end{infobox}

\subsection{RNN Architecture}

\begin{definitionbox}{Recurrent Neuron}
A \textbf{recurrent neuron} computes its output based on:
\begin{enumerate}
    \item The current input $x_t$
    \item The previous hidden state $h_{t-1}$
\end{enumerate}

\textbf{Formula:}
\begin{equation}
h_t = f(W_x x_t + W_h h_{t-1} + b)
\end{equation}

Where:
\begin{itemize}
    \item $h_t$: Hidden state at time $t$
    \item $x_t$: Input at time $t$
    \item $W_x$: Input-to-hidden weights
    \item $W_h$: Hidden-to-hidden weights (recurrent weights)
    \item $b$: Bias
    \item $f$: Activation function (typically tanh)
\end{itemize}
\end{definitionbox}

\subsection{Unrolling the RNN}

When we ``unroll'' an RNN across time steps, we see that it's equivalent to a very deep network where each layer shares the same weights:

\begin{verbatim}
   h_0=0 --> [RNN Cell] --> h_1 --> [RNN Cell] --> h_2 --> ... --> h_T
                ^                     ^                      ^
                |                     |                      |
               x_1                   x_2                    x_T
\end{verbatim}

\begin{importantbox}{Weight Sharing is Key}
The same weights ($W_x$, $W_h$, $b$) are used at every time step!

\textbf{Benefits:}
\begin{itemize}
    \item Dramatically reduces parameters compared to a fully connected network
    \item Allows processing sequences of any length
    \item Encodes the assumption that the same transformation applies at each step
\end{itemize}
\end{importantbox}

\subsection{Computing RNN Parameters}

\begin{examplebox}{Parameter Count Example}
For a simple RNN layer with:
\begin{itemize}
    \item Input dimension: $n_{in} = 2$
    \item Hidden state dimension (neurons): $n_h = 3$
\end{itemize}

\textbf{Parameters:}
\begin{align}
\text{Input weights } W_x &: n_{in} \times n_h = 2 \times 3 = 6 \\
\text{Recurrent weights } W_h &: n_h \times n_h = 3 \times 3 = 9 \\
\text{Biases } b &: n_h = 3 \\
\textbf{Total} &: 6 + 9 + 3 = \boxed{18}
\end{align}

\textbf{General Formula:}
\begin{equation}
\text{Parameters} = (n_{in} + n_h + 1) \times n_h
\end{equation}
\end{examplebox}

\subsection{Keras Implementation}

\begin{lstlisting}[style=pythonstyle, caption={Simple RNN in Keras}]
from keras.models import Sequential
from keras.layers import SimpleRNN, Dense

# Input: sequences of length 200, with 2 features per time step
model = Sequential([
    SimpleRNN(3, activation='relu', input_shape=(200, 2)),
    Dense(1)  # Output layer for prediction
])

model.summary()
# SimpleRNN layer: (2 + 3 + 1) * 3 = 18 parameters
# Dense layer: 3 * 1 + 1 = 4 parameters
# Total: 22 parameters
\end{lstlisting}

%========================================================================================
\newsection{The Vanishing Gradient Problem}
%========================================================================================

\subsection{Why RNNs Struggle with Long Sequences}

\begin{warningbox}
\textbf{The Vanishing Gradient Problem}

When training RNNs via backpropagation through time (BPTT), gradients must flow backward through many time steps. Consider the derivative of the loss with respect to early weights:

\begin{equation}
\frac{\partial L}{\partial W} = \sum_{t=1}^{T} \frac{\partial L}{\partial h_T} \cdot \underbrace{\prod_{k=t}^{T-1} \frac{\partial h_{k+1}}{\partial h_k}}_{\text{product of many terms}} \cdot \frac{\partial h_t}{\partial W}
\end{equation}

Each term $\frac{\partial h_{k+1}}{\partial h_k}$ involves the derivative of the activation function:
\begin{equation}
\frac{\partial h_{k+1}}{\partial h_k} = W_h^T \cdot \text{diag}(f'(z_k))
\end{equation}

\textbf{Problem:} For sigmoid/tanh, $|f'(z)| \leq 0.25$ or $|f'(z)| \leq 1$

When multiplied $T$ times: $0.25^{100} \approx 10^{-60}$ --- effectively zero!
\end{warningbox}

\subsection{Consequences}

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{4cm}p{5cm}p{5cm}}
\toprule
\textbf{Problem} & \textbf{Cause} & \textbf{Effect} \\
\midrule
\textbf{Vanishing Gradient} & $|f'| < 1$ multiplied many times & Early inputs have no effect on learning \\
\textbf{Exploding Gradient} & $|f'| > 1$ multiplied many times & Training diverges, loss becomes NaN \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Gradient Problems in RNNs}
\end{table}

\begin{infobox}
\textbf{Gradient Clipping for Exploding Gradients}

A simple fix for exploding gradients:
\begin{lstlisting}[style=pythonstyle]
if gradient_norm > threshold:
    gradient = gradient * (threshold / gradient_norm)
\end{lstlisting}

This caps the gradient magnitude while preserving direction.
\end{infobox}

%========================================================================================
\newsection{LSTM: Long Short-Term Memory}
%========================================================================================

\begin{definitionbox}{LSTM}
\textbf{LSTM} (Long Short-Term Memory) is a specialized RNN architecture designed to learn long-term dependencies by using \textbf{gates} to control information flow.

\textbf{Key Innovation:} A separate ``cell state'' $C_t$ that acts as a highway for information, allowing gradients to flow unchanged across many time steps.
\end{definitionbox}

\subsection{The Highway Analogy}

\begin{summarybox}
Think of LSTM as having an ``information highway'' (the cell state $C_t$):
\begin{itemize}
    \item Information can travel down this highway with minimal change
    \item \textbf{Forget Gate}: Controls what to remove from the highway
    \item \textbf{Input Gate}: Controls what to add to the highway
    \item \textbf{Output Gate}: Controls what to output from the highway
\end{itemize}

Unlike vanilla RNN where everything gets squashed through tanh at each step, the highway allows information to persist.
\end{summarybox}

\subsection{LSTM Gates}

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{3cm}p{4cm}p{6cm}}
\toprule
\textbf{Gate} & \textbf{Function} & \textbf{Computation} \\
\midrule
\textbf{Forget Gate} $f_t$ & What to forget from $C_{t-1}$ & $f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$ \\
\textbf{Input Gate} $i_t$ & What to add to cell state & $i_t = \sigma(W_i [h_{t-1}, x_t] + b_i)$ \\
\textbf{Cell Candidate} $\tilde{C}_t$ & New candidate values & $\tilde{C}_t = \tanh(W_C [h_{t-1}, x_t] + b_C)$ \\
\textbf{Output Gate} $o_t$ & What to output & $o_t = \sigma(W_o [h_{t-1}, x_t] + b_o)$ \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{LSTM Gate Functions}
\end{table}

\subsection{LSTM State Updates}

\begin{align}
\text{Cell State: } C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\
\text{Hidden State: } h_t &= o_t \odot \tanh(C_t)
\end{align}

Where $\odot$ denotes element-wise multiplication.

\subsection{Why LSTM Solves Vanishing Gradients}

\begin{importantbox}{The Gradient Highway}
In the cell state update:
\begin{equation}
C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t
\end{equation}

When computing $\frac{\partial C_t}{\partial C_{t-1}}$, we get $f_t$ (the forget gate).

If $f_t \approx 1$, gradients flow through unchanged! The network learns when to remember ($f_t \rightarrow 1$) and when to forget ($f_t \rightarrow 0$).
\end{importantbox}

\subsection{LSTM Parameter Count}

\begin{examplebox}{LSTM Parameters}
LSTM has 4 sub-networks (one for each gate), so:
\begin{equation}
\text{LSTM Parameters} = 4 \times (n_{in} + n_h + 1) \times n_h
\end{equation}

For $n_{in} = 2$, $n_h = 16$:
\begin{equation}
4 \times (2 + 16 + 1) \times 16 = 4 \times 19 \times 16 = 1216 \text{ parameters}
\end{equation}
\end{examplebox}

%========================================================================================
\newsection{GRU: Gated Recurrent Unit}
%========================================================================================

\begin{definitionbox}{GRU}
\textbf{GRU} (Gated Recurrent Unit) is a simplified version of LSTM that:
\begin{itemize}
    \item Merges cell state and hidden state into one
    \item Uses only 2 gates instead of 3
    \item Has fewer parameters while achieving similar performance
\end{itemize}
\end{definitionbox}

\subsection{GRU vs LSTM}

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{3cm}p{5cm}p{5cm}}
\toprule
\textbf{Aspect} & \textbf{LSTM} & \textbf{GRU} \\
\midrule
\textbf{States} & Cell state $C_t$ + Hidden state $h_t$ & Only hidden state $h_t$ \\
\textbf{Gates} & 3 (forget, input, output) & 2 (reset, update) \\
\textbf{Parameters} & $4 \times (n_{in} + n_h + 1) \times n_h$ & $3 \times (n_{in} + n_h + 1) \times n_h$ \\
\textbf{Performance} & Slightly better on very long sequences & Often comparable, faster to train \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{LSTM vs GRU Comparison}
\end{table}

\subsection{GRU Gates}

\begin{align}
\text{Update Gate: } z_t &= \sigma(W_z [h_{t-1}, x_t]) \\
\text{Reset Gate: } r_t &= \sigma(W_r [h_{t-1}, x_t]) \\
\text{Candidate: } \tilde{h}_t &= \tanh(W [r_t \odot h_{t-1}, x_t]) \\
\text{Output: } h_t &= (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t
\end{align}

%========================================================================================
\newsection{Bidirectional RNNs}
%========================================================================================

\begin{definitionbox}{Bidirectional RNN}
A \textbf{Bidirectional RNN} processes the sequence in both directions:
\begin{itemize}
    \item \textbf{Forward}: $x_1 \rightarrow x_2 \rightarrow \ldots \rightarrow x_T$
    \item \textbf{Backward}: $x_T \rightarrow x_{T-1} \rightarrow \ldots \rightarrow x_1$
\end{itemize}

The outputs are concatenated: $h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$
\end{definitionbox}

\subsection{Why Bidirectional?}

\begin{examplebox}{Context from Both Directions}
Consider: ``I love [MASK] because they are so fluffy.''

\begin{itemize}
    \item \textbf{Forward context}: ``I love'' $\rightarrow$ Could be anything
    \item \textbf{Backward context}: ``fluffy'' $\rightarrow$ Suggests animals (cats, dogs, etc.)
\end{itemize}

For tasks like sentiment analysis or translation, understanding the entire context (past AND future) often helps.
\end{examplebox}

\subsection{Keras Implementation}

\begin{lstlisting}[style=pythonstyle, caption={Bidirectional LSTM}]
from keras.layers import Bidirectional, LSTM, Dense, Embedding
from keras.models import Sequential

model = Sequential([
    Embedding(input_dim=10000, output_dim=32),
    Bidirectional(LSTM(32)),  # Output: 64 dimensions (32 forward + 32 backward)
    Dense(1, activation='sigmoid')
])

model.summary()
# Bidirectional LSTM: 2x the parameters of regular LSTM
\end{lstlisting}

\begin{warningbox}
\textbf{When NOT to Use Bidirectional:}
\begin{itemize}
    \item \textbf{Real-time prediction}: You can't use future information that hasn't arrived
    \item \textbf{Autoregressive generation}: Generating text word-by-word
    \item \textbf{Time series forecasting}: Future values are unknown
\end{itemize}

Use bidirectional for tasks where the entire sequence is available at once (classification, translation, named entity recognition).
\end{warningbox}

%========================================================================================
\newsection{Training RNNs: Practical Considerations}
%========================================================================================

\subsection{Learning Rate and Data Scaling}

\begin{warningbox}
\textbf{Why Default Learning Rates May Fail}

Default learning rates (e.g., $\alpha = 0.01$) assume your data is scaled!

If one feature ranges from 0-1 and another from 0-1,000,000:
\begin{itemize}
    \item The loss surface becomes elongated
    \item Gradient descent zigzags inefficiently
    \item May diverge or converge very slowly
\end{itemize}

\textbf{Solution:} Always scale your inputs before training.

\begin{lstlisting}[style=pythonstyle]
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Use same scaler!
\end{lstlisting}
\end{warningbox}

\subsection{Dropout for Regularization}

\begin{lstlisting}[style=pythonstyle, caption={RNN with Dropout}]
from keras.layers import LSTM, Dropout

model = Sequential([
    LSTM(32, return_sequences=True, input_shape=(timesteps, features)),
    Dropout(0.2),  # Drop 20% of connections
    LSTM(16),
    Dropout(0.2),
    Dense(1)
])
\end{lstlisting}

\subsection{Return Sequences}

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{4cm}p{4cm}p{6cm}}
\toprule
\textbf{Setting} & \textbf{Output Shape} & \textbf{Use Case} \\
\midrule
\texttt{return\_sequences=False} & \texttt{(batch, units)} & Classification, final prediction \\
\texttt{return\_sequences=True} & \texttt{(batch, timesteps, units)} & Stacking RNN layers, seq-to-seq \\
\bottomrule
\end{tabular}
\end{adjustbox}
\caption{Return Sequences Options}
\end{table}

%========================================================================================
\section*{Glossary}
%========================================================================================

\begin{table}[h!]
\centering
\begin{adjustbox}{width=\textwidth}
\begin{tabular}{p{4cm}p{10cm}}
\toprule
\textbf{Term} & \textbf{Definition} \\
\midrule
\textbf{TF-IDF} & Term Frequency-Inverse Document Frequency; measures word importance \\
\textbf{t-Test} & Statistical test comparing means of two groups \\
\textbf{p-Value} & Probability of observing data if null hypothesis is true \\
\textbf{RNN} & Recurrent Neural Network; processes sequences with memory \\
\textbf{Hidden State} & Internal memory vector passed between time steps \\
\textbf{Weight Sharing} & Using same weights at all time steps \\
\textbf{Vanishing Gradient} & Gradients becoming too small to update early weights \\
\textbf{LSTM} & Long Short-Term Memory; RNN with gates to preserve gradients \\
\textbf{GRU} & Gated Recurrent Unit; simplified LSTM \\
\textbf{Cell State} & LSTM's long-term memory pathway \\
\textbf{Gate} & Learned mechanism to control information flow \\
\textbf{Bidirectional} & Processing sequences in both forward and backward directions \\
\textbf{Epoch} & One complete pass through the training dataset \\
\textbf{Time Steps} & Number of sequential elements in an input \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

%========================================================================================
\section*{One-Page Summary}
%========================================================================================

\begin{tcolorbox}[
    colback=lightblue,
    colframe=darkblue,
    title=\textbf{CSCI E-89B Lecture 02: TF-IDF and Recurrent Neural Networks},
    fonttitle=\bfseries\large
]

\textbf{1. TF-IDF}
\begin{itemize}[noitemsep]
    \item $\text{TF-IDF} = \text{TF}(\text{word, doc}) \times \text{IDF}(\text{word})$
    \item High TF-IDF = frequent in document, rare across corpus
    \item IDF computed ONLY on training data
\end{itemize}

\textbf{2. Feature Selection with t-Test}
\begin{itemize}[noitemsep]
    \item Compare TF-IDF distributions between classes
    \item Small p-value $\rightarrow$ discriminative feature (keep)
    \item Large p-value $\rightarrow$ non-discriminative (remove)
\end{itemize}

\textbf{3. RNN Architecture}
\begin{itemize}[noitemsep]
    \item Hidden state: $h_t = f(W_x x_t + W_h h_{t-1} + b)$
    \item Same weights at every time step (weight sharing)
    \item Parameters: $(n_{in} + n_h + 1) \times n_h$
\end{itemize}

\textbf{4. Vanishing Gradient Problem}
\begin{itemize}[noitemsep]
    \item Gradients shrink exponentially over long sequences
    \item Result: RNNs can't learn long-term dependencies
    \item Solution: LSTM, GRU with gating mechanisms
\end{itemize}

\textbf{5. LSTM}
\begin{itemize}[noitemsep]
    \item Cell state $C_t$: information highway with minimal transformation
    \item 3 gates: Forget, Input, Output
    \item Parameters: $4 \times (n_{in} + n_h + 1) \times n_h$
\end{itemize}

\textbf{6. GRU}
\begin{itemize}[noitemsep]
    \item Simplified LSTM: merges cell/hidden state
    \item 2 gates: Reset, Update
    \item Fewer parameters, often similar performance
\end{itemize}

\textbf{7. Bidirectional RNN}
\begin{itemize}[noitemsep]
    \item Process sequence forward AND backward
    \item Concatenate outputs: $h_t = [\overrightarrow{h_t}; \overleftarrow{h_t}]$
    \item Use when full sequence available at inference
\end{itemize}

\end{tcolorbox}

%========================================================================================
\end{document}
%========================================================================================
