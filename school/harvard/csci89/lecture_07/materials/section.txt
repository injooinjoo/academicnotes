89 day 7 section - YouTube
https://www.youtube.com/watch?v=Z7VRt8jVJl4

Transcript:
(00:04) Hello. Hello. So last time we introduced Latin allocation. Remember we would create some kind of Latin topics and we would estimate probability with which every topic contributes to each document. Last time we also considered some examples but basically today we are going to almost like repeat repeat those examples using first of all Python and secondary using R.
(00:44) So first example is LD latent latent dish allocation using secret learn in this case allocation from secret learn first of all I create documents in my case I have eight documents in this case I typically prefer to have multiple documents because the whole point of doing this kind of procedure is try to understand what kind of topics each document consists of.
(01:10) If I pull everything together and if I say like it is like almost like one document it's not interesting because I want to kind of differentiate first document talks about cats second document talks about cats again third one about dogs the whole point of basically introducing lesson topics is this type of this type of uh no sort of almost like clustering I would say this type of unsupervised learning so now I simply take count vectorzer as before Right.
(01:39) I say stop words will be English. It means things like air, there the and they will be removed from the vocabulary. They are sort of, you know, kind of common anyway. They're not really helpful. I'm going to remove those. And then I create X which is essentially matrix which I train on my documents. I fit my transformer and I get my matrix A.
(02:04) And then finally I'm going to this is my X X matrix as you can see here. So it look this way first document means zero 0 0 means first document. All of those are basically going to represent first document and it looks a little bit strange maybe not convenient maybe not something you got used to but six means that basically six word from my vocabulary is presented two times as you can see there is no like fifth one from my vocabulary simply because it is zero now we use this type of kind of representation where we don't really don't really have to present zeros right
(02:50) so fifth is not there fourth is not there and so on because if you look at the vocabulary they basically don't present in my in my particular document in the first document so six means what if I have two words count count is two you can see what occurs twice clearly cats occurs twice it means basically six words in my vocabulary will be yet that's why I say it occurs two times 36 is what something else occurs once and so on and this basically describes my first document then I move to the second document and the band creatures and so on will be already one six word again
(03:36) cats occurs one time let's see okay cats indeed occurs one time and so on and this is how we represent it again the x is basically just counts as we got used to X will be for every document we compute sort of bag of words we compute like counts of every word essentially and that's how we present our X now next one is going to be operation which is basically fitting LDA no we maximize likelihood as we discussed this latend allocation this allocation from secret learn if you feed it and our uh uh x on this um sort of data is going to basically do the job. It will maximize
(04:26) likelihood. Remember how we discussed we assume that basically every document is generated in the following fashion like someone like a robot or whoever whoever generate text says let me first if you pick a topic for the first word and then first of all let let me pick a number of words the document will will consist of using post on distribution then let me pick a topic for first word then from this topic let me pick a word and I generate first word I move to the next one and procedure starts over. I say let me pick a topic. I pick it and I pick
(05:01) from this topic already. I pick it a word and I write second word and there is no time component. So it is clear that it is basically kind of bag of words. Whatever we generate in this case is kind of assumed to to be to to have no order, right? And then we maximize likelihood.
(05:22) What it means? It means we we kind of try to estimate maximal probability of getting what we get as a function of parameters. This way we get estimators of parameters. So at this point we say let allocation we kind of specify this create this in instance which is like this model and we say number of components will be two.
(05:41) Number of components means exactly number of latent topics we say will be two topics. Okay, this is decision we should have to make sort of a priority is like hyperparameter basically. Okay, two two topics I want to have random state just for reproducibility not as important but we typically have it right. So random state and then we fit it.
(06:02) So, LDA fit on my X. By the way, LDA object I will say this guy uh LDA will not actually will not know your vocabulary because X itself doesn't have any words anymore. It only has references, right? So, if you try to extract some kind of vocabulary, LD doesn't have it. It doesn't know about those. We simply supplied X matrix.
(06:28) If you want to know like vocabulary, you have to go back to your victory. That's where those vocabulary words from vocabulary are stored. You can use a syntax and sort of refer to your vocabulary from vtorizer. Now yeah. Um so you said that uh the first the first entry 06 that means the the first document in the sixth word. Yeah. Zero means the very first document which mean this one.
(06:57) So six means six in our vocabulary whatever. Yeah. So so that's why that's why there's a 36 cuz there don't seem to be 36 words in Yeah. There are no but but it is referring to the vocabulary. Vocabulary has so many. Okay. Perfect. Thank you. Remember how we did bag of word length of our bag of words. The length of this representation will be length of vocabulary.
(07:20) Yes, we'll have zeros and that's indeed what we have. For example, for the first document, only a number of things is different from zero. 1 2 3 4 5 6 7 8 9. And you can compute 1 2 3 and is dropped. 1 2 3 4 5 6 7 8 9. Cats is same. Four is dropped. Is is dropped.
(07:58) That means nine right that's what we have that's why only nine non-zero elements but whatever is in between like 05 04 07 they will have zero frequency zero count we don't we don't represent it because it is like kind of python is like very efficient tool when you have like sparse data when you have a lots of data it is much more efficient than than R actually it does in this kind of way so it is efficiently represented So now this um victorizer will have information about your vocabulary.
(08:31) If you have to access that have to refer to the six basically guy from victorizer. I mean if you essentially look at your get features names out you refer to the six one you will be getting those right. So um just give me one second. Um yeah and uh X itself doesn't have those terms because vocabulary is already kind of kind of stored aside and X doesn't have it.
(09:08) So now let me display basically top uh so let me just display let me display uh simply those uh uh topics probabilities and say this represents first document 05 05 is probability that first first topic probability with which first topic contributes to first document So 5% is contribution which comes from the first topic 95% contribution which comes from second topic.
(09:47) Second case 6% from first topic 94% from second topic and in next case it is going to switch 93% from the first topic and 7% from the second topic. Now topics are latent topics more than that when we feed it actually your LDA doesn't have any idea what your words are it doesn't see it even it means now kind of we have to understand what topic means it is latin topic we have to somehow extract this information from here you can already kind of guess if 95% uh contributes to first document it probably means that second topic is somewhat about cats essentially right because 95% of this kind of no kind of
(10:28) document is essentially second topic. It means second topic is most likely about cats. If you look at the third document it is on the contrary 93% comes from first topic. It means first topic is somewhat like that right somewhat like that you have to somehow create a label for that topic. It is Latin topic. This kind of difficult part like almost like manual work.
(10:56) Basically you have to understand how to create name for this topic and you say okay since 95 94% contributes from topic one most likely topic one is somewhat about dogs right you never know essentially but we can look at those documents which are mostly associated with particular topic and we can understand what topic is about. That's how we do it.
(11:16) There is also another way to do it. You can this is just nice way to plot it right document one consist of 5% of first topic 95% of second 6% of first 94% of second and so on it is same from here it is same same data I just wanted to present it this way so you understand what we are talking about for your reference now in this case um I can tell you that essentially you could look at Uh you could look at um your um most frequent words. Let me run it.
(12:04) So this ones topic one will have dogs enjoy long walks exploring. What is what is this? Remember every topic will have sort of words associated with this topic because during construction we assume that we pull words from particular topic right it means we can pull words with particular probabilities.
(12:25) If I say let me sort of arrange those um those those um those words and let me uh print out five most frequent ones. I would say five most associated words with particular topic and I will say with first topic dogs is most associated enjoy is a little bit less associated but still associated and so on with second topic I have cats pure love and so on so that's how you also can sort of extract label for the topic you can understand what topic is about typically in difficult cases when you have a lots of common words let's say especially if you don't remove stop words in those cases it's not really helpful because you will have a lots of things like and
(13:08) a there and so on even if it if even if you remove stop words you still may have something very common it means it maybe not very helpful to use just stop words so there is alternative to that which I I like much more you actually say let me on the contrary not just pull words which correspond to particular topic that is like top words basically ally we display to top words which correspond to particular topic we look at this prevalence and say if I'm talking about no let's say a second topic let's say first topic then this 94 is maximal it
(13:47) means seventh document is going to contain 94% of topic one it becomes like most associated in some sense with topic one because topic one contributes most to to to document seven. It means I can display document seven and say okay it talks about about dogs. It means most likely topic one itself is about dogs.
(14:12) Then you can display second most frequent one. After 94 you can say this one it means the third one and you get convinced that okay two most frequent documents given given particular topic will be talking about dogs. It means your topic is mostly about dogs. Most likely is about dogs.
(14:37) And second case is already second topic. Let's assume I don't know what it is about. I say let me take 95% as the top one. It means cats and so on. And second most frequent topic is going to be 93 90. No, first is the most frequent one and then which one and last is most frequent one. It means first and eight. So first cats, last is also cats.
(15:08) And that's what we're doing. Exactly what I display next. I say let me now literally look at this prevalence as I demonstrate and display the ones which which are the highest given topic right and display also corresponding documents. And from reading these documents it is a little bit easier to understand what this topic is about.
(15:31) If 95% like almost 95% comes from topic one almost 95% comes from topic one it means those are sort of nice representations of my topic and I say okay regularly dogs enjoy long walks and so on okay looks like it is about dogs right and second case exactly same I plot documents which have highest prevalence like top prevalence basically and I see cat p gently and so on cat softly and so on that means Okay, second is about cats.
(16:01) You see how powerful this tool is actually know this a simple example only eight documents and we are able to sort of sort of say sort of um uh um run this unsupervised learning and kind of not not really cluster but kind of say what document what each document consists of. So any questions about this approach LDA right in this case drawback of LDA I can tell you we don't have any any variables if someone who writes this documents have specific characteristics for example or if someone about whom would write this document has specific characteristics or maybe you pull news from different sources and you know that
(16:45) this comes from this journal this comes from that journal and so on. It means you actually want to maybe somehow add those characteristics from for example from where it was pulled as a part of your modeling. In case of LD we don't have this opportunity.
(17:03) That's why next time we're going to study as structural topic modeling. In that case you can also incorporate cover variables. It is even more powerful tool. Actually if you have more data if you have not just text but something else in addition to text that one is more powerful. So now we can also use LDA from uh John Sim from here. Yeah. One quick question.
(17:28) So can can we use the uh non-ext like images etc also to classify with LDA? H I've never heard about this actually application. Uh uh. Oh, let me think. So you're trying to say that your images will consist of two different topics and your topics will be will be kind of so you kind of think that your topic will be like theme about what all those images about basically right that's exactly like uh if I if I have a corpus of images with animals images I want to classify either it's a four-legged or two-legged so uh what is going Maybe your
(18:17) vocabulary. Let's think about this. What is your vocabulary? What is your unit of information? In this case, word kind of natural reference, right? One document talks about cats, second about dogs. In your case, you think about pixels probably, but pixels is a very bad idea because the meaning of um well, it's not just color, right? Mhm.
(18:45) You you can kind of classify by colors and say those are dark ones, those are light ones, this image is mostly dark, this image is mostly light. But it's not interesting. So you have to have some kind of unit of information which will differentiate between image about cars, image about something else, right? So if if you try to somehow first extract information about your image, basically if you try to describe it, let's let's say this way.
(19:11) Okay, in this image I see car and also dog running running running running you know nearby. in that image I see something else like a building and also you know airplane and then you do it then it will be basically applicable but it's not just pixel as you understand there should be unit of information maybe description of your image via sentence and then you can do it but it's going to be already again natural language processing but if you want to just say my unit of information is pixel it's not going to be helpful because
(19:42) pixel doesn't tell you what it is about it only tells you what is like dark or like light or kind of blue or red, right? Okay. You will get you will get ultimately that your image consist of you know 70% of blue and 30% of you know red colors. No, it's not interesting. Okay. Right. Okay. Correct. Yeah.
(20:10) Yeah. Yeah, but if you introduce some kind of um almost like description of your image know what I mean by this basically extract what what is in this image and then you right use those labels to this approach of course you can do it because you're going to eventually work with essentially work with text. Yeah. Yeah. Like vision model we can get the information. Exactly.
(20:32) Exactly. You can do it first and then do this kind of uh modeling of of the labels but it will be already kind of natural language processing applied on the top of those labels. Got it? But not like pixels because okay you will know that 30% is you know blue 70% is red and what it's not interesting right so now um next package is uh so LD basically LD but it comes from different package we can import English stop words in this case we can also um uh split it right and we don't use stop words and we use somewhat similar as before and apply this LDA to our corpus in this case our corpus
(21:29) will look slightly different this is how it is designed it's going to be like first document uh it's going to be like uh first word from the vocabulary is only once in my text. Second word from vocabulary is only once in my text and so on. Next word is only once. It is represented this way just different kind of way to represent it and then we do the same and we can also create um topics and say remember when we said what are most frequent words which correspond to particular topic once it is it is um it
(22:09) is it is estimated we can say topic one consist of enjoy which is like coming with four 4% probability which corresponds to topic one dog room client and so on. Second topic will consist of cats not consist of I don't say it correctly but it is like probability of basically choosing this particular word given topic two cat comes cats come in case with 4.8% 8% right chance and so on.
(22:44) That's how we can see it also we can sort of recover or we can similar to what this is the representation of our contributions from different topics topic one topic two it means first document mostly is about topic two which we will learn is about cats essentially then second document mostly is about first topic next mostly is about first topic um so it is okay this is probably so cats cats, dogs and I got second topic, first topic, right? So it is something is going on already. See, so different result. So it can be quite
(23:26) um quite uh misleading. It can be quite um um not clear what you mean by topic because who said that topic cannot talk about cats and dogs easily. So topic doesn't have doesn't know anything about cats and dogs. So topic can be combination of different things. That's why you have to be always like quite careful careful in order to interpret this results.
(23:51) Okay, there are topics but you may get impression that topic one is about cats, topic two is about dogs but maybe it is not the case, right? So just be careful and in order to understand you have to essentially again plot those ones display those ones which have highest prevalence. In this case again dogs two documents with highest prevalence and second case two documents with highest prevalence.
(24:23) Now it means probably second topic is mostly about cats but you have to be kind of careful. Now any questions about that? Now there is important question how to choose number of topics basically. So this is most important question because this is like your decision which you have to make. You cannot really uh well not the right techniques to kind of do it uh in a sort of systematic way right let me say on so-called coherence and exclusivity.
(24:54) So I mark on coherence and exclusivity. So there are two matrix I will not uh define them. Let me maybe better show you some references. So this is a famous publication STM right so let me show you it is about structural topic modeling and basically uh trivial example of STM is going to be LDA when you don't have coariance STM head more interesting it has also coariance models here we have also some kind of you know parameters and so on which we can take from also um data which are co variables but here on the way you can find definition of coherence
(25:54) and exclusivity. So I will not walk you through this formal you can read about this if you really want. I will just say that coherence basically tells you how u close words within particular topic you display like literally like we did like five top uh awards for example and you try to see how kind of here and they are right you look at some kind of correlations basically you can see what what happens here they they uh They kind of look at this type of metric which computes number of times words v and v prime occur together in the document. Now typically it is not in the document. Typically they use kind of
(26:44) sliding window and then see how often they accur sliding window and also the denominator which computes just frequency of particular word right in documents and uses kind of metric to to assess it is kind of metric of um correlations I would say of of your words this kind of transformation of correlation but this kind of representation of correlation of your words within particular topic.
(27:17) It is almost like if I said let me I compute somehow correlations between this and joy and let me look here my correlations between a dog and joy along and so on and understand that there are some correlations between dogs and enjoy because in text they occur together it means basically I kind of I'm able to estimate those correlations and then based on this I can let going to say first is uh coherence I will not produce formals but I will just explain coherence and in that case I'm going to say let's say topic one topic one consist of let's say dogs
(28:06) is my document right enjoy then it is long and so on so This is my these are my documents. So basically I can somewhat compute correlations. So dog enjoy almost calendar along as some somewhat caler or like long walks maybe dimension and so on. And I look at some kind of kind of correlation between things which occur together within top words.
(28:38) Typically there is like default parameters such as S10 top words we use in our at least. Well, you can change it clearly. Then second topic. Second topic is going to be uh like in my case uh cats for example then love and so on. I take like top again only top words and I compute correction. I sort of visually represented that way.
(29:08) Okay, cat look this way that that way that way and I say in first case my correlation is going to be somewhat like let's say 8 for example there is CV metric.8 eight in second case CV metric is point let's say 9 then I compute a sort of average metric right which corresponds to particular uh particular model so this is going to be CV average I have multiple topics it is a coherence within first topic coherence within second topic then I compute average and I get like 85 for example this what I sort of look at as a sort of coherence which corresponds to particular model right
(29:53) here. I can look at average or you can look at individual topics as well. So I know that this is just a sketch but is like visual presentation how to compute correlations basically how often they coeue in documents. We have to run through documents and see how often they co occur.
(30:13) That's exactly why this metric which tells me um where is it? um which tells me that I have to sort of compute uh co accurences normalize it properly take algar that take some of those things and so on or different indexes because I have to look at all cable all possible cables and this way we compute this symmetric so any questions about coherence So coherence is kind of powerful tool right we can even try to now plot it.
(31:04) So the first one is let's say y-axis is coherence and second axis is going to be different metric so called exclusivity. So exclusivity second one is uh so coherence is where is it so coherence is available so there is this function right I can compute coherence I don't have to actually this is type CV is type basically there are different types for coherence different packages also provide different things uh Python this this particular package provides you also options to select what type of coherence you can compute and so on top end you see top end 20 default is 20 I told you 10 in our I believe it is
(31:54) 10 in in this case 20 we look at 20 top words which correspond to particular topic and compute this kind of kind of co occurrences together take this transformation sum over all this possible combinations of indexes and we compute it.
(32:17) No basically procedure is straightforward if you just use this you know function right so you can um you can use it uh so I get 34 now there is second metric exclusivity so basic exclusivity tells you how different they are how different uh how different they are like my topics and we can uh plot now let's say u let's say I have my uh 85 in this case for example is my coherence let's say 85 is my coherence then I also compute my exclusivity let's say exclusivity is uh something.
(33:01) 7 for example go introduce it and now exclusivity exclusivity is second metric and I get this point so this point represents model. Basically when I select number of topics to be two in my example number of topics was two number of topics was to negate this point essentially. So my goal is to maximize coherence and exclusivity at at the same time.
(33:29) Let me even emphasize that exclusivity can be completely on different scale. It means it is more difficult than just okay this 7.7 uh in my example this 7 let me say 3.1 will be example different from the screen and I say I want to maximize both of them basically it's not like it's not like half of coherence plus half of should be maximized we have to somehow normalize it first it is already more difficult question how to properly do it we have to sort of maximize a weighted sort of sum of coherence exclusivity. Now maybe if you
(34:06) run if you run multiple things let's say four number of topics equals to four you get something here number of topics which is equals to uh three you get something over there this way over here and you can kind of visually inspect it and say it looks like if I want to maximize exclusivity and coherence at the same time Maybe this one is a winner like two topics.
(34:41) For example, if you don't like this approach, you can sort of standardize across X dimension, standardize across second dimension and sort of already maximize corresponding standardized values. I mean you can already take average literally average of standardized coherence and standardize exclusivity and maximize average will be kind of nice approach. So now in this case um exclusivity is not available. So I computed it.
(35:07) So let me try to explain how it how it works. Um exclusivity exclusivity uh it is like topic uh one topic two this way. Then I also look at the top particular words let's say 10 as is in my example and I say dogs enjoy long and so on not particular number of top words I have to take I don't want to take like all of them because if I take all of them topic one will will contain clearly all possible words it's not interesting topic two will consist of same words I always take top ones top for topic one let's say top 10 words for
(35:58) topic one top 10 words for topic two then second case I have cats and then love and so on so next what's going to happen next is I'm going to kind of um compute a u sort of relations between between not only correlations no not correlations I'm going to see uh how often I get the same word same words right so I have to to see how often the same word occurs I sort of look at dogans and compare it to every from there and see how often they sort of cure if you if you look already not through the same topic but through different topics and that's how we kind
(36:55) of compute put it. So let me see if I have a kind of nice uh representation. You can actually read it from code. Uh I always don't don't remember uh the specifics. You can read it from here if you want. Or maybe you can find uh find reference which looks a bit more readable.
(37:20) Let let me see what what better to do in this case. So I want to define exclusivity exclusivity we can start it from here. So we take top indexes, we take exclusivity, we take dictionary, we take probability in particular topic normalize it by total word probability and then exclusivity will be sum of those probabilities over number of words.
(38:01) So let me say structural topic modeling. I just took it from somewhere and and um programmed it but it was like a long time ago. So I may better refer to the original formula. Let me let me let me take some kind of reference. So the idea is you want to see how uh unique every every uh how different they are from each other basically.
(39:02) So let me see exclusivity So this is like one of the matrix um harmonic mean of the wor in terms of ex frequency. This is slightly different. Uh yeah by the way this is how they typically plot it and they say let us essentially plot every every specific um uh topic right 1 2 3 four means like different runs we have typically multiple runs of of the model even if you have same number of uh topics you have multiple multiple runs and then You say let us because thisic process when we maximize likelihood it doesn't reproduce all process we run it multiple times and we say red one for example first run and we
(39:54) look at different topics topic topic topic topic just as I told you for every topic I have its own like coherence for example so it means for every topic we can we can plot couple exivity and coherence and then we can take averages basically as I told you here of your coherences and that's where we place number One.
(40:16) Number one will correspond to average coherence and average exclusivity for particular model run. And then we simply say here okay which run is best? Not the one which has average closer to this corner. Basically maybe one is fine right? Maybe take one. One seems to be kind of fine. That's kind of idea behind this behind this uh uh selection step.
(40:43) I will I will not uh find now right now this definition of uh of exclusivity right but uh but you can see how I did it here. So it is probability over total probability. Reclusivity is defined as total probability will be sum of those probabilities and then I say So we define 10 words by default. Then
(41:52) we compute uh word distributions. We sort by uh probability in decreasing order that is second line this line. Then we take exclusivity. Then for every word in my top indexes which means I loop over this 10 words here I have to compute topic distribution and will be war probability in my topic and then I say total probability.
(42:38) I take sum of those and then exclusivity is going to be award probability in particular topic over total award probability uh over total what probability. Okay. So it is um I think it is something like um so must be something like something like I compute like roughly speaking probabilities like 0.1 and then I say one for example then I say one two so I I don't want to recover it from the from here.
(43:26) I just there's like 05 02 for example 1.01 0 01 then I say let me exclusivity. So they should be showing how different every every topic from the diff from the next topic basically. So I say that I want to get only those swords. So I have to think about this. I will probably I don't want to tell you something we should not correct.
(44:09) Well, let me see if So I think it must be something like if I'm not mistaken something like you look
(45:31) at second uh topic and you find same word among your highest highest 10 words. If you find it then you sort of connect in this way and say okay now I finally found it and let's say that this probability is somewhat like 01 for example then my uh my exclusivity exclusivity not for this particular word at least is going to be somewhat like point.
(46:11) 1 from here it occurs over over.1 plus and over there is like 07 for example and plus 07 it will be kind of you know kind of sort of fraction of probability which belongs to first topic if you refer to partic particular word and then you take it for every single word and get some kind of um uh sum of those uh fractions to define exclusivity somewhat like that.
(46:44) So now let me so how many minutes we have done let me now so we obtain exivity let me now say this is how we choose the max maximal exclusivity so the truth exivity is something which we actually don't really compute because if you run it in R R has a variable exclusivity you don't have to do it manually you don't worry much how this computes you just know that there is a package which does computation of coherence for you computation ofivity for you and you don't worry much about kind of you know uh making mistake here. So now let me uh
(47:19) open R and show somewhat similar in R. So any questions about anything? So number of topics uh uh professor you said how do we identify? Yeah. So in this case our goal to make coherence and exclusivity highest possible. If coherence grows exclusivity typically decreases right if exclusivity grows coherence decreases.
(47:57) That's what happen. That's why we have to sort of balance there is like they compete against each other. We have to balance those two and we choose typically we sort of almost like we almost like rescale it essentially and choose the one which has highest average of coherence and exclusivity.
(48:17) It is not like literally average because average wouldn't make much sense since they often on different scale. But if you kind of plot it and visually say okay this seems to be best. That's how people typically do it like literally visually you can incorporate some kind of metric where you literally scale exclusivity kind of this score of some kind this score of coherence and you see it right you you you can do it um let me show you one thing u so this is So this is how it looks in practice. is we
(49:24) take we plot the 60 and coherence this way and we say for example if I choose seven topics every cross corresponds to particular topic and color corresponds to a particular um run of the model. Then you take averages and you see here the one which is which which is closer to the kind of right corner.
(49:54) Basically that's how you kind of in practice do it. So if it makes sense what what I'm saying. So it is like almost like this area in some sense but since coherence could would look this way andivity could look completely differently. It's not just average of those in cocoherence after sort of rescaling right when you put it in the same box.
(50:20) Essentially you choose the one which has which is closest to the corner which has highest average. In this case for example you can you can formally define score if you want. You can say let us define score which is not just visual expectation but kind of nice score which you can use. Uh just give me one second. I'll find the Nice. Is this paper available in the file section, professor? No, no, no.
(51:40) Okay. Can it be made available or it's not possible? I send you the link. Okay, thank you. So I'm trying to find a place where so basically we can like let me just tell you what we can do. Formal is straightforward anyway. So we can sort of compute this score across one direction this score across second direction and then simply maximize average of those those scores which sort of equivalent to send that we are trying to take the one which is closer to this corner and we take maximum which corresponds to like highest rectangle
(52:36) here highest area of rectangle which means like topics in this case for example that's how typically we uh we do this type of thing type of make this type of decisions So any question there's a good question about how to make decision about maximal about optimal number of topics. There are different techniques as well you can try to maximize um so-called held out likelihood it's like almost like test like test likelihood basically. Yeah.
(53:05) So different techniques. So now uh let me open R and say you already had uh a section on R. Correct. That's correct. Yeah. Yeah. So as soon as you know how to install it and you know how to to work with this.
(53:35) So now in order to create reports similar to uh uh to Jupyter notebook reports, we can use so-called R mark Markdown files. RMD. So this is how it is. RMD. Then if I want to create my report, I need to say. So if I say NIT, it will run everything here and will create nice report for me which you can save as PDF even as HTML whatever you like. So it it will look this way. It will consist of code of your descriptions almost like Jupiter notebook.
(54:00) In this case I can say what title is right table of content true. I want to have it. If I say hashtag it means it is like section. If I have double hashtag will be subsection and so on. If I have this apostrophes opening closing whatever is in between will be basically our code which will be executed during your knitting. If you say include false it will be not displayed in your report but will will run sort of will will run during during knitting but it will not be displayed. If I say include true I don't have to say it. I can simply ignore it. If I
(54:34) ignore it, that means by default it will be included. Then my result will include this chunk of code as well as well as outputs, right? No outputs will be included included. So as you can see, first chunk of code is not there. This hashtag become my uh my uh uh title.
(54:56) I can click on it if I want, right? It will move me forward. And then this piece of code is here. And then output is over there quot output and so on. And now what we can do first of all if you at this stage when you try to uh try to create this file it means you can also like in case of notebook you can run it chunk chunk by chunk you don't have to run all things together in notebook case we call it cells in our world we call it chunks it is like official name chunk I think you can say here uh need where is it here run chunk particular chunk So chunk means particular chunk. I can say I want to
(55:36) run only this chunk over here. I can say it this way. It will it will run it. I can highlight part of it and say controll enter. It will run it as well. Or I can just place my cursor and say ctrl enter. It will run one line. So this is very handy handy way to actually uh debug your code.
(55:59) You can run it piece by piece or if you're at some particular location you just place your cursor and say run all chunks before above and it will run it for you and then everything is executed and now you can work with your particular chunk. You can run it one by one and so on. So basically you never have to uh create uh entire kind of document and run it. You can work with one chunk at a time.
(56:24) Basically if there are some dependent dependencies as you can say place it here run everything above everything above is executed and now you can work with your particular chunk and you can try to understand what you want to do here controll enter it will move you through the lines of code it will execute them or you can execute entire chunk if you want so sometimes if you have time you can just you know run it see what happens run see what happens and so When you run it this way, by the way, it means this particular variables will be here in kind of in console. It
(56:57) will be filled in console. It means they will be sort of available. You can also extend them. My documents for example will be available right here. So they are there. If I say remove list ls, then my documents are not available anymore. If I say let me run my entire document, it means let me need it. Documents in console will not be available. So there's my result. Okay, it will store it right away.
(57:30) HTML file by default in my case. If I say documents, it is not available because I didn't run it. If you want to during not during kind of, you know, um um during your um development phase, right, you can sort of run or you probably want to run everything here in console which means you place your cursor here and you say run all all of them above.
(57:55) So now it is here. Now you can say let me run my documents. Now my documents will be will be will be available and you can continue. So this is nice way to kind of run it or you can simply run all of them. It was also sometimes useful because it will bring all the variables all the things to your console will be executed there as well.
(58:16) So you know how to debug it. So now what's happening here? This is basically very similar to what we just discussed. Uh in this case I use library uh topic models. Then I say let me create documents. Now C means like combine it is like almost like vector of things.
(58:37) If if you if you will combine things corpus will be my documents. I create some kind of make it make it lower case of punctuation stop words and so on. So I just did a number of things which is pre-processing my data. Then document time frequency matrix can be obtained this way. I apply it to my corpus and then I take my LDA right my LDA from topic models and then I simply apply it to my document frequency matrix.
(59:11) I specify two topics and I say kind of seat right seat in this case and then I print my results. So you see what's how it looks basically it tells you that if I plot display first two documents it tells you that if this is my essentially vocabulary it tells me dogs has one occurs one times cats occurs two times and so on. That's how it looks.
(59:36) Now if I want to actually uh display also most frequent words there is a way to do it as well. As you can see topic one consist of those words topic two consist of those words those are most frequent ones using the same kind of approach right we extract the most frequent ones.
(59:58) If you decide that you want to display this way as we did before you can do it as well. Now I will not walk you through like details but you can see that everything is basically doable in R in the same same fashion we can say that most prevalent document which has most of topic one is document four which is about dogs and document seven which is about dogs as well and the second case topic two I can display my first two most relevant sort of documents those documents which consist a lot of topic two will be document one and document 8 is also about cats basically
(1:00:43) and that's how we can can do it in R any questions about that so next time we are going to discuss u discuss uh the structural topic modeling so It is going to be a little bit more advanced and you can see here of example result which shows how uh students evaluate teachers right in this case we able not only discovered topics.
(1:01:21) These labels are basically labels which we place for every discovered topic. We choose 11 topics in this example and we say uh some some students mention in documents caring and enthusiastic instructor. Some some mention lectures are interesting and relevant and you can see relations with gender for example which becomes quite interesting.
(1:01:44) So you in this case for example uh there were almost million of student relations. They were accompanied by data such as gender of instructor, department where they teach and so on. And we able to in this case extract information about where they how it how those latin topics correlate with gender for example.
(1:02:10) And this is quite interesting to see because you can see that when they talk about females they say about being kind of a caring professor, facilitate effective discussion and so on. Feedback is nice. When they talk about males, they talk about humor, about interestingly relevant and so on. There is some kind of bias. Clearly, students could be biased, right? Uh when they take female female the class taught by female professor or they taught by male professor, they may get different impressions.
(1:02:47) So um it is kind of interesting because uh if you control for departments and for type of class and so on you probably should not see much difference but this different this type of difference is consistently reported across different um different publications. So that's how we can use in practice. Again this is based on almost million of student evaluations.
(1:03:08) One almost one million of student evalations. Yeah, that's how we can use that in practice. Any questions? Or maybe you can plot it. Plot it versus u uh divisions. For example, in science, in science they say explain complex concepts effectively occurs a lot, right? Of course in humanities there are it is not what people discuss usually in for example uh science that facilitates effective discussions it's not going to happen as much in science and computer science right so because in that case professor typically teach like it is lecturing most most of the time this uh f
(1:03:54) y www means freshman se seminar freshman seminars locate positive uh timely feedback whatever. So that is quite interesting to see how this works in practice. It means you can actually work with a lots of text as many as like millions of relations and you can extract interesting information and interesting correlations.
(1:04:20) Any questions about anything? Okay, let's now stop then. Thank you so much. Have a good night. Yeah, sure. Thank you. Good night. Could I ask a question about um about sort of the homework in general? Yep. Okay. So, uh I I did have one little question like what makes an auto encoding model undercomplete or not if you don't mind me asking.
(1:04:50) So, if dimensionality of your uh representation is going to be less than dimensionality of your input, it is called undercomplete. Oh, okay. I I thought okay so you said you said um the dimensionally of the input is less than the output no not yeah because output is same as in input so basically you shrink if you shrink information it is undercomplete if you don't shrink it is over complete okay cool um I also noticed that in a lot of these scripts there will be like a a comma at the end of a list so a layer will will somehow have like a list
(1:05:26) in it and there will be a comma at the end of the list without an element at the end of it. Oh yeah yeah yeah this is this is because um you know what it is like when you design your design your um recurrent network you don't you don't care how many time steps you take so because your um uh LSTMs they are copies of each other whether you take five or 10 it doesn't matter at this stage where when you design your architecture of LSTM for example recurrent network of any And whether you take 10 time steps or million time steps, it is not important. It is it it is going to be kind of same
(1:06:09) architecture in some sense. But when you supply data when data comes in, your network will know the length of your time step. That's why you don't have to specify length of time step at the stage when you design when you kind of paint paint your network.
(1:06:27) All right? But when when when mini batch comes in, it will already know. That's why we say kind of common. They don't don't specify. That's interesting. Interesting. Okay. Um, it might be giving away the answer to the homework question. So, I I wouldn't mind if you didn't answer it, but I I know the question is asking like make the encodings as small as possible.
(1:06:47) And I was wondering like whi which part of the neural network is like the encoding size itself? I' I've actually finished everything but it. So I'm wondering is like um is the sort of that dense layer so so first of all I want to mention one important thing when I say make it as small as possible I don't mean make it like five or 10 right as small as possible in order to sort of u get something reasonable even even this is quite difficult you may not get anything reasonable because when you try to when you try to sort of u compress
(1:07:26) your sequence of vectors into one vector. Even if your vector is going to be highly dimensional, you still make lose lots of information. When I say make it as small as possible, it doesn't it doesn't literally say make it small. It says make it as small as possible. If if small possible is 10,000, keep it 10,000. Mhm.
(1:07:52) Secondary uh where is it? You know what? when there is like LSTM or whatever layer you use and you say output uh return sequences is false and the return of the single vector this single vector is exactly your encodings it is it is end of your encoder you have encoder you every time return sequence of vectors sequence of vectors sequence of vectors at some point you say I don't need whole sequence of vectors I need only last vector This very last vector is going to be called your encodings. Okay, I think I got it. Yeah. All right, then. Um,
(1:08:31) yeah, that's pretty much it. Well, thank you. Okay. Yeah. So let's sto them.