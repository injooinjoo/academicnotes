89 day 7 - YouTube
https://www.youtube.com/watch?v=0u335w9CM9g

Transcript:
(00:01) Hello everyone. Hello Dr. Croket. Hello. How is everything? How you assigned? Well, I'm moving along with the fifth homework assignment, so that's good. I'll have more to say during our section. Okay. Yeah. Yeah, we'll talk about some stuff on Friday. So today we have uh um lecture seven. I will talk what we're going to to cover. But first let's review the quiz.
(00:39) So I'm not sure if I posted your uh quiz grades. Let me maybe do it right now. So now quiz number six then we have first question disadvantage of one hot encoding is first of all it is sparse we understand it has a lots of zeros right what it means sparse sparse means it has a lots of zeros as a result when you compute any kind of um uh linear combinations.
(01:29) So any kind of you know functions of it you will have to use lots of zeros which is basically not really doing anything but you still have to do this computations 0 * something and so on. It means sparse is never really preferred. It doesn't uh represent much information but it represents information in a sort of note efficient way. That's why we use embedding embeddings and so on.
(01:49) That's why sparity is is indeed the case in case of one encoding. This is issue. This is disadvantage. Second problem is as a result of this basically is going to be high dimensionality. It means B is also issue has a lots of zeros. As a result, it has a lots of a lots of uh many dimensions. So now uh C is correct. A and B is correct. It means correct answer is C.
(02:13) Any questions about this? I hope it is clear. So next one. Second question. Auto encoder is called undercomplete. At some point we discussed if we basically reduce dimensionality we use this representation vector of encodings which has less dimensions basically right this vector has less dimensions in that case we say undercomplete.
(02:41) This is basically goal which we're trying to achieve here. We say you want to represent input using some kind of compress compression like last time we did for example we would use list and the way we create created bottleneck maybe you remember how we did it let me show again this is lecture two and we have in this case LSTM and there was architecture we can use encoder decoder in case of LSTM for example where we create bottleneck This bottleneck will represent entire sequence via a single vector. That's how we can do it.
(03:18) We can we we can do it also for any kind of input actually. It could be image. We could basically reduce dimensionality and like compress it. We could also take full content network and also create bottleneck. So bottleneck is kind of useful if if you want to uh compress data.
(03:41) So this is what we mean by undercomplete the shar presentations is lower that's why B next one stacked out encoder in this case refers to what in this case when we say stacked out encoder we basically mean that we we can you know train it using multiple multiple uh phases you would add more and more in between it means stacked sort of by definition already refers to deep deep network Because we assume that we kind of can stack something inside. That's what we mean by stacked out encoder. It is not shallow one.
(04:14) Shallow means one hidden layer. No more than one hidden layer. If it has at least two hidden layers, it is already called already called deep network. That's why ST encoder since it has more than one one hidden layer is is basically deep network. So next one you remember how to how to train it, right? I we discussed it at some point last time.
(04:37) We can simply train or maybe maybe I don't don't have it here. I don't remember. Maybe it is not presented on my slides but we discussed it last time. We can basically say let's with no hidden layer tool as you see on the screen. Let's train only input hidden first and output right away. Let's train it first during phase one.
(05:05) Then we freeze coefficient we stack hidden layer two in between this type of kind of kind of sandwich approach right. Then we train already middle part but we freeze coefficients of this two two layers and near to the input and also to two layers near to the output that's what we mean by stalled encoder it means it is deep clearly. So next one um is going to be uh question about layers.
(05:32) Layers of STO encoders do not need to be trained together as we just discussed. Yes, they don't have to they don't have to be trained together. You can train them sort of one at a time. It means you can basically train only shall out encoder every time. No need to train it right away. Now maybe it is important to mention why it is so right at some point we discussed that coefficients W's for different layers could be on different scales.
(05:59) It is also a result of my signal being on different scale. Right? Everything is is kind of interrelated. Let me say note it is related to basically stastic descent and also modifications of this algorithm. If you have let's say W first and then W let's say 200 it is far away from W first maybe it is somewhere like close to the output already. they may be on different scale.
(06:30) So basically my levels of cost function can can be stretched this way. It means my gradient not it means my negative let's say alpha* gradient of J will be pointed not towards question mark but somewhere off the result training may take either forever if you take alpha not sufficiently small it may even diverge.
(06:55) You can imagine about case where it is much more extreme than than what I sketched because I sketched like maybe how much 10 times more 15 times more but what if it looks like literally like that right so you you can't really differentiate between two lines then it means gradient will be pointing that way like negative gradient will be pointing that way well minimum is somewhere over there the question becomes how to kind of practically move towards question Mark you know that there are some modifications first of all Adam optimizer itself will use this type of rotation of vector based on like like local curvature I would say we didn't
(07:32) cover it in detail but I'm just telling you that Adam will adjust vector based on kind of local curvature it helps secondary also quite quite interesting approach so-called local batch normalization signal which propagates network can be locally kind of normalized as a result that will also impact how WS look. It means we we will mitigate this issue using local batch normalization.
(07:57) Next approach so-called shortcut connections. We literally connect like one of the layers to layers like maybe few layers later like using shortcut. It also allow signals sort of propagate around some of the layers. Basically as a result you don't have this issue of signal being on different scale. As a result w on different scale.
(08:22) So this is nowadays is not actually issue to be to be precise. People often train deep uh out encoders using like all layers at once. But if you really want you can do it this way using phases. It means you can use only shell out encoder at a time. The question is can you train? The answer is yes you can.
(08:47) If you want basically even even though nowadays I would say because of all this techniques which we just covered right allow you to avoid this issue of you know not being able to train deep network you can train it so autoenccoder can be trained right away without this approach even though this approach is like elegant I think this is a great idea so it's it's nice to remember about this and finally last question all encoders cannot be used for sequences this is not correct last time considered example not to mention we considered this slide where there was out encoder for sequence as input and
(09:22) sequence was output itself it means we would match it to the same sequence so yes we can use auto encoder for sequences as well typically we create bottleneck which is not not going to have time component anymore we just basically compress sequence into a single vector that means with we say sentence sentence is sequence of words every word is some kind of vector which refers to my vocabulary it means basically vector sequence formally speaking as you can see x is bold in this case right it means vector weight sequence but then we say let's basically
(10:02) map entire sentence into a single vector there is no any kind of time component anymore in entire sequence should be represented by a vector so this is possible so we can do it. So any questions about this quiz? I just wonder about the the term undercomplete where they come up I understand what it means but where they come up with um why they say under.
(10:33) Yes, undercomplete. So that if you go to um the question about I think number three. Yeah, I understand you mean why terminology sounds this this way? undercomplete. So I just wonder I understand it it's it's a it's a bottleneck. It's it's a squeezing layer. I understand that uh but what what is what is the word undercomplete? No, it is like so I think uh you're talking about terminology undercomplete.
(10:56) I I I'm making this up right now but I'm sure it is the case. So basically if we just create like uh same dimensions kind of right it will be like complete like basically we say four dimensions in four dimensions out in the middle we also have four dimensions for example to represent that we have sort of complete we can kind of without loss sort of represent our input under complete means we create bottleneck it means at some point there is bottleneck it was like four dimensional now it is like two now
(11:35) it is four again so it means my representation is sort of undercomplete because if I have like four inputs and now I'm allowed to use only two numbers to represent my four inputs that's why we say undercomplete under complete so it is like related to dimensionality of my representation being less than input thereity of input.
(12:06) It means we kind of don't use complete you know kind of complete representation which is required to represent my my input. You can think in terms of maybe data set x1 x2 x3. It is your data set and you say let's say it is three dimensional and I map it to two dimensions three. This is also three. It is complete. So if I want to represent threedimensional uh data set I need to have three dimensional space.
(12:36) Basically in this case we say no no no we take only two dimensional space to represent our data set which lives in three dimensions. So that's why it is sort of undercomplete because what we represent is not basically uh going to live in two dimensional space.
(13:01) That means our representation is not complete sort of we take only projections under complete is related to idea that we take only sort of kind of projections in this case it is pro projections if you use linear activation functions but this is how how we can think about this now clearly there is another case when we have like three inputs and let's say seven uh encodings it will be called over complete complete.
(13:33) So it is quite understandable why terminology goes this way right under complete means basically we take a lower dimensional representation it means we sort of lose something it is not complete representation it take some kind of projection maybe it depends on efficient functions if they literally like linear functions it means we literally take projection onto some kind of plane in this space that's why undercomplete is like not complete basically right there any uh any um uh quantitative methods in information theory that will allow us how much um or how big this squeezing layer can be because I I believe at certain points if we squeeze too much we will not be able to
(14:12) reconstruct the original. So that's very important question actually. So uh we have to think about ultimate goal. What do we want ultimately? uh I mean do you want to be able to reconstruct construct novel variance? Yeah. So it is very very important question actually.
(14:38) If you want to reconstruct your original input if this is your goal right then the answer how to do it in the best way is obvious. What is your answer? It will be I would I won't squeeze anything. I would you would not squeeze anything. Right. Exactly. Why would you squeeze it? Right. You don't even need layers to be precise. You just use input and call it output right away probably.
(14:58) Right? So it would be kind of smart idea to do it this way if your goal is to recover. Right? Now there is another thing. What if your goal is different? What if your goal is to actually squeeze it and then use this result as a kind of representation of your input which you map somewhere further let's say for classification for something else.
(15:19) Which means in practice my goal is typically not my goal is typically not to try to squeeze it because if you try to squeeze it without loss of information you probably don't want to squeeze it. Your goal will be to take your encoder this way, right? And then on the top of this build something whatever you want.
(15:44) Let's say some kind of classifier, right? And I have some kind of let's say classifier for example. So this is my basically uh classifier. And then the answer what you want to do here actually depends on your goal which is your now cost function over there right you minimize ultimately this cost no maybe not even this cost maybe test cost which is related to your which is related to your some kind of classifier and then now it becomes iterative process.
(16:24) You say let me take two dimensional presentation build my classifier and see what I get. Let me take five dimensional representation. Let's say input is like 100 dimensions. Let's take five dimensions. Is it better or not better? 10 dimensions better or not better? Then you sort of take the best kind of approach to squeezing. Does make sense? Yes. Experimental approach. Yes. Yeah.
(16:47) So it is kind of iterative approach because you have to define your goal. What I doing here? Are you literally like squeezing? If you are squeezing and hope to recover it in the best possible way, just don't just don't squeeze it, right? Okay. Thank you. Yeah. So, and and why is it helpful? Because the second part will require a number of parameters.
(17:04) So, if you imagine that is like not three, if it is like millions of inputs, right? Maybe it is not something you want to feed feed to your classifier. Maybe you want to squeeze it first, maybe you want to compress it first. Then it becomes quite important to somehow compress it because later on you may have like literally millions of parameters.
(17:23) If you want to reduce dimensionality only millions of parameters means also that you cannot really work on this space of parameters and hope to find global minimum. That's a problem basically. That's why if you want to somehow how efficiently reduce number of parameters without not losing much basically you don't lose much but you still be able to somehow classify whatever you classifying then problem become like well defined and you say okay I'm going to ultimately use my encoder in order to sort of build classifier on the top of this output from my encoder on the top of this encodings that becomes already clear
(18:01) what you have to So uh so now um yeah so now today uh we have actually uh different topic it is quite interesting it is called legendary allocation it is kind of introduction to topic modeling next time we're going to expand it we're going to talk about structural topic modeling it this type of approach is quite popular among social scientists you can find a lots of works um we think like government departments right political science departments they really like it because they work with text and they have to extract information from text somehow in some kind of efficient way I mean
(18:46) computational way basically not manual way and they need to somehow classify text so this is basically what's called unsupervised learning algorithm LD is unsupervised learning you don't have to have any kind of labels. In this case, we have to try to uncover structure in our text.
(19:09) Maybe we have to even ultimately classify it into some kind of different types of different types of uh documents. Let me let me say this way LDA allocation right. Let me talk about motivation first. motivation. So um it is um you know about unsupervised learning algorithms right? Let me say unsupervised unsupervised learning unsupervised learning.
(19:58) This is some kind of uh let's say type of models where we don't have labels essentially and we say let us assume that x1 x2 is like my my sort of um uh characteristics whatever not outputs there are no outputs there are no outputs and we have some kind of observations right it may be quite important want to identify clusters here. We understand why it is important.
(20:33) Let's say marketing for example, right? So we can identify clusters it could be quite important. Not to mention we may if if if you have output later on you can build different kind of subm models maybe as well. So clusters typically clustering means we run some kind of algorithms.
(20:57) K means clustering right? hierarchical clustering for example and we say um they don't require any any outputs and we say now we know that there are some kind of clusters this way so typically in this case is typically I mean this is basically how it is done right every point belongs to one of the clusters to one and only one of the clusters Um even with regular data set it's not really necessary right you may be like retired and know do data science as a freelancer for example this data scientist those are retired people but who said there is no like someone who belongs to two classes but typically we say okay like this is kind of
(21:47) mclassified or maybe it is someone who is still more belongs to one into different classes classify this way and those algorithms basically break our data set into this type of uh subsets. That's all it does. Essentially you do it by uh kind of trying to uh let's say maximize ratio between some kind of differences between things between between things from different classes over difference between between things from one class this way basically.
(22:29) So now we understand that there are a set of clusters. What if we have clustering of documents? Let's let's think about this in this case. Maybe those are female, those some males, maybe like a retired people, maybe those are students. So, and usually understand that there are some kind of clusters.
(22:49) We can break into clusters and it's going to be fine, right? But in case of text things are much more complicated because in case of text it is very possible that people can start talking about economics and they shift to politics easily right to artificial intelligence within the same document. It means clustering is like not naturally kind of available.
(23:11) Let's say clustering in quotes. Clustering of documents, right? You can think about like news like news for example. You read like news and you understand that every document can be represented by a number of features. We understand that like why a vector not a problem. We can do it.
(23:48) Then this is like document first, document second, document third, document fourth, document fifths, documents six and so on. So we have this type of seemingly clusters maybe, right? But then when you read it, you realize that actually clusters like nature, they don't have to be non-over overlapping clusters. So maybe all these articles talk about economics for example easily right.
(24:12) So economics maybe this things already talk about uh politics right example and there is maybe another cluster which they talk about artificial intelligence maybe completely different topic and basically what I'm saying is every document easily can can belong to multiple clusters this is basically what often happens one document may may consist of sort of literally may consist of different topics and uh lat allocation as well as structural topic modeling will will essentially tell me let's assume that every document consist of multiple multiple topics right to be
(25:02) precise it may consist of all topics actually with different probabilities for example D2 in my sketch consist of all three topics Maybe topic one economics contributes like 60% to my document two. Let me say example my document two is not like equally right but it will be economics example 60% let's say politics will be 30% and they mention something about artificial intelligence on the way okay 10% this That's typically how documents will be. They will not belong to particular class. We know they are students, right? Okay. Students, those
(25:54) are retired people. They're completely different. Usually there could be retired student obviously, but it's not like often. In this case, often is this the case that's why we have to sort of apply different kind of unsuper algorithms. LD is exactly type of unbras algorithm which is used to assign to each document assign different topics.
(26:17) But first we need to realize what kind of topics we have right we have to first from the text extract topics and we say okay this text talks mostly about economics mostly about politics and somewhat about artificial intelligence and basically nothing else. Now this decision nothing else is kind of your decision.
(26:41) You have to say up front this hyperparameter that I assume that this particular corpose this particular this particular set of documents talks only about three topics. So number three is hyperparameter which you have to assign up front. That's how it works. So let's now look at some at some um some uh interpretation of what I just said right just some for your reference this statistical modeling will will talk exactly how it looks it tries to discover some kind of latent latent topics from the from the documents those are latent topics why they are latent topics it's not like we say let's look at econ economics we don't want to
(27:24) invent anything here on The contrary will say the model will have to decide what topics are. Essentially it will say there are three topics. It will have to decide itself what kind of topics they are. Our task will be ultimately only to kind of produce a label for every topic.
(27:45) We'll say okay those who who consist mostly of economics talk mostly about economics which means we can say okay it seems like first topic talks about economics. It's not like uh something we up front decide. We on the contrary later after the we finish the processing of data we can sort of assign label and we can say first topic seems to be about economics and you see like there are percentages right? It means we can always look at those documents where economics is going to contribute most. So we can take top top documents which consist of
(28:23) mostly economics. Then we can read those documents and say okay it seems like they are talking about economics. It means first topic seems to be about economics. This will be simply label of the topic. That's why they are latent topics. We don't see them directly but we can sort of produce labels for those topics oppos kind of after the experiment.
(28:49) Right? So we kind of try to discover hidden uh themes from the text right like literally and the method which we are going to discuss as LDA NMF and STM LD and STM are quite quite related lately allocation and also structural topic modeling STM they're kind of related because STM was based on LDA it was a sort of enhancement of that model with indial allocation where people also allow it to introduce some kind of covered variables.
(29:20) Typically or I would say often not not always but often documents can be can may have some kind of associated uh variables. Now let's say it could be like gender of a writer or maybe it could be even uh some kind of um uh let's say uh journal to which it belongs right and so on. It means it can easily have a lots of parameters lots of data which sort of accompany a document.
(29:51) LD sort of ignores it. It just says we don't have it almost like we just create those uh topics. Maybe later on we can try to look at correlations. If economics correlates with you like you know type of your journal or not doesn't correlate so clearly it will correlate.
(30:12) STM structural topic modeling will on the way on the way of modeling already incorporate your cover variables. It means it will basically create those topics a little bit more accurately because it is part of your modeling part of your model. LD doesn't really consider any kind of covert variables even though afterwards you could technically speaking look at correlations between topics and your covert variables but is only after STM does it on the way.
(30:36) So NMF is quite uh quite simple procedure. It is just represent. We'll talk about this today, right? It's called ngative matrix factorization. We'll talk about this today. It is basically the deterministic procedure. You try to also solve similar problem.
(31:00) You try to kind of represent your topics via a number of your text by a number of even topics. Maybe you have a lots of uh documents. Maybe your vocabulary is huge. It may consist of like 100 words for example, maybe thousands, but you only have like three topics which is like basis kind of basis for your like coordinate system for your document, right? And you say my document is 60% of first, 30% of second, 10% of last topic. That's how we kind of uh want to want to do it.
(31:32) So now um now clearly if we have this type of information, it's a lot, right? How how can we benefit from knowing that knowing that second document consist mostly of economics? No, we can already say it is mostly economics. We can classify documents for example. We can implement some kind of search and so on.
(31:49) Right? So this is quite quite useful. Clearly social scientists typically do different things. They try to say example uh what um no let's say let's say what how topic correlates with like gender for example right uh then you can discover it from the text you can see if there is some kind of bias for example right if this topic is about politics the topic is about like economics about democracy or whatever right then you can correlate it with some kind some kind of some kind of cover variables. So, LDA models documents as mixture of
(32:33) topics, right? Every document is a mixture of topics. That's exactly what I mean by this. It belongs to multiple classes. Basically, so now uh overlapping topics in this case is what we get. Unlike clustering, we get overlapping topics. In this case, we assume probabilistic approach, right? It means we don't really do it in a deterministic way in case of LD was initial allocation.
(33:01) Assume that we do it in some kind of probabilistic way. We'll talk about this like in a second. But it is not deterministic already. It is probabilistic. It means if you run this model second time essentially may get different result. So it is also going to assume that your text is generated in some specific stoastic way and by uh trying to maximize so-called likelihood we can discover parameters.
(33:28) So now um let me talk about LDA itself right LDA so this is LDA model latend allocation let me try to explain what's happening here first of all we have to uh talk about likelihood what it means likelihood and how we discover parameters by maximizing likelihood So on maximization of likelihood is what I want to make remark on maximization of likely put let's take simple example so basically I want to tell you how how parameters are discovered from from model using approach which is called maximization of likelihood. I'm going to
(34:36) give you very simple example. Let's assume that we collect observations of X variable right X is my random variable single random variable I collect like some kind of some kind of records and I get histogram. So histogram of my observations this way. Now you can think about maybe something like u like frequency right frequency is a function of x.
(35:08) No it means histogram basically. Then I say I have to in this case assume particular model. This is this is first step. No this is my data. Histogram represents my data. It is obvious. Now I also have to assume specific model when I maximize likelihood. It means I always have to assume how data is generated by nature.
(35:33) How data is generated by those who for example produce text. Let me say assume on this case or just to kind of you know illustrate it assume that my data basically come from normal distribution and the only problem is I don't know mean I don't know variance of this normal distribution mean and variance basically where it is located and also I don't know how wide or narrow my distribution is going to be and let me say I'm going to assume that my distribution normal distribution is over here let's say it is what I assume model A means specific choice of mu and
(36:19) sigma squar average of my observations is somewhere here but I assume that mu is over there and then I ask you a question what is actually probability to observe observe my specific data. So this is axis is probability to observe our data right given given basically choice of parameters which corresponds to a let me in this case say it is typically like function of multiple variables because you may have multiple parameters.
(37:04) Let me say this line represents mu comma sigma square that is like a vector basically I have to place maybe second dimension I will sketch this one dimensional kind of you know kind of arrow but you you can think that is like two dimensional then I say this is my case a so a tells me that mu is quite quite low far away from where it is probably supposed to be then I say like literally I look at this and say now Everything is available.
(37:32) I say this is my data. This is my assumption. Capital A means particular choice of my mu and sigma squar very very specific choice. Then I can say what is probability of getting this sample assuming my mu and sigma squar are such as I sketched over there. So what do you think probability of getting my histogram given it is on the left that far away small or high? Like it would be small wouldn't it? Would be very small. Yes, very small. Would be tiny. Right. Exactly tiny.
(38:08) So it will be will be very small. Likelihood will be like almost zero basically like very small chance of getting this. Then I can say okay I can try to adjust it right. I can say okay let me move it to the right. What about B option? probability of getting my distribution my data given it is coming from B distribution is it higher or smaller already higher a little bit right higher so second choice of my parameters higher then I sort of scan through different you know various of parameters and say let me now choose it that way this C
(38:49) option then it seems to be quite nice actually right so C option option will be somewhere over here and maybe even much higher actually. So maybe much higher and then the option when I sort the hershoot I say there is no I have two parameters I can change variance as well. So D it could be quite small as well.
(39:20) That means it is quite small maybe even smaller than the first case. And this is my likelihood function. I sort of scan it this way and I get maximum. So maximum of this function corresponds to basically best solution to this problem. This is called my C is called uh is called MLE. All right. So this is called MLE maximum likelihood estimator.
(39:50) It corresponds to the best best option for parameters and sigma squ using this specific approach. This approach is called maximization of life. It is very natural approach. It is so much powerful. I can tell you it is used in many many applications and time series analysis in this for example latocation in threshold topic modeling. We look at data.
(40:13) We make assumption about model and we scan through parameters and we basically say what is best best option for the parameters in order to make probability of observing whatever we observe being highest then we can recover parameters. Even if you have like tens of parameters like dozens of parameters we can actually recover it from optimization of likelihood and results could be quite impressive.
(40:36) So any questions about this approach itself? So we should understand likelihood to sort of move forward and we can understand how parameters are being obtained in this case in case of LD and also STM. Okay. Now let's talk about our framework for this um LD itself. So um you may you may see some kind of distributions which you don't recognize like dish for example I know that you may not know what it means.
(41:08) So basically dish means uh generalization of a better distribution. Let me explain what's happening here. Um um you know what first on high level I want to explain on high level first in this case um in case of LDA in case of HTML also in case of structural topic modeling we assume the following we have to assume like some kind of model it means we have to assume how data is generated in this case we say data is generated by drawing samples from normal distribution okay this is my assumption I don't know parameters but
(41:44) it is okay later I can recover parameters but by maximizing likelihood I can basically scan through different parameters and get it. Uh now in case of um LDA what's happening is we say first it's like we assume that whoever generates text acts act acts this way they say let me first pick a topic which I want to talk about when I want to generate first word on let me pick a topic so I decide to pick a topic economics okay I made a decision maybe I have to start from the from the beginning First I have to decide how many numbers how many words my document
(42:28) is going to have. So n comes from person distribution. I decide how many words it's going to have and I assume as if someone who generates text acts this way. They say okay how many should I choose? Let me pull random number from post on distribution. Okay it happens to be four. Okay my my document will consist of four words.
(42:50) First dec decision second decision. What should I choose? What kind of topic should I should I discuss? Let me choose economics for example. Okay, I chose economics. What word should I pull from topic which is economics? Okay, let me pull on. And I write on is going to be my first word generated this way.
(43:08) Then I say okay, what's next? Let me talk about car calls. I decide on the topic basically. Okay, I want to talk about car goals. What word should I choose? What should I pick from cars? Okay, maximization. and I write maximization and I continue more than that when I do it this way I describe it as if there is like type time component in reality this model assumes uh doesn't assume that there is time component basically we're talking about bag of words they are sort of no kind of in any order generate this
(43:38) words when I generate maximization I don't care what was output from the previous uh previous kind of generation process right so maximization will be generated completely independently of previous result. That's what what I assume if you understand that if you generate sentence this way, it may not make much sense.
(43:59) Right? Maybe if you shuffle if you kind of try to rearrange words because order of words doesn't matter anymore. In this case, we don't don't look at time component. If you maybe sh kind of rearrange it, maybe it make a little bit more sense. But if you generate text this way, it may not make like exact sense.
(44:18) We probably don't want to generate this way but we can assume this model and we can say what are the parameters of this model which corresponds to uh maximal likelihood of generating a particular given document. That's how we recover parameters. Now let me maybe formally talk about this uh in reference to exact procedure how we generate it.
(44:49) So this is LDA letting dish allocation and we say let us uh first decide how many words particular document is going to be. So n is pulled from so-called plus distribution. Pson distribution is particular distribution. Let me just sketch it which looks this way. Probability that capital n is equal to lowerase k.
(45:26) K is number of the number which I sect basically probability that it is zero is very small. For example it is one is going to be slightly higher two will be slightly higher and I get this type of you know distribution. Let me sketch it this way. So it's going to look this way.
(45:51) similar to binomial distribution if you know how binomial looks but it goes all the way to infinity basically. So that's how plus distribution looks and we say let us pick random number from this distribution not will be somewhere around you know maximum right around where expectation occurs. Expectation is given by parameter xi exactly if you compute formal expectation of such distribution will be xi.
(46:16) So parameter xi basically refers to expectation of number n. If your documents of length 25 on average when you fit your model will be approximately 25. So it's not surprising sometimes some documents will be shorter some documents will be longer but this is how it will will be ultimately because expectation there is expectation somewhere it is exactly right describes describes expected number of expectation of number n basically or expected number of words in your documents. So first step is finished. We determine how many words we want to have
(46:53) in our document by pulling random number from here. Next, next we say let me pull parameter theta from so-called dish allocation. So parameter theta is going to be coming from dish allocation. So dishly which is parameterized by alpha but alpha is actually vector valid parameter formally speaking right.
(47:27) So this way no it is um so this vector valid parameter so something which we don't know we want to estimate by maximizing likelihood. This is also by the way something we don't know but we assume this structure then when we see text we can already by maximizing likelihood recover all this parameters just like in case of sample for normal distribution we can recover here using same same procedure. Let me better tell you some kind of example.
(47:54) So what is digital allocation? Digital allocation will produce essentially a number like a vector of probabilities. So digital location digital distribution digital distribution is going to produce a vector of probabilities. That means my theta m is nothing but some kind of vector of probabilities. Let's say 0.
(48:23) 1 as example then 2 as example then 6 and so on for example. And this one corresponds to topic first. This one is topic second. This one is topic third. So basically length of my theta will be exactly length of number of topics which I define a priority. So I will say I want to have only three topics. Okay will be three numbers which add up to one.
(48:50) Right? So this way so this is both going to be vector of probabilities. In case when we have two topics it will be exactly so-called better distribution better distribution if if it is not two dimensions not two topics will be more general case you understand that basically some of this numbers will will be will be one it means it is like vector of probabilities essentially so next step is finished we decide what are probabilities which will contribute to my to my basically document Right.
(49:25) I decided my document will consist of so many um so many uh uh I mean will consist of my topics with this probabilities. Now next one is actually next step which is going to be generation of words. By the way probably I didn't say it correctly. I have to take it back when I said like um we decide topic for the first word and then pull pull word decide topic for the second word and pull pull word it's not correct I have to take it back we have we decide extra topic for the entire document kind of right and uh we then pull then we pull uh words right uh it is like um well um Yeah,
(50:13) you have to clarify. We decide about the vector of of my topics up front for the document, right? Like 60 30 10 and then for each word we have to already be specific and say which topic we choose. Let me continue. It will be clear. So it will be clear. So for the for this document I decide about probabilities.
(50:32) It is like basically like 60% economics, 30% politics, 10% IE and okay we we made decision. But what about the words themselves? In case of words I will say for each basically for each n which is from n through capital n right I'm going to do the following I'm going to select a topic first so it means I'm going to select already exact representation exactization of my topic it will come from no what would be kind of binomial distribution if it was like two topics but then more general case it is called
(51:12) multi multinnomormal distribution multinnomial distribution multinnomial distribution and parameters of distribution will be theta m again theta m is fixed for my document but what kind of topic I select will be random process specific to word for first word I can do it one way for second word I can choose different topic this is totally fine and let me see example so basically in This case I say ZN is going to be like as an example.
(51:44) Now you see most likely it must be topic three basically but it is probabilistic kind of procedure. It means I can say no it is not the first topic. Oh yeah it happens to be second topic not third one because it is random number basically random vector from this distribution.
(52:05) I say 20% chance of selecting second topic. It means uh it is possible and this what happened topic essentially two is selected right is selected that's what we say topic two is selected during this process for particular for particular word for the next word it can be different probabilities will be same they fix for the entire document but word but topic which we select ultimately maybe already different for second word and so on and then Next next step would be already word itself. I will say what word to generate.
(52:42) I also pull it from multinnomal distribution characterized by already set of parameters some kind of betas but they are specific to my topic. So those betas are topic specific. They will be different for different topics. and I say example. Now let's say I'm going to generate my word and a specific one like first one for example remember it was on okay I say zero zero zero it is already from my vocabulary right and I say one for example and zero and so on.
(53:26) So this uh one corresponds to for example on remember from my example if n is equal to one first one was on on on maximization so on on was first word and that's basically how how I do it and of course I run this algorithm right so I run this algorithm for every word in order to generate text this is explicit assumption about how text is being generated Under LDA assumption, any questions about this we select which topics it will consist of in what probabilities and then for every single word we decide what topic to select. every time it most likely will be topic
(54:14) three because 60% chance of selecting topic three but may deviate may deviate sometimes in my case it happens to be second topic then for every chosen topic or given word we generate word itself that comes from multal distribution itself that means we generate this type of no kind of vector of zeros and ones we say which word you want to pull from the vable it refers to vable The Zen itself refers to topics.
(54:45) First topic, second topic and so on. One means second topic is selected. Word refers to vocabulary. Okay. Force word and my vocabulary happens to be on. That's why force location this way. And betus is another set of parameters which you have to estimate from data. Remember if you specify how to to generate data even if you don't know parameters it is totally okay.
(55:11) Maximation of likelihood will take care of this, right? It will already be able to it will be able to find the parameters. Any questions about this? In this case, we have some kind of random variables which we don't directly observe like ZN like what is ZN actually what was what was decision from what topic we chose? Okay, we chose on like think about like data we chose on from second topic but how do we know that it was second topic? We only observe like document on maximization of likelihood. Yes, we observe on but how we know it was second topic. It means this Zen is actually also latent
(55:54) variable. It wasn't observable. That's why there this approach is is called um EM algorithm right expectation expectation maximization. So it is classical algorithm which is used in many applications. Expectation maximization algorithm. We sort of um start with some kind of u uh parameters. Then we say what is expected value of parameters ZN kind of what what it should be. We plug it in.
(56:30) We get our likelihood and we maximize this likelihood. Next time likelihood is a little bit better. Then we also decide what ZN could be under already some under already new parameters updated parameters and we plug this already under new parameters and so on that is like iterative process it's called expectation optimization algorithm so any questions about this it is required if you have le parameters like ZN for example it is required to to use also this step expectation.
(57:08) So now um okay uh if you don't have any questions we'll make a break now. Let's let's make a break. Sorry I have to go. I have deadline tomorrow morning. So I I I will watch the the the second half. Yeah, sure. Yeah. So now any questions about about this model?
(1:07:38) It is kind of interesting because in this case we assume that uh someone almost like robot right like whoever generate text let's say a person first decides how to ch what length of document is going to be it is post on okay you can try to understand why it is post on maybe there is some kind of you know uh justification because it is number of well there's no really like justification because person assumes independence and describes number of events during particular interval of time basically we assume person distribution this type of distribution then we select uh what kind
(1:08:22) of contributions will be to this document which we're talking about we select those probabilities as some kind of random number basically random vector generated from distribution parameters to be specified by maximation of likelihood and then for every word we decide how to basically generate this word.
(1:08:47) What topic to choose topic will be always around those which have maximum prevalence this is like prevalence basically some kind of probability of contribution from particular topic to to my document like prevalence if prevalence is 0.2 do like 20% from topic two.
(1:09:07) Now it means that it's possible for particular word I decide to choose maybe second topic not third one I decided to do it this way then I pull from my set of parameters my betas which characterize another bin distribution which allows me to generate word and word comes from binomial but it corresponds to specific set of parameters which are specific to my topic which I chose if I chose topic two will be like better which correspond to second topic but is also vector of parameters.
(1:09:34) Then I will say okay now it's time to generate words and I say which word from my vocabulary will be okay force word from my vocabulary. That's how it will be sort of it is how it assumes text is generated. If you do if you do this way of course whatever you going to generate using this uh approach is not going to be readable. It's not about generating text. It's not like generative model.
(1:09:58) This is model simply which is simply trying to describe your text tries to identify topics. So now uh if no questions let me move forward um uh let me say implementation in Python in Python it is available in R it is also available but next time we are going to introduce structural topic modeling which means generalization of this this model and that one will be essentially in R because in Python even though it could be available by some kind of you know some kind of not official I would say um uh implementations but typically R has it but Python doesn't have it. You
(1:10:43) can find some implementations online clearly but R has like implementations provided by authors of this of of this approach but LDA has is implemented in both Python and R. Let me start with Python. In my case, I have a documents which consist of u uh this three + 3 plus eight documents. So I have eight documents.
(1:11:15) Some documents talk about cats. Some documents talk about about about dogs. We are trying to say what kind of topics uh we can extract from these documents. Then we first of all have to vectorize it. This is count vectorizer as we got used to it means we're going to essentially on the way create vocabulary and we're going to create matrix of token counts literally.
(1:11:38) So my x capital x is matrix of of counts for every document I create by table I say cats in first document two times it means be two comma pure is going to be like one time comma one and so on and so forth. It is like bag of word. Basically it will refer to my vocabulary. Order is not important in this case.
(1:12:03) Order is not preserved. There is no time component though. X will be kind of almost like bag of words but it will be counts. It is different from bag of words. I mean depends on the implementation but it will be counts. So it will be counts. It will refer to my vocabulary. Now next it's time to basically fit my LDA.
(1:12:26) So this simple line specifies this this is what I want to do. I assume that my text is generated this way. Every text every document is generated this way. Every word is generated this way. And I specify how many uh how many um topics I'm going to to use here. I say number of components is equal to two.
(1:12:49) It means basically two topics random state is zero because this procedure itself of likelihood is not actually deterministic. So it maybe may result in different different um different topics right because there are some kind of latent variables which would be identified differently if you think about this why first topic and second topic should be in this order why second doesn't become first first doesn't become second for example it is like easiest what you can think about but there problem is more complex the problem is let's say in this case I identify that first topic consist of dogs, walks. So probably it talks about
(1:13:26) dogs and second topic consist of cats, greater they talk about cats. Now who said that first topic cannot be simply about dogs and cats at the same time and second topic could could not be about walks and trees. It could be actually could kind of select different bases, different coordinate system.
(1:13:49) Basically it means that this puzzle that next time it will be completely different. In this case, we have to kind of, you know, run this experiment maybe multiple times and use some kind of metrics to identify which result is better. In that case, we use things such as semantic coherence and um and um coherence and semantic semantic uh adherence and uh exclusivity exclusivity.
(1:14:18) So now uh let me walk you through. So we fit this model. It means we maximize likelihood. We fit it and then we say let us try to print most uh associated terms with particular topic. Let's say I choose topic one. My topic one is right here. And I say let me arrange my uh my terance right my my terance which correspond to this topic and take basically top five words which correspond to this topic.
(1:14:57) Now I sorted in this regular way but I take from the end five of those most probable will be dog already less probably will be enjoy but still quite probable long walks and so on. So we have to understand that every every document consists of basically all topics right there is chance to to be included right for every topic but some will be contributing more some will be contributing less that's why we always in this case try to look at the most probable ones we look at prevalence basically and say which one is most probable in order to identify what topic it is if I like pull all of them it will be like all of them will be
(1:15:32) entire vocabulary first one will have dogs second one one also will have dogs somewhere clearly but in this case I say let me look at top five only then top five you see they don't even overlap because top five are different in first case and then second case that's why I can see clearly that first topic seems to be about dogs second topic seems to be about cats no this is by the way you can run it in Python you can see that the same result because I have seat you're going to have same results and I I know now that my
(1:16:04) topics which identify Right? Probably could be labeled as dogs and cats, right? Documents which discuss dogs and second topic will be documents which discuss cats. Any questions about that? Every document ultimately is going to be consisting of two topics. First one and also second one. In this case, they probably don't overlap as much.
(1:16:33) It means maybe first document for example will be mostly about topic two maybe slightly a little bit also from topic topic one because some words could be coming from there as well but most likely it will be about topic two most likely but little bit from about topic one as well because again topic one if I didn't cut it if I didn't take like five most most frequent words it would actually have all words from my my vocabulary will be just rearranged differently basically right will be this way like all of them from my vocabulary and second one will have
(1:17:10) also all of them from my vocabulary but in different order that's why I have to sort of chop the high the the one which have highest frequency and see what they correspond in order to sort of recover my topic it is already kind of it is quite important in practice to know what topic is about right but for the model it kind of says I don't care what it is about it's kind of you decide what it is about if you want to create some kind of label for for the topic.
(1:17:36) Yes, you can take a look and see what it consists of. First topic consist of mostly dogs and whatever is related to dog and second one consist of cats related to cats. But the model itself doesn't really tell you what label it should have. You should decide it yourself like after after the experiment.
(1:17:56) So now in our Dr. Crouch, can are you are you surprised that barking or bark is not included in the topic for dogs? You know what? It's it's it's a good question actually. Um let's see. Um first of all, the thing is stoastic, right? If you run it second time, you may get it. You may get it as part of your topic one. But um it is first of all, it's not like it is not included.
(1:18:21) It's just far away, right? Because my topic one consists of all words on my vocabulary. Every every word is included everywhere basically. But you are saying probably why it is not not close to dogs, why it is not frequent ones, right? So somewhere else I mean if you put more there will be bark definitely.
(1:18:44) So topic one consist of bark more than that topic one consist of cats as well somewhere over there later on as well as topic two. But you're saying why it is not right away. Let me let me see. Cats uh cats enjoy dogs bark. So bark is nice candidate to be included with dogs. Clearly now there is a dogs then bked is different.
(1:19:16) So we are clear about this because barked I don't do any stemming or anything. So now you see like dogs there's no stemming as it stays. It means basically bored is different. So bked I see. So so they're independent words. So they wouldn't it wouldn't be that that high. Probably. Yes. Probably wouldn't. But I'm not saying that it is not possible. Maybe for next time it will rearrange it slightly. It is possible to rearrange it slightly.
(1:19:45) Actually it is interesting in in this case if you experiment. I mean you can if you think about example where you say it is my coordinate system first one is dog second one is scat right but I'm trying to kind of kind of split into topics maybe but it is possible that my coordinate system will be chosen slightly differently because my model doesn't care about you know association of a dog with barking Right? So it means maybe this will be like somewhat like let's say this will be somewhat like dog plus cat. Now it means one topic will consist of dogs and cats essentially and
(1:20:34) second topic also may consist of dogs and cats. It is even possible that we get this type of mixture. It's not going going to be ultimately best representation but it's not like against the rules. So it is also possible in this case I say cat minus dog probably right so cat minus dog will be second basis vector so it's even possible that it will rotate in some kind of weird way that my topics will be basically not exclusive right in second case what what what is what is bad exclusivity is bad they are not exclusive they sort of consisting of
(1:21:11) similar things it means it is not probably the best approach we have to look the kind of matrix then and try to run it multiple times and select the best model basically best best representation but yes what I'm saying is bark bark is not coming with dog well dogs and bark together so in this case count vectorzer can you remind me does it um does it use lower case approach must be right must be must be by default right Count vectorizer. I'm just thinking maybe dogs and bark. There's different dogs from the dogs. So, let me say count
(1:21:54) vector uh lower case through. So, okay. It it does it does transfer it to lower case. It means these dogs and bark they sort of together. But where else? Um in this case we say dog and uh bked it is not together because barked is different word right? So cats, dogs, no bark and nothing barkked is different. Dog, no bark. Dogs and bark.
(1:22:40) Uh no basically only one time dogs and bark, right? So it's not it's not much. You can say why this doesn't come with dogs using same same logic, right? Why loudly doesn't come with dogs, right? Can we use different? Yeah. And I guess um I guess cats and purring happen close enough that u that it does give it uh precedence, right? It does show up close to cats. Yes. So this is like together.
(1:23:18) Where else does it have it? There's purring cat in the in the third to last sentence. So even though it's purring instead of purr. Well, it just you see it just because so in this in such cases it is actually advised to do stemming, right? Stemming is something which uh packages in R would do by default basically.
(1:23:42) So I I just wanted to kind of have example but maybe it would be more representative to to do stemming. So stemming is is is advised to do in this case but you're right. Yes. because um it is different like puring is not going to be same as pure that's why it is I mean it happens to be together but this is different wordwarmly speaking in my example cats yes so I have to maybe do stemming and see what happens you can do it yourself you can try to run it and see yourself we'll see more examples like next time but this is just to give you idea how it works you can try to understand is very good right I'm trying to understand but
(1:24:22) it is not guaranteed that we are going to have like what we expect to have it looks at no basically because we exactly because we use this type of approach and we say topic is selected and the word is chosen from particular topic right it means basically um effective effectively there are correlations involved because they kind of from same topic it means we assume that do I say dogs and enjoy they come from this topic.
(1:24:52) It means they are more likely to be pulled together from from my pulled together from my topics. But because we pull it at random basically everything is completely nondeterministic next time you may get different result because the way text is generated is assumed to be stastic by itself. That's why it says dogs and enjoy sort of together because maybe at some point they they occur together.
(1:25:17) In order to recover this dogs and enjoy together, we probably better assume that dogs and enjoy will belong to the same topic because next time when we pull it from there then probability of pulling such specific you know text which you observe is already higher. It means likelihood is higher. That's how it does it but there is no guarantee.
(1:25:42) It is like reproducible first of all and there is no guarantee it will actually make sense. Sometimes it is difficult to make sense of this results. You have to maybe run it multiple times. Maybe you have to change number of topics. In this case text is so small it is just two example. Clearly we don't apply to such small texts. Yeah.
(1:26:07) Any further questions about this? By the way, it may seem that dogs and enjoy are like close to each other. Long is also close to dogs. It's not because of that. It is not really related anyhow, right? Because we completely lose time component. In this case, we use almost like bag of words. We use count vectorzer which refers to my x will be in order of my vocabulary basically, right? That's why it is not as important if they are close or not in my text. They just must belong to same text or not same text basically because by assumption my words are coming from
(1:26:41) specific distribution given by parameters and parameters are topic specific. Zn in my case is like second topic that means it will be better which corresponds to second topic. It means if I look at the same document more and more words will be coming from essentially second topic right it means when I maximize likelihood I better assign those same words which occur together to the same topic and if they don't occur together better be belong to different topics so now let me move to implementation in
(1:27:16) in R it is very very similar my documents by the way I know that you're not supposed to know R it means we are going to talk about this we're going to talk about R from the sort of start right we will tell you how to install it how to work with this now I can tell you that in this case documents will be equal this is like style in R basically we say equals um C is like uh is like uh those things together like like a vector basically right then we say first element comma second element comma and so We have our documents. Then we say let's create
(1:27:56) purpose. So I create purpose this way. Transfer transfer to lower case. Remove punctuation. Remove numbers. Remove stop words. And I also remove white spaces and I get my result corpus. And I say DTM is my document term matrix applied to this already kind of cleaned corpus. DTM is is is document term frequency.
(1:28:25) No, it means exactly same as capital X. In that case my matrix which consist of those counts. Basically first document will be represented by a vector of a length of my vocabulary. for every entry from my vocabulary will have a count of that word from my vocabulary. Sometimes it will will be zero.
(1:28:49) Right? If for example first document doesn't have a dogs let's say first word in my vocabulary is dogs it means first entry to the TTM will be zero right because no no dogs and so on. And I will have this matrix which will have 1 2 3 4 5 6 7 8 rows and number of columns will correspond to length of my vocabulary.
(1:29:14) So any questions about this and we can we can get this results. Let me plot it. So I say LDA and I say K is number of topics is equal to two. I specify seat in this case just for reproducibility and I get my LDA model. this way and then I also can using the same approach I can pull my most frequent terms from here terms LDA it is like terms apply to my LDA model result and I get five most frequent ones and I get I upload it later I get dogs enjoy love bark enthusiasm so bark is now here right you can see it now even if you apply the very same package but we use different city you may get different result then
(1:30:04) cats purchasing client fun so slightly different from previous love this mouse now I don't have mouse I have fun instead so any questions about this uh models or model uh implementations okay now um we have yeah Yeah, I cannot hear you. I'm sorry. So now let's move to the negative mis factorization.
(1:30:55) This is already much much I mean from kind of um uh from not even implementation but from kind of fundamental point of view it is it is quite different. We are not using any kind of um uh stoastic model in this case like in this case we use stastic model we assume how text is generated we maximize likelihood and we obtain all these parameters right in case of non- negative factoriization this is much much kind of faster much more straightforward procedure but the idea is kind of similar we are trying to say how to try to uh find topics within our documents this is basically the
(1:31:36) question. So ultimately we're going to answer the similar question. What our documents basically consist of but using matrix algebra. So now uh in this case we assume that uh vector v uh v is matrix v which is a matrix of those columns right in in our case we will have like six rows and we'll have number of columns which cor which corresponds to lens of vocabulary.
(1:32:12) Let me give you some examples so it is clear what's what's happening here. Capital V will be this matrix which contains the frequencies document term frequency basically let me say example. So this is non negative matrix factorization. Non- negative means that every entry of my matrix W and H in this case will be non negative by construction. I say maybe zero zero is okay or maybe positive.
(1:32:55) Let me say let me assume that W is my first matrix. Let me just take some numbers 2 3 4 for example. Let me also assume that there is some kind of capital H know which sort of refers to I would say a way we try to uh describe topics latin topics. Let me say 1 1 0 02 exam this way and then I say what is you can see on the screen W * H what is W * H we are talking about matrix multiplication in this case it means it will be 1 2 3 4 times using matrix multiplication time 1 1 0 0 02 this way. Let me explain how we compute it. Now first of all if my matrix is of size
(1:33:59) 2x2 and second one is 2x3 my result is supposed to be 2x3. So first number comes from here. Number of rows from the first matrix will correspond to number of rows in my result. And this three is number of columns in my second matrix. It will correspond to number of columns in my second matrix.
(1:34:30) So this will have one this will have um uh 2x3. Right? So 1 2 3 1 2 3 this way. How do we multiply matrices? As we say first row here and first column over there will result in my first entry here using that product 1 * 1 + 2 * 0 is 1. So this one is basically 1 * 1 + 2 * 0. That's how we get it. Now let's take second one.
(1:35:06) If I want to get second one I still have first row. That means first row from the first matrix and then second column from second matrix. I'm going to get 1 * 1 + 2 * 0 I get 1 again. This one comes from 1 * 1 + 2 * 0 because second one so this is like first one this is second one and I can continue this way. So I get 1 * 0 + 2 * 2 it means 4 in this case.
(1:35:41) Next one the second row 3 * 1 + 4 * 0 is 3. 3 * 1 + 4 * 0 is 3. 3 * 0 + 4 * 2 is 8. So this is my result and this is exactly what I basically say my V matrix right is like my V matrix. V is my original matrix which would contain my um counts right those term counts. Now in this case I just give you W and I give you H and I say let's try to compute V.
(1:36:16) But typically in practice it's all the way around. We are given V and we trying to reconstruct W and V. Right? So in case of um in case of non negative matrix factorization things are going to be all the way around actually we're going to say V which is my document frequency matrix right is going to be approximately given by some w * h why approximately because it is not always possible not always possible to represent my v this way kind of why any matrices I want then there is a question how to do it basically we're trying to use like L2 norm basically right we say this matrix
(1:37:00) and this matrix they have some kind of components we take difference between components square plus difference squared plus difference all sum all all them together and we try to maximize this way minimize it I'm sorry minimize deviation this way that's how we try to represent it is completely deterministic procedure basically and we trying to represent some kind of square matrix square matrix and then I'm going to have another H which is already representation of my topics basically and the assumption is that all entries
(1:37:36) of W will be non negative all entries of H will be non negative because my V itself consist of counts one time I observe first word from my vocabulary one time observe second one from word for my vocabulary and so on. Four times I reserve third word for my vocabulary.
(1:37:57) It is exactly my kind of representation of documents, right? First document, second document and I want to somehow reduce dimensionality. Basically in this case dimensionality is not really reduced but if my second matrix had for example five rows and this one had let me show you example on the screen. So this is example. So in this case we have three documents.
(1:38:24) Cats mail dogs bark and so on. And my vector my my document matrix V is going to look this way. My vocabulary cats dogs bark P grow. Then I say first document has cat. So one doesn't have a dog zero has male one bark zero pure zero grow zero second document dogs bark I get cats zero dogs one and so on this second document last document cats per dogs grow so it means one one here one one over there and zero for me and bark this is my document term matrix v just as an example and I say let me try to create some kind of
(1:39:11) decomposition, right? Decomposition of my capital V and also H. H is going to represent my essentially going to represent my my my topics. So I can interpret H as a sort of representation of topics. And what happens is now you can see here maybe 1 1 * 1 + 0 * 0 is 1. 1 * 0 + 0 * 1 is 0. 1 * 1 + 0 * 0 is 1 as I want and so on.
(1:39:47) Now this one will not work. 1 * 1 + 0 * 0 must be one. But I have zero. That's why we say approximately. So it's not going to be exactly equal because you cannot like always represent it this way you want right if you try to reduce dimensionality you kind of lose something but as I mentioned you have to basically minimize square deviations right like left hand side here capital V right hand side here w * h there will be some two matrices you have to compute L2 norm basically if you assume that first one is like a vector second one is like a vector if you reshape it right then
(1:40:24) you can compute L to norm norm and minim minimizing the s to norm you can decide how to represent and u what happens is essentially out of three documents we extracted two topics first topic second topic first row is like representation of first topic in some sense what is w is going to tell you that first document basically consist of mostly of topic one this one means mostly of topic one and zero of topic two. That's how it is.
(1:40:59) Second document zero of topic one and one of topic two. That's why it is about dogs. And you can kind of understand that probably first row means first topic means like about cats. Second topic means about dogs. What about last one and cats and also dogs? No. Okay. Five of first one and five of second one.
(1:41:22) It is not exactly correct. Clearly if you say.5 * 1 +.5 * 0 you get.5 not one but we say approximately so we just approximate in this way it is like deterministic way of trying to kind of represent your text where this capital H and WW basically tells you how much your topics contribute to every document and H itself is representation of your topics.
(1:41:48) Now you can try to understand what are those things mean. You can say maybe one means particular word. You can try to do it that way, right? Or maybe you can try to just uh uh it is like almost like embedding in some sense. Interpretation is not always like like straightforward. In order to try to understand what it means, you have to maybe pick particular documents and see what they correspond to.
(1:42:15) So that's how we we do it. Any questions about this? deterministic procedure. No any stoastic model in this case. No mixation of likelihood. We just literally try to use matrix algebra as you can see in order to no kind of reduce dimensionality. Three documents become like represented by two topics basically and this w itself is like matrix of weights in some sense.
(1:42:45) I can tell you that there are no restrictions formally speaking. You can see here like 1 0 0 1 I just made it this way. So it is like a little bit interpretable. So you understand that that we are talking about like basically like probabilities maybe but W formally speaking has to be only non- negative matrix.
(1:43:02) It means every entry must be zero or positive where it's not like you have to have like probabilities. Probabilities is like another constraint that would be even more difficult. We don't do that. We don't say we have to have probabilities of any kind. No just positive numbers. It could be easily like two and five. No. Okay. Contribution from first topic is two. Contribution from second topic is five.
(1:43:27) A little bit maybe more difficult to interpret but it is kind of straightforward procedure to reduce dimensionality this way. This is a little bit less popular. It is like um not so flexible. doesn't allow you to use uh this stochcastic approach where you assume some model right interpretation is more difficult in this case.
(1:43:51) It is like straightforward procedure to represent your text where this matric is but interpretation is more difficult. People less often use it actually in practice people try to use LD and also ST if they try to also find nice nice interpretation of results. So now uh applications u we can basically do the same things as as before right we try to kind of uh represent document why combination of different topics again if you refer to this representation like this example first row tells you first of topic one plus zero of topic two that's how we should interpret basically second document is mostly coming from second
(1:44:40) topic. That's exactly as we had before but no no kind of stoastic model anymore and that's how we should apply it in in practice right and uh let me show you implementation right here um we take count vectorzer as before create x which is we have to have this matrix x it is like v on my slide right V capital V is this matrix of my accounts and then I simply apply NMF.
(1:45:21) and MF specify how many topics I want to have and I get my results WH and W and H will be will be obtained this way after I fit it right I fit on my X I get W and also I can extract components like H those like representations of topics basically and I can also now if I want to kind of understand what what they mean I can also display my frequent uh frequent words right cats pure high trees clamp dog love enjoy humans well this how we can get it somewhat similar to previous results now any questions about this so now at some point as we as you probably know already right it is on the syllabus also it is And I mentioned
(1:46:15) first time during first class you will need to install R don't have to install it you can use actual cloud version if you want it's not important but you will have to use R for STM there will be assignment on on STM on structural topic modeling that is similar to LD but generalization and uh I I want you to use R because Python doesn't really have this package you can find some kind of variance of it implemented by someone but I didn't check how they work so I I I don't know if they reliable but R has STM package
(1:46:51) it has everything you need in order to perform this tasks that's why at some point you'll have to learn how to use Right very basic stuff so don't worry much but first we need to kind of install it let me say you go to posit.com and you can find here R you can see you and see uh let's say R studio right id R studio and you can scroll down and you can find uh there is a free for our download R studio desktop first of all you want to download R itself I think it tells you about this so first install R itself because you know you have to have this R rg is um it's like
(1:47:45) environment where you develop your code right it's almost like editor in some sense but it's more than editor it allows you to debug everything and so on so R is something which you need to sort of install before you install R studio itself and that's why first step will be tells you install R you have to install R let's say for Linux for Mac for Windows on this machine I use Windows right here so you can install it for Windows easily you and find it uh the first time and so on. For Mac, it is also quite quite
(1:48:18) straightforward. You can do it as well, right? So, it works I mean exactly same. And then second step, you install R Studio. Now, I recommend you to first install R and second as a second step. R Studio so R Studio knows where R is located and you don't have to change anything manually.
(1:48:37) It will right away know and will work right work right away. Then once it is installed R first and then R studio after you install R you will have R available um let me say R itself will look this way you can you can open it you can you know basically do whatever you want using R.
(1:49:12) it is a little bit less convenient because it doesn't allow you to efficiently develop algorithm code. Then our studio will look this way. You will click on it and you start will start creating your uh projects. So now um I'm not sure if uh I want to go like into detail details here right now but you can see here like one two three planes right you can also say file new let's say our script will have four the first one you can save it later and here is where you can start writing your code actually um in in R if you want the install package you just say install uh packages right this way and then you
(1:49:58) say hm for example STM structural topic models you can sell it and install it I think I already have it but that's how it will look now it is installed you have it let us uh use a library to sort of load it and it will be available then this one is something which I don't need anymore I just install it once So once I have it, I can use it next time.
(1:50:24) I simply need to load it. Many things already pre-loaded because base package has a lots of stuff. But STM is something which you will have to install and and load. So now STM is available. example. No, by the way, same about LD. Install packages. Um, must be LD. Then library LD and I have it. By the way, what I'm doing is I just say Ctrl enter.
(1:50:56) I like it this way because it runs particular line and moves to the next one. Uh now you can run it this way if you want, right? You can highlight the line if you want and run it. So this will will load LDA package. So now any questions about anything please install install RNR studio. All right. Try to play with this.
(1:51:26) Maybe we'll have more more uh meetings on that. Our TS also will show you how to use R. So don't worry about this. Um cloud doesn't require installation. Yes, you can work on cloud. Definitely you don't have to have to work it on your local machine if you don't want. Yeah, you will not have maybe very difficult problems in the sense that you can work locally. So it's not going to be problem.
(1:51:56) Any questions about anything? You can I'm not sure if you want to kind of so you can say get working directory you can see where everything is right now you can change it you can say set working directory you can some type something else it will be different and you start working in your particular folder it is now different We'll we'll see how to do it.
(1:52:34) I will not you I will not ask you to do maybe too much. Maybe you know what for the reports maybe you can present uh using uh using something similar to Jupiter notebook we have this capability in R as well. So we can represent it using using reports RD RMD report markdown files. I will show you examples later. Uh any questions? Okay, let's uh then stop. If you don't have any questions, uh we'll talk about R later.
(1:53:11) I will ask T first of all and also I will show you a little bit more next time maybe on Friday. Okay, let's stop now. Thank you.