% CSCI E-89B: Natural Language Processing
% Lecture 9: Classical Machine Learning for NLP
% English Version

\documentclass[11pt,a4paper]{article}

% ============================================================
% PACKAGES
% ============================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{adjustbox}  % 표/박스 크기 조절
\usepackage{booktabs}
\usepackage{array}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}

% ============================================================
% PAGE SETUP
% ============================================================
\geometry{margin=25mm}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{CSCI E-89B: Natural Language Processing}
\fancyhead[R]{Lecture 9}
\fancyfoot[C]{Page \thepage\ of \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% ============================================================
% COLORS
% ============================================================
\definecolor{harvardcrimson}{RGB}{165,28,48}
\definecolor{codegreen}{RGB}{34,139,34}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{148,0,211}
\definecolor{backcolour}{RGB}{248,248,248}
\definecolor{darkblue}{RGB}{0,0,139}

% ============================================================
% CODE LISTING STYLE
% ============================================================
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{codepurple},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{harvardcrimson},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2,
    language=Python
}
\lstset{style=pythonstyle}

% ============================================================
% TCOLORBOX ENVIRONMENTS
% ============================================================
\tcbuselibrary{skins,breakable}

\newtcolorbox{summarybox}[1][]{
    colback=red!5!white,
    colframe=harvardcrimson,
    fonttitle=\bfseries,
    title=Summary,
    breakable,
    #1
}

\newtcolorbox{overviewbox}[1][]{
    colback=blue!5!white,
    colframe=darkblue,
    fonttitle=\bfseries,
    title=Overview,
    breakable,
    #1
}

\newtcolorbox{infobox}[1][]{
    colback=cyan!5!white,
    colframe=cyan!60!black,
    fonttitle=\bfseries,
    title=Information,
    breakable,
    #1
}

\newtcolorbox{warningbox}[1][]{
    colback=yellow!10!white,
    colframe=orange!80!black,
    fonttitle=\bfseries,
    title=Warning,
    breakable,
    #1
}

\newtcolorbox{examplebox}[1][]{
    colback=green!5!white,
    colframe=green!60!black,
    fonttitle=\bfseries,
    title=Example,
    breakable,
    #1
}

\newtcolorbox{definitionbox}[1][]{
    colback=purple!5!white,
    colframe=purple!70!black,
    fonttitle=\bfseries,
    title=Definition,
    breakable,
    #1
}

\newtcolorbox{importantbox}[1][]{
    colback=red!10!white,
    colframe=red!70!black,
    fonttitle=\bfseries,
    title=Important,
    breakable,
    #1
}

% ============================================================
% CUSTOM COMMANDS
% ============================================================
\newcommand{\metainfo}[4]{
    \begin{tcolorbox}[colback=gray!10, colframe=gray!50, title=Lecture Information]
    \begin{tabular}{@{}ll}
    \textbf{Course:} & #1 \\
    \textbf{Lecture:} & #2 \\
    \textbf{Topic:} & #3 \\
    \textbf{Date:} & #4 \\
    \end{tabular}
    \end{tcolorbox}
}

% ============================================================
% DOCUMENT
% ============================================================
\begin{document}

\metainfo{CSCI E-89B: Natural Language Processing}{Lecture 9}{Classical Machine Learning for NLP}{Fall 2024}

\tableofcontents
\newpage

% ============================================================
\section{Introduction to Classical ML for NLP}
% ============================================================

\begin{overviewbox}
This lecture covers classical machine learning techniques applied to natural language processing. While neural networks dominate modern NLP, classical methods remain valuable for smaller datasets and provide interpretable baselines.
\end{overviewbox}

\subsection{Why Classical Methods?}

\begin{infobox}[title=When to Use Classical Methods]
Classical methods are particularly useful when:
\begin{itemize}
\item You have a \textbf{small dataset} (neural networks need lots of data)
\item You need \textbf{interpretability} (understand why predictions are made)
\item You need \textbf{fast training} (no GPU required)
\item You want a \textbf{baseline} before trying complex models
\end{itemize}
\end{infobox}

\subsection{Text Representation for Classical Methods}

\begin{importantbox}
Classical methods require \textbf{numerical vectors} as input. We cannot feed raw text directly. Common representations:
\begin{itemize}
\item \textbf{TF-IDF vectors}: Term frequency-inverse document frequency
\item \textbf{Bag of Words}: Simple word counts
\item \textbf{N-gram features}: Sequences of consecutive words
\end{itemize}
\end{importantbox}


% ============================================================
\section{Naive Bayes Classifier}
% ============================================================

\begin{overviewbox}
Naive Bayes is a probabilistic classifier based on Bayes' theorem with a ``naive'' assumption of feature independence. Despite its simplicity, it works surprisingly well for text classification.
\end{overviewbox}

\subsection{Bayes' Theorem}

\begin{definitionbox}[title=Bayes' Theorem]
\[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
\]

For classification:
\[
P(C_k | X) = \frac{P(X | C_k) \cdot P(C_k)}{P(X)}
\]

where:
\begin{itemize}
\item $C_k$: Class $k$ (e.g., spam or not spam)
\item $X$: Feature vector (e.g., TF-IDF representation)
\item $P(C_k)$: Prior probability of class $k$
\item $P(X|C_k)$: Likelihood of features given class
\item $P(C_k|X)$: Posterior probability (what we want)
\end{itemize}
\end{definitionbox}

\subsection{The Naive Assumption}

\begin{definitionbox}[title=Conditional Independence]
Naive Bayes assumes features are \textbf{conditionally independent} given the class:
\[
P(X | C_k) = P(x_1 | C_k) \cdot P(x_2 | C_k) \cdot \ldots \cdot P(x_n | C_k) = \prod_{i=1}^{n} P(x_i | C_k)
\]
\end{definitionbox}

\begin{warningbox}[title=Why ``Naive''?]
The independence assumption is rarely true in practice:
\begin{itemize}
\item If ``cat'' appears, ``dog'' is less likely in the same document
\item Words are correlated through context
\end{itemize}

However, TF-IDF already loses much contextual information, so the assumption is less problematic than it seems. The classifier still performs well in practice!
\end{warningbox}

\subsection{Classification Decision}

\begin{definitionbox}[title=Classification Rule]
Choose the class with highest posterior probability:
\[
\hat{y} = \arg\max_{C_k} P(C_k | X) = \arg\max_{C_k} P(C_k) \prod_{i=1}^{n} P(x_i | C_k)
\]

In practice, use log probabilities for numerical stability:
\[
\hat{y} = \arg\max_{C_k} \left[ \log P(C_k) + \sum_{i=1}^{n} \log P(x_i | C_k) \right]
\]
\end{definitionbox}

\subsection{Applications}

\begin{examplebox}[title=Common Applications]
\begin{itemize}
\item \textbf{Spam detection}: Classic application
\item \textbf{Sentiment analysis}: Positive/negative classification
\item \textbf{Document categorization}: News topic classification
\end{itemize}
\end{examplebox}

\subsection{Implementation}

\begin{lstlisting}[caption={Naive Bayes for Text Classification}, breaklines=true]
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split

# Sample documents
documents = [
    "Cats are wonderful pets",
    "Dogs enjoy long walks",
    # ... more documents
]
labels = [0, 1, ...]  # 0=cats, 1=dogs

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    documents, labels, test_size=0.25
)

# Create TF-IDF vectors
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)  # Don't refit!

# Train and predict
nb = MultinomialNB()
nb.fit(X_train_tfidf, y_train)
predictions = nb.predict(X_test_tfidf)
\end{lstlisting}

\begin{warningbox}[title=Data Leakage Warning]
Always fit the TF-IDF vectorizer on \textbf{training data only}, then transform test data. Never fit on test data---this causes data leakage!
\end{warningbox}


% ============================================================
\section{K-Nearest Neighbors (KNN)}
% ============================================================

\begin{overviewbox}
KNN is a simple, non-parametric algorithm that classifies based on the labels of the K closest training examples. It requires no training phase---it memorizes the entire dataset.
\end{overviewbox}

\subsection{How KNN Works}

\begin{definitionbox}[title=KNN Algorithm]
For a new data point:
\begin{enumerate}
\item Compute distances to all training points
\item Find the K nearest neighbors
\item \textbf{Classification}: Take majority vote among K neighbors
\item \textbf{Regression}: Take average of K neighbors' values
\end{enumerate}
\end{definitionbox}

\begin{examplebox}[title=1D Example]
Data points: $\{(1, 0), (2, 0), (3, 0), (5, 1), (6, 1), (7, 1)\}$

To classify point $x = 4$ with $K = 3$:
\begin{enumerate}
\item Nearest neighbors: $x = 3, 5, 6$ (or $3, 5, 2$)
\item Labels: $\{0, 1, 1\}$
\item Majority vote: 1 wins
\item Prediction: Class 1
\end{enumerate}
\end{examplebox}

\subsection{Distance Metrics}

\begin{definitionbox}[title=Common Distance Metrics]
\textbf{Euclidean Distance} (default):
\[
d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
\]

\textbf{Manhattan Distance}:
\[
d(x, y) = \sum_{i=1}^{n} |x_i - y_i|
\]

\textbf{Cosine Similarity} (often better for text):
\[
\text{similarity}(x, y) = \frac{x \cdot y}{\|x\| \|y\|}
\]
\end{definitionbox}

\subsection{Feature Scaling}

\begin{warningbox}[title=Scale Your Features!]
If features are on different scales, larger-scale features dominate:
\begin{itemize}
\item Age: 20--80 years (range: 60)
\item Income: \$20,000--\$200,000 (range: 180,000)
\end{itemize}

Income will completely dominate the distance calculation! Always normalize features to similar scales.
\end{warningbox}

\subsection{Choosing K}

\begin{importantbox}
\textbf{Use odd K} for binary classification to avoid ties.

\textbf{Choosing K}:
\begin{itemize}
\item Too small K: Overfitting (sensitive to noise)
\item Too large K: Underfitting (ignores local structure)
\item Optimize K using validation set or cross-validation
\end{itemize}
\end{importantbox}

\subsection{Is KNN Deterministic?}

\begin{warningbox}[title=KNN Can Be Stochastic!]
KNN is \textbf{not always deterministic}:
\begin{itemize}
\item \textbf{Even K}: May have ties in majority vote
\item \textbf{Equal distances}: Multiple points at same distance---which to include?
\end{itemize}

Ties must be broken randomly, so results may vary between runs. Default implementations often run multiple times and average.
\end{warningbox}

\subsection{Implementation}

\begin{lstlisting}[caption={KNN for Text Classification}, breaklines=true]
from sklearn.neighbors import KNeighborsClassifier

# After TF-IDF vectorization...
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_tfidf, y_train)
predictions = knn.predict(X_test_tfidf)

# Optimize K
from sklearn.model_selection import GridSearchCV

param_grid = {'n_neighbors': range(1, 21)}
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train_tfidf, y_train)
print(f"Best K: {grid_search.best_params_}")
\end{lstlisting}


% ============================================================
\section{Logistic Regression}
% ============================================================

\begin{overviewbox}
Logistic regression is a linear model for binary classification. It's equivalent to a single-layer neural network with sigmoid activation.
\end{overviewbox}

\subsection{The Model}

\begin{definitionbox}[title=Logistic Regression]
\[
P(y = 1 | X) = \sigma(W \cdot X + b) = \frac{1}{1 + e^{-(W \cdot X + b)}}
\]

where:
\begin{itemize}
\item $\sigma$: Sigmoid function
\item $W$: Weight vector (learned)
\item $b$: Bias term (learned)
\item $X$: Feature vector
\end{itemize}
\end{definitionbox}

\subsection{Decision Boundary}

\begin{importantbox}
The decision boundary is where $P(y=1|X) = 0.5$, which means $W \cdot X + b = 0$.

In 2D, this is a \textbf{straight line}. In higher dimensions, it's a \textbf{hyperplane}.

\textbf{Limitation}: Logistic regression can only separate classes that are linearly separable.
\end{importantbox}

\subsection{Connection to Neural Networks}

\begin{infobox}[title=Logistic Regression = Simple Neural Network]
Logistic regression is exactly a neural network with:
\begin{itemize}
\item No hidden layers
\item Sigmoid activation on output
\item Binary cross-entropy loss
\end{itemize}

Training via maximum likelihood is mathematically equivalent to minimizing cross-entropy loss.
\end{infobox}

\subsection{Non-linear Decision Boundaries}

\begin{examplebox}[title=Feature Engineering for Non-linearity]
If data isn't linearly separable, add polynomial features:

Original features: $x_1, x_2$

Extended features: $x_1, x_2, x_1^2, x_2^2, x_1 x_2$

This allows curved decision boundaries, but requires manual feature design.
\end{examplebox}

\subsection{Implementation}

\begin{lstlisting}[caption={Logistic Regression for Text}, breaklines=true]
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_tfidf, y_train)
predictions = lr.predict(X_test_tfidf)

# Get probabilities
probabilities = lr.predict_proba(X_test_tfidf)
\end{lstlisting}


% ============================================================
\section{Support Vector Machines (SVM)}
% ============================================================

\begin{overviewbox}
SVMs were the dominant machine learning method before deep learning (pre-2012). They find the hyperplane that maximizes the margin between classes.
\end{overviewbox}

\subsection{The Maximum Margin Idea}

\begin{definitionbox}[title=Maximum Margin Classifier]
Given two linearly separable classes, find the hyperplane that:
\begin{itemize}
\item Correctly separates all points
\item Maximizes the \textbf{margin}---the distance to the nearest points (support vectors)
\end{itemize}
\end{definitionbox}

\begin{examplebox}[title=Visualizing the Margin]
Imagine two parallel lines (in 2D) separating two classes. The gap between these lines is the margin. SVM finds the position that makes this gap as wide as possible.

Points touching the margin boundaries are \textbf{support vectors}---only these points determine the decision boundary position.
\end{examplebox}

\subsection{Mathematical Formulation}

\begin{definitionbox}[title=SVM Optimization]
The hyperplane is defined by $W \cdot X + b = 0$.

\textbf{Hard Margin SVM} (linearly separable):
\[
\min_{W, b} \frac{1}{2} \|W\|^2 \quad \text{subject to} \quad y_i(W \cdot X_i + b) \geq 1 \quad \forall i
\]

This maximizes the margin $\frac{2}{\|W\|}$.
\end{definitionbox}

\subsection{The Overfitting Problem}

\begin{warningbox}[title=Hard Margin Overfitting]
With hard margin SVM, only support vectors affect the decision boundary. Moving one support vector changes everything!

This is \textbf{overfitting}---the model is too sensitive to individual points.
\end{warningbox}

\subsection{Soft Margin SVM}

\begin{definitionbox}[title=Soft Margin SVM]
Allow some points to be inside the margin or misclassified:
\[
\min_{W, b, \xi} \frac{1}{2} \|W\|^2 + C \sum_{i} \xi_i \quad \text{subject to} \quad y_i(W \cdot X_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
\]

where:
\begin{itemize}
\item $\xi_i$: Slack variable (how far point $i$ is from correct side)
\item $C$: Regularization parameter (trade-off between margin and violations)
\end{itemize}
\end{definitionbox}

\begin{importantbox}
\textbf{Parameter C}:
\begin{itemize}
\item Large C: Few violations allowed, narrower margin (risk overfitting)
\item Small C: More violations allowed, wider margin (risk underfitting)
\end{itemize}

Tune C using cross-validation.
\end{importantbox}

\subsection{Non-linear Decision Boundaries: Kernels}

\begin{warningbox}[title=What if data isn't linearly separable?]
Consider points arranged in a circle (class 0) surrounded by points outside (class 1). No line can separate them!
\end{warningbox}

\begin{definitionbox}[title=The Kernel Trick]
Add a new dimension: $z = x_1^2 + x_2^2$ (distance from origin).

In this 3D space, points are linearly separable by a plane!

\textbf{Key insight}: We don't actually compute the transformation. Instead, we replace the dot product $X_i \cdot X_j$ with a \textbf{kernel function} $K(X_i, X_j)$.
\end{definitionbox}

\subsection{Common Kernels}

\begin{center}
\begin{tabular}{ll}
\toprule
\textbf{Kernel} & \textbf{Formula} \\
\midrule
Linear & $K(x, y) = x \cdot y$ \\
Polynomial & $K(x, y) = (x \cdot y + c)^d$ \\
RBF (Gaussian) & $K(x, y) = \exp(-\gamma \|x - y\|^2)$ \\
\bottomrule
\end{tabular}
\end{center}

\begin{infobox}[title=RBF Kernel]
The RBF (Radial Basis Function) kernel can approximate any decision boundary. It's equivalent to projecting to infinite dimensions!

The $\gamma$ parameter controls how ``local'' the influence of each point is.
\end{infobox}

\subsection{Implementation}

\begin{lstlisting}[caption={SVM for Text Classification}, breaklines=true]
from sklearn.svm import SVC

# Linear kernel
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train_tfidf, y_train)

# RBF kernel
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
svm_rbf.fit(X_train_tfidf, y_train)

# Grid search for optimal parameters
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf', 'poly']
}
grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_train_tfidf, y_train)
\end{lstlisting}


% ============================================================
\section{Decision Trees and Random Forests}
% ============================================================

\begin{overviewbox}
Decision trees recursively split data based on feature values. Random forests combine many trees to reduce overfitting and improve accuracy.
\end{overviewbox}

\subsection{Decision Tree Algorithm}

\begin{definitionbox}[title=Building a Decision Tree]
At each node:
\begin{enumerate}
\item Consider all features and all possible split points
\item Choose the split that best separates classes
\item Recurse on each resulting subset
\item Stop when a criterion is met (max depth, min samples, pure node)
\end{enumerate}
\end{definitionbox}

\subsection{Splitting Criteria}

\begin{definitionbox}[title=Gini Index (Classification)]
For a node with proportion $p$ of class 1:
\[
\text{Gini} = 1 - (p^2 + (1-p)^2) = 2p(1-p)
\]

\begin{itemize}
\item Pure node ($p=0$ or $p=1$): Gini = 0
\item Maximum impurity ($p=0.5$): Gini = 0.5
\end{itemize}

Choose splits that minimize total Gini index of child nodes.
\end{definitionbox}

\begin{definitionbox}[title=Entropy (Alternative)]
\[
\text{Entropy} = -p \log_2(p) - (1-p) \log_2(1-p)
\]

Information gain = parent entropy minus weighted child entropy.
\end{definitionbox}

\subsection{Overfitting in Decision Trees}

\begin{warningbox}[title=Trees Easily Overfit]
Without restrictions, a decision tree will:
\begin{itemize}
\item Split until every leaf contains one sample
\item Perfectly classify training data
\item Perform poorly on test data
\end{itemize}

\textbf{Solutions}: Limit max depth, require minimum samples per leaf, or pruning.
\end{warningbox}

\subsection{Random Forests}

\begin{definitionbox}[title=Random Forest Algorithm]
\begin{enumerate}
\item Create $B$ bootstrap samples (sample with replacement)
\item For each sample, build a decision tree:
    \begin{itemize}
    \item At each split, consider only $\sqrt{n}$ random features
    \item Grow tree fully (no pruning needed)
    \end{itemize}
\item Final prediction: Majority vote (classification) or average (regression)
\end{enumerate}
\end{definitionbox}

\subsection{Why Random Forests Work}

\begin{importantbox}
\textbf{Two sources of randomness}:
\begin{enumerate}
\item \textbf{Bootstrap sampling}: Each tree sees different data
\item \textbf{Random feature selection}: Each split considers different features
\end{enumerate}

This creates \textbf{diverse} trees. When averaged, individual tree errors cancel out, leaving only the ``signal.''

Random forests are nearly \textbf{impossible to overfit}!
\end{importantbox}

\subsection{Bootstrap Sampling}

\begin{definitionbox}[title=Bootstrap Sample]
Sample $n$ points \textbf{with replacement} from dataset of size $n$:
\begin{itemize}
\item Some points appear multiple times
\item Some points don't appear at all (~37\%)
\item Each bootstrap sample is slightly different
\end{itemize}
\end{definitionbox}

\subsection{Interpreting Black Box Models}

\begin{warningbox}[title=Interpretability Challenge]
Random forests are ``black boxes''---hard to understand why predictions are made.

\textbf{Solutions}:
\begin{itemize}
\item \textbf{Partial Dependence Plots (PDP)}: Show how each feature affects predictions
\item \textbf{SHAP values}: Explain individual predictions
\item \textbf{Feature importance}: Which features matter most
\end{itemize}
\end{warningbox}

\subsection{Implementation}

\begin{lstlisting}[caption={Random Forest for Text}, breaklines=true]
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(
    n_estimators=100,       # Number of trees
    max_features='sqrt',    # Features per split
    criterion='gini',       # Splitting criterion
    random_state=42
)
rf.fit(X_train_tfidf, y_train)
predictions = rf.predict(X_test_tfidf)

# Feature importance
importance = rf.feature_importances_
\end{lstlisting}


% ============================================================
\section{K-Fold Cross-Validation}
% ============================================================

\begin{overviewbox}
K-fold cross-validation is essential for classical methods where datasets are small and we need reliable performance estimates.
\end{overviewbox}

\subsection{Why Cross-Validation?}

\begin{importantbox}
For neural networks with large datasets:
\begin{itemize}
\item Split into train/validation/test
\item Plenty of data for each split
\end{itemize}

For classical methods with small datasets:
\begin{itemize}
\item Can't afford to reserve much for testing
\item Single split may not be representative
\item Need K-fold cross-validation
\end{itemize}
\end{importantbox}

\subsection{K-Fold Procedure}

\begin{definitionbox}[title=K-Fold Cross-Validation]
\begin{enumerate}
\item Split data into K equal parts (folds)
\item For $i = 1, \ldots, K$:
    \begin{itemize}
    \item Use fold $i$ as test set
    \item Use remaining $K-1$ folds as training set
    \item Train model and evaluate on fold $i$
    \end{itemize}
\item Average performance across all K folds
\end{enumerate}
\end{definitionbox}

\begin{examplebox}[title=5-Fold Cross-Validation]
Data split: [Fold 1] [Fold 2] [Fold 3] [Fold 4] [Fold 5]

\begin{itemize}
\item Run 1: Train on 2,3,4,5, test on 1
\item Run 2: Train on 1,3,4,5, test on 2
\item Run 3: Train on 1,2,4,5, test on 3
\item Run 4: Train on 1,2,3,5, test on 4
\item Run 5: Train on 1,2,3,4, test on 5
\end{itemize}

Every data point is tested exactly once!
\end{examplebox}

\subsection{Using Pipelines}

\begin{warningbox}[title=TF-IDF Must Be Refit Each Fold!]
When using K-fold, the TF-IDF vectorizer must be fit on training folds only, not test fold. Use sklearn Pipeline to handle this automatically.
\end{warningbox}

\begin{lstlisting}[caption={Pipeline with Cross-Validation}, breaklines=true]
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Create pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(stop_words='english')),
    ('clf', MultinomialNB())
])

# K-fold cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(pipeline, documents, labels, cv=cv, scoring='accuracy')

print(f"Mean accuracy: {scores.mean():.4f}")
print(f"Std: {scores.std():.4f}")
\end{lstlisting}


% ============================================================
\section{Evaluation Metrics for Classification}
% ============================================================

\subsection{Confusion Matrix}

\begin{definitionbox}[title=Confusion Matrix]
\begin{center}
\begin{tabular}{l|cc}
& Predicted Positive & Predicted Negative \\
\hline
Actual Positive & True Positive (TP) & False Negative (FN) \\
Actual Negative & False Positive (FP) & True Negative (TN) \\
\end{tabular}
\end{center}
\end{definitionbox}

\subsection{Key Metrics}

\begin{definitionbox}[title=Classification Metrics]
\textbf{Accuracy}:
\[
\text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
\]

\textbf{Sensitivity (Recall, True Positive Rate)}:
\[
\text{Sensitivity} = \frac{TP}{TP + FN} = \frac{\text{Correctly predicted positives}}{\text{All actual positives}}
\]

\textbf{Specificity (True Negative Rate)}:
\[
\text{Specificity} = \frac{TN}{TN + FP} = \frac{\text{Correctly predicted negatives}}{\text{All actual negatives}}
\]

\textbf{Precision}:
\[
\text{Precision} = \frac{TP}{TP + FP} = \frac{\text{True positives}}{\text{All predicted positives}}
\]

\textbf{F1 Score}:
\[
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\]
\end{definitionbox}

\subsection{Imbalanced Datasets}

\begin{warningbox}[title=Accuracy is Misleading for Imbalanced Data]
If 95\% of emails are not spam:
\begin{itemize}
\item Always predicting ``not spam'' gives 95\% accuracy!
\item But sensitivity for spam is 0\%
\end{itemize}

\textbf{Solutions}:
\begin{itemize}
\item Use precision/recall/F1 instead of accuracy
\item Adjust classification threshold (from 0.5)
\item Use class weights or focal loss
\item Resample data (SMOTE, undersampling)
\end{itemize}
\end{warningbox}


% ============================================================
\section{Model Comparison: Text Classification Example}
% ============================================================

\begin{examplebox}[title=BBC News Classification Results]
Task: Classify news articles as ``tech'' or ``not tech''

\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Sensitivity} & \textbf{Specificity} \\
\midrule
Naive Bayes & 0.90 & 0.43 & 1.00 \\
KNN (K=5) & 0.97 & 0.94 & 0.98 \\
KNN (K=19, optimized) & 0.98 & 0.94 & 0.99 \\
Logistic Regression & 0.92 & 0.71 & 1.00 \\
SVM (linear) & 0.92 & 0.73 & 1.00 \\
SVM (optimized) & 0.99 & 0.97 & 1.00 \\
Random Forest & 0.98 & 0.89 & 1.00 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Winner}: SVM with optimized parameters (C=10, linear kernel)
\end{examplebox}

\begin{infobox}[title=Observations]
\begin{itemize}
\item Models predicting mostly ``not tech'' have high specificity but low sensitivity
\item This happens because ``not tech'' is the majority class
\item SVM (pre-neural network era champion) performs best overall
\item KNN performs surprisingly well with optimized K
\end{itemize}
\end{infobox}


% ============================================================
\section{One-Page Summary}
% ============================================================

\begin{summarybox}
\textbf{Classical ML for NLP}: Still valuable for small datasets and interpretability.

\textbf{Naive Bayes}:
\begin{itemize}
\item Assumes feature independence: $P(X|C) = \prod_i P(x_i|C)$
\item Fast, simple, works well for text
\item Use: Spam detection, sentiment analysis
\end{itemize}

\textbf{K-Nearest Neighbors}:
\begin{itemize}
\item Classify by majority vote of K nearest points
\item No training---stores entire dataset
\item Scale features! Choose odd K
\item Tune K via cross-validation
\end{itemize}

\textbf{Logistic Regression}:
\begin{itemize}
\item $P(y=1|X) = \sigma(W \cdot X + b)$
\item Linear decision boundary (hyperplane)
\item Equivalent to single-layer neural network
\end{itemize}

\textbf{Support Vector Machines}:
\begin{itemize}
\item Maximize margin between classes
\item Soft margin allows violations (parameter C)
\item Kernels enable non-linear boundaries (RBF, polynomial)
\item Dominant method pre-2012
\end{itemize}

\textbf{Random Forests}:
\begin{itemize}
\item Ensemble of decision trees
\item Bootstrap + random feature selection
\item Nearly impossible to overfit
\item Interpret via SHAP, PDP
\end{itemize}

\textbf{K-Fold Cross-Validation}:
\begin{itemize}
\item Essential for small datasets
\item Each point tested exactly once
\item Use Pipeline to refit TF-IDF each fold
\end{itemize}

\textbf{Metrics for Imbalanced Data}:
\begin{itemize}
\item Accuracy misleading when classes unbalanced
\item Use precision, recall, F1, AUC-PR
\end{itemize}
\end{summarybox}


% ============================================================
\section{Glossary}
% ============================================================

\begin{definitionbox}[title=Key Terms]
\begin{itemize}
\item \textbf{Naive Bayes}: Probabilistic classifier assuming feature independence
\item \textbf{KNN}: K-Nearest Neighbors---classify by neighbor majority
\item \textbf{Logistic Regression}: Linear model with sigmoid output
\item \textbf{SVM}: Support Vector Machine---maximize margin classifier
\item \textbf{Kernel}: Function replacing dot product to enable non-linear boundaries
\item \textbf{Support Vector}: Point touching the margin boundary
\item \textbf{Soft Margin}: SVM allowing margin violations
\item \textbf{Decision Tree}: Recursive feature splitting
\item \textbf{Random Forest}: Ensemble of trees with bootstrap + random features
\item \textbf{Bagging}: Bootstrap AGGregatING---averaging bootstrap models
\item \textbf{Gini Index}: Impurity measure for splitting
\item \textbf{Cross-Validation}: Rotating train/test splits
\item \textbf{Sensitivity (Recall)}: TP / (TP + FN)
\item \textbf{Specificity}: TN / (TN + FP)
\item \textbf{Precision}: TP / (TP + FP)
\item \textbf{F1 Score}: Harmonic mean of precision and recall
\item \textbf{Data Leakage}: Using test information during training
\item \textbf{Bootstrap Sample}: Sample with replacement
\item \textbf{PDP}: Partial Dependence Plot for interpretability
\item \textbf{SHAP}: Shapley Additive Explanations for feature importance
\end{itemize}
\end{definitionbox}

\end{document}
