89 day 9 section - YouTube
https://www.youtube.com/watch?v=Z3hp-rWmZfg

Transcript:
(00:01) Hello everyone. Hello Dr. Karki. Even how is your assignment? Just moving along with the our project that's been assigned. Just been juggling it with the exam preparation. Exam preparation. Okay. Like for different class. Yes. Yes. E106 which is actually where I've been learning R. Okay. Okay.
(00:38) So, um last time we covered uh STM, right? So, you're working on this and next time we are going to apply classical techniques for text classification. Let's consider some examples about text classification. using classical techniques. So in this case you can see I import libraries right classifiers of different kinds and let me now uh uh show what what data set I'm using. In this case, I take data set of a specific news BBC data.csv.
(01:22) So some kind of data file which I found I believe on probably it was u so where was it? Uh I think it was kegle or something. So this data set about news and uh in this case we have around five different types of news about sport about tech and so on and our task is going to be basically train model which will classify them.
(01:52) Now if you have five different classes that means formally speaking it means that we have to have some kind of model which can classify into five different classes and maybe as you know in such cases we can easily use for example support machine but we take like one class at a time basically take one class at a time versus the rest of those take tech in tech um news about tech and the rest news about sport and the rest and we run different like five different types of model models and then we decide which one it is going to be. It means even if model classifies into only two different classes, we
(02:25) still can use it. So now in this case uh let me just display information about this data set. We have um 2,225 observations. It means to 2, 225 news. And we have also in this case u one two three four five different categories. In this case you can see what I do.
(02:58) I say df category and I count and and I display my values right values. No basically I take unique categories and count how many news we have for each category. And you can see that it is approximately evenly evenly distributed. Maybe a little bit not exactly evenly but almost like equally distributed 500 about sports 500 about business and so on 400 about tech.
(03:23) Now in this assignment in this kind of exercise I focus on the problem of trying to identify if it is tech or not tech. I use only basically binary classifier. I say tech or not tech. Even though we understand that we can run it multiple times basically we can um find which one is about tech which is is not about tech we can find which is about sport which is not about sport and so on running like five different models basically using different labeling we can apply to this type of problem and classify into five different classes even if classifier itself doesn't really allow us to classify into five different
(04:00) classes like support vector machine would not be would not allow us to classify into five classes. It is like splitting with a line remember with a hyper plane but we can apply five support vector machines to kind of see what's happening here. There is actually in secret learner is very fast way to do it.
(04:20) But I I will simply say my t is going to be label one. Whatever is not t is going to be label zero. Let me first try to analyze my data set. I say let me look at my news. Then I apply know this function which computes number of words in every document in every news x.plit. It means it will split based on spaces.
(04:44) It will split based on spaces and then I will simply compute a length of those words. That means I will create number of words for every document which I will then display and I will see I will most simply uh see the distribution of number of words in every case in case of entertainment for example number of words on average is around you can see around 300 maybe and some of them go up to 3,000 okay 10 times greater than average maybe it is good idea to see what's going on there with those sort of outliers. Maybe they are not really
(05:22) actual news. Maybe there is some garbage. In this case, I think that 3,000 versus 300 is kind of okay. No kind of 10 times longer news. So, it is possible probably I didn't even actually display the results. But maybe it is good idea to look at outliers and always try to manually read what's what's happening.
(05:42) Try to clean it is you understand that it is quite important to manually clean your data set, right? because they say garbage in garbage out, right? So, we don't want to have any any kind of garbage in our news. I didn't display it.
(06:02) I just wanted to kind of mention that it is good idea to try to analyze it somehow like this way for example and try to see if those um outliers are not. Again, I don't see anything anything suspicious. If it is like 10 times longer than actual average news, maybe it is okay. 1,000 is probably okay. 3,000 is probably okay. Now business also seems to be kind of okay about sport is also on the same scale about politics is on the same scale about tech is also on the same scale it means I will kind of say that probably it seems okay I don't have any kind of issues and then my x is going to be news so news will be my news articles my documents remember 2225 documents will be my x my news and then
(06:43) I want to create my label in This case remember label was my category five types but I decided to use some kind of binary classifier such as support mastering it means I decide to label one tech and zero not tech because problem which I'm currently solving is is this tech or it is not about tech that's what I'm asking that's why I say apply then this my lambda function it is one if it is tech and zero otherwise and y will be label one for t and zero for other cases.
(07:21) So any questions so far and let's our first naive base classifier in this case. Let me say a couple of words about this pipeline. I'm not sure if you know about pipeline uh but let me maybe briefly mention what was happening here. So let's say I have case where I want to use neural network to maybe run some kind of classifier right. So let's say neural network.
(07:46) Now in this case we typically since we have a lots of parameters we typically have huge data set we have to have usually large data set in such cases and we say in case of neural networks we basically say let us sort of randomly split data set and say it is going to be like train observations train observations and the rest whatever is rest like 25% or whatever is going to be used for testing.
(08:19) To be precise, there is also typically third chunk which is used for validation, right? It is nice idea to have it reserved as well. When we train your network, we typically have to run it multiple times. We have to use this test performance and understand how it does and choose number of epochs, maybe hyperparameters and so on.
(08:44) The result test data set is used multiple multiple times kind of and in order to estimate ultimate performance as if it was like completely on new data set like when you put in production you want to know how it performs. Validation is used at that point validation will be used when you already chose your model chose your hyperparameters and you use it.
(09:04) So in this case case of neural networks you can basically simply take a data set split it into chunks and maybe you can sort of um reprocess your text and try already start fitting basically right when you build for example TF vectorizer in that case you understand that you have to train it on the train data set only because we have to train our inverse document frequency Then we will apply it already to test and to validation.
(09:36) Now it means information from train will sort of be trans transferred to test to validation data set. So in this case when I describe how we can do it probably we don't need to use any so any so-called pipelines. The pipeline in this case is not really important.
(09:56) You can just kind of manually program it and that is like few lines of code and it is done. Right? So that's how we do it. However, in cases such as support vector mastering, especially like logistic regression and so on, uh in those cases, we don't have to have too many observations. It is possible to use like small data set maybe and still be able to train the model and achieve nice performance.
(10:20) No, it means in that case maybe it is kind of not nice to split it this way reserve those data for testing for validation and train only on the part of my data set because it becomes maybe potentially sensitive to how I split it that's why in this case it is good idea in case of let's say classical classifiers right we can use k let's say classical classifier bars.
(10:56) Typically for new network we don't do it that way but for classical classifiers we use so-called kfold validation by two reasons. First reason data set is usually much much smaller. And second reason, our result is kind of reproducible. I mean I mean what I mean by this um if you train again on the on the same data set we're going to get most likely same result.
(11:25) If you run like linear regression you get you're going to get exactly same result. Even if you run SPM support on the same data set you get same result. That means it's not like in that case in case of neural network even if you use the same train data set second time you may be getting different result another student may get different result because of stasic approach in this case we don't have typical stastic approach or just minor stoastic as we discussed last time not so maybe important not so much stasic that's why in this case uh we can actually assume that
(11:59) uh we can sort of train this model sort of multiple times on different subsets of data. This way it is called keyfold validation. Let me say let me say as an example in this case example it is going to be K is equal to five. So I can sketch something and my data set is going to look this way.
(12:31) So this is my basically data set right. So this is my data set and I will say five old validation means I take one two three four five chunks of my data. I split it into five pieces and just like in that case I say validation is not needed anymore because we're not going to like use test data as much. We use test data only once to estimate performance. That means validation is not needed in this case.
(13:00) And we say let me run training algorithm first time. Think about specific example like support vector machine. I like in that case I say train data set first chunk second chunk next chunk next chunk and the rest is going to be reserved for testing. Right? So this way next is going to be reserved for testing. That's how we basically do it.
(13:26) But as I mentioned typically we don't have like large data set and maybe not so accurate in terms of estimating test performance because why we took last chunk maybe it should be should be different chunk. Okay let's take different chunk let's say second time we run it we take first one as training as training as training.
(13:46) Next one is going to be for testing only and then training this way. So we can do it that way as well. Now you can wonder how to estimate test error. Basically since we assume that model kind of doesn't change much when we move from this training number one to training number two performance can be estimated based on all points from last chunk and from the one before the last chunk.
(14:14) More than that we can do it five times clearly right. So we can do it five times. It means we can now say third time. First chunk is in, next is in. Third chunk is going to be used for testing only. And the last two will be for training. Then training number four. First is used, next is for training for for testing and the rest is for training this way.
(14:54) And last one train number five means first one is for testing then train train this way. So again what I mean by this that this is going to be for testing test data set and the rest is going to be for train in every case. Now if I do it this way it means I basically run three type of optimizations of the same kind of model same support vector machine basically yes it will be simated differently in all cases but it is kind of same type of model in case of neural networks it would not be nice idea because network in the first case and network in second case could be quite
(15:30) different from each other because of stoasticity because of many local minima in case of such things as support machine I assume that first model is not as much different from second model right it means no except for random chance because of because of splitting that's why it makes sense to run it five times every time I have some kind of data which were not used to train the model now I can make literally predictions of those observations in my test data set based on train number one second time predictions based on train model number predictions based on train model number
(16:11) three, train model number four, train model number five and number of points which I'm I'm able to predict now will be basically equal to this data set size like in my case it will be 2,225 data points which I'm able to predict based only on those observations which were not used based on those which are not in my test data set every Then I can compute my error based on all the observations right.
(16:43) Yeah, they can come from different models kind of formally speaking they were trained differently but because not much variability in my model I believe it is nice approach nice estimator of my tester I can estimate my tester again by two reasons first of all my model is kind of kind of reproducible not like normal network secondary I assume that extra data set is not huge I cannot just waste so much to reserve for testing I want to do it this way that's that's Why I do it this way like in our case 2,225 documents is not as huge. So maybe K-fold validation is a nice approach.
(17:22) Okay. Now there is also one thing which is I hope obvious okay we are obtain test there and so on let me ask you a question which model should you put into production like you want to literally say now to your management now I have obtained support vector machine I found best parameters and so on I'm ready to basically put this model into production they tell you okay train it and give it to me which one should I give to to the management what they Think which one should I put to production? Does it does it make a difference? It does because different splitting.
(18:12) Okay. Uh let me just tell you it it makes sense when you're already done with your estimation of test performance and stuff like that and you already want to predict tomorrow or it is like for example some kind of forecasting algorithm right tomorrow then you can use actually entire data set and train it on entire data set with your parameters and put into production you can do it this way.
(18:37) So now those give you idea how your model will perform given your parameters and stuff but of course you can retrain based on all observations you will not have to already uh test you don't have to estimate any error you already have it so now if I do it that way if I do it like in case of neural networks pipeline is not as important if I do it that way now you can imagine it means I have to split my data set five different times basically then train it test train test train test becomes a little bit heavy in terms of
(19:09) um number of lines right to to write a loop maybe that's why there is this pipeline I basically tell it is like plan what I'm going to do essentially construction so blueprint what I'm going to do I say create this object pipeline where I specify first I want to apply my t ID vectorzer I can specify parameters right stop words will be English stop words will be dropped and so on and second I will apply my knife base classifier so pipeline is like instructions basically my blueprint my my plan what I'm going to do then I say cross validation
(19:50) I split it into five different chunks just as I sketched this way shuffle through means what shuffle through means it's not like I shuffle every time no it's not about that it means I take my data set. Maybe there was some kind of specific order. Maybe it is it is like some kind of you know survey and there is some order.
(20:15) This will like initially like those at the start will have like five years old observations at the end will be like yesterday's observations. I don't want to split it this way. I want to kind of shuffle first because I don't know how my data were stored into the file. That's why I shuffle only once.
(20:33) Then I split it and I keep going and I keep those this splitting like forever basically for all five validations. That's what split shuffle means only shuffle once and split it and you keep it random state just for reproducibility you typically we say something here right okay now everything is basically already uh predetermined and I say cross validation score as simple as that cross validation score and I will say pipeline it will know what I want to do it will know that I want to apply TF IDF vectorizer first then it will know it will know that I want to apply my naive
(21:10) my um naive base classifier. X is my inputs. Remember those documents, those news. Y is my label one and zero. One is check zero is not tech. I specify my cross validation. I already obtain this object here and which score I want to use accuracy. Okay, I want to I want to obtain accuracy.
(21:36) So now uh that's how we do it like in a sort of kind of um compact way. So it doesn't have many lines like loops and stuff. Now in my case I want to emphasize in my case pipeline does make sense because every time I basically have to uh apply TF IF vectorizer differently. Remember TF vectorizer should never use data from test data set.
(22:03) It means when I move to second splitting I have to refit my TF IDF vectorzer. I have to refit this use this pipeline. So next time it will be obtained differently. I have to refit my idea factorizer. Now if you actually decide that you split it in particular way like in my case of neural networks and then you want to try different models for this particular splitting it does not make sense much to use pipeline.
(22:32) Maybe on the contrary it is counterproductive because next time you move to different model like some kind of logistic regression. Why would you want to refit your TF vectorizer? It doesn't make much sense if you use the very same splitting next time. Then you probably want to stick to the same TF factorizer result. That's why in your case on your next assignment, I will not ask you to do K fold validation.
(22:55) I will ask you to probably split it, right? And then try different models. So in your case, you actually better it is more kind of economical to apply TF vectorzer first. stick to this representation and move forward with different models. So my pipeline is nice as long as I understand that I have to refit it basically right.
(23:17) So just keep this in mind and okay now let's see what we obtain predet values we can uh compute those um printed values and let's now compute the matrix my accuracy score is here sensitivity which is also called recall maybe you know it is called recall right sensitivity is called recall um no it means um essentially Um essentially it means uh um how many uh so it is like accuracy means what proportion were printed correctly essentially right and sensitivity means out of those positive ones what proportion was printed as positive ones same about
(24:08) specificity if I basically re label and say that zero is now kind of It means what proportional of zeros was treated as zero essentially it's called specificity and then confusion matrix also can be computed in this case and let me display these results. So my um cross validation accuracy scores it is like five chunk that's why I get five different things but typically we don't look at those we typically look at mean accuracy in this case.
(24:42) 9 is my mean accuracy sensitivity sensitivity which is like recall for tech right is 43 specificity is one so basically everyone who was zero was correctly predicted as zero that is As you look at this confus confusion matrix if actual was non techch it was always predicted non techch that's why it is one. So basically out of 2,00 1,824 all 1,824 were predicted as not check that's why specificity is one of as for the uh sensitivity it means how well we predict documents tech out of true tech we have to take 230 plus 171 out of this number we compute how much 171 is
(25:35) because only 171 were rated as tech. Now there is something to think about. First of all, accuracy seems okay like 90% I don't know it depends on the context but seems okay uh at least model works specificity one is is perfect right but there is a catch if you have a lots of lots of uh zeros like in this case non techch means zero then model will sort of want to predict those to be zero to be on safe side kind of right if our minimization problem tries to minimize something like like cross entropy Google cross entropy or whatever it is safe to to say it is zero essent that's what's
(26:19) happening in this case it means in this case you want to maybe ultimately if you are really concerned about predicting some of the zeros zero predicting some of u uh let's say if you really concerned about uh let's say the this number being sold large right we don't want a actual check to be often predicted non tech because it seems like many many many things simply predicted to be non tech to be zeros just to be on safe side because it knows the model knows that I have a lots of non tech articles now in this case you have to think what application we
(27:00) have in our in our example if you say I want to just get a number of articles so it is enough for me to read something but I want to read only about tech and you say 171 is more than sufficient for me. Everyone will be about tech.
(27:18) Then you kind of don't you know maybe not concerned about missing some some of the tech articles, tech news. You only kind of worry about getting only tech articles. It is one kind of application or you can say I want to basically don't miss any of articles. I want to make sure that whatever I look at I have I have all tech articles selected. Maybe it is huge. Maybe there's more than it is supposed to be.
(27:41) I can manually read it and sort of filter it later. Not the same about like disease. Do you really want to tell sick healthy person that you potentially could be sick and you have to take another test or you kind of concerned about missing sick people and you say someone is sick but model doesn't tell you that person is sick. So it depends on application.
(28:03) Clearly in many cases you don't want to miss sick people. it is okay to send to the second test then it is concerned whatever we get is concerned clearly right like in my case if I really want to select all tech articles my model is not nice if I say I just want to select some articles I want to make sure that every single article in my data set is actually tech articles so I I have something to read during the weekend during my trip then this model is actually perfect for you so depends on applications there are
(28:34) some ways to correct if you want to correct first of all there is typically threshold like 0.5 probability 0.5 and depending on which side we selected is zero or one you can change threshold with this first approach second approach if you use some models like no network for example you can use so-called focal um uh focal loss so what it means it means we basically say if we don't predict correctly zero we have to punish it more Right? You have to punish it more. Basically on the contrary, maybe if you want to get um if
(29:11) you want to get ones to be predicted, you have to punish more once. It makes sense why we want to punish those more because we have much more zeros than the ones. It means effectively zeros which are mclassified will be basically punished already more because we have many of them.
(29:29) That's why there is this kind of let's say weighted some of those terms which you can see in case of cross entropy they will be weighted some of those things so-called focal focal focal uh loss focal cost and focal loss so there are things to correct for this now you can say what is 43 it is worse than coin flip so it is not actually worse than coin flip because in this case we have only about 400 out of 2020 to be tech articles. It means it is actually less than 20%.
(30:04) You cannot obtain it with coin flip, right? If it was like 50/50, you can say just coin flip will give me 50% uh uh sensitivity. Not in this case. In this case, probably to be less than 20%, if you do it by random, it means you will be getting less than 20% sensitivity. If you do it by random, it means the model is still better than by random chance. just to keep in mind.
(30:31) So now let me display my results. I create data frame where I say model accuracy specificity just for comparison and I display it here n base classifier accuracy is8966 sensitivity is43 specificity is like 100%. This is more not the best clearly but that's how it does it. Any questions about this approach? Okay, now let's move to next one.
(30:59) K nearest neighbors. Remember we discussed how it works. Again, I'm going to use Kfold validation. I'm going to use five folds, I'm going to use TF vectorizer. Every time when I move from from one splitting to second splitting to third splitting, every time I move from splitting to splitting, I have to refit my TF factorizer in this case. In first case, in case no, not works.
(31:28) You don't have to do it clearly because the same training data set all the time. In my case, I have to do that's why I I make it as part of my pipeline, my TF factorizer. And second, it will be my K nearest neighbors. I take default which is five just by default. So assume that I don't don't want to optimize it yet. Let me take five.
(31:53) Then I do the same as before across validation this splitting. Then I supply my pipeline from here. This one I supply my documents, my labels. I specify that I want to copy accuracy. I kind of uh I want to minimize accuracy and then I say let me obtain values and see my results. So this chunk of code is same and I can see now that accuracy is 97.
(32:22) It is better already because it was 0.90 before now sensitivity is better much better because before it was 43 now 94 it is much better now and specificity got of course a little bit worse because before it was like 100% like one uh it is not nice approach probably all right again depends on the applications but probably it is not nice approach to have just one because everyone will be assigned to zero group okay sensitivity specificity a little bit uh went down but not as much. We much we obtain much better metric for sensitivity. This is
(32:58) confusing confusing confusion matrix. Actual non techch will be predicted as um non techch in majority of cases. No basically in 97% cases it will be predicted this predicted one as non techch. Only 31 of those n were classified mistakenly as tech articles. It happens of course. Now actual tech in most cases again in 98% cases it would be I'm sorry I have to probably take it back.
(33:39) Uh when I talk about tech it is sensitivity right? So in this case in non tech case I'm talking about 2% uh about u so sensitivity refers to check because tech is one. So it means 9.94 corresponds to this 376 out of total 25 plus 376 376 out of this total will be 94%. And for the specificity it is already this way 1793 out of this total will be 98%.
(34:11) In 2% cases we not correctly classify actual non techch we say this tech. In 6% cases we mclassify tech articles. So any questions about this? Okay. Okay. This metrics are kind of different ways to look at the same results, right? There are different ways to actually approach this problem. Again, I believe it always in the context of what you want to get actually because again, if I want to go to trip and I wanted to take like a bunch of articles about tech, maybe the very first model was the best for me because I know that it would always provide me only purely tech articles, right? Not
(34:47) all of them, but only purely tech articles. If there is not your problem, if your problem was to try to get all of them for kind of further inspection, then okay, maybe this model is already better. And there are different ways to also look at this uh problem. You can look at different metrics such as such as precision kind of area under precision recall curve and so on.
(35:13) Now we will not talk about this but keep in mind that there are ways to kind of try to introduce metric because if you use different threshold remember I told you 0.5 threshold which one is zero which one is one you will be getting different sensitivity and specificity.
(35:34) It means you can basically plot sensitivity versus specificity or you can plot um your precision versus uh recall right for different thresholds you're going to get different couple different results and you're going to get let's say precision recall it is called area under curve right so you can load precision versus recall for different thresholds you'll getting different numbers and you get something like this as a dependence and then you can take literally area under the curve and this will be your metric.
(36:08) So people sometimes use such metrics such metric if they don't really know how to choose threshold what is most important what is not most important but this is I think this is not like the best approach because you always have to look in the context of particular application and say what do we really want then it becomes clear what you maximize and specific sensitivity what or just accuracy in case of balanced data set you may want to specify accuracy right if data set is unbalanced it becomes a lot more challenging because model tends to actually ignore one of the classes because it is say if you take let's say
(36:41) if you take a test and I happened it happened that I gave you correct answer always to be option A a A a most of the time and you already know know about this that most of the time correct correct correct answer is A even if from time to time one out of 100 questions I kind of randomly provide as a correct answer to be like a b or c for you.
(37:12) It is much safer to just always select a okay you miss like tot of 100 and you still get a right that's exactly what model is doing if it know that most of them are zeros anyway why to bother just predict zero that's what happens like same as if you know that most of the answers will be a answers then why why to even worry about you know trying to risk and predict some answer something differently if you know that most of the time a is correct answer if you say a a aaa all the time basically you'll be getting 98% so that's what's what's happen in case of unbalanced data set that's why different metrics are quite important
(37:49) precision recall uh area under the curve so precision recall in this case I didn't define what u uh precision means recall is basically as you understand same as um uh sensitivity right there is recall recall is same as sensitivity but there is also precision. So who knows what precision is? Let me say it is um predicted positive.
(38:29) Predicted negative it is true positive. It is true negative this way. And if it is true positive it to be positive we say true positive value. If it is true positive but we pred it to be negative we say it is false negative it is kind of pred to be negative but it is false then true negative predicted to be negative it will be true negative and in that case we pres to be positive but in reality it is not positive it means false positive it is positive result like you take a test and the result is positive but it is false positive reason to be positive but in
(39:11) reality it is not positive And in that case um we can define uh let's say recall this way as we already discussed recall means we take uh uh how many we use of positive things how many we predicted to be true positive right out of true positive plus false positive it means out of basically those true positive values They're equal to each other.
(39:46) Number of true positives will be true positive plus false negative. How many we pres to be positive out of total true things. It is called recall. Precision is different quantity. So precision means uh precision means uh that we say how many true positives we have out of predicted positives. out of true positive plus false positive.
(40:17) True positives plus false positive and we also can map kind of sketch this dependence. If you choose different threshold for the probability which which is like splitting probability then we can pull dependence on the area under this curve is different metric already which is kind of not not really sens not not dependent on my threshold that is kind of objective objective metric.
(40:43) So that guy is that guy precision is basically out of those things right through positive out of predicted positive. Now whatever basically you get as a result you get like 100 people were tested positive how many are truly positive out of this 100 this called it is called um precision right does make sense now okay so this how a matrix work now maybe you already know about this matrix I'm not sure but this are quite important for this application so when we classify things in biomeical research and this this type of applications you should be concerned of Of course accuracy is not the kind of nice metric
(41:21) if you have unbalanced data set and so on but depends on applications. So now in this case we have this matrix we get 94 98 it is like recall for tech recall for text means means um sensitivity if check is one and recall for non tech means sensitivity if if if if I call non tech to be one which is called also specificity with respect to original label 0 and one.
(41:52) So okay now let me say that um let me simply add this results to my table and it will be can neighbors with default and I get 97 for accuracy it is better 94 for sensitivity is bad it is better 98 is specificity which went down it understandable because I because this is where I lose basically I lose a little bit of sensitivity but get accur I lose specificity But I get sensitivity and accuracy boosted.
(42:26) So any questions about this? So next question is how how about optimal number of neighbors optimal k in this case I basically create my pipeline. I don't specify a parameter which is the number of my neighbors. It will be hyperparameter. So later I will decide how to choose it. I do this stratification and then let me specify my parameters.
(42:54) So you see how key of this of this uh uh key of this dictionary looks KN&N underscore underscore and then this parameter K neighbors it is not occasionally this way because I want to match it to how my function is and how my parameter which specifies number of neighbors is. So remember how it was uh it was n neighbors right here it was n neighbors and this is my k&n and n neighbors that's why I specify this key to be this way so kn double underscore and parameters which I'm going to use for which I'm going to try different values let's say this way and then value
(43:34) itself in this dictionary is entire list of things from 1 to 20 I We'll try one two three and so on like up to 20. So it ranges from 1 to 20. You know that 21 is not taken. The last one is not taken right. So and that's it. I can say grid search CV. I specify my key pipeline this one.
(44:03) Then I say my uh parameters my dictionary which holds this holds this information. what I want to do about my nest neighbors parameter and end parameter as before CV and scoring and I get my result. Now if I display my best parameters then it will tell me that 19 neighbors is actually best. So 19 is better than five and default five. Okay then best cross validation accuracy is 98.
(44:32) Then I can specify my number of parameters to be 9 19 and I can now display my confusion matrix as before and say and see that here it is not much different. Maybe specificity is a little bit better since accuracy is probably same almost sensitivity is almost same.
(44:57) Now let me add it to my table and see that accuracy went up. sensitivity doesn't change. Specificity went up. So basically I don't lose anything by trying to kind of optimize parameter key. I don't lose anything but I got a little bit better result for every almost every every metric except for sensitivity which didn't change. So it means it is of course better. 19 neighbors is better in this case.
(45:22) So what it means? Remember what it means? That means from my 2,225 news, every time I look at 19 nearest neighbors, right? 19 nearest neighbors when I decide how to classify. Okay? Logistic regression very similar idea. My pipeline I specify that I have to use TF vectorzer again every time I have to do it because I use different splitting. Logic regression isn't so sensitive to any parameters. You have to have maximum number of iterations.
(45:55) Let me say 1,000 when you maximize likelihood. You have to have maximum number of iterations otherwise it is almost like deterministic procedure basically. So it means it is like like almost like linear regression. You don't have to optimize for any parameters here. I choose like 1,00 and I do the same then I do the same the same and I display ultimately my confusion matrix and uh you can see what happened accuracy went down sensitivity went down and specificity again one so it wants to kind of be on sort of safe side again and says let
(46:32) everyone be if it is on check let kind of kind of predicted most of the time to be non tech right I mean what happens is if it is even tech it predicts it to be non tech only rarely says it is tech of course um sensitivity still went up comparable to knife base classifier it means it is better so 285 is better than it was before it was like 171 nevertheless it is probably not not what you want if you missed like 30% of both uh I will add it to my table.
(47:11) Accuracy went down, sensitivity went down. No specificity is back at 100%. Because my model tells me now let's count everyone to be to be not but most of the time to be non nontech. Next one support vector machine again my pipeline my vectorzer my SVM I use default values uh radial basis functions it means my kind of you know lines hyperlines will be not really hyperlines they will be like circles like ellip kind of circles and my C like remember that parameter C which is like almost like land in case of almost like land in case of
(47:54) L1 autoizations C is one. So there is some margin. So some things some points will be inside of my margin. And I use default then same same as before everything is same and I get my confusion matrix and you can see what happened now 98 sensitivity is better than case of logistic regression but it is still worse than than before in case of canerous neighbors and specificity is again one.
(48:26) That means my SVM support vector machine says let's let let let's count most of them to be non tech for to to be safe and that's what we got here. This one is not comparable to can default K nearest neighbors. I would say that one is comparable to maybe default can neighbors but even worse and this one is slightly better because again it is closed one.
(48:52) That means it wants to be on safe side and counts most of the things to be non check because most of the things in my data set will be non check. Now second case optimal kernel and see parameters how to do it. Again this is my uh pipeline svc support vector m machine is going to be uh not going to have any parameters right not going to have any parameters. It is like a classifier sysc and then no parameters.
(49:26) Then I say split and here I will say let me loop over different parameters of C. Again SVM I call it SVM underscore underscore parameter C is what my function CV CV C SVC accepts. it accepts parameter C. That's why here I will say explicitly that I want to have SVM double underscore and parameter C that will be.
(49:59) 1 1 and 10 over which I'm going to loop then SVM kernel I will try linear radial base functions polomial and test different kernels again those guys for example polomial also will have some kind of parameters those powers how many powers how many terms to take I take default polinomial but default now next one I'm going to do same same same and I'm going to get my let me say my optimal result so outputs I'm going to get my optimal results so I think I didn't put plot it here anymore I just used my um use my optimal results. Yeah, this parameters best parameters. Best parameters which I pulled from here. All
(50:59) right. From here I can use it to estimate my support vector machine and then I can display it for for best parameters. So optimal parameters is the 10 for C and kernel is a linear not not basis function then my accuracy is.99 um then my sensitivity is.97 and specificity is one already almost no it is not truly one because you see seven it is 1817 out of 1824 that's what it is no it is almost one so it is probably the best now right We get.
(51:41) 99 accuracy is the best sensitivity is.967 it is very nice best and this one is not best but those don't count because when it is one we get so not nice sensitivity it means we don't want to probably use those cases it means it is almost like the best maybe default would have a little bit better result but again if I feel that it's not not what I want if I don't want to just count everyone is non tech and probably I want to move to support vector machine with my optimal parameters. Finally, random forest. Again, my TF random forest. I take my
(52:22) default parameters, whatever it is. Number of estimators, which means literally number of trees I'm going to use. Then maximal depth none. Uh metric which I minimize is gen index. Last time on the slides, we demonstrated what metric is minimized in case of Rena forest. So it was a journey index specifically this function in case of classification what is P.
(52:53) So basically for every sub region when I do the splitting I will have um for every sub region right I will have things of two types of zero type of one type and ideally every sub region will have only pure kind of kind of pure pure cases only once for example or maybe only zeros and p is basically proportion of ones out of all of them in every sub region it's kind of impurity It means basically my p squ some of p some of those uh p squ needs to be um uh needs to be uh largest right so I don't want to I want to kind of be pure pure cases I don't don't want to have like.5
(53:46) squar +.5 squar I want to have 1 squar + 0 squar for example let's I have two regions. In the first case, I have zeros on the left, ones on the right. What it means? On the left, I have P to be one. On the right, I have P to be zero. 0 2 + 1 2 will be like one. 1 - 1 is zero. In this pure case, G index is zero.
(54:13) What happens if I have like mixture of things on the left in my sub region? Only two two regions, only two sub regions. On the left I have 50/50. On the right I have 50/50. Then probability I get kind of P. Proportion of ones on the left will be 0.5. Proportion of ones on the right will be.5.1 squar.5 squ +.
(54:35) 5 squar will be 25 + 25 which is total.5. 1 -.5 gives me.5. So this is not pure case. I don't like it. I like when it is when every sub region has like only zeros or only ones. That's how we do it. That's what I specify here. G index is going to be minimized. Minimal value of gen index is clearly zero.
(55:00) And maximum feature of features. What is that guy? So remember when we build random forest we based it on number of different trees. Number of trees is right here 100 trees. Now those trees I built on bootstrap samples. Why so? Because I want to have some variability in my trees. Ultimately I'm going to be kind of average my trees.
(55:26) The result I don't even have to actually try very hard building best possible tree. I want to have some variability. Maybe I want to even on the contrary introduce randomness in those. That's why when I say let me decide how to produce next split typically I have a number of variables. let's say uh let's say 16 variables I have to take one by one first variable second variable next variable and decide how to split in the best possible way and which one to split but if I specify here square root it means what it is default parameter by the way square root means out of 16 variables every time during every
(55:59) splitting only four randomly selected will be considered x first x 5th x 10 x16 randomly selected will be considered and My split will be having only along those four variables. Other ones will be ignored. Then I move to next splitting and I randomly select again next four and do the splitting.
(56:19) The result all matrix will be quite different from each other because of first of all bootstrapping and secondary this approach which is a very smart approach not to select all variables when you make best split. It is square root by default you can use it right and uh that's how I organize my pipeline. Then I do the same and I get my results which is 98 for accuracy, 89 for sensitivity, one for specificity.
(56:52) This is my confusion matrix and I get my rena forest which is probably a little bit worse than my support vector machine. Let's optimize. Let's try to see if rena forest can perform a little bit better. Uh, assignment 7 hotel. Uh, which one is next? Which one is due? Due this weekend. You're asking about assignment seven.
(57:21) Which one is due this weekend? Yeah, this one. This assignment. Yeah. Okay. So, assignment seven is about those uh STM use are correct. So, what I'm explaining right now is basically going to be used on your next assignment. But this TM was covered last time, right? Thank you. Yeah. So, it's not going to be on your current assignment because it is not, you know, it is not there, but it's going to be on your next assignment.
(57:47) But by the way, on your next assignment, you probably don't want to use my key fold validation because there's not doesn't make much sense to refit your refit your uh uh TF vectorizer because you're going to split it and forever use it. Now in this case I say my pipeline I don't specify those parameters in instead I create my dictionary where I say number of estimators will be 50 100 200 then uh cost which I'm trying to minimize will be gen or entropy second type of uh cost number of features could be square root maybe logarithm or maybe none none means all of them every time should be used
(58:30) when I decide to split and uh okay number of estimators is probably uh not going to be used. I think it took like long. I just decided to use default one and this one is going not going to be used. It means only uh metric which I minimize and maximum number of features will be tried and then I do it and I display uh my optimal results which is gen index indeed and square root basically default very same values right where yes so very same values my defaults and my optimal things can side and uh model comparison I just plot this chart
(59:16) to kind of show how they compare to each other. Uh no it is same table is even better probably. So in this case test accuracy accuracy itself it means I'm talking about first column accuracy itself I get support vector must be the best then random forest then random forest then canist neighbors and so on. So basically support machine is winning.
(59:47) It's not not surprising because it was actually the best model available in some sense until neural networks in 2012 would sort of outperform support vector machines. Yes. So support machine if you don't consider neural networks makes perfect sense why it is winning model.
(1:00:08) So any questions about anything? Dr. Crushian I don't remember what was bootstrapping. So we want to build a model right and we say let's not use entire data set or at least let's not use data set as is let's take a subsample from there and build model on this subsample but it would be just sampling we don't do that that way we we use sample with replacement if you have 10 points for example I take a point out write it down put it back Take second point out, write it down, put it back. If I put it back, it means second time I get I I can potentially get the
(1:00:49) very same point. Right? That's what's called bootstrap sample. 10 points but chosen with replacement. With replacement literally means I take it out, write it down, put it back. It means next time when I pull second point, I can potentially pull the same point out again. This is called bootstrap sample.
(1:01:08) Every tree is built on such bootstrap samples. Which means technically speaking every tree is built on unique data set. They practically will not be same. They will be slightly different. Does make sense now? Yes. Thank you. And do do I remember correctly that you you like random forest for a lot of your problem solution? Yeah, that's that's correct.
(1:01:39) I like run for because it gives you capability to kind of you know uh experiment with those parameters and you know what I kind of generally speaking like when kind of randomness and optimization right I kind of really like uh things which are based on kind of stastic models of any kind and so on like Monte Carl for example gib s of bayonian approach I don't speaking like it it's not like I'm not saying it is the best model But I kind of get kind of aesthetic, you know, kind of kind of like it that that's why. Yeah.
(1:02:12) But yeah, sometimes like in this case vector machine is better than you have to use support machine. No one need to move to random forest. Yeah. But random forest is nice because you you cannot possibly overfeit because you introduce this randomness which allows you to actually avoid overfeitting. Prenorus cannot possibly overfeit by design. Okay, let's now stop.
(1:02:43) Thank you. Thank you. Yeah.