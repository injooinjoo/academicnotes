89 day 9 - YouTube
https://www.youtube.com/watch?v=ycRSyDDRAtw

Transcript:
(00:01) Hello everyone. Good evening Dr. Kroskin. You see my slides right? Yes we can see them. So today as you maybe know according to the schedule today we are covering classical techniques which we are presumably able to apply to apply to natural language processing as well. There are no issues we can apply definitely.
(00:41) Now of course if you have a lots of data then maybe this is not the best approach but if you don't have like much much data it could be quite quite applicable right so definitely we cannot just disregard those models. So today we are going to cover classical models and let me first uh open the quiz. Quiz number eight. First question we are talking about STM. Which statement describes STM? Structural topic modeling.
(01:16) In this case you can see a tells STM as a method that incorporates coverates into topic modeling to understand influence on topic prevalence. If you remember the model, it indeed uses coarance which incorporates them into the mo into the modeling right remember there was logistic normal distribution which means basically soft max applied to multiate normal and then mu would be expressed in terms of covert variables it means yes that is incorporated under the model you can read the other ones you can see that they are not appropriate
(01:53) require Whereas label data is not right and so on everything is not k means clustering is not so we understand that by the way STM is not actually actually clustering it is some kind a kind of unsupervised algorithm it means it doesn't require a labels but it is not clustering because every document may belong simultaneously to multiple multiple topics.
(02:18) So next question which function is used to prep-process text data for analysis in STM right in this R package in this case. So you remember we used text process processor it means option C is correct option in this case select model is not that STM is not that it is just maximization of likelihood basically STM is like fitting but C is correct. Now next question.
(02:47) Question three. Select all statements in this case which are correct. Uh let's see. A it serves the temporary method for tokenization is not correct. Right. Not correct. Primary method for tokenization. No it is not correct. Influences both topic prevalence and topic content inference during modeling.
(03:10) So uh role of document role of document level metadata it influences as both topic prevalence and topic content. No basically quite related to first question. Yes, it does influence it indeed influence influences us um how we assigned prevalence right and topic content during modeling because topic content is also related to um to this uh cover variables now next C it helps adjust the model to account for external factors influencing topics yes it is indeed the case they could be considered as external factors right and we include them explicitly to to our Model D.
(03:52) Metadata is used for visualizing topic. Well, metadata could be used to visualize topic modeling definitely, but it is not the only reason why we use metadata. It means D is not correct. Formally speaking, metadata could be used to visualize topic topics, but it's not the only reason why we use it, right? So, if D is not correct. Next one. Question four.
(04:17) In the context of topic modeling, what distribution does topic prevalence follow? Do you remember it is logistic normal distribution? Answer is D in this case mate normal to which we apply soft max. So next one question five. What function in the STM package can be used to the effect of coates and uh it was estimate effect. Last time we saw some examples.
(04:45) Estimate effect will estimate the effect of coverage on the estimated topics. Right? So any questions about this um quiz? Yes, Dr. Gashkin. For question four, are there any cases in which uh I kept doing a lot of reading on this and then I was getting myself confused cuz there were like um some articles that were saying that if the if there was independence that then it would be multinnomial distribution but that it was typically logistic normal.
(05:18) Is it ever multinnomial or is it or is that uh was what I was reading incorrect multinnomial in case of dial allocation but in case of STM we use logistic normal. Oh, so it's L it's more LDA. Okay, thank you. Yes, yes, it is it is how it is designed. Yeah. So, basically in case of uh STM we use some some cover variables. It means we have to incorporate those somehow into the model and multinnomoral is not the best tool to incorporate it.
(05:45) uh I don't know if it is even possible but we can use logistic normal to do it because expectation mean is going to be expressed linearly in terms of this linear combination of those co variables that's why it is logistic normal it is assumption of the model basically I mean can you do it different or not probably you can do it differently but this is how STM does it uses logistic normal it is assumption explicit assumption of this model yeah and Any further questions? You may remember at some point there was assumption of STM right and there was
(06:25) this uh uh this uh logistic normal right here right so now let's move to uh today's uh content lecture n we have a number of different models naive base classifier can neighbors logistic regression support vector machines random three and random forests You may know some of those, you may not. I don't know.
(06:54) I mean, depends on what you took before and on your experience as well. But those models are quite quite important models actually in data science. And we'll today kind of cover those in sort of application to natural language processing even though it could be applied to any problems. Basically, in case of natural language processing, we still have to somehow represent text as a sort of vector, right? Maybe this vector, for example, U frequency inverse different frequencies as TFS IDFs maybe if we represent text as sort of TF vector we can now apply any basically
(07:29) technique to do for example class classification right so that's how we should view it first we need to somehow represent text using numerical representation like vector weight representation maybe and then we can apply this type of classifiers for example now let's uh now uh make like very brief overview first and then we'll kind of dive in and see how models are designed.
(07:59) So naive base classifier in this case uh we actually assume you'll see what it means. We assume that features are independent. So basically if your text is vector of those TFS first word is cat in our vocabulary. Second word is uh roof in our vocabulary. TF IDF of cat and TF IDF of roof must be basically independent. This is assumption of this naive base classifier.
(08:24) Na refers exactly to this assumption. Naive means that we naively sort of assume that they independent basically right. So they are kind of independent. Um no given given they belong to particular class. If you have some kind of classification problem, there could be some specific class chosen. We sort of stay within class and say within this class of documents.
(08:49) For example, let's say we're talking about spam. Spam. Okay. You given it as spam those uh two entries to my vector should be independent. Uh it is by the way it says often use used for spam detection and sentiment analysis. Right? So this is case where you probably can use it especially if you have ITF IDF you can tell how is it independent if you have context and stuff like that right yes we have context and stuff like that but basically we assume this we assume independence and also when we talk about this uh yeah dependence is there right maybe TF is not the best way to represent dependence and you can argue since it is not the
(09:30) best way to represent dependence it means dependence is already kind of lost on the way of of this computations of those frequencies. It's it is not quite lost actually to be precise. If you have cat, right? We may have like um already less likely dog in the same sentence. Maybe for example, it means there could be correlations.
(09:53) Formally speaking, there could be some dependencies. Even though TF is not best way to represent context and stuff like that in correations, but correlations still could be there. It means we sort of assume that we further ignore those correlations even if they exist. We further ignore those. Uh in this case, uh we kind of prefer to use a small data sets, right? typically can nearest neighbors.
(10:24) So in this case we basically say we are trying to understand what customer is going to buy today and we say okay how old is the customer what is gender and uh maybe if you have data what is income and so on. So we have all these variables and how to predict what customer is going to do. There is this algorithm that is called Ken neighbors.
(10:48) Basically, we're going to look through our data set and say who had the same income, same gender, same age and whoever was the closest to current customer. We are going to basically assume that this customer is going to be doing something similar to previous ones to previous K customers. not not like from from not not one who came before but but those customers who the closest ones in terms of some kind of metric in this space of characteristics similar age similar income and so on.
(11:19) So what they did before those are the closest using this metric. This current customer is going to do probably something similar. It is called K nearest neighbors right. Um in our case maybe we can use something like cosine similarity for example. Typically it use ukidian matrix ukidian metric but it doesn't have to be ukidian metric. It could be for example cosine similarity.
(11:46) In this case, by the way, if you use you use some kind of Ukrainian metric and you try to build a model which u compares ages, compares income, it is quite important to actually normalize data because if you say 20 and 25, what is difference? 5 years income $50,000, $100,000, what's difference? $50,000. If you use ukleian metric, basically it will always ignore age.
(12:16) We'll only look at income because distant is huge right comparable to difference in age. It means u it is important to sort of normalize it. So they kind of equally contributing just keep it in mind because if metric is designed this way then it means you have to be careful in this case. If you want to contributions from all characteristics simple to implement right.
(12:37) So no kind of real training involved. We will see how it is done useful for smaller data sets. So now logistic regression in this case we fit a model which basically produces as output probability of success probability of being of particular type type one for example and uh it is essentially similar to shallow network with single layer basically and with with no hidden layers input and output right away which uses sigmoid function.
(13:11) If you use sigmoid function it will be exactly what's called logistic regression and in terms of like classical models such networks will be called logistic regression. Of course logistic regressions will maximize likelihood networks will do what will minimize will minimize cost function and you can argue that this different results could be slightly different.
(13:32) Now first of all mathematically speaking maximization of likelihood in this case is completely equivalent to minimization of cross entropy completely equivalent. If you write down likelihood, you will see that maximization of likelihood is minimization of power and power looks just like just looks exactly like neg kind of cross entropy, right? It means minimization of cross entropy is exactly same as maximization of likelihood.
(14:03) But of course, since we used different algorithms for maximization of likelihood and for minimization of cost, we may get still different results. Probably not much different results. If there are many local minimum we can get quite different results but most like in such cases we don't have many local minimum it means we are probably expected to get same result because network is quite simple no hidden layers so it means it is basically equivalent to shallow network where you have sigmoid just to keep in mind right so now um next one support machines it used to be a huge actually at some point before neural networks no
(14:44) not before they were introduced but before we got um Alex net famous network for image classification so basically until 2012 support vector machions were most popular most dominant techniques support vector machions um and people didn't even want to invest time in into uh developing of neural networks because it didn't seem to kind of go anywhere or at least uh it it wasn't clear how to efficiently train it.
(15:21) But later on of course because of computational resources, computational power also data data is quite important for neural networks would increase increase and basically networks would outperform support vector machines. Now we'll see how it how it is designed how support vector machines work. It basically maximizes margin between two different classes. We have classes that we try to split in a way that margin is maximum.
(15:42) We will see decision trees. So decision trees uh is based on simple idea that every time we try to actually split our variable we pick one variable and split it in a way that it maximizes u optimize a specific metric. Let's say say this week a purity of result and we every time every time when we build decision tree split only only one of the variables at a time the one which corresponds to best splitting and in the best possible way from optimization point of view and then we move forward and we split second time next time next time we will see how how it how it looks. It is called decision tree.
(16:25) Decision tree itself is difficult um kind of model to work with because we can easily fix it. We never know when to stop but we can combine them in so called random forest. It is basically begging of begging of trees many many many trees slightly different ones design slightly differently and then we take average of those trees basically and we get rena forest which works works works much better. Marina forest is is very powerful tool by the way. You can see it performs nicely in many cases.
(17:00) So now let's uh start talking about first model naive base classifier. So this is my this is my assumption not assumption well second part is assumption. First part is just base theorem which is true. It's there's no any kind of assumption is always correct.
(17:24) Right? So let me well first of all let me let me say that um what it is about right let me say recall from basically probability theory let's recall that probability of event A given B in this case event means basically specific set of features for example B could be set of features it is like date in our case basically we observe some kind of data we observe text which we which we got into our email box and this is basically kind of class which we which we believe it belongs to right so it is now let's say class class we want to make do classification let's say class that one or that one some sort of decision we are
(18:07) going to make ultimately generally speaking two events A and B it will be defined as probability of event A and also event B over probability of B. Now in this case we assume of course that event B may may may happen. So probability of event A is actually positive. It means it may happen if if this data will already happened.
(18:36) We already observe it. So probability of A is of B is strictly positive because we we did observe it already. Then this is relation and the definition. This is not the way this is definition of conditional probability. Then from here I can easily derive.
(19:02) No you can see from here my P of A and B is basically P of A and given B * P of B. I just rewrote it clearly. Numerator here is product of those two. I got it. Also I considered P of B and A. Because if I call a b call ba is going to be different. It's going to be p of b given a * p of a this way. And then I notice that p of a and b and p of b and a is exactly the same event because a and b is exactly the same as b and a.
(19:38) Now there's a typo, right? Typo this one. Pa. That's why right? Yeah. Thank you. So a and b is exactly same as b and a it is same event that's why left hand side is same exactly the result we are going to get that's called by base theorem right we are going to get this guy is equal to that guy now it means probability of uh uh class a given data essentially is going to be you can see have to equate those two this one is equal to That one that means P of A given B is is basically that quantity P of B given A time P of A over P of B and we get that
(20:27) one. No, essentially what I'm saying is that P of A and B can be also expressed as P of B given A * P of A. I can sort of plug it there and I get it. So this is always correct just the way we define conditional probability implies it must be correct. So it is always correct. Now if you think that there is some kind of specific vector of features X is like bold in this case vector of features maybe it is TF vector which represents our text.
(21:02) Okay X is vector of features in our data. You can call it text itself but typically we think in terms of like vector like vector of specific represent vector which is specific representation of text and CK is given class let's say we say it is spam okay CK is spam C first is spam C second is not spam probability that this is spam given our particular vector of features is going to probability that we obtain a vector of features given it is spam time probability of being spam over probability of this X.
(21:38) That's how we can rewrite it. We can flip basically condition around probability of um uh class C is prior probability, right? Not typically if you don't have any information, we can say 50/50 for example. If you have information from previous experiments, we can use the previous information. You have some questions. Yes, sir.
(22:02) Uh my question is how can we calculate the probability uh of um of a vector uh like feature vector given given a specific class. Yeah. So in this case uh let me uh um let me um say that we have to uh we have to have uh some specific assumption right? We have to incorporate um we have to um choose specific assumption what our probability of uh particular word is going for for example particular frequency is going to be g given particular class.
(22:47) So this is something which you need to impose as a specific assumption of this model and uh also in addition to that I will say before that we actually have to impose different assumption we have to like you say how to to comput it is quite difficult clearly to compute it that is a very good question how to comput of obser vector of this TFS given particular class it is quite quite difficult but assumption in this case of naive bas is going to be that probability of vector is going to be a product of individual probabilities. So this is going to be huge simplification in this case and we say probability of our term
(23:28) frequency TFS is going to product of probabilities for particular TF that's how so simply just we multiply the probabilities for every word uh independently because that's the assumption about behind naive base that the yes exactly yeah so so if you want to compute that one in case of naive base classifier we assume that for individual ones we have to multiply them out.
(23:55) Exactly that. Yes. Okay. Thank you. So exactly that. Yeah. So now um yeah so using this formula we can ultimately maximize this probability of class being of particular type. No, it means we're going to for each class we're going to compute some kind of probability that it belongs to first class to second class and then we choose class which corresponds to maximum maximum probability.
(24:25) In practice we typically don't like maximization of products not because it is kind of unstable numerically quantity right it means we in practice maximize logarithm of this quantity just to be logarithm of this quantity to be a little bit more accurate from numerical point of view and it means we maximize logarithm of this quantity where each one each product is sum of logarithms essentially.
(24:54) So this how how this models work. Uh now next one uh maybe I should say something about like applications for example it is often used in spam detection right spam detection also sentiment analysis it is very kind of a popular technique for for those problems. spend detection it is first efficient this computationally not expensive right easy to interpret because we get probability of class B of first time class being second time and so on we can also take as a prior distribution as prior probabilities of our classes results from previous basically experiments if you want right
(25:40) it means if we collect more and more kind of information we can kind of know that there is some kind I'm biased. Maybe it it could be 50/50, could be not 50/50. Depends on how often you get spam, not spam. So, it doesn't have to be 50/50. So, uh this way uh here is example.
(26:08) So, in this case, I have like eight documents and I label them correctly. So, labels are correct labels. Zero one and so on. First is about cats. So zero second about cats zero about dogs one and so on. So one means dog zero means cat and I want to use my knife base classifier to do that. So in this case I split it into 25% versus 75%.
(26:39) It means my my test data consist of two documents out of eight. It happens to be about dogs and then about cats and the rest those six documents will be for training of this classifier and then I uh first of all have to initialize my TF vectorizer then I train it on my train documents right so I train it on my train documents I don't want to even compute anything anything from test documents clearly to avoid data leakage which we understand it right.
(27:13) So we basically compute everything using only our train documents because inverse document frequencies will be used further in further analysis. Then I apply this pre-trained vectorzer to my test documents as well. X test will be my ve this X basically this X vector of those TFS will be pre-trained a vector which is applied which I apply to test documents after the trans after my vectorzer was pre-trained and next train is vector of TFS which is obtained from train document and then uh multial professor why is TF TF vectorizer is used why not
(27:59) other vectorzer like say it again when when is it used? Why why TF vectorizer is used here? Yeah, because you see we want to have some kind of X representation of our our data, right? This is going to be data. But what is data? Data is like sentence. We cannot possibly fit sentence to this model. We have to have some kind of vector.
(28:23) Remember there was multi-dimensional vector X and we assume that conditional on class this is product of marginal probabilities no condition on class again it means we need to have term frequency each XI is specific term frequency corres which corresponds to particular word from the vocabulary so we need to get those sexes that's why we do it it's not the only way to do it X I could be whatever formally speaking but in this case we use TFI IDF.
(28:52) So X first is TF IDF of first word from vocabulary. X second is TF IDF of second word from vocabulary and so on. That's why we get so X test for example is vector right it is bold and then I say mult from here from secret learn and I can uh fit it on my X train and also Y train.
(29:21) Y train is my correct labels which correspond to X train those six documents. Then I say predict for my X test to see how it does it for the new new data which it never saw before and uh I also can compute accuracy in this case. I can display it right. I can display what was predicted essentially uh uh and first one which talks about dog was predicted to be dog which is very nice second one is about cats was predicted to be about cats so that's how how it works very simple example of course if you run if you split it differently you may be getting mistakes
(30:03) and understandable because data set is not large but it does does the work right So it it does does a job. Accuracy is one in this case. Uh no it is like discrete if you have only two documents. Accuracy is only zero one or.5. No we got to one at this time. So any uh questions? Uh professor I have a question uh on theizer.
(30:30) Um is there is there a reason maybe I missed it uh for using TF versus you know word to wick or something because we don't want negative values here or are any other reason versus just using TF or is there option other you know what there is no actually requirement to have non negative values those x's could be whatever probability of -5 okay we and use it right of 05.
(31:00) The reason is we want to have independent features. So our features must be actually independent of each other. So if you use some kind of some kind of uh embedding in that case I believe that they are more correlated. I believe they should be more correlated. It means it is less appropriate. On the contrary our vector should consist of things which are not correlated with each other.
(31:27) And in case of embedding they are sort of in this compressed compact form and I believe those um things are probably more correlated right and also I'm not sure about embedding it will be like sequence of vectors if you have sentence it will be sequence of vectors we need in this case only single vector so but nevertheless if you use some kind of you know embeddings or some kind of embedding and then maybe you take average of embedding to represent entire sentence.
(32:00) Still those entries of your vector of which represents your sentence could be more likely more related I I believe in practice than the case of TF. TF refers to my dictionary to my vocabulary and they of course they still correlate it's potentially right because words in in the document are related but maybe this correlation is less pronounced than in case of those embeddings. Yeah,
(32:32) makes sense. Thank you. Yeah. So in this case we kind of want to represent it in a way that they kind of less correlated basically because of assumption of naive base classifier. So next one K nearest nearest neighbors right K nearest neighbors uh not is like metric which you can use obviously let me give you one example maybe maybe two examples just to show how can neighbors work.
(32:59) Now again you can keep in mind example where we look at people who are very close to our current customers in order to predict what this customer is going to do. You can see what those people who were similar or most similar to current one what they did there's ID behind K nearest neighbors. So KNN is K nearest neighbors in this case. Let me say I want to use example uh of k nearest neighbors where k is equal to for example three.
(33:37) Let me sketch it first. So this is my data. Uh let me say one dimensional example. It means my output is 1D. X is over there only X only like single X even in this case I can use K nearest neighbors so this will be my data you can try to fit some uh some model understandable you can try to fit some model maybe it is not a linear regression there is some kind of zigzags you can see right so what should we do in that case turns out that in such cases when there is some kind of nonlinearity going on any neighbors is actually not not the best technique. If I say let me look at
(34:23) particular input let's say input by the way it doesn't have to be point given point let's say I'm looking at number 12 right here and I want to make predictions for number 12 I look at number 12 and see let me see who is the closest this guy is closest if I assume that my k is here is three nearest neighbors it means I take first one is nearest neighbor.
(34:54) Second one is nearest neighbor. Then who is next? Okay, this seems to be closest according to X. X is I don't worry about Y. So why is not important? Even I can do it this way. So maybe itself is over there and I ask you who is closest across X-axis in my domain. It means this is closest, right? essentially that's why I take average of y's now which corresponds to those three neighbors because k is equal to three and my average look this way so this how my average looks then I can scan with my 12 and I can get all kinds of results so this will be y head which corresponds to
(35:39) 12 exactly my y of 12 and now you can scan through different axis then you get as you understand you get uh like uh some kind of um function. Let me see. So three of those might maybe maybe this way then maybe some kind of jump. Usually it is pisswise constant as you understand right this wise constant. Now let me sketch it. So it's going to look this way.
(36:20) Okay, there's my prediction. Looks this way. So very easy to implement as you as you understand. Any questions about this model can neighbor this way. We look across dimension to see who is closest. Now let me look at different example two dimensional case. That means input is two dimensional basically k is equal to three and uh I have x first I will sketch it this way x first x second and output is going to be whatever goes that way is y and then I have my data set one point second point next one next one next one next one next one then I ask a question what should I
(37:16) choose for uh let's say this number there is some point 12 7 what should I choose for this this part 12 7 for this one it's not in my data set but I want to make predictions so basically same as before I need to look at my three neighbors and I say according to specific metric in this two dimensional space already I have to look at nearest one this is first guy.
(37:45) This is second guy and that one is close as next guy. So basically we look at kind of circle right around which we used to incorporate to incorporate neighbors this circle in this case because I use uh ukidian distance. So this is because I use ukian is not uklidian it may be circle. So it depends on what metric I choose.
(38:15) In this case it is uklidian distance and I can take average of those guys of those y's which correspond to these three points and construct my prediction. How to choose k question to you how to choose k what what do you say it can be sensitive to k. So how to choose K this hyperparameter I have to choose it a priority before kind of a priority right how do we choose hyperparameters any ideas uh well we have to u maybe optimize certain metric uh metrics if if we're looking for accuracy or precision or recall No Yeah. Yes.
(39:07) So basically we have to optimize test accuracy of some kind. Exactly. Yes. So we we have some test data set which we reserve and we optimize performance. Now in this case again you can you can keep in mind that maybe in this case nice practice would be to have train data, validation data and test data.
(39:32) Validation data is used to actually optimize for your hyperparameter and test data is going to be used only to assess performance. It is never used even for optimization of your parameter but only to assess performance. If you have like much data, it is nice to split into three chunks. Train, validation, test. If you don't have much data in this case, it is okay maybe to just use train and test.
(39:53) test is going to be used to basically choose choose the most appropriate key. Okay. So what about the distance itself? No, it is same idea. You have to choose some distance and maybe you can look at your performance such as test error of some kind. In this case classification you may use accuracy.
(40:13) You can use recall F1 score whatever you you want. Now it depends on if your data is balanced not balanced. If it is balanced accuracy is totally okay. If it is uh not balanced it is different conversation right in that case in this case by the way I do prediction it's not even about accuracy it's about test error in this case there is no there is no there is no actually classification in this case maybe in second case there is I don't know I didn't sketch but in the first case it is simply prediction problem that means we're talking about some kind of test error which you minimize in second case if you do classification 01 cases it
(40:51) means majority Majority vote 0 0 1 okay majority is zero it means prediction is going to be zero majority vote that's why by the way k is often chosen to be not like even number but odd number if you do classification we don't want to be like 50/50 that's why we often choose odd number for convenience and uh yeah if you use classific if you do classification as you said like accuracy different matrix f1 one score, recall, right? Um, sensitivity, specificity, whatever you like. So, um, recall is recall is same as
(41:34) sensitivity, right? There are different metrics by the way. If your if your data set is unbalanced, you can use even different metrics. um math coefficient there is also uh uh when you plot yeah [Music] just give me one second I have to mute it so you can also uh there is also this metric this uh a metric which is using uh area under curve where you plot uh recall versus precision depend on each other. All right.
(42:19) When you use different threshold and then you can use area under this curve it's called a UK area under curve on the precision recall curve and it is also used often in case when you're concerned if your data set is unbalanced. So yeah you're right. So there are different different techniques but the most important point and this this in this case that we have to look at test data and maximize K with respect to performance on test data.
(42:50) What we mean by performance depends on application in this case some kind of test error in classification case classification some kind of metric we have many many variable metrics. So now there is also metric coign similarity right manhattan distance it looks like um it goes along axis x or along y that's why it's called manhattan distance reminds you like how streets look in Manhattan um so any questions about this approach it is by the way quite nice approach maybe very very powerful in some cases now of course we can use something like a random forest for example or whatever
(43:26) to outperform it but as a starting point at least it is quite nice symmetric if you have nonlinear data set it would be quite quite efficient algorithm. So now one more point if x1 is h x2 is income this distance could be like uh this one this distance across h could be 5 years distance and cross income could be like thousands and thousands of dollars it means in order to make it circles I have to first rescale my data otherwise it will be not not really representative it means the one which corresponds to those variable which is
(44:06) on different scale which on high scale like thousands will be contributing more to my to my distance. That means of course I want to rescale it. It is quite important to rescale it first before we apply Ken's neighbors.
(44:24) If you want to similar contributions from different variables otherwise effectively the one which is using high values right like thousands for example will be dominant in this formula right here at this distance will be large for those where I use thousands to measure it should we supply some weights to the value to make prediction what do you uh supply some weights to the y value of neighbors to make to do predictions. No prediction is is is right here.
(45:00) So we just take one two three neighbors take average. So this one is is is average. This line represents average. This line we compute average. If you do predictions if you do classification we use majority world. If it is like 0 01 okay 001 means 0 is is winning. So zero is going to be prediction there. You don't need weights. We just take average.
(45:28) Uh okay. This is how we do classification. It is like for formally formal how it looks. Now again it means simply that if it was let me say another example. If it was like let's say 1D and I say classification problem in that case we are going to have 0 0 0 1 0 0 one one one one this way so how to do classification so basically let's say if I'm somewhere like at 12 somewhere over here want to do classification that means I have to look at let's say again K is equal to three uh it is basically similar to what
(46:23) we had before but we have to come to see what is majority neighbor neighbor neighbor well two zeros and one one it means prediction will be in this case yhat at 12 is going to be zero because zeros are winning in this case so this is my one level, zero level. Now let me ask you a question.
(46:55) Do you think that K nearest neighbors is deterministic model or not deterministic model like you know random forest? You run a second time you get different result but by design it is random. It is called random forest random. In this case if I build can neighbors is a deterministic model or stoastic model. It means if you run a second time can you get same can you get different result? It is deterministic. Is deterministic.
(47:29) So it seems like it is deterministic. The answer is no. It is actually not deterministic. It can be statistic. Right. We even uh I believe uh by default run it run it 10 times because it is not deterministic. Now you can say how is it not deterministic because you see in my case uh it is indeistic but there could be cases where even if you okay if I say k to be four it could be already not deterministic because we don't know how to how to decide two ones and two zeros means what who is going to be what is output zero or one we have to somehow uh make this decision random decision
(48:10) ultimately because 50/50 But okay you can say why not always take uh k to be odd number. Okay we can take k to be odd number. If k is even number there will be cases where we have to decide which one to take in case of classification for example will be two zeros to ones. How to decide what is the output? We have to basically break ties randomly. It means it is random.
(48:35) You can say what if k is odd odd number because I don't want that. I choose k to be odd number. It can be still stastic. How is it possible if K3 that it is stastic? Any ideas? I can say here without even changing much I can say what if there is second guy at the very same location.
(49:07) Why did I choose that one but why didn't I choose the the second guy? So this is my no not here let's say here another one second one is here. So first one is obviously this. Second one is that and which one should I choose as third one. What is your answer? I should decide randomly because there's no way to decide otherwise.
(49:31) So in some cases where the distance is same essentially right I have to decide randomly. If you think about like age for example of course there should would be cases where distance is same. age for example even income if it is measured in thousands of dollars many people may have same income so they could be basically equivalent and many in some sense it means distance will be equivalent that's why this was was not chosen only by random but second time I run it it may be chosen actually u by this reason I believe default is like 10 times we run it 10 times every time we build this
(50:12) dependence and then take ultimately average for K nearest neighbors average is going to be basically based on 10 different runs because there could be some ties still even if K is three it could be sometimes that's why KN is formally speaking not not deterministic algorithm statistic algorithm does make sense now yes very interesting but so my understanding is now this randomness comes from breaking the tie so that breaking ties exactly from breaking ties or whenever It's odd. Say it again. Equal distance.
(50:49) So the case when when K is even then I will have to break the tie. If if if there's equal vote um right if K is even then I will have to break the tie in case of equal vote. Two and two. Yes. But K even something we can easily avoid. So K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K K being even is like probably something you don't want to use.
(51:11) odd uh when K is odd also I can I can have more than K at to be points at the very very equal distance and I don't know which one to select so also I will have to select them randomly okay yeah so so in both cases for k= to 3 for k= to 4 basically for all cases even and odd odd cases we may have ties because distance is same for both guys we have to break it somehow but But yeah, but if K is equal to three at least we avoid cases where we have to break ties because of majority vote. So if for classification it makes sense to never use even K only odd K.
(51:53) All right. Yes sir. Thank you. Yeah. So that's how it is done. Again this is just a formula which I sketched over here how white hat is chosen. We just it is exactly majority vote. We see how many ones we have, how many zeros we have and we decide simple to implement, simple to understand. effective for life training data sets.
(52:20) If you have data set which is like dense across X, you can imagine how nice it could work. If you believe that it is more important to see how neighbors did and if data set is large, you have many many many neighbors around to choose from, right? Then this model could be quite effective. uh performance is sensitive to choice of K. It means of course we have to experiment with this also distance metric is quite important choice in this case. It means we have to experiment with distance as well.
(52:58) Ultimately our performance will be some kind of test error for example. It it doesn't care about your distance. It means you can try all all kinds of distances and see what is best performance. Ultimately it could be commodation expensive though Python implementation from secret learn. take uh again TF vectorizer we can we take trans split train split we do as before right so this is same same slide essentially so I didn't change anything same slide and then we take K neighbors classifier in that case we take accuracy score we specify this instance K neighbors
(53:46) classifier we choose five you see I took odd on purpose all on purpose five and by the way five is probably too much so it's interesting but five means what five means uh only one is out essentially since I have like six train observations train data maybe it is not the best approach well five maybe is too much maybe not actually I don't know maybe no not too much you know what and I'm now thinking maybe it's on the contrary good idea to take five because you kind of um take majority water and you kind of understand you get a little
(54:23) bit more into your into your sample and you can understand better maybe. So to me if it was like that case five years neighbors if you have only six data points something which is probably not the best approach but if I think in terms of sentences I don't know somehow it feels a little bit more more uh appropriate to take five because taking like two sentences is probably not enough like three sentences you can experiment with this of course we typically we don't have like six six documents we have much more that's why
(54:56) uh five is nice choice if you have like a lots of data. Okay, I feed it on my X train Y train. I predict using predict on for my test data as before I display it and I see that dog is text about dog is labeled as dog and text about cats is labeled as cat. So very nice. Any questions about this model neighbors? It is quite transparent how it works.
(55:33) Just keep in mind that K better be odd and it is for speaking not deterministic. By the way, let me check. Uh key neighbors. What's your fire? I want to see how many times we run it. So maybe it does it only one time. Let me see. Do you see anything which uh defines number of times we run it?
(57:11) What is P equal to? P is equal to two. It is you know it is minski. Um it is it means ukidian. Okay. So yeah it means minski this space too. It means effectively ukidian distance. Yes, here two means ucidian distance. So I'm trying to see um how many times it runs. Um like one time I'm trying to see also documentation of R and see they run it multiple times.
(57:55) Oh, nearest neighbor sweep defeated it, right? Then we Okay, we can check it. So there should be some kind of you know parameter maybe
(59:09) in the predict method which tells me how many times to to run it. Uh so okay let's make a break. You can try to also look at this documentation and see how many times it will run it effectively when we make predictions because this uh when we feed it so when we feed it maybe it doesn't it doesn't actually predict yet.
(59:45) So maybe it doesn't run multiple times during the fitting. It only needs to know kind of distances and stuff right it only sort of prepares data to kind of work with this further and when we call predict it will maybe run multiple times when we run predict we have to find documentation of predict to see if it is the case. Yeah but it should run multiple times I believe on the step when we when we do prediction.
(1:00:10) So maybe it is u when we say the predict here maybe it runs multiple times to make prediction. But I think your point is clear. So the based on the implementation the the outcome is going to be arbitrary. Yeah. So the question is what what what fit does basically fit fit needs to know like all the distances and stuff right so it sort of computes all the distances uh okay let's now make a So now next one logistic regression.
(1:10:33) Let me explain what is regression maybe in the simplest possible way given you know you know neural networks. So basically what is neural network is essentially is neural network right. So is neural network of following kind. We have x first, we have x second and so on. Now let me say x k.
(1:11:30) Typically when we use regression we use k notation in this case also one is my bias and then output y hat. So this type of network is called logistic regression. If I use sigmoid function, if I use sigmoid function here, that's all essentially that's how it is. Now you can see what it means. Sigmoid function is is given by the by the sigma linear model is over there.
(1:12:05) uh and it means we are going to have probability that class is of first type like priority of success. So something is happening. Just give me one second. Uh froze. Okay. So probability of what? Are the rest of you still online? I am. Can you hear me? Yeah, I can hear you, James. Hm. I wonder what happened to Dr. Kruchkin.
(1:12:59) He's frozen. Yes. And when I try to expand his video feed, it's all gray. But when I focus on something else Oh, okay. This is Okay, this is even worse. His video feed's completely gone. Yeah, he may have disconnected to try to reconnect again. Can you please tell me at what point you you lost me?
(1:19:32) Hello. Hello. Can you please tell me at what point you lost me? Did you see what I was when I was writing it here or you already didn't see it? You wrote that and then your video feed froze and then when I tried expanding it, the video feed was gray. So basically, you saw up until the end of this uh last statement, right? You still saw me, right? I mean, if you could hear me.
(1:20:03) So now, let me um I'm sharing my screen. Well, let me we heard you tell sigmoid function. Uh the next part was not not there. Yes. So sigmoid function we can write it this way. Then we say it will not work means we minimize loss function typically like categorical cross entropy in such cases and then loation uses maximization of likelihood and uh turns out that maximation of likelihood is completely equivalent to minimization of cross categorical cross entropy because we in case of likelihood going to have expression in terms of like exponent to
(1:20:45) some power and that means one problem is completely equivalent to second problem mathematically. But in practice we may get slightly different result because in practice we use algorithm to minimize cross function different from what what is used to maximize likelihood but typically it is like very similar typically even though could be some some differences.
(1:21:22) So now uh my slides are nine. So that's how it looks. So maybe I want to sketch it. Let me see example. So in this case I'm going to use one one dimensional example first. And basically I say it is going to be applied to problems where my output is zero and one. It is binary case. So 0 0 0 maybe one one one.
(1:22:00) So if you think how sigmo it looks basically it means means that we are trying to fit this type of curve to our data. This is sigmoid function specifically precisely. Now we have to also add some bias also some multiplier in front of x. It means this function in this case this function is going to be y let's say yhat is going to be 1 / 1 + e to the uh x * w let's say w first * x.
(1:22:32) Now there's no other w so I will just say w minus b. So that's how it looks and my function I shift it rescale X shifted and take this transformation. Now when we make decisions we have to essentially use some kind of threshold typically 0.5 and we say if I use threshold which is 0.5 I can make decisions how to predict if my way ahead which is probability of class one is going to be above.5 typically 0.
(1:23:11) 5 even though you can change it especially if it is unbalanced you can do something different if you want right but typically 0.5 and my prediction will exactly point probability of class being of first time as I exactly point on this curve if it is above.5 I will decide it as first class if it is below five I will decide it is second class class zero and also you can similarly visualize how it looks on two dimensional case let's say there's my y x first x second and know it means So I'm going to fit this type of cylindrical surface which corresponds to this curve exactly and the surface will be given by yhat
(1:23:58) equals to 1 / 1 + e w x first - w2 x 2 minus b it's going to look this way also I can choose threshold around 0.5 and then there will some kind of level which will cut my surface somewhere right and whatever is predicted to be above is going to be one below then five will be assumed to be assumed to be uh zero and there is some points points points which are equal to zero which I used to train my certain there will be some points which will be like one one one those are ones those are zeros and my logic
(1:24:51) regression will decide how to do it now you can see that so-called decision boundary is basically going to be a line in this case if you try to project where my level 5 cut the surface it is always line it means my decision boundary this is decision boundary decision boundary.
(1:25:21) My decision boundary is basically a line in x1 x2 plane, right? No, it is not parallel to the axis. It is some kind of decision boundary of this type and uh it's going to be always line. What if I don't like line? What if it is not not possible to use line? In that case, I can use actually some kind of uh designed already features.
(1:25:47) So I can use x for example maybe x1 * x2 x1 * x3 and so on will be more general than line. So there are techniques to do it but already becomes quite complicated problem because I need to use feature engineering sort of manual approach in this case. So I can generalize it effectively essentially in practice but so it is not line but uh in this format is going to be line and those variables or any questions about that.
(1:26:14) Yeah. So we can use it in test classifications. No, no problems, right? Yeah. So we can use it simple, efficient, we can easy implement, easy to interpret. We produce probability that it is of first type of of type one. So that's why it is nice.
(1:26:39) But the problem is assume linear relation means basically in terms of those axis which enter my exponent here, it is always a line. decision boundary is always a line. So if decision boundary in in reality is not line this model can definitely underperform. No unless you find appropriate transformations. If you find appropriate transformation which you also add here that means you design different features like x1 * x2 for example x2 squar and so on in those uh original features that will be already not line but it is already difficult problem it requires a kind of manual kind of approach to try to understand how to implement this transformations
(1:27:20) in python we can easily implement it we have documents labels Everything same as before to test documents and now we say from secret learn we can import logistic regression uh with take logistic regression maximum number of iteration during maximization of likelihood we have to specify number of iterations will be 1,000 and no usually it will achieve it before because if it is already converged it doesn't have to do like 1,000 then we feed it on train data set x train y train and make prediction on the test data set and again the model predicts
(1:27:59) correctly. Dog and cat in this case is created correctly. So it turns out that in this case it worked it worked fine. So the decision boundary is most likely some kind of line or close to line. That's why it was possible to do this prediction. Maybe if you have more data maybe you can already see that it is more complicated.
(1:28:19) So again this logical regression is is nice tool. If you have like explicit separation of two classes by line then relation will find the position of the surface in order to locate line correctly but only if you have your data separated by line. Any uh questions? Okay. Now next one support vector machine.
(1:28:49) It used to be again used to be quite quite advanced basically a top model for a while for maybe 10 15 years definitely more maybe and uh uh in 2012 it changed because of neural networks would outperform support vector machions and let's see how it works actually support vector machions surprisingly is quite easy easy kind of model I mean in a sense of kind of visualization for example but it is quite powerful and let me explain why it is powerful and how to use it when we don't have boundary decision boundary to be align so in this case we let's say support yeah questions support vector machines
(1:29:47) So in this case it is super algorithm we have to have output right typically we use for classifications we have to class as zero once it is possible to also use it for prediction but typically it is done for classification and uh let me you know what let me show example let's assume that we have two uh classes first one and second one and also two variable X first X second that means I'm looking at two dimensional example also I have labels and let me say I have some some points first type of points which is going to be labeled as zero everyone is zero the
(1:30:32) zero type this this stands for zero right this stands for zero and second type of points is over here. So it stands for uh one second type is one have zero zero labels and one labels. And now if you look at this and you ask a question how to best split it we have to have some kind of decision boundary.
(1:31:08) Okay, large regression is a nice nice nice case in this case a line will be splitting nicely because when we cut it at 0.5 level it means the surface will be cut line in terms of x1 x2 and I can split easily. So logic regression is indeed nice approach in this case. But there is a second case. It is called second model another model which is called support vector stren.
(1:31:32) In this case we try to say let us fit a sort of a margin sort of gap sort of two lines. So if I say something like this okay it is okay but it is not not not the best approach right. I can try rotating those two lines to maximize gap. So basically what it does it place my two lines in a way that gap is maximal. So it places lines in a way that gap is maximal. Probably this is my position. This is maximal position.
(1:32:07) If I rotate you can imagine if I rotate gaps only becomes smaller. It mean this is my best probably position and then I can split with a line which is exactly in the middle of this gap. So it is going to be decision boundary. Decision boundary. So any questions about that we use basically kind of geometrical approach. We maximize gap which you're trying to fit here.
(1:32:43) Those vectors which you see here which touch those which touch my uh boundaries my my lines will be called support vectors. So those are support vectors one to three in my case those will be called support vectors. So any questions about this? How do you define the decision boundary? How do you identify that? How do you find this position? Yeah. So this is kind of geometry right.
(1:33:16) So the problem is how to maximize this. In this case I will not derive it but in this case we try to actually uh minimize uh those w squared no kind of metric kind of distance of w^ squ while while we try to keep this relation.
(1:33:44) So this is something which actually it's interesting to discuss but I decided not to go into detail detail details here. So basically problem becomes sort of minimization problem of this type uh subject to some constraint. It is geometry. We have to incorporate here a vector of W's. Let me say what we mean by this. So there is like vector which is like w first w second and so on w uh n let's say k whatever like in my case two.
(1:34:17) Okay, let me say two only two W's, they will define a vector which is perpendicular to this line. Then if I take X * W plus B, it effectively becomes equation of my line itself. So X * W + B = to 0 is equation of this line essentially. It is like that product of my of my um so it is like um supposed to be that product between these two vectors is supposed to be a zero right so and that's how we can obtain um equation of this of this line essentially so this is going to be exactly what what you see is this equation is going to be equation of this line of hyper plane generally speaking and in order to maximize the
(1:35:13) gap we have to actually uh take this type of constraint and we have to minimize this quantity then we will get exactly position which maximizes gap so again this is something which we have to derive I I decided not to do it in this class what is like geometrical problems basically it is pure purely geometrical problem where we have to find position of this line which maximizes the gap which is formally uh going to be mathematically whatever you see on the screen this type of problem right now let me ask you maybe different questions
(1:35:48) not about like how to derive it because I don't want to derive it here but let me ask you if we have to kind of go back to you know equations of hyper plane and stuff but let me ask you a different question how to actually uh what kind of concerns can we potentially have in this in this case What concerns do you have? There should be some concerns I hope.
(1:36:17) Okay, I say maximize gap. So concerns, let's say concerns. First concern, what is your concern? If I want to maximize gap, how to do it kind of mathematically or numerically? So basically now we understand we can find position which maximize this purely geometrical problem or minimization problem subject to this constraint which you see on the screen.
(1:36:45) But there is there is some important point which which I want to make even few points. So first of all let me ask you if I try shifting this point somewhere this point okay if I try to shift it somewhere will it impact position of my line or not? If I try to shift it will pos will my decision boundary be impacted by this point or not? No.
(1:37:19) No. So which points will impact position of my of my of my b decision boundary? The uh support vectors right support vectors only only support vectors will impact my decision boundary. What does it remind you? How is it called? There is notion of overfeitting, underfitting. How is it called? Only three points impact entire position of my line.
(1:37:46) How is it called? If my model is sensitive to only overfeitting yes, this is overfeitting case. Only three support vectors will define position. I shift to one and position is already different. So it is called overfeitting. So we don't like it. We don't like it. be somehow somehow want to introduce a model where I actually uh sensitive to more points than just three points.
(1:38:10) So let me say overfeitting over fitting is one concern. Let's say only support vectors matter, right? So there's a problem. Second concern. What is second obvious concern which you may have in this case and also third concern that is also two dimensional case. Um do you have any more concerns with this approach? again remember yeah remember it was like dominant technique for for a number of years so it it shouldn't be so bad you should should be able to improve it somehow how to find position is we understand it is pure geometry but it is equivalent to minimization of that uh function now how
(1:39:10) to actually implement it in practice if you don't have so you want you want to answer other concerns uh probably there's no such single hyper plane solution for some problem. Okay, no single solution. By no single solution, you probably mean that they are not separable. Right. Right. So this is concern. Exactly. This is concern. They could be not separable.
(1:39:41) This is what I have right now. So how to fit this you know gap this kind of margin if they are already mixed is more kind of realistic case right. So it turns out that it means I should in order to avoid that only three points contribute to position of line and avoid this problem and also in order to be able to handle such cases where there are some violations I need to incorporate actually um uh into my cost function.
(1:40:15) I need to incorporate some kind of possibility of uh violations. So I should be able to allow some violators. Basically it means if you look at this problem I say it is it is okay this margin but if I allow some violations margin will be bigger or someone will be inside. Okay who cares I allow for that so they will be there will be some violations but they don't contribute.
(1:40:41) I don't punish as much if there are some violations. That's exactly what I'm going to do. So I'm going to say if someone is inside of my margin or even on wrong side of my margin it is okay I allow that means my margin will look somewhat differently now uh let's say let me say this way maybe someone is even wider let me let me say this way this way so there would be some this is violation violation violation violation right maybe someone is even on on wrong side also violation and then yeah this one is on wrong side and this is my margin if I only allow this and I
(1:41:29) say okay if some few points will be inside of my margin it is okay as long as they don't really um like not really too much on the wrong side for example and so on so this formula is done this way allow this um additional term Remember Xi basically represents how far away it is from the right side.
(1:41:50) Let me say what it means. So my margin will be this way. If I say what about uh this let's say this point no there is some kind of guy which is like s and I measure this here will be like zero over there will be like one and for this guy for this specific guy I will be like how much 30%.3 so I can compute how far away it is from where it is supposed to be.
(1:42:29) It is supposed to be on that side right below this um below this margin but it is not it is inside. So there is some violation it means this guy this particular guy will contribute 3 that side which you see in my cross function will contribute. Now let me ask you a different question. Okay, this seems to be nice because already whoever is inside or whoever is on wrong side will be contributing to position of my line.
(1:42:53) So this is kind of regularization technique. Basically this size allow me to uh already introduce uh more points which will impact position of my line. Now let me highlight support vectors. This is support vector. This is support vector. Support vector. Support vector. Support vector. Uh that one is inside is also support vector. Support vector. Support vector.
(1:43:22) Do I have more support vectors? What do you think? Who else impacts position of my margin? So those guys which are on wrong side will also impact. So basically S is even more than one for this guy and also this red red point on wrong side will also impact position of my margin. All these guys will be support vectors.
(1:43:55) Now it is much better like half of my observations will be impacting position of my line. It is already not overfitting right. So this is overfitting case overfeitting case. And this is already not overfeitting case. And there is a question how much should I allow my points to be inside or on wrong side.
(1:44:21) Basically how wide this margin is allowed to be. When I make it wider position will change the right it will rotate. And uh there is this parameter which is um C the C parameter which is essentially like lambda plays role of like lambda. regation parameter. You can play with the C and see best performance ultimately based on based on based on or best performance for each C and then you can decide which C is most optimal.
(1:44:50) It is already soft margin hard margin case soft margin. This is called hard margin and this is soft margin. My parameter C is somewhat like lambda basicallyization parameter which I can play with and find best best best position. So I fitting only few support vectors uh matter. Now second issue uh there maybe No splitting but this issue is also um fitting maybe no splitting.
(1:45:45) These two issues will be resolved using this soft margin. What other issues can we potentially have in this case? Any any other issues? Remember it was like for maybe couple of decades like best model basically available. What was um what what is obvious issue which you see also another example I will help you. So this is my x1. This is my x2.
(1:46:32) And I have data points, data points, data points, data points. Those are zeros. Then I have other data points. Yeah. Questions. Decision boundary limitations. decision boundary limitations which means not always it is separable with line right so this case can support vector machine handle this case the answer is yes easily it can handle this case any ideas how to do it circle maybe circle yeah but it it fits margin basically Right.
(1:47:20) So, so decision boundary maybe not linear, right? Decision boundary. Okay. Let's say only let me say different. Let me say um maybe not separable with lion. Not separable with line, right? Not separable with line. This issue. How to solve this issue? You say you said correct correct thing. Circle some kind of ellipse or whatever, right? So we indeed can do it that way.
(1:48:07) So this is two dimensional case. Not separable by line. Example, not separable. by line example. So we can yeah imagine it uh differently. We can say there is x first x second and let me artificially introduce another variable which is going to be x1 2 + x2. So problem becomes artificially becomes three-dimensional right because I introduce another feature I can say let me kind of artificially introduce another feature x1 2 + x2 and so on.
(1:48:57) Now let's assume that my center was around zero. Maybe for convenience let me assume that this was zero. This was zero for convenience. Then when I sketch it here it means I'm going to have points over there over there over there and then I move away from zero.
(1:49:30) I'm going to have already once x is greater x 2 is greater but x12 + x2 is also greater it means those points will be over there and now it it is easily separable with what plane so one plane one margin second place second margin and then I can say there is something in between which is going to be my decision boundary Right? So this is my decision boundary.
(1:50:05) If I only introduce this type of feature x1 2 + x2 problem becomes easily separable in the original coordinates. It means I'm introducing circle as you said and my margin will look this way. Now I still keep soft margin. So some of them could be inside some could be violators. It is okay. But my decision boundary effectively is is going to be as you said like circle or maybe ellipse of some kind can use other transformations easily we can do it.
(1:50:39) Now beauty of this approach is that I didn't have to do any kind of you know featuring of of circle of any kind. I simply introduce new variable and problem on this threedimensional space is exactly same support vector motion in this multi dimensional space is kind of linear problem.
(1:50:58) My still introduce hyper plane which is linear hyper plane right it means problem is exactly the same that means minimization of my cost function is exactly the same minimization problem which by the way is is is is quite easy to implement numerically it means it is it is going to be efficient algorithm as well now it turns out that you don't don't even have to incorporate this third dimension manually you can instead of trying to incorporate manually you can actually change geometry of your original space. We see here w * x it is some kind of dot
(1:51:36) product equivalently rather than going to threedimensional space we can change dot product in that space we can manually change it and say it's going to be different now and will effectively correspond to circles for example maybe to some kind of higher boat or something else and uh that's exactly what is going is going to happen instead of trying to use that product we are going to introduce different that product differently which means problem is effectively the same but the product is going to be defined differently in case
(1:52:08) of support vector machines this is called kernel so basically we're going to in introduce no different geometry format speaking in this space it means dot product is going to be different and there are some options polomial kernel for example radial basis functions for the kernel and so on if kernel is going to A simple 1D is equal to one a simple line in that case is going to be called support vector classifier speaking it is called support vector machine. So it is so powerful you can just play with different kernels basically which means
(1:52:47) with different geometries which equivalently means you introduce different uh variables and that space and that multi-dimensional space you still use hyper planes linear hyper planes but problem uh is still quite um quite easy to solve because we basically minimize same we solve same type type of problem numerically And uh if you try to approach particular problem especially if you don't know how to choose your kernel you just try different kernels basically different hyperparameters and then you see best
(1:53:23) performance using test test data set some of them could be for example radial basis function of this kind which means circle effectively and it may separate your data set better and you can play with hyperparameters as well. So any questions about this approach? So in this case u let me say that uh it is very powerful to actually support some support machines and can be used for text classification easily and uh here is Python implementation.
(1:54:02) We say let's take support vector machine from secret learn and then we train it on our using kernel linear kernel which means original problem I try to split with a line but if I change the kernel to something else I can split with a circle with something else basically if you don't know how to split we can write different kernels different hyperparameters and choose best performance based on test some kind of test metric then let's fit to the train to the train then predict for test and see that we got do and get as well. So that's how SVM support machine works. Kernel is quite
(1:54:40) important notion. Kernel equals to linear means we use always line to split it right but it's not the only option. Any questions about support vector machines. So you can see here C is equal to one. You can see that by default radial basis function. You can see here options for kernel functions.
(1:55:23) It means so effectively think this way. We introduce different geometry in this space as a result lines which you used to split is going to be different. Even though formally speaking the problem will be solved in higher dimensions kind of it will split in this hyper plane but numerically it will simply modify that product and will not even do do much different computations much different will not do it much differently.
(1:55:52) So see this uh default parameterization parameter as we discussed right which allows you to use soft margin and so on. Uh any questions about this? We need soft margin because we don't want to overfeit right we want to don't want to be sensitive to only few to only few um support vectors. So finally very last model radial basis function uh yeah nothing is different right result is still same rad basis function polomial kernel you have to see other parameters what else you can change right of course because polomial polinomial kernel by default maybe not the best clearly because there are also some parameters if you're talking about
(1:56:38) like polomial what kind of parameters would we choose and so on. So now finally random tree and random forest. Let me say um let me let me talk about this random tree and random forest. Again let's look at some example. Let's say I'm talking about prediction problem.
(1:57:30) So in this case let's assume I have some some data set and I want to build predictor X is in Y is out then I'm going to actually trying to split my X variable the first iteration I'm going to try to split my X variable in a sense that if I then choose constant on the left and constant on the right it will be best prediction.
(1:57:55) This is not the best approach, not the best split. Then I want to scan through different axis and somewhere maybe in the middle I will split it and choose constant on the left, constant on the right. It will be my predictor. So this is like first iteration. First iteration then I move to second iteration. Second iteration is exactly same.
(1:58:20) I'm going to say let me choose X and Y. I'm going to use my data set this way. There already was split. I keep it. So from first iteration split location where splitting happened should be kept. Then I say let me choose split of the first region. Now let's say this way and constant. Let's say uh let's say for example this way then constant constant let's say this way constant constant and I get this type of tree already after two iterations for every sub region I have to decide how to split basically it's already second iteration and so on
(1:59:07) so this is my tree after number of iterations I'm going to get the result Now obvious issues this tree is going to overfeit because I don't know when to stop other people use some kind of you know idea like tree pruning you have to decide when to stop you introduce some penalty and so on but it's not the best approach probably in practice because you still have to you know experiment with this a lot you have to decide when to stop otherwise most likely you're going to have overfeitting because ultimately every point will be in its own kind of region right by the
(1:59:42) way how to split it in In case of how to choose a location in case of prediction it means we minimize sum of square deviations typical typical error. In case of classification we are going to actually use criteria such as entropy or gen indexes is also quite uh quite popular. We're going to minimize this type of matrix entropy and gen index.
(2:00:15) If you do classification for prediction it is simply sum of square deviations. If you have more than one x x1 x2 every time we split only single x it means we have to scan through first x scan through second x and decide which one to split and where to split and we split across only one of the x's. Then we move to the second and so on. Ultimately we're going to get still nice result but it will be most likely fitting.
(2:00:40) Now the solution to this again tree pruning and stuff like that is used but the best solution to this would be actually to use a random forest. Random forest uses uh ID that we can actually build multiple multiple trees using same data set and then simply take average of those trees. Let me say this random trees is clear how it works. Now let me show how random forest works.
(2:01:08) And forest is basically combination of different trees. And let me say example I have some kind of data set X and Y. That is my data set. Then I say let me use bootstrap sample first one second one and so on. Let me take a random bootstrap sample. It will be kind of resembling my original data set.
(2:01:42) But some points like this one for example is not there because I didn't pick it approximately like 27 30% of points will not be picked because bootstrap samples happening with a replacement. It means instead of this point I may potentially be getting second time the very same point. I'm going to sketch it this way. So it comes two times. Maybe this point also comes two times now. And then I build my tree.
(2:02:04) So this is going to be tree number one. I build my tree. I don't really worry much about ever fitting. It is okay with this ever fitting case. This is first three. Second times I take another bootstrap sample which is also random sample. But it happens with replacement.
(2:02:25) It means maybe now I have different points twice. This point is missing. And this way maybe this point twice as well. This will be my three. Now let me ask you um why do we bootstrap? What's the point of taking bootstrap? We get secondary which looks quite different right? So the point of taking bootstrap is that I want to take ultimately average of those many many many trees and my average will will look quite nice already.
(2:03:08) So it will be smooth and nice, right? This should be three. I'm sorry. This will be forest. This is already random forest. Now let me say I have to take one over number of trees. If I have capital number of uh capital n number of trees, it means I take average and I get my random forest. And this is exactly what I call random forest.
(2:03:31) It is already kind of smooth and nice. And now the thing is if I take bootstrap samples every time it means every time my data set is slightly different it means my tree is slightly different and since I take average at the end I don't care if it is overfeitting case I just build my tree for as long as I want build it can split split split split split overfeitting I don't care because ultimately I take average with other guys other guys will be quite different slightly different from the current one it means all this overfeitting will be sort of canceled out and I get nice
(2:04:01) smooth model which is random forest. Any questions about this? It is average of trees. Now there are some things to kind of think about. First of all, it is important that I have different samples because I have bootstrapping case. It means every sample is going to be unique in some sense. Every tree is going to be unique. Random forest is average of those.
(2:04:24) Even this classification that means I take majority vote again and no kind of average and I decide more than 0.5 less than.5 based on this I I decide what outcome is going to be secondary if I want my trees to be different from each other it means actually bootstrapping helps but it also means that actually if I want to ultimately have some consolations it also means my trees better be different from each other somewhat different from each other it means when I build every tree I don't have to try hard I can actually do like
(2:04:58) a lazy joke in this case when I build tree number one I can do lazy job if I have a number of x's let's say x first x second x third 100 x's I don't have to use all hundreds when I decide which one to split and how to split I can actually pick every time at random like 10 of those example 10 x's out of 100 x's and optimize my splitting based on only randomly picked 10 variables and I split it in the best possible way but based on those 10 variables when I move to next iteration within building the tree there
(2:05:35) was second iteration then I pick 10 x's at random and decide how to split in the best possible way based on those 10 randomly selected variables so the result because I do this kind of random selection of my x's every time my trees will be even further different from each other also It will allow me to speed up convergence because I don't really have to maximize over 100 x every time over maybe 10 x's only but typically like square root of number of your x's if you have 100 square root of 100 is 10 so I maximize over 10 variables every time
(2:06:09) and secondary because I do this randomly it means even further variability in my trees it means average will cancel this kind of randomnesses this kind of overfeitting things and stuff and my random polish will be quite nice and smooth and no overfeitting. Basically, it means I can, you know, train my tree for as long as I want.
(2:06:29) Train as many trees as I want. Maybe more more trees is better, right? Then I take average and my random forest is done. That's how it works. So, HB in this case, HB means like if you refer to notations, it means like H first X is what I built, secondary as a function of X is what I built and so on.
(2:06:53) Then I take average in this case is actually B okay over B. And then I build my Y head which is random forest that is called begging right begging of trees. You can actually use any models of this of this kind and build it on bootstrapped samples and see and then take coverage to get your be. So any questions about this algorithm? This quite nice.
(2:07:20) So I have a small question please. So um just quickly because I'm aware of the time. So we did discuss a random forest in our previous class. I guess it was uh S84 or or ethics 184. So uh personally I really like random forest uh the way it uh uses randomness to uh mix with diversification.
(2:07:43) Now my question is one of the main criticisms of it as I remember from previous class was it's kind of a blackbox model so to speak and that people don't know what's happening under the hood. So um is there any way around it or or is there any choice? Yeah. So so the thing is it's a very good point. The model itself is black box. You are absolutely correct.
(2:08:04) This is kind of maybe kind of drawback right kind of you know disadvantage of this model. But if you want to find the kind of interpretability, if you want to understand how this model looks, there are techniques which are common for all kinds of blackbox models, right? So you can do so you can do so kind of to interpret your results.
(2:08:29) There is no kind of um fast way to do it. It's not like linear regression where you look at coefficients, you have statistics and stuff. In this case, you basically can do what's called um uh let's let me recall how called PDP partial DP. So basically let me try to recall the the term.
(2:09:04) So we can basically try to see how each variable X impacts your output and we can build this you know dependence and then what it means it means we essentially saying let us u take x1 value as for the other ones we are going to take all kinds of x2 values and then we average over entire data set and see what output is going to be output for basically whatever other variables are and we this way can scan and see how dependence with respect to x1 is similar we can do for x2 we take specific x2 and we sort of plug different different different different values for the variables and then take average and
(2:09:49) becomes dependence of yhat on my average right so it will be of course not linear because model is not linear but this is kind of ideal here similar to how you interpret linear regression where you have like slopes better one better two it is like bet one basically but it is more complicated it is called partial so PDP PDP partial doesn't even know like PDP term partial let me checkpot Um partial partial dependence plot is called PDP is stands for partial dependence plot.
(2:10:40) Partial dependence plot is what you can use here. Partial dependence plot and it will be basically way to interpret your results. Yeah. But there is no kind of readily available way to interpret your results. But this is the way to see how your output depends on particular variable. Again it means for other ones you just scan through all of them from your data set and take average.
(2:11:06) That is if you run like linear regression and use PDP partial dependence plot. In that case you will basically recover your slopes your beta one beta 2 and so on will be lying. Also if you want to say how particular observation impacts your result. So you want to know maybe not how variable impacts your result but you want to know how your particular case impacts your result.
(2:11:36) So you can say there is specific guy x1 x2 and so on and I want to know how my result is going to be inputed by this specific variable right how significant this guy is for my result. So there is um this approach let me recall it's called um you kind of take impact with this point on your output without this point and your output take some kind of specific average and if it is called um doesn't know the term I forgot the term if I want to check impact of particular point on my result is like black box like random forest.
(2:12:40) What? So this called um um ship. So this cold ship sharply editive. So this type of value which is widely used to see impact of your particular observation. So in this case we take like all your observations with particular point I and output without without your I S stands for
(2:13:51) observations where we don't consider your particular point and they use this type of metric to see how it impacts your result. So widely used sharply also there was something else like a lime or something.
(2:14:12) So b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b b basically there are ways to interpret what I'm saying is there are ways to interpret black boxes box models. So like sharply metric and also partial depend split will will give you some kind of idea will tell the story. So sharply you see what I'm saying right? But yeah, it it is drawback of this model. Definitely no readiness readily available uh interpretation. So let me show in Python we have the same model. We take random forest from secret learn.
(2:14:46) We train it random state number of estimators that means number of trees feed it and we predict do and get. So any questions? Okay, let's now stop.