89 day 8 section - YouTube
https://www.youtube.com/watch?v=g0R7sAoq750

Transcript:
(00:01) Hello everyone. Hello Dr. Cross. Uh good evening. Hello. So today we're going to run STM structural topic modeling using R. This is how we do it in R. Let me show you RMD. RMD is basically similar to what uh uh to how Jupyter notebook works, right? It it is a little bit kind of maybe different because we first of all organize code here differently.
(00:43) We basically have text everywhere. But but for in order to understand what is text, what what is code which it needs to execute, we use this apostrophes. Whatever is inside of this apostrophes will be a quote which R will execute when we so called need the file right we need means we're going to basically create this report which will consist of our text and also our code and also results which on the way we are going to produce in this case we specify like title and stuff then we apostrophes we place some code in this
(01:20) case I'm going to load required libraries library deeper and so on the glo and so on. Now if I have this hashtag basically it means this line will be ultimately um uh section section title it going to be just text essentially. If I uh describe something else um uh next to my uh title it also will be text but whatever is between my apostrophes is going to be which will be included into my result into my final report.
(02:02) In this case, I also have some commands for you to kind of take a look. This is where data set lives which I'm using here. Let me maybe open this uh website. It is kegle right. You all probably know Kgle. If not, you have to maybe create account. It is very kind of resourceful place where you can get data sets also quotes. People try to also compete.
(02:36) They try to uh uh implement different models and see which performs best. So this is very nice nice tool. I took a data set which consist of million news headlines. So basically my text will consist of headlines. It is not like kind of news themsel because news will be too much. I just took basically headlines. Oh, maybe if I scroll down, you can see how it looks. I have my data set which has headlines.
(03:02) Every entry here is headline of my new artic, news article. And second variable which is basically cover it in my case cover variable is going to be date of publication. So this is date of publication which I consider to be essentially my covert variable.
(03:21) Typically or at least hopefully we're going to have more cover variables than one. In this case I have only one sometimes you have more right in this case only one date. D date will be in the format year month and day. It means we have to somehow deal with this we have to somehow extract it from there. By the way, if I take it as is and I say date is kind of a categorical variable for example, it means I will have to have a lots of cover variables because I will have to use like one hot encoding of some kind. It's not probably the best practice. I well well depends if if I believe that there is some kind of trend
(03:55) then maybe time should be represented simply as a number as a numerical variable and I can basically include it as simply numerical variable to my model. In that case it will be effectively on the single variable or I can say it is indeed categorical. Sometimes things go up sometimes down. In this case when I say up or down that means basically prevalence of some kind.
(04:20) Prevalence of some topics may go up or down over time right if I believe that I have no idea how it is. If I don't know if there is a trend of of some kind typically in this case we assume that time is basically going to be categorical variable right it would be too expensive to run with this model you have to have a lots of data for every date it is not what what I'm doing I'm saying let me assume that there is like linear trend maybe prevalence of some topic goes up or goes down it means I assume like linear trend it means I keep my time to be numerical variable
(04:53) specifically I will day to for example first date it will be 2003 + 2 out of 12 I will ignore days it's like four basically 2003 + 2 out of 12 will be my date in second case let's say if I scroll down no maybe I will not scroll down because I I cannot find it but basically if it is like March already it will be 200 3 + 3 out of 12 that's how you construct date which is numerical variable in my case and use as a single cover variable that is okay it may be single as well right so that's how I that's where I
(05:34) take my my data set and then I say let me this by the way a result if I need it it takes like a while if I need it you can actually uh get this result and this is what I I want to show you this is my result and um Remember I told you if I say hashtag for example this way is going to be essentially uh subtitle some kind of section and title of this this section this exactly what I observe over here I say this is my first section because of hashtag which I placed over there I can even click on this and I can basically move to the appropriate
(06:16) location right away and let's see what's happening let's just kind of walk through and see first of all I want to uh construct my structural topic modeling using STM package right now it is it is what what is used in R in order to construct STM STM model first I'm going to read my data set in this case my data set is stored on this CSV file so I read it from there using read CSV string strings as factors as false it is not as important you can later always transfer it to numerical or whatever right now I kind of like to do it this way. If I know that it's it's not going
(06:55) to be like categorical, I can say this way. And this is going to be my data set. I can compute dimensions. So I have like over million of titles. It is by the way in in Australia right Australia over a million of titles and two variables we already saw on the website. First variable was date. Second variable was title itself.
(07:23) I can apply structure str to my data frame. News data is my data frame. When I read it, it becomes by default data frame. So I can say structure of data frame. And you can see what happens with it displays my dimensions. First of all, secondary, it shows me fields. First field is publish date. Maybe you remember from the website, it was the same.
(07:48) First field was publish date. Second field headline text. That's exactly what I have here. So it is going to be first publish date and it will display a few first values. So we have idea what it is. So it is dates which are presented this way. They are kind of numbers right? By default if R sees numbers it will read it as numbers. Okay, it is number but it is not what you want.
(08:19) If you don't want to use it as a number as is you want to transfer it somehow to different format and second is a headline text that means like headline of those news they look this way right that's what we going to analyze in this case now let me first of all take care of the date itself my date looks this way year months and date I can transfer it to date format there is this function as date and I deliberately say I'm talking about character. It means s character.
(08:55) So I don't want to transfer number to date. I usually transfer character to date. So in this case since my publish date is probably just a number. This is just a number. No quotes. As you can see it means I will transfer it to character first then apply as date and I specify when what format it is year, months and day.
(09:15) this way then it will understand and basically it will create new variable date which will be date format then I can extract here from here I can say let me take from here year if I specify what I want year it will extract it from there then I say as numeric and my year will be like numeric variable for convenience I want to just use numeric variable similar I can extract months from here I say as numeric field will months, first month, second months and so on.
(09:49) And then ultimately I'm going to artificially construct date this way. I sort of ignore days. I could have added days as well, but you have to take care of like 30 days, 30, 31 days. I just ignore date basically like full. We often do it actually. If we talk about age and we say like model age uses a sort of variable, what do we say? Someone is like 25 years old. Maybe there's already not 25.
(10:14) Maybe there's already like almost 26, right? But we still say 25. It is like common practice kind of number of full full years for example. In this case, you can you can think that I'm talking about number of full months if you will. It means I say year plus months minus one because first month is first.
(10:41) I want to start from zero over 12 will give me exactly basically proportion of a year and a year plus this quantity gives me exactly time in expressed on years. So any questions? Um first of all this data set is actually uh on canvas under sections uh under data there is there is like a uh folder the data you can you can find it there secondary it is actually on kegle website right here right here kegle thank Yeah. And uh so we have this data.
(11:26) You can see my first month look this way. 2003.803. No because one out of 12 and so on. This is okay. This is what I want exactly. So my date will be just numerical variable and I will assume that there is like linear trend. Maybe it is upward or downward but linear trend is not always correct of course. Right.
(11:50) Or maybe you can introduce like some kind of um transformations like x data maybe data squared it is okay you can assume like parabola or you can keep it as is sort of categorical variables in that case it is quite expensive but it is possible you can do it as well actually you can keep it as categorical variables it is somewhat similar like if you know like time fixed fixed you can keep it that way if you want in this case I chose to use time as a sort of single number as a numerical variable It means my results will will expose some kind of linear trend.
(12:26) I mean my results for prevalences right those parameters status which we discussed last time. So now I say let me take a fraction because I didn't want to use like this median I wanted to just to make sure that I can run it quickly. So actually this results are based on only fraction of my observations. I take sample uh this way.
(12:50) This is actually using a library which is um which must be which must be uh uh this one deep deeper and uh you can do it differently. You can simply take sample uh uh this way actually you can say sample let's say 1 through 10 my like indexes I want to take two only I can get uh two out of 10 numbers this way it means you can you can then later say my data frame example right then I say I have a sample of maybe maybe let's say I have through n where n number of rows and then I take like only 100 observations. You can do it this way. It will work as well. You can specify which indexes you want to take
(13:43) using sample. You don't have to do it that way because there is nice function which we use here sample fraction and I specify what fraction I want to take. So it will do the same effectively. My news data will now look this way. So I kind of know kind of throw away the rest.
(14:06) I I just this is kind of uh the stage of um uh the initial stage. You can want to do it this way. Maybe later you can actually already remove this command out of this line and run for everything. So it would be nice way to do it once everything is working right. But to demonstrate I wanted just to take some uh a sample. I wanted to take only 5% of data.
(14:24) Typically when you have like huge data set you don't want to try to develop everything for right away for your data set because something may not work as you as you as you want. It means it is nice practice to maybe take some sample and see if you can first write your code and then later you can uh comment out this sampling.
(14:47) So now we have this headlines dollar sign means we extract particular field. My news data extract particular field will be headline text will be my documents. My metadata in this case will consist of only single basic column. In this case I say my date variable will be news date dollar sign date. In order to make it again data frame, I have to specify I have to say data frame because if you extract a particular column from your data frame, it may be it may be actually not data frame anymore. It will be like a vector in order to make it like a table again table which has only single
(15:28) column. You kind of want to specify this way and say it is data frame. So my metadata my meta is going to be data frame. Actually even though it has only one column I want to say data frame and now I say text processor last time we already discussed how it works. So basically in this case I use almost like all default parameters even the lower case is equal to true is also default.
(15:51) I specify whether my documents will be my documents from here. Those ones metadata will be my meta from over there which is data frame and I specify that there is going to be lower case. This pro text processor is something you may want to look at.
(16:15) Let me say you can Google it or you can say question mark text processor and you can read it here. So it has number of parameters. Lower case is true. It means I'm going to create lower case. Remove stop words is true. I will remove it. Remove numbers is true. Remove punctuations is true and so on. Right? This parameters. And uh sometimes since we remove the things actually your text may become quite short or may become even empty or maybe come quite short.
(16:49) In that case we are going to actually we going we will have to remove some some some um observations as well. Let's see what happens. So next um we are going to remove uh 9,000 observations from 21,000 due to frequency. So basically we are going to essentially we are going to um first clean it remove like quotations numbers and so on and then they become empty in some cases or become quite short right there is like threshold below which we don't like it and we just remove it.
(17:21) Sometimes if there is like term which is a rare term it may also remove it. You have to see like specifications which you which you selected right and it may remove even those cases which consist of like very very rare words and uh uh removing 50 with no no words right and that's what we're going to ultimately have we going to have um so removing terms because they are not frequent enough so terms will be gone from vocabulary completely 9,000 terms which means 9,000 words will be will be gone from vocabulary. 50 documents become basically empty. We remove those
(18:02) and corpus now has 62 different documents and 12,000 terms. That's how you can you can kind of understand this output and then I say out which is the result of this uh processing right of preparing my documents. I can extract my documents vocabulary meta and this what I'm going to have. So this case I sort of extracted from here because remember something was removed. I don't want to just use original meta.
(18:32) If I use original M it will be not consistent because some rows will were removed. That means now I wouldn't kind of redefine my meta data. It will be already output from this uh object from this uh function call right output from here. That's why my meta is now a new and then I say STM. That's exactly what is is going to do the job is going to maximize my likelihood.
(18:56) Likelihood is designed just as we discussed last time this way. So my likelihood is likelihood which corresponds to this model. I'm going to maximize likelihood in order to estimate all this parameters right and that's what this basically step is doing. STM is going to maximize it using uh expectation maximization algorithm.
(19:16) In this case expectation maximation algorithm is quite popular. So what it does effectively it says if there are something like some kind of hidden parameters basically some kind of latent variables we can't really write down likelihood because we have something missing in that case we have to essentially use given set of parameters to estimate latent things latent variables or something what is what is missing we estimate those using current current our current understanding of likelihood and then we use those things to actually those expected things basically to to compute expected
(19:53) likelihood and then we maximize expected likelihood. That's why there's two steps expectation maximization basically consists of two parts. First we estimate likelihood we literally compute expected value of likelihood right and then we maximize expected value of likelihood update parameters. It means then things starts over again.
(20:14) We use this new parameters to get expect to to get estimated likelihood to precise expected value of log likelihood and then we maximize it again. That's what it does. This STM does exactly that. You see E step m step expectation step maximization step expectation step maximization step. So in this case we specify documents will be docs vocabulary is my vocabulary. K is equal to five.
(20:42) It means I chose to use five topics. So you can ask how do I know that is just basically first step. I want to just show how it works. Later we'll be able to choose it. Actually we have to run a CM basically multiple times for different case. If you do it we can then look at our performance at our different metrics and see which is best.
(21:02) Uh in this case there is kind of oper which does it actually like search K. You can run it later. I will show you. But now let's assume I know what K is. I plug it in and I run it for K equals to 5. This is my formula. I have only one corate variable date which is remember just number numerical variable not even categorical of any kind my meta and I say maximal number of iterations right so I hope things will converge before that before that if they don't converge I have to kind of stop iteration process and then initialization which basically corresponds to NMF as we last time and
(21:42) reduced and then I we start maximization. So E step M step and then move forward east M step and so on and it takes takes a while I don't remember like about maybe 1 hour or so and we get my we get our results so summary of STM model and we can see how it looks in this case it will display basically your topics and corresponding corresponding most associated words from your vocabulary so highest probability correspond towards council, Australia, government and so on.
(22:24) Rex, lift, score, different metrics basically. Maybe you remember last time at some point we looked at those metrics this official publication and there were a matrix uh defined for example 3x is defined that way and so on. So we can display them and based on those we can display most associated words.
(22:49) I personally don't like to look at this results this way. I like to look at most associated documents a little bit more informative. You can read and see what those documents are talking about and this will be basically u basically what your le topic is probably is about. That is because your words which you look here at could be like Australia it is not informative.
(23:11) Okay, Australia and what Australia is everywhere maybe because it is news about Australia and Australia is not supported we didn't remove it that's why it is not so useful say how is it useful it's not useful right and so on that's why we we don't really like maybe looking at this words it is less less informative now let me say plot if I apply plot to my STM model object that is like output from and I say summary n is equal to 5.
(23:45) So now it means I want to plot all my prevalences. This is expected topic prevalence. So again remember prevalence means something which you pull from normal distribution. We pull numbers from normal distribution from multi normal distribution we apply uh soft max and we get theta. So theta is basically random variable but we can actually compute expected value of theta.
(24:11) We can say for every sort of document what is the expected value of stata. We can compute it like literally and basically your output actually will have it already. You can access them from your uh from your object and expected value of that is available for every single document. Then you can say let me now talk about let's say topic one document first consists 70% consist of topic one document second 20% consist of topic two document third 30% consist of topic two let me fix topic let's say first topic first document consist of 70% of topic one second document in the reference to
(24:52) the same topic will consist of like 20% of topic Next doc consist of 10% of topic one very same topic all the time. Then you can loop over your all all documents and see how much basically your topic contributes to every document on average and you're going to get basically average value of your expected preance.
(25:13) That's exactly what we plot here. We say topic uh two for example contributes on average as 30%. And of course to one document it will contribute as 50%. And to different document that may contribute more than 50 or less than 50 on average will be like 30%. That's what it displays. It means in our corpus basically topic two is most prevalent.
(25:38) Again of course some documents will consist of topic two only slightly. Some will consist of topic two more than even 30% but on average it is 30%. Then topic five is uh second most frequent topic and so on. That's how we how we can uh understand this chart already. Then I can also plot my cloud right so it is again something I don't usually don't really pay attention to because I will see some Australia Sydney it's not so informative maybe or maybe it is if it is like news from all all the words maybe but in my case it is not so so useful especially since I didn't remove it is not stop words by the way it
(26:17) basically means that maybe you want to modify your stop words and you can say Australia is something which is not useful maybe I want just remove it maybe so you can add it to list of stop words if you want that's how we do it okay now um we can estimate effects this is something which I wanted to talk about a little bit estimate estimate effects so uh in our case remember that let me say estimate effects In our case, you can see that my uh prevalence, this data parameter which is basically prevalence comes from logistic
(27:11) normal distribution that is centered around su which is given by over it and parameters which we estimate right we estimate this parameters X is just data it is given in my case it is it is basically date and sigma is also something we estimate from data so we have it now how to actually say how for example my prevalence changes over time relation is highly nonlinear it is kind of kind of you know difficult relation because it it is through this um PDF we plug in and we sample from this PDF. So typically what we do is we basically say
(27:55) the following let's say I fix particular topic let's say I fix uh topic one topic one I want to look at topic one I want to say how topic one changes over time basically right so formally speaking I have here a date I have what was first 2003 I believe then 2004 it means and so on 2005 five my observations were like monthly observations or maybe even daily observations.
(28:28) It means I have a lots of lots of observations here. Now let me say that according to this formula according to my model essentially my prevalence even if I know parameters even if I know parameters exactly my prevalence is actually random variable I can take it and sample from the distribution but if I sample second time even for the very same x I may get something different all right same for different year same for different day so it means effectively I understand that even if I have very same X in this case very same data
(29:05) basically I should understand that my prevalence is not predetermined prevalence by design of this model is going to be essentially going to be random variable that's why we do in this case the following we say let us use estimated parameters let us plug them in and this is going to be like posterior distribution like from bian basian infinite So basically we're going to look at this sort of distribution of my prevalences and I'm going to run this model run this sort of sampling multiple times and plot my thetas. So my theta to the D right it
(29:47) come it corresponds to different D as well right different documents for different documents I have different data in my example and then I can essentially fit a linear regression here. That's exactly what I do. I can fit linear regression. I can construct confidence intervals for expected value of my prevalence.
(30:07) So this is essentially this is confidence interval, right? And this value is essentially expected expected prevalence data not given given date in my example. That's exactly what estimate function is doing. It is going to take my already estimated parameters that is called posterior distribution.
(30:34) It is going to sample from posterior distribution and fit basically some kind of model here in order to estimate how my prevalence changes over time. That's what it does. Uh and that's what we do here. We say estimate one through five means I don't fix topic one. I say do it for every topic 1 through five do it for all of them my form is going to be that.
(31:04) So because date is my uh variable here STM object is my result of STM metadata is meta uncertainty is global I just specify uncertainty how I estimate my confidence intervals then I can apply plot to my objects and here it is important to correctly specify what you want to display in this case I say I want to display all topics from one through five my model is STM model my Co variable is date in this case right not is the only one which I have in my example and method is going to be continuous because in my case the date is numerical variable continuous numerical variable basically remember it was like 2003.083 083 something. So it
(31:45) is continuous date. That's why I say continuous and then I say X label and main like title whatever I want and then I upload it. So this function will display it this way. Now you may not like it. You may not like how it displays. In that case you have to basically take your object and extract from there your variables, your estimates, confidence intervals and everything and display any way you like it. So you can display it a little bit better than that if you want. Right? But this is kind of default
(32:16) behavior how it will display it for us and show that topic one looks like over time it becomes more and more prevalent. What was topic one? Council. So what was topic one? Council government death here. It's not clear from here. It is not clear.
(32:40) I have to look at maybe most associated documents in order to understand what topic one is about. And topic um for example green one topic three becomes less and less prevalent. So topic three over time becomes less prevent politics say plan and so on especially because this um words already uh stemmed it means it is even more difficult to understand what's going on.
(33:06) I typically prefer to look at uh documents. Nevertheless, this is how how you should interpret it. Any questions about that. So in this case, we make assumption that trends of prevalence are linear trends. Of course, you can modify it, right? You can introduce for example at the start your X doesn't have to have only date. It may have date and date squared.
(33:31) for example will be ultimately some kind of parabola where you can introduce actually even one hot encoding. So basically can you can say that my data is going to be factor variable for example it will be already um not continuous anymore will be discrete you'll construct confidence intervals and see how your prevalence changes over time it is probably not preferred because power of your kind of of your test is going to decrease in that case because of more more variables but you can do it if you don't if you're not willing to assume that there is linear trend you can do it differently of course. Now what if we didn't know number of topics? No, basically we need
(34:08) to run STM multiple times for different number of topics. There is this function search search K. I can specify that K runs from two to through 10. I will try different case 2 3 4 and so on up until up until 10. I specify my formula, my metadata and so on and then I run it multiple times.
(34:33) This takes much longer because I have to run it for two for for for two topics for three topics for for four topics and so on. That's why it takes much much longer. It is same E step M step and so on. Every time I take for example two topics and I run it run then I move to next number of topics at some point not is already up to six you see right and then it is up to seven already and so on and then I scroll scroll down and it will show you results.
(35:18) So it will display this way. If you plot your results, it will display your residuals held out likelihood and so on. You can actually pay attention to those if you want. But if you want to kind of uh interpretation of your text, maybe likelihood is not the best and metric to look at.
(35:37) Maybe sematic coherence and exclusivity is what you want. it will display only symmetric coherence but not not exclusivity. That's why you can actually extract it from your object K results. If you take K dollar sign results, it will have sematic coherence and also exclusivity. So I can extract it from here will be vector of coherence and also exclusivity will be vector of exclusivity.
(36:08) Then I create data frame based on those coherence exclusivity and I specify how many topics it had. And then I can plot it using G plot. I can plot it this way. My data frame all statics X is exclusivity. Y is coherence label is number of topics which I had two three and so on. And I say points I want to display text. I want to display next to it. Right? labs labels and I choose team to be minimal.
(36:39) That's what I have exclusivity coherence versus exclusivity and then basically here I have to decide what is best for me exclusivity and coherence must be maximal but they on different scale so I can't really just take average of coherence and exclusivity because they on different scale here 9 and here I have -260 so basically uh uh so for your assignment ment uh for your assignment you have to choose what was for your assignment date of your choice you say right was it of your choice I think I I gave you options to choose whatever you want right who remembers let me double check
(37:24) so it is assignment six select news article whatever you want and so yes it is of your choice here. Yeah. So now in this case when we maximize we sort of want to maximize kind of kind of rescaled exclusivity resal coherence if you have a number of data points basically have to place it in a in a box from 0 to one then you already allow to take average of this reskilled coherence and also reskilled exclusivity you can apply function rescale from package scale or you can just manually you know rescale it. So basically what I want I want for two exclusivity to be zero for
(38:18) 10 exclusivity to be one. I just want to want to rescale it this way. It will be rescaled to for coherence. I want coherence of nine topics B 0 coherence of two topics B1. Then it will be from 0 to 1 across first dimension from 0 to 1 across second dimension.
(38:40) Then I can easily take average of those reskilled matrix and rely on this average of those matrix. In my case I think it will be four anyway. Right? So four four will be four and that's how we can obviously five maybe five here five. So then I choose five and I say select model K is equal to 5 will be my result. So basically this select model will be essentially same as if you just take STM and you say let me take five uh topics.
(39:12) But there is a catch. This is not reproducible right? That means if you just run STM using K equals to 5, you may get different result. That's why it is better to specifically select the model which corresponds to K based on your particular runs. So any questions about this you can also actually look at your coherence and exclusivity this way.
(39:43) It will display your coherence exclusivity for every single topic. For example, those circles mean that you're talking about uh model which corresponds to specific run and every circle circle represents every circle represents topic. Then this red number here the red number one. So where is one? I don't see it.
(40:17) A is behind for this one is average across all topics and average across of all topics for coherence and for exclusivity and you basically have to choose which number is highest here. This is difficult to see but that's how essentially we decide we have to say which number is highest 2 4 3 1 it looks like three maybe is highest it is closer to the right corner and that's what um essentially they ultimately would choose and say three is highest but this is not number of topics I want to specify this is not number of topics this is actually different runs
(40:53) first second third fourth run because every time when we run it, we may get different result. So we choose third run and this is going to be our ultimate model which we use to basically move forward. In our case, we're going to choose like the one which corresponds to maximality and coherence and uh it will be our best model.
(41:18) No, let me let me see what's next. So this is what we have. So we can take best model. We have a number of different runs and we can see those numbers basically present averages across topics. If you fix like circles, one circle corresponds to to one topic, second to second topic and so on and then average of across all topics will be here at location variable one occurs.
(42:01) Then 1 2 3 four what is highest? Maybe one is highest for example run right or maybe two. So I chose two in this case okay I chose two that will be my selected model which correspond to second run. So again the logic is I chose number of topics it was five then I run it multiple times to choose even best out of best for each each of those correspond to five topics then I choose the best run which corresponds to five topics and will be my favorite model which corresponds to five topics will be second model second round I'm sorry second round and then I can put summary and sort of move forward with this right and basically that's how I would do it. Any questions about that?
(42:46) I recommend you to read this uh documentation that is very very very and kind of a informative source. So you can see what they did here, right? You can apply u uh for example you can apply find thoughts to display documents which correspond to topics from here you can actually understand which those topics are about right it is nice way to do it if you don't like to do it this way you can actually go directly into the object and you will see prevalence there you will see prevalence using this prevalence you can sort of extract documents which
(43:30) correspond to highest prevalence. So essentially what happens in this case so we understand we have a number of topics let me say note we have a number of topics like topic one topic two topic three topic four topic five and here I'm going to have documents document first document second document third and so on so what happens is my basic object which corresponds to this results We'll have this matrix those prevalences right those static coefficients those like stata coefficients you can look at data and you will see like 2 for example five.1
(44:21) uh let's say one and let's say.1 total is five so document one mostly consist of essentially document one mostly consist of topic two. That's how you should interpret it. You can extract this matrix from your object. Then in this case you say.1 3 4.1.1 it means second document is going to consist most of uh topic three and so on.
(44:59) And you can basically uh if you want to understand what topic is about you can basically say let me for every topic let me take maximum so let me say let me say uh no let me say I have only three three documents so it is clear and I'm saying what is what is topic five is about it means um if I if I want uh look at topic five. I need to pick pick document three to understand topic five because for topic five each which document is most related most of the document is last document is 5.
(45:43) 6 six at this highest here tells me that basically document three is the one which is most associated with topic topic five so for each topic I want to display document and I want to read it like literally which is most associated with my topic for topic five for example this document is most associated that's what I want to display basically uh basically um find thoughts is exactly what it is doing it will tell me for topic three this document is basically most most kind associated with my topic.
(46:14) The second document which is also associated with the same topic. They just literally say let me focus on topic five for example and display this document in this box. You can see it. The next one right next to it will be second and most prevalent. Then you move to next topic like topic one.
(46:34) In this case you would want to display document one because it has highest highest probability. So you can look at uh highest across maybe topics or maybe let me even even change it so it is clear. You can take topic at a time and say for topic one highest is this one. It means document one is what you want to look at.
(46:53) For topic two highest is again document one. For topic three highest is document two. That's what we want to display. In this case not clear what to display right. So you can change it uh this way and say okay it is going to be uh this way it means for topic four I want to display maybe document three that's how we can ultimately understand what topic is about topic one contributes most more to topic to document first more more than to anyone else it means if I display it like you see on the screen I can say okay topic one is about what topic document topic one is about what
(47:35) document one is talking about but notice document one also has other topics. It means not everything which you find in document one is what topic one is about. So you have to be careful topic one is only about part of document one that's why number is also quite interesting quite important it will be not like that it will be not like point 2 maybe it will be higher right so for topic for topic for example four you will display document three and say okay document three is talking about that that and
(48:04) that means topic topic four is about such such uh such uh themes so any questions about that from these thoughts you can kind of understand what topic is about and you can deduct this information. So they say I think that one is like uh liberal second one is like conservative or something. So yeah they produce some kind of labels basically right.
(48:34) Okay. So any uh questions? So this document please uh please use this document for the reference. It is quite useful. Our package for structural topic models. You just can find it uh you can just type like structural topic models and you will find it right away. It is very popular. Could I ask you a basic R question, professor? Yeah. Yeah.
(49:05) Um, at the beginning of your document, are you is there a specific command that removes the 50 empty documents or is it just automatically done? It is automatically actually. So what happens is if it becomes empty, it will remove it. You see, we say we say remove stop words, we say remove numbers. If it becomes empty, we can't really work with it, right? It means it will remove it.
(49:31) Oh, good. So, I don't have to do anything. It just does it automatic. You don't have to do anything. Yeah. And also sometimes it also will remove words which are not frequent enough. Right. If they occur like I think one time only. I think this is sparity level. If some words are not not frequent enough, they will be removed from vocabulary.
(49:53) It means even more documents effectively will become empty because it is not only about like having something but it is about having something from from the vocabulary. If vocabulary is shorter now because some words are removed from vocabulary it means more documents will be actually empty and they will be removed. Yeah. So they will be removed.
(50:14) You don't have to do anything but it is important that you uh actually specify that you you you say here I want to prepare my documents right and your meta for example must be new meta not original one you see some some something was removed it means your out will not have same meta it will have less observ observation. That's why my meta is is is redefined.
(50:49) So it doesn't refer to original one. Original meta here may have more observations than met over there. It will have more observations because something was removed. That's why it is important not to forget to redefine my meta when I supply it here. Right over here you can simply supply out those sign over there.
(51:15) Does it make sense? Yes, but we'll have to go go through it to understand it a little more. Yeah, because uh your reprocessing basically removes some of the observ so it it removes numbers, punctuations, stuff like that. Also remove words which are not frequent enough in this case by default.
(51:32) I think if it occurs only once it will be removed not interesting. Basically if those after removal removal of those things such as error rewards numbers uh fluctuations if your document is empty we don't have it we don't want to have it we want to remove it that's exactly why 50 documents we removed when it says with no words it's not quite with no words it is with no words after removing punctuation after removing error words after removing stop words they become empty. If they become empty, they will be removed. So my metadata for
(52:08) example will be will have less number of rows. That's why this out will have already meta which is shorter. That's why my meta now must be shorter before I supply to STM. I have to sort of update it. Does it make sense now? Okay. Um yeah. So any further questions? Okay, let's stop now. Okay, thank you. Thank you. Have a good night. Yeah, you too.
(52:45) Good night.