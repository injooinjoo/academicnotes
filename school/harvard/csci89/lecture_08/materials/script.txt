89 day 8 - YouTube
https://www.youtube.com/watch?v=QWxSyhnOkKc

Transcript:
(00:01) Hello everyone. Hello Dr. Krosk. Hello. Good evening. How are you? So, how is everything? How is your assignment? You know that it is due not uh on Sunday but it is due basically uh today right by the end of the day. So now um any questions about anything? It's just more of a request not not a not a um not a question.
(00:39) Um is it possible if we can submit separate Jupyter notebooks for each problems because sometimes I will have to restart the the kernel and just running everything from the beginning means that I will have to repeat other experiments. So uh it was not against the rules. If you see what we discussed in the first class, it was not against the rules, right? But later, sir, there was a posting on PA. I have to check it. But it what what you're asking is not against the rules.
(01:08) Is is not what was announced in the very beginning. I will see what what post you're referring and I will I will I I will try to clarify. But yes, again, you have to submit your code and also your report. If your report is created using Jupyter notebook, it is okay. Okay, you submit like PDF file which you which you create using Jupyter notebook and also Jupyter notebook if you want to submit multiple files because you solve problems maybe differently or one is using Jupyter notebook second one you don't even use any Jupyter notebook it is totally acceptable right
(01:45) yeah so it it was not against the rules it was it was actually mentioned in the very first class now I can tell you uh even maybe once again let open the very first um first slides. By the way, um apparently some people are having a problem using Zoom. What? What do you mean? Well, they're getting 400 errors on their web browser saying bad header, whatever. I don't know.
(02:15) So, if the number of people who show up today is much lower, that'll be why. So, 21. No, I mean I think it is more kind of typical number. I'm sorry. Are you like a support uh support staff for your student? Who are you? No, no, I'm just monitoring the the WhatsApp where some people are posting that. Yeah. Okay. So 22 students I think it is reasonable is usually how we get it.
(02:42) We have around 100 students and uh typically you know because students live in different time zones. Some students work and so on. we get around 20 to 30 students seems to be kind of reasonable. Uh so for the problem, do you have any hints? Are we supposed to use the advanced algorithm for which problem? Problem three of current assignment, right? Yeah, of the last assignment, do you have any hints? Uh, yeah, I think it's very difficult to um do the auto encoder without significant loss.
(03:35) So, assignment number uh current is six number six, right? Or number five? Number five. We had a lots of discussion in the WhatsApp, right? And uh it seems that nobody could uh come out a solution that perfectly recover the uh sequence 200 words. So yeah, let's let's talk about this. So in this case, we are going to build some kind of autoenccoder, right? And we on the way on the way trying to sort of train uh train embedding layer.
(04:12) So this is perfectly okay. So this is the question now you are saying that it is not basically is not possible to reproduce it in a way so it is readable right no okay so if you don't change architecture of this network it means we have basically layers layers layers of recurrent type then we take only last vector which is like representation of our sequence our sentence in this case then we take it as uses as input to the Next layer already is a part of our um uh decoder right first layer of decoder and then try to reproduce it. So this is
(04:55) exactly what's called out encoder. It means there is this natural kind of kind of bottleneck which we create and we try to represent entire sequence of vectors in this case basically sentence using single vector. Nobody said that it's going to be easy. It's not going to be similar to what you you can get in case of images. Don't even expect that.
(05:20) Right? In case of images, it was it was much much easier than in this case. Uh if you just you know make it um in a sort of best possible way. If you have to you can increase dimensionality of your latent representation. Even if this doesn't help basically you know the answer for this particular model number of observations is not sufficient.
(05:46) It means so we have to somehow maybe get more observations which we don't have on this case but basically this is a this is kind of answer we are trying to build quite difficult network which uses basically sequence of vectors and try to call kind of compress it into one single vector and try to sort of decompress it back also in the way we have this embedding layers it's going to be it is very possible it is it is difficult right if unless you have a lots of observations And this exercise is not for you to kind of build nice and you know uh efficient uh autoenccoder which will recover your sentence. Most likely it will not but it is exercise for you to see how much more difficult this
(06:25) model is than for example for example um convolutional out encoder when we when we deal with images. So what is your question? Uh it's it's along these lines. Um so what I did is specifically for this question uh so the the um the last layer in my network is um is loft max which out probabilities.
(06:49) So I use the the function the temperature function and instead of selecting the the top uh the highest probability the word was highest probability I tried to select one word near the one that was highest probability and that more or less generated some sentences. So this is Yeah. Mhm. Yeah, they're not very coherent as the original ones, but but they were they were English sentences.
(07:12) They were um Yeah. Mhm. This is smart approach. Basically, I said that I'm trying to somewhat u change my kind of uh not even not even as um cost function. You're trying to change like interpretation of your output, right? So yeah, this is smart way to do it. Definitely. Yes. Yes, you can you can do it this way.
(07:33) But what I'm saying is you you should not expect that you're going to get very nice result. Actually there is no really kind of way to do it if you first try to compress into embeddings and then you try to compress into single vector. There is kind of one way to do it but it's not going to be called out encoder.
(07:52) But one interesting way to train you to train your embedding layers, you can actually avoid the second part. You can simply take a layer of recurrent uh neurons a layer layer layer layer maybe LSTM for example on on the way at the start you can incorporate your embeddings and you can with no compressing into latent vector you can actually output your result in that case you will get almost like 100% you know reproducibility you can try that approach but it will not be formally speaking answer to this question but if you kind of concerned how to kind of in practice you know build maybe your uh embedding headings. This is kind of
(08:25) solution. You don't have to do it v out encoder. You can do it via this type of you know neural network which simply transfers sequence of vectors to sequence of vectors to sequence of vectors and ultimately will match your original sentence original sequence of vectors on the way you can incorporate your embeddings.
(08:44) It would be nice actually approach if you want to just train your embeddings. In that case you can see that you can shrink more more and more and of course there is no surprise your repres will be quite quite accurate because you don't create this bottleneck you always keep time component all you do is you are trying to represent your index via your embedding representation which is quite possible to do right you don't lose as much now if you have like index 1 2 3 4 5 which represents particular And we say now let's use instead of this index not one hot encoding but let's use vector in like 30 dimensional space
(09:25) typically it is going to be like basically sufficient for us to reproduce it with high accuracy for our small data sets at least. Well, now if you say if I want to stick to out encoder where you have bottleneck so you will you will kind of see that it is not so easy there are techniques to actually train such models so called um so-called um augumentation it means you will have to basically create artificial s sample sample data points if you don't have enough data points not that means you can take your text and you should somehow alternate it maybe you replace
(10:02) your words with synonyms, right? You can cut something and drop something from your sentence and so on. Basically, you have to artificially enlarge your data set. It means every time you supply your sentence, you you make little twist, make it slightly differently different every time.
(10:22) This may help because it effectively means you increase your data set. That's such techniques widely used argumentation. Even for text it is widely used. For images it is much easier because for image you can simply rotate it you know stretch a little bit. So for images augumentation is widely widely used but text is possible but you have to have kind of u uh additional tools to sort of know how to alternate your sentence how to replace it with synony. But it is still also used.
(10:53) So if you want to kind of think deeper how to do it this would be the way to go. But uh again for this problem if you don't get like complete repressibility or even if it is not quite not very readable actually it is expected because if your text is not large enough in our in our case I I gave you text right so it is not your text it is our our uh our um um our movie reviews or something right here movie reviews.
(11:24) So in this case uh even even though it is quite maybe large for out encoder of this type of recurrent type it is not not sufficiently large to get nice uh reproducibility. This is not surprising. By the way at some point people try to develop uh translators using this approach this type of auto encoder.
(11:44) I would say seems like nice idea with compress sentence into a vector and then try to recover from this vector everything we we have to like the same sentence for example maybe sentence in different language. Turns out that at some point it would be kind of probably best available model but then people would move away from this approach because it's exactly what you observe on this assignment.
(12:09) Of course you don't work like with a huge data set but this is for you to kind of get get this experience and yes it is difficult. Yes. Uh it turns out that this is not sufficient to even get like uh somewhat reproducible results, right? So this is expected actually. So don't worry much about this. Just say whatever you got like as a best you know result and uh that's what it is right.
(12:37) You can discuss why you think it is not going to be better. So now because bottleneck we lose time component and we create vector which represents entire sentence. In order to do it you can understand you should understand that it requires you lots of lots of data points lots of basic sentences a lot of documents. So any further questions? Oh yeah you mentioned about like files.
(13:03) So again let me say uh assignments and at some point I clearly stated that you have to submit uh your word to PDF document as a sort of report. This report can be created using Jupiter notebook or you can manually create it at whatever you want. It depends on how you basically solve this assignments.
(13:28) You can run Jupiter notebook if you like. You don't have to run Jupyter notebook in that case. probably it is kind of quasi manual work maybe for you. Uh D notebook means you can simply run it and create PDF file. So we want to see a report which you can see right away without even downloading downloading this report and also you have to supply all your files.
(13:47) So now it says if you have uh multiple files if you have multiple files I actually asked you to create a zip file so you don't you don't like you know post like 20 files on the canvas. So one zip file which contains everything. If you have like single Jupyter notebook for example just upload Jupyter notebook on your report.
(14:15) If you have multiple files you can upload your report and also multiple files as a zip file. If you have like two or three it is okay. It is acceptable if you submit like two or three files without zipping them. If you have like 20 files no I would prefer if you zip it right. So it's most important that you submit it. So we have it first of all right whether you z is like already less important and I didn't request that you do it using Jupyter notebook I never I never I never um I never said that I say you may generate right but I didn't say that you have to so you can use Jupyter notebook in order to kind of make it more efficient for you if you don't want to you know create
(14:54) your report in addition to your coding then you can use Jupyter notebook which will create a report right away. But nevertheless in both cases you have to submit like report which we can read. I mean this was discussed during first lecture. I don't know about the post. I will I will check what was posted.
(15:14) I will I will again clarify u but basically the bottom line you want to see report and you want to see original code which was used. If you have multiple files, for example, one file for each one uh basic notebook for each uh problem it is okay or somehow later combine it together. So just run average Jupyter notebook and you know combine this PDF files together to create single report.
(15:37) It is already quite easy, right? So more questions. Yes. So I think that the confusion is about the report itself. Thank you so much for clarifying. Now I understand it's okay to submit multiple Jupyter notebooks but the report itself can can I have the the report um in each Jupyter notebook or the report has to be only one file. So report um is not Jupyter notebook.
(16:00) Report is PDF file which you may create using Jupyter notebook. Yeah. And uh I would I mean I mean I I would kind of combine those PDF files together. So there's no confusion where it is right. So you just submit one PDF file which has all outputs from every single Jupyter notebook. You just combine them together afterwards into one single PDF file and that's it. But when you create it, you can use different Jupyter notebook files.
(16:27) Every problem may may be solved on individual Jupyter notebook. It is totally okay. But please submit report as well. So we can quickly open on canvas and double check everything. That's why PDF or W report and also your original code which you used to run it. I will see I'm not sure what yeah I'm not sure about the post which you mentioned but the logic is is very simple.
(16:57) We want to see it so we can open on on canvas and also you want to download it and we want to run it if you want to if you have to run multiple if you have to use multiple files multiple notebooks for example it is okay. I would personally prefer if you zip it right but if you don't I mean I always mention that please zip it but some students don't don't do it we kind of don't really worry much about this but it's important that you submit it if it is missing that is already a problem right because we don't know how you created your report so uh any further questions about that so okay now uh today we have
(17:38) Quiz seven. Uh first question first question is about uh your is about your uh LDA allocation and uh the question is which statement best describes what LDA does basically right the first case is LD assigns a single topic to each document is not correct.
(18:05) We discuss that every document will be a combination or kind of collection of different topics and it will assign probabilities to each topic. We will say 60% of topic first, 40% of topic second which means a is not correct. LDA is a super algorithm that requires labeling. It's not correct. We can simply have text without any labels. This is the beauty of this approach.
(18:24) This basically a kind of unsupervised learning. We don't have to have any labels. B is not correct. C assumes each document is a mixture of topics. This is exactly what it does. It assumes that every document is a mixture of topics. It is quite important because we understand that text can actually describe multiple multiple topics at the same time.
(18:49) You read the news and understandably you can notice that they talk about politics at the same time. They mentioned something from economics and so maybe something from sport that means of course it is mixture of topics you typically that's why it is by design by design document is going to be mixture of different topics LDA use K means but no it's not the not the case K means is not used in this case the is not correct the only correct answer is C next one which of the phone is a common challenge associated with using LDA For topic modeling the need for label data set is not correct. You don't have to have any labels. Second one deter
(19:27) determining the number of optimal topics is indeed the case. It is a challenge. It is basically hyperparameter. We have which needs to be chosen some somehow. Basically before experiment typically we do this experiment and we kind of already see the results and choose the one which maximizes specific matrix as we last time discussed such as coherence and exclusivity for example there are different there are also other metrics sometimes people try to maximize almost like test test likelihood it is not called test likelihood it is called held out likelihood right
(20:05) computed for points which are not used to train the the model. So different different methods can be used but basically it is quite quite important and difficult task how to actually choose the best number of optimal number of topics. If you have too many topics what's going to happen two different topics will be similar. First one is about cats second one is about cats not interesting.
(20:30) If you have only few topics not enough it means different ideas will be combined together and topic will be about like cats and dogs and everything maybe you like it maybe you say it is about animals and it is okay in that case right but what if some things which aren't completely unrelated will be combined together they talk about computers and also about cats or maybe it is some kind of uh some kind of uh some kind of uh computer vision then it is okay but if it is not then it is not okay to combine cats and computers maybe that's why small number of topics is not nice large number of topics is
(21:08) not nice because different topics will contain similar similar ideas it means we have to find somewhat optimal number of topics which is often manual work yes we can rely on those metrics and it is kind of nice practice to have this symmetric sort of predetermined before experiment so you can say I didn't cheat I just basically decided up front and now I'm using it.
(21:32) That's how people typically do it but it is still challenging basically because results maybe not what you expect or may be not what you like and not clear what to proceed further. You kind of feel that maybe topics combined together but it's not what you what you like. So next on LDA what is the role of um uh directly distribution in this case dial distribution in this case.
(22:02) So dial distribution remember it is used to uh to generate uh proportions of topics in each document. So let me open maybe maybe slides from the last time we introduce this algorithm and there was this specific step which uses dish distribution right the distribution dish means almost like uh it is both probabilities from the distribution right probabilities those status should sum up to one basically that's what we do in this case and we say it is used to generate those status and that is basically no what's called prevalence what's called uh probability kind of probability with which I say proportion proportion uh of
(22:53) uh this document which is assigned to particular topic right kind of probability of contributing particular topic to a document that's why we can answer that it is prior pro distribution which defines proportion of topics within each document. So those status are quite important quantities those are what you want to look at ultimately you say first document that the first at the second that the third you have like three topics and you have that the first to the second second that the third you will have like 70 20 and 10% will will
(23:29) be telling you what your document consists of th those are quite important. So next one um question four uh uh which of the following best describes the negative matrix factorization. In this case we basically try to uh somewhat u reduce uh dimensionality right somewhat reduce dimensionality.
(23:56) No I mean it is almost like trying to represent our text via some kind of topics. They are not explicitly topics but they could be sort of interpreted as topics. Now let me so deco compos matrix with n negative entries into smaller matrices. Let me again open slides and we last time discussed there was example that we can say v is basically representation of our text. Every row is a document.
(24:20) Those ones represent that we have particular word in our document. First row represents first document. Second row is second document. Next row is last document and we have a number of columns. Every column corresponds to particular word in our our vocabulary. Let's say cat corresponds to first column, dogs to second column and so on.
(24:43) And what we obtain? We obtain matrices which are going to be kind of smaller. Why happening? Because W will have same number of rows but number of columns in W will be sort of related to number of topics. And number of topics which we want to choose here is typically less than number of of of words.
(25:04) Right? We have for example in this case six different words in our vocabulary. Number of topics doesn't have to be six. It is smaller because topics will combine different words together. Basically words which occur together often will be combined together. That's how it works.
(25:24) Uh that's why we have like only two columns because only two kind of topics basically. this this kind of idea and h H is going to have two rows if you have two topics topics like in quotes a number of columns will correspond to a real number of columns that's why it becomes effectively smaller right W has less number of columns H has less number of rows that's how it is used in practice that's why B finally last question how do latitation and NMF differ in the approaches right I mean what is different so basically LD is probabilistic model which maximizes likelihood and uh NMF is simply going to be matrix algebra we're going to
(26:11) decompose this matrix into product of two matrices in the sort of best possible way there is no really kind of maybe exact way to do it generally speaking but in the best possible way it will do it for us. So any questions about this quiz? Dr. Kajkin is is item C uh true but not related to algorithms? So LD works by identifying themes based on word coances patterns while NMF reveals hidden structures by breaking down documents into editive parts with no negative values.
(26:55) Um so in this case uh this occurrence a little bit confuses me even though formally speaking it is probably correct ultimately because co occurrences sort of play a role in this case but we don't like explicitly try to model coccurrence or try to maximize co this kind of issue which I see here right and second issue which is um uh re reveals hidden structures by breaking down documents. We don't break down documents into anything.
(27:28) We just simply represent this matrix as a product of two matrices. That's why it's it's not correct way to say that we try to reveal hidden structures by breaking documents documents into editive parts. We don't do it. So first sentence is kind of more or less okay.
(27:49) It is indeed the key that we are sort of ultimately effectively I would say use occurrence it's it's going to be happening kind of implicitly right when you maximize likelihood it's going to happen that your topics will sort of combine those words which occur together or often together will combine together into topics is indeed the case so first part is okay but second part by is not okay second part is saying that we're going to break down documents into editive parts we don't do it.
(28:21) So by no means right the one word that was was sketchy for me is additive. It was multiplicative even so the this this this factorization will bring it down into into two matrices to be multiplied together not to be added together. Yeah. Yeah. So yeah. So additive means we kind of break into kind of pieces right literally. But no we don't do it. Yeah.
(28:43) So now uh let me think uh what I want to mention now uh first of all uh LD itself maximizes likelihood right let me sort of again repeat what it means maximation of likelihood we observe uh data and based on data we are going to say what basically process what stoastic process cost our observations this is very difficult problem so we understand I mean it is widely used.
(29:15) It is quite interesting if you have if you have many many observations. It gives you actually very interesting results maximum of likelihood. This MLE estimator is quite quite important estimator because it has nice properties and uh widely used. But let me give you one example. Let's assume that uh you see that your cat came home and your cat is wet.
(29:41) Right? So it is your day to be observed that cat is wet came home from the from the outside and your cat is wet. So the question is how to kind of deduct from here what happened to the cat. This is basically maximization of likelihood we are saying it is possible that outside it is simply raining outside that's why your cat is wet.
(30:05) It is one possibility, right? Or maybe someone kind of deliberately like explicitly tried to, you know, push something onto your cat. Also possibility, right? So you have to conclude, you kind of don't know what happened really why your cat is wet. But you you have opportunity to sort of try to maximize likelihood.
(30:25) That means you compute probability that your cat is wet if it is raining. It is basically 100% right almost. So like probability is one if if it is raining your cat is wet is probability one and u uh well so maybe it's going to be a little bit uh uh let's say a bad example but because you have now thinking deeper understand that it could be a little bit more confusing but let's say we are trying to sort of deduct from this data what really happened, right? So, what really happened? Uh, and we conclude that most likely it was rain because we maximize likelihood. So, that's how we that's how we kind of apply it in practice. That's
(31:10) why this algorithm is first of all is not um always correct is not like stable, right? Especially in complicated models like LDA, it may produce not always same results. It is stasisic algorithm. There's a difference between NMF and also LDA. LDAS the high algorithm uh more than that you may get different different uh topics you may get different topics uh when you run it next time I mean you can literally first of all they could could be switched why first topic is about cats second topic is about dogs
(31:47) why not all the way around it could be different first of all it is quite obvious secondary it is possible that your topics no they will often have to combine different words together. They try to combine words which co occur together. But it is actually more complicated if you think about examples where it is not obvious how to combine coccurrences as if there are like millions of co occurrences and only like 11 topics.
(32:16) That's why it becomes uh actually difficult and not stable problem. Every time you run it you may you may be getting different result. It is important to understand this this concept in order to understand why we run this algorithm multiple times. Actually what happens is for example STM structural topic model which is already kind of modification of LDA which is sort of more advanced already next level model which allows you to use so-called meta data.
(32:43) So that model already uh example by by default will be uh you will run it multiple times basically and if maximization is not going well the algorithm will even stop maximization will start it over. It will kind of by default run it multiple times and present you only best results and then you look at those best results and you choose the best basically basically best uh model which means you maximize some kind of matrix of your choice.
(33:16) For example, you maximizeivity and coherence together and you choose your best model. So in this case it's not like for example for example for example linear regression. You just run it and you get same result. Everyone in in class will get same result because we do the same minimization problem. In this case u we kind of do same optimization problem but this problem doesn't have unique solution.
(33:43) Right? If you have like in simple case if you have one document which says cat and nothing else second well okay you can flip things right. First one is about cat second one is about dog. You can definitely flip things which means it is already not unique solution. In this case there is no unique maximum of likelihood more than that.
(34:01) There could be cases now let me say like example let me try to make it up but I want to make one point which will be helpful when we talk about STM today. Let me say document first is going to be talking about uh let's say let's say excellent but difficult for example excellent but difficult. Now I keep in mind, let me say I keep in mind student evaluations because I work with student evaluations.
(34:40) It was part of my job and uh there is even publication which you can read if you want last time I provided your reference uh in the chat. You can read this publication. We analyzed 11 years of data of student evulations at Harvard and we tried to use uh STM model structural topic modeling to understand how how basically results depend on things like gender of instructor field like department and so on and so forth and that's why I keep this type of examples in mind excellent but difficult assuming that it maybe students say something right it it doesn't have to be about evulations it could be whatever you kind
(35:19) of want. Let me say excellent but difficult. Let me say uh next one is interesting but fast but fast. Uh and let me say something like let's say three documents. Uh let me say uh difficult. Let me say difficult but interesting and now question if I decided I have two topics only so K which refers to number of topics is is two let's assume it so this is number of topics which I aor decided I want to three documents and two topics.
(36:17) Now remember turns out that if I maximize likelihood of such models where I assume that my I assume that my uh documents are generated this way as we discussed last time. Remember number of uh words will be chosen as post on random variable. Then we choose uh uh topic assignments using dish distribution.
(36:46) Then we decide which which topic we we use and we pull word right to generate every word. So we kind of assume specific specific way documents are generated and then we maximize likelihood. So it turns out that if you do it that way words which co occur together will tend to belong to the same topic. That is kind of feature of this algorithm. Let me see what you write.
(37:04) Uh uh it's okay. I will maybe do it later. If you want you can tell me. So now number of topics is equal to two and I say how to choose topics. Let's let's think about this. Let's say I have uh maybe topic one and then uh topic two. I can assume that I take only maybe two uh let me leave more space. So topic one and topic two.
(37:37) So remember we have to sort of combine words together. Let me take three words at a time for example, right? How can we do it? Okay, maybe you can say excellent and difficult. Okay, you can say excellent. Excellent but will go away because of we trying to remove stop words but a stop word that has gone. Difficult is second word difficult. Maybe it can consist of different words as well but probability of those words always will be less.
(38:10) So basically most prevalent words which correspond to topic one will be excellent difficult. Second topic will be for example interesting comma past maybe something else but those probabilities already less less and less. So it will be like options options all kind of results which I may potentially get after running my algorithm.
(38:43) So I may I may be getting this result then if I have option first let me say if I have option first here then there there are no more topics because only two topics then you can see that excellent and difficult is basically what my first document consists of I'm getting almost like uh almost like almost like 100% of topic one this way of topic one and uh I'm going to get 0% of topic two right it contributes to my first document what about second document so you can see first topic is different from my content it means second document will be 0% which contributes from topic one and 100%
(39:33) comes from topic two this way. What about last one? What do you think how the last is supposed to be? If my topic happens to be excellent, difficult, second topic interesting, fast, it will not be like truly zero. It will be somewhat like small in practice because we have other words as well.
(39:59) But those words sort of associated with topic less probability of association is less. Let's assume it is only two and two. So towards here, towards over there. What my last document consists of? Which topic does it consist of? What do you think? 50%. 50/50. Yeah, 50/50 makes sense. So 50/50. And I got my result. So it is one way to do that.
(40:26) Now if I tell you, but what if I choose it completely differently? I mean like I could, right? So option two could be different. It could be easily. Now you can you can flip things around. You can say interesting fast. So interesting fast. It means the topic which was second one now became first one. More than that second topic doesn't have to be first topic which we currently have.
(40:54) Second topic could be for example difficult interesting. So it will be difficult, interesting. As you can see, if you try to sort of combine text into limited number of topics, we sort of inevit inevitably lose something. It means in this case, for example, it means we are going to have a second option.
(41:19) First document will be partially topic one because in because uh because um oh maybe first topic is let me see interesting fast doesn't contribute to the first one right so difficult interesting is the last one difficult interesting is uh so difficult corresponds to difficult and uh excellent is not even there it means like 100% of topic two probably no let me say 100% comes from topic two and then this example interesting fast will be 100% from topic one and what about last document now it looks like in this case uh difficult interesting is also second topic 100% from second topic probably this way
(42:12) because first first topic doesn't seem to to contributing anyhow to first document and then we kind of see that what happens second topic also has interesting but it's not really found in my first document so it may maybe happen easily that's why even though it is 100% of topic two but it's not exactly what topic two is it doesn't have to be exactly that and you see the challenge right the challenge is basically my result will be different next time uh any questions about that you can. Yeah. So even though in option two topic one
(42:50) has interesting, you wouldn't give it any percentage for for document three. Document three uh is interesting and yeah, you're right. So in this case 50/50. Yeah, thank you. So it is 50/50. Yeah. So 50% topic one and uh 50% of topic two. Yeah,
(43:17) that's correct. Yeah. But first case is u some somewhat like 100% from the second topic even though uh interesting is not there but I have to do this way because first topic is kind of irrelevant already. So I'm just trying to kind of uh tell you what may be happening when you maximize likelihood in this case and you never know which option it is going to be.
(43:36) That's why in practice we're going to do it multiple multiple times. We run it multiple times and we choose the best option. Let me show you a publication which is this famous publication STMR package for structural topic models. It is almost like documentation basically you can find it. It is actually on the syllabus as suggested reading for structural topic modeling and you can see here people uh run this algorithm multiple times and then they get this result.
(44:12) What it is first, second, third, fourth. It means actually different runs. Four different runs, right? We run it multiple times because we never know what result is going to be. We have this cloud cloud cloud of points. This circle, triangle and so on. They show like centers of those. They show let me say every triangle here shows particular particular topic.
(44:38) We have a number of topics which is like 1 2 3 4 5 6 7 8 9 10 and so like 20 topics maybe you can see up here when they run it they specify uh number of topics is 20 exactly. So they run it only they intend they want to run it 20 times but result shows only four uh four best basically performances right.
(45:05) So because 16 times it was kind of not right either not finished or was found to be not like the best. So this maximation of likelihood will help us multiple times just like in case of neural networks basically it happens multiple times because we understand that processes we never know what what we're going to get because of non uniqueness of this maximum and then we plot all these 20 topics let's say triangles we plot this 20 points each point corresponds to single topic topic one okay can compute coherence and exclusivity and I plot it topic two I plot it as topic three I plotted as
(45:41) well. So for this specific run which corresponds to triangles I plot like 20 22 2D points here exclusivity versus coherence and then I compute average of those things like midpoint basically averages of coherences and average of exclusivities and number two will be exactly at the location where average occurs.
(46:06) So you can see two then four and so on and then you choose the number which is kind of closer to this to this corner right upper corner because it will sort of simultaneously maximize exclusivity and coherence. That's how this package is designed. That's how we interpret this results. And uh again it's important to understand that we not going to get like always same result.
(46:27) Sometimes it is like somewhat worse, sometimes better in some specific sense and we have to rely on some metrics. Um in this case if I would if if I if I try to interpret this results maybe for me second case is slightly worse I don't like that like 100% comes from topic two but topic two is not not not very much related to document one because there is also word interesting right so that's why it is difficult problem so now uh let me move back to uh to my uh slides and uh uh say what limitations of LDA let's kind of recap it but let's see what we discussed last time so last time
(47:16) we discuss that we first uh generate no kind of model which which is used to generate text sort of generates as if it was the case kind of we decide to talk about something right so today we meet and we just kind of, you know, we choose a topic. Oh, let's talk about, you know, like a baseball. Okay, let's talk about baseball.
(47:40) We kind of don't think about this maybe much, but topic comes comes up and we discuss it. That's exactly what's happening. We decided right now during this conversation, we discuss baseball. Okay, that's why we generate this probabilities and probability of of of political topic contributing to my current document is denoted by theta and theta to be specific is not just one number.
(48:06) It is actually a bunch of numbers which sum sum up to one because my conversation ultimately make a list of multiple topics right that's why I have this probabilities and then I sort of generate topic itself so there is like distribution of probabilities from where I'm going to choose my topic and then I generate my topic and I start conversation and every time when I already know what topic it is about every time I generate my words specific words which are related to this topic. This is idea.
(48:37) Now if you you imagine if you try to generate text text this way it will be much worse than result of your problem three or maybe same. So it is kind of understandable but this how we assume text is generated. There is only time component that is just bag of words basically and we try to see which set of set of parameters will correspond to maximum likelihood given our particular document or set of documents and that's how we run this uh algorithm alpha and beta parameters which need to estimate from data those like Z for example actually topic which was selected is not observable directly because I don't know what topic was
(49:16) selected I look at my text. Who knows what topic was selected when this text was generated. This is kind of problem which I need to sort of recover in this case. Z is called latent variable. Latent means not directly observable kind of latent variable sort of hidden kind of variable.
(49:36) Not even parameter but some kind of hidden latent variable of the model. And we say that in this case basically when we maximize likelihood we have to on the way sort of um understand that there is also this latent variable. There are techniques to maximize likelihood in such cases. It is called EM algorithm.
(49:54) Expectation maximization algorithm for a number of specific choice of parameters. We basically generate the Z to be precise. We compute expected lo likelihood. Z could be that or could be that or could be that. We don't know what it is. And we compute expected value of likelihood. And we maximize expected value of likelihood rather than likelihood because we don't know likelihood simply because we don't know Z what topic was selected. We have no idea.
(50:19) That's why there is like one step which is expectation step. This expectation algorithm EM algorithm famous is used in many applications. We first compute likelihood for every possible selection of Z. We waited with probabilities and we obtain expected value of log likelihood and then we maximize it and then we move to previous step updated set of parameters we generate kind of kind of generate differences we don't really generate but we compute expected value of likelihood again and maximize it again this is like going going this way iterative process EM algorithm and we maximize likelihood
(50:59) so any questions about this approach so complicated is that we have to also um uh compute expected value of likelihood because likelihood itself is not a variable because variables which are used here latent variables they are not observable directly. So now limitations uh for this algorithm are quite obvious.
(51:24) We actually don't have metadata or if you have metadata we can we don't use meta metadata. Let's say I have a bunch of you know documents. Let's say news. One news I found in New York Times right? Second news was found in um uh financial times. Next one was found in you know cooking or something. I have this information basically is called metadata. information about the text.
(51:50) Now for example, it is about where it was published, who was the author of this publication or maybe about whom it was in case of student evaluations. For example, we have who wrote evaluation, student gender, you know, like uh age, age, basically everything we may have and also we may have uh information about the professor, it's him or herself, right? We may have information about professor, gender, what department, what what course was taught, when it was taught, fall, spring, summer, when and so on.
(52:22) So it means we have a lots of lots of potential metadata which we don't want to just disregard. And again, if you can think about applications where you understand that uh at least you know maybe ours or like journalist name of journalist who who wrote it and when you do classification it is not really efficient to disregard this information.
(52:41) It may be helpful for us to use it in order to create those topics. That's exactly what STM is about. It is going to use meta metadata so-called metadata to better assign those topics which we just discussed. Let me say remark on metadata. So it is clear what it means. It is called metadata. uh instead of covered variables basically let's say I have documents and the first document says cat set and so on my document and then I say that there is also some kind of uh so-called cover variable so also called metadata right let's say this is a
(53:36) gender. No, maybe again gender could be gender like author's gender for example. This is second column and okay this is like female for example wrote this document we know that right? Next one dog ran away something okay male wrote this document there could be also ID typically so it is understandable but most importantly we have this metadata we may have something else let's say uh author author's name for example it could be also information where it was published when it was published right you can a publication date
(54:27) for example everything will be will be basically metadata and let's say it was uh Amanda Amanda Smith uh for example date it was published so let's say it was u 2024 for example let's say year maybe and second one is Douglas Parker was the author published in 2025 example.
(55:06) So everything which you see here this particular uh data frame this data frame is what we call metadata basically. So this is meta data also you will you will hear terms such as cover variables. So this the core variate variables this way. So at this at this step on this step when we try to assign topics to to documents we actually want to use it because it may be helpful to know who was the author because maybe you know style style of this author right you know what she is writing about why to to to just throw away this information.
(55:47) If you know style what she typically writes about it is I mean you don't have to manually do it model will do it if she writes about the same things all the time it will be useful when we build this model will be useful to utilize this information even when it was published if you're talking about like fall of 2024 so news news more likely will be about politics about elections for example right if you talk about fall fall 2025 Five maybe about government shutdown and and so forth, right? So it's clearly data is also important
(56:23) piece of information when we try to assign topics and uh so basically what it says doesn't incorporate document level metadata. We can use different type of data. Um and this metadata may be helpful in in in sort of creating those topics. uh so we basically all this information more than that I can tell you that actually uh at some at some point when you already build your topics you you may want to try to relate your topics already to gender for example or to publication date and let me maybe even open some examples
(57:06) so it is u it is clear what I'm talking about so in this case you can see Let me show some examples. Now it is variable which is uh uh uh liberal and conservative, right? They extract topics and they show that Obama for example is is mentioned by those who are more more conservative and more liberal in this case. They talk about Bush. This happened this way.
(57:41) So now uh in this case I also can plot uh the topic proportion as a function of time as well. So you can see what happens. So you can you want to try to understand how you know mentioning of some particular issue for example how um you know uh uh for example layoffs or for example is is being discussed and press and you can basically find a topic which talks about that if I mean you don't have to manually actually decide what topic is model itself will decide what topics are and you will essentially create label for the topic and you will say this
(58:19) topic seems to be talking about uh you know kind of spending cuts for example and then spending cards will be discussed and press with specific kind of intensity right there is exactly going to be prevalence of topic in our documents and we can show that this prevalence changes over time this this way maybe before election it wasn't as much or maybe it was already actually it was part of election but maybe at some point it wasn't and it became like issue and would be discussed much more for example such as uh spending cuts for
(58:53) example. So this quite interesting way to analyze data. This approach is very very popular among social scientists. I can tell you like it is all this uh government departments all this political scientists also economists economists also as part of social scientists also like this model. This is quite popular model.
(59:19) And uh let me now say how we going to uh change it. We going to basically incorporate this metadata into the model. So STM structural topic modeling is nothing but extension of of of LDA extension. We kind of extend it by incorporating this metadata. STM is not the only model by the way but this is kind of probably most popular model or incorporating metadata in structural topic modeling in topic modeling.
(59:48) So now let me show you how we're going to do it. By the way, let us actually take uh take a break and we'll do it after the So now we understand the the issue we need to incorporate metadata somehow into the model. It means we have to
(1:08:20) modify the model. So let's do it. So this is how we formally introduce structural topic model. Let me uh maybe write down something on the board. structural topic model. So remember parameter theta the one which was also asked on the quiz. Basically parameter theta is speaking a vector of parameters or a vector of probabilities those probabilities must sum up to one and we generate before it was generated from the distribution.
(1:09:03) Now we generated using so-called logistic normal distribution. Let me explain what it means. So specific to it is specific to document sub index D means it is specific to my document which I look at. Every document will have its own set of probabilities. Then I say given in this case given those XD XD basically is exactly what we have up here. This metadata is what I call XD, right? XD.
(1:09:32) Now, of course, XD corresponds to specific document. It means if I fix XD, it means specific row, specific document. So, this metadata is what I call XD. If D is chosen, D is document first, it means first row, D second, second row from here. And then I say XD multiplied by gamma.
(1:09:57) No, it means it is like linear regression linear combination of those coefficients standard not matrix notation of linear regression I will elaborate on this in case you don't know and also given a matrix of those variances and coariances and I pull it from a logistic normal distribution so logistic normal distribution centered around mu which which is exactly XD * gamma it is like linear linear sort of linear function. Let me explain what it means.
(1:10:30) Gamma 0 plus gamma first time x first which corresponds to my document first. Right? My document D plus gamma 2 X 2 and plus so on. That's what it means. My mu is basically centered around gamma 0 plus gamma 1 * gender. In this case gender is 01 for example. Then it is just 01 and plus gamma * x2 x second could be name name already means it must be one hot encoding it means basically gamma * x2 itself is actually that product so it may be happening that my x2 is for example names it means it is like categorical variable right it means gamma * x2 is nothing but
(1:11:14) basically vector of gamas times using that product vector of zeros and ones and so on and then second variable is just uh just my capital sigma which is matrix of variance coances. Now this normal what normal the normal we understand what it means we take multi normal distribution right we say there is space of u k dimensions k is number of topics which we decide to use.
(1:11:48) So this is k dimensional space we use normal distribution from here centered around my mu which I specify up here and this is um my mu right and then I say from this distribution I'm going to pull random variable centered around mu but this random variable is not actually going to be this like random vector to be precise vector uh random vector is not going to be vector of probabilities I have to create vector of probabilities it's a logistic Just like in case of logistic regression we apply we apply sigmoid function in
(1:12:24) case of multi multi-dimensional by logistic we basically mean soft max. So this is nothing but soft max. We take this random vector apply soft max to it to get my status. That's how it is done. Let me say this. This part is basically effectively is what we know as soft max. soft max applied to vector which you pull from normal distribution applied to this particular vector and this uh distribution lives in this k dimensional space no plus one for for pdf itself right so k k inputs because I have k number of topics so any questions about
(1:13:05) this specific step how we generate our set coefficients suggestion Dr. Kashkin, the the labels uh for the for the graph are not visible uh on the video. You may want to draw them a little higher. What is not the the mu? Oh, you mean that one? Yeah, we can see them. Yeah. Yeah. So, but you can see me, right? You can see me. Mu we can see. Yeah.
(1:13:39) The other one is I just mean that I pull I just pick pick random vector from the distribution. This corresponds to this part and then I I apply soft max to my result to get probabilities not just a number in this k dimensional space but I want probabilities that becomes probabilities after applying soft max to this result which comes randomly from normal distribution centered around mu that's all I meant okay does it make sense now just just one question so the normal distribution, a multivariate distribution or is just uh uh single variable normal distribution and we have to sample it maybe. No, in this case normal is is multiate
(1:14:27) normal multiate normal we pick vector from the distribution centered around multivariate mu. All right. And we get also those uh also those um matrix of variances coariances here assuming if if you know them then we this how we pick we pick it from here and then we apply soft max to get probabilities. So this is kind of a modification of previous approach.
(1:14:57) Previously it was dish now it is logistic normal which is technically speaking normal normal vector. Then we apply soft max to it and we get it. We just need kind of mechanism to generate this status. This me mechanism was chosen and then we move forward and basically we do exactly the same as before.
(1:15:19) We basically compute we choose topic for every word we choose we choose specific topic using those status right and we also choose specific word using some kind of better coefficients here. So this is how we how we do it. Yeah. The variance will be hyperparameter or which one? The variance the the variance of the variance this sigma capital sigma is hyperparameter. Exactly. Yeah.
(1:15:47) Sorry not hyperparameter parameter which we will obtain during maximization of likelihood. The only hyperparameter is number of topics. In this case nothing is hyperparameter except for number of topics. Okay, that's how we do it. Now, let me let me just say that let me make one little remark.
(1:16:18) So, we often say like in in linear regression, we often say like beta times x, right? But sometimes by for example x we mean actually some kind of vector. In my case, x could be vector which represents my name. It means x for example x gamma gamma second gamma * x2 could be easily not just scalar time scalar it could be actually vector time vector so x2 is basically this variable which could be represented via one hot encoding right so amanda let's say let's say she's first she's first in my kind of vocabulary whatever you call it now is going to be my vector which
(1:16:58) represents x2 two then gamma must also have a lots of things. Gamma like first right gamma second and so on gamma last one will be also vector that corresponds to second one because this name second variable from metadata and I use my dot product. So this becomes dot product.
(1:17:24) So effectively when you write it this way if X happens to be vector itself for example it corresponds to categorical variable gamma basically means vector of gamas. It is like typically how we do it like linear regression as well. We run linear regression and then we have a bunch of coefficients which corresponds correspond to every case. We have a number of different cases uh which variable can take.
(1:17:48) We run seemingly simple linear regression with single variable. But the single variable on the fly will become a number of different variables because we have to use one hot encoding effectively. It means my corresponding coefficient becomes effectively vector of coefficients. So we keep it in mind also about this notation XD time gamma. So it may be confusing.
(1:18:13) Let me just say that also as an example. Let me say note. So basically we often use this matrix notation to represent a linear model. Typically we say that linear model looks this way. It is some kind of I coefficient but zero plus let's say but the first x first let me simply say x i and nothing else single variable it could be multiple x's um let me see uh I can hear someone but mute you so we have x i plus some kind of error term epsilon i.
(1:18:59) This I is no specific observation no specific person our case specific document let's say specific specific person right I I corresponds to specific person for example right that's how we can write it but we can equivalently say it means uh it means that y first is equal to beta 0 plus beta first beta first x first observation plus plus epsilon first y Second is beta 0 + beta first x second observation plus epsilon 2 and so on.
(1:19:42) Last one yn is beta 0 + beta first x n + epsilon n. So this type of relation typically means or basically means and different equations every time you have same coefficient and what happens is it can be written in a matrix form right. So it is equivalent to saying that my y first and so on yn is left hand side equals to my matrix of my observations which is x first and so on x n also for convenience because of slope here intercept here we place one artificially and then multiply it using matter is multiplication multiplied by vector of coefficients beta one beta zero and beta one this way
(1:20:36) and then plus residual this error epsilon one and so on epsilon n that's how we typically say it which means uh exactly this notation right so exactly this notation so it is already equivalent to shorthand notation where y is a vector equals to x matrix and multiplied by b which is a vector plus epsilon which is also vector. So it can be written in matrix form.
(1:21:09) Ultimately if I say that by y I mean this guy by matrix x I mean I mean this guy by beta I mean this guy by epsilon I mean this guy. It becomes like matrix matrix equation. When you see it in this form it means basically matrix equation right? matrix equation. Uh any questions about this model? So now uh you can see that we incorporated metadata explicitly into the model.
(1:21:43) It means when we maximize likelihood, it will be somewhat used. It means it will be more efficiently uh distribute document over different topics. So it will kind of assign assigned probabilities. So it is called also prevalence for every topic and uh advantages are quite obvious obvious. We already discussed we can incorporate this metadata into the model itself.
(1:22:08) That means assignment will be more accurate essentially, right? And u also if you want to test some some kind of hypothesis for example you want to know if gender for example right is significant variable if it specific topic let's say we extract some kind of topics ultimately from there we even can create label and say this topics to talk talks about cats okay then we can say that females for example more often talks about cats than males because we can look at all these coefficients essentially and can can say if coefficients are significant or not.
(1:22:49) More than that we can actually using this model we can try to generate prevalence already using kind of a posterior approach. We can generate prevalence using using our estimated parameters and try to literally regress generated prevalences posterior prevalence basically regress it on this variables and we can see those coefficients are significant or not and we can make a judgment if our topics vary with gender for example or with date that's exactly how they would plot it right that's how they would produce this results where they
(1:23:27) Basically the topic proportion changes over time this way think about specific example I extract a number of topics first topic talks about cats for example this is expected proportion of my topic about cats and they didn't talk much didn't talk much didn't talk much and then people started you know discussing somewhere on Twitter or whatever started to discuss some topic about cats that's how we can understand it And there is also this quantile. This is confidence central basically which allows you to see whether it's significant or not. If
(1:24:03) it is different from zero, if it is different from maybe a specific level or different from for example uh uh no basically you can you can use this confidence in many different ways. Typically we see if there is a difference between for example males and female right or if it is changes over time.
(1:24:22) If it is white you you don't have evidence. In this case you can see that is short here and then short over there. That means of course the topic proportional topic prevalence of the specific topic went up from June to September. That's how we interpret this results. I can I can show you one maybe result. This is what we published just few months ago.
(1:24:48) We discuss we analyzed in this case uh 11 it was um 11 years of uh 11 11 years of uh student student relations and we would in this case have u different covered variables which is instructor gender, instructor age, squared instructor gender of enrollment and so on and so forth like academic division for example and we would run this model that is described here how we did it basically what we just discussed then we ultimately would obtain results which look this way that is proportion so engaging lectures humorous lectures is what occurs most in all the evulations
(1:25:36) approachability for example occurs a little bit less often and this is 11 topics we chose exactly based on matrix ivity and coherence. We chose that 11 topics was most optimal. And then we in this case would let me jump to for example this result. You would see that uh topics such as explains comp complex concept effectively more often occurs.
(1:26:00) This is like preparence probability more often occurs in science. complex concepts would be often uh uh effectively explained in science. Basically again just want you to understand that this is just you know label which we assign to particular topic topics relating topics. You would have to look through most associated documents and extract information from those documents to understand what topic is about.
(1:26:31) So this topic is about uh explains comp complex concept effectively and then we uh see that in in in computer science it is also the case if you talk about facilitates facilitates effective discussions it is actually less least often occurs in science on the contrary for example right and uh if you talk about like feedback it is freshman seminar for example example. Um uh so this is how it looks.
(1:27:04) You can also jump to this chart which shows you how it varies by gender again because metadata contains contain gender gender of instructor. In this case it is interesting to see this is like lines which represent confidence intervals. You can see that for example in case of females students more often mention that female instructor is caring enthusiastic instructor right they less often talk about males in the same fashion for example when I mention something when I discuss uh males they say that lectures are interesting relevant this stuff so basically we try to analyze if students are sort of have
(1:27:42) biases right based on gender of instructor this There's no important work which is used to no it is it is used to let me say this way it is important to understand because when we interpret results we want to know how actually if there are there are biases which students have if you talk about promotions of faculty if you talk about getting tenurs then we want to know if we should actually sort of trust the sulation so we should kind of adjust a little bit because we understand that students may have some biases so this is
(1:28:15) kind of idea behind this type of application of such models. We can see that uh students basically use different words when they discuss male instructors and female instructors. So this is kind of application. Now let me ask if you have any questions. Um it is also interesting to see this results here. They have some interesting examples.
(1:28:48) So you you got idea right? So any questions about this? So one question on the logistic normal um so do we know the shape of the co-variance is that like how do we decide the number of topics or like where is the number of topics being? Number of topics is hyperparameter.
(1:29:12) Basically number of topics is the only hyperparameter except for maybe number of like runs, number of times you run this algorithm when you decide which one is best and so on and also which metric you're ultimately going to use to kind of make selections. But essentially number of topics is hyperparameter in this case number of those.
(1:29:32) So this the theta is basically this theta is basically vector of probabilities right. So as example this theta will be as an example will be probabilities like 2 then one and so on so on and last one is 03 for example will be vector of probabilities they all should add at to one this is because we use soft max. Now what is length of this vector? This is hyperparameter. We have to decide up front.
(1:30:04) For example, in our case, I can tell you that we used uh this metric. Let me show you chart. This is chart which shows somewhat similar to what we just discussed like 20 minutes ago. We choose seven topics and we run it multiple times. You see different colors basically show you different runs different runs and those pluses crosses they show particular topic.
(1:30:36) Then we take average of semantic coherence and exclusivity and plot center in every case we have like six different runs and then we look here and we maximize basically almost like this area it is maximal in case of 11 topics and we decided that 11 topics seems to be optimal because of this area is maximal that's effectively what we did I I get that part so so that decides the shape of the coariance right in the logistics um on the Okay, because you're talking about probably of diagonal elements of sigma matrix those are parameters they will be estimated during maximization of
(1:31:14) likelihood. Yes. So this sigma so we are clear about this the sigma means it is like variance no it is like what let me say variance right first variance second and so on. Now let me say variance 11 if I have 11 topics if k is equal to 11 example it will be this way and there will be kind of coariances right so coariance coariance and so on that's how matrix looks so you asking a question about coariance coarance basically means correlation time sigma 1 sigma 2 so in this case those parameters are all parameters to to be estimated by maximation of likelihood we Don't assume
(1:32:02) anything here. We just maximize likelihood. We assume that we what we assume we assume that topics could be correlated. First topic has variance sigma 1 squar to be estimated right. No I mean it's not even topics which has variance. It is like normal distribution which has variance.
(1:32:24) When we apply a logistic regression when we apply a soft max variance always will be different but a variance of this intermediate variance of this variable which you pull from here is sigma 1 squ and then sigma 2 squared for two different components and coarance between those is presumably different from zero potentially different from zero. We have to estimate it.
(1:32:43) So we allow model to use whatever it wants and it will estimate it for us. And then it is interesting to see like how my topics are related. There is a way to actually plot it. This is by the way chart which depends on this liberals conservative how they discuss. Right now let me show you one chart.
(1:33:07) It shows you this correlations graphical display of correlations. We estimate all the topics and we can we can see what correlates with what. This way that is kind of visual representation. You can see that maybe topic 12 and topic 15 are highly correlated. Right? It means this of diagonal element which corresponds to 12 and 15 is high related. So we assume that there could be correlations but model will estimate by maximum of likelihood.
(1:33:34) So this is we just plotted this object. If you plot it, if the subject u kind of comes from structural topic models plot will know what to do with this. So we could deploy cloud it will show our words topic seven size clearly indicates probability right kind of it means we can see what which words are more prevalent which words are less prevalent for specific topic.
(1:34:02) If it says topic seven it will display it for us. So any questions about this? So now uh let me uh see what else we have. Uh now applications again we understand that we already saw applications. I you applications we understand that we can apply to problems where first of all we want to we want to assign efficiently our documents into topics or I will say assign topics to to topics to documents this dist probability distributions right of topics uh and if you believe that metadata is helpful then we can incorporate into the model in order to enhance this this precision enhance this assignment. Secondary we can already
(1:34:50) after we optimize it everything we already know parameters we can try to uh make u uh judgments about significance of some of the variables here for example about gender with respect to the output which is prevalence how gender impacts prevalence here you can see that it is quite difficult to say how my how gender impacts uh prevalence it is by coefficient we can say if coefficient is significant or not significant but it is still difficult because it is not linear relation right. So what happens is we basically after we know parameters we
(1:35:29) basically already try to regress coefficients theta on our variable for example gender and we can see what kind of effect it has to be specific we don't just regress our status because that is random variable right we take all parameters which we have we take the model basically we take the way this supposed to be generated And we repeatedly generate multiple of those. It is called kind of like bootstrapping.
(1:35:59) Basically in this case it is not bootstrapping. It is like post we use posterior distribution. Posterior distribution. It means we use parameters which we estimated and we sample from there and we take those sampled values already to regress it on on gender to be precise. What I have here I would say what I load there like in example in case of this results like u this one example this prevalence is not prevalence itself uh let me show you so this prevalence is not really prevalence because prevalence
(1:36:42) is considered to be random variable generated this way it means what we put here we put Expected prevalence, estimated prevalence. If you want to know how or particle variable actually going to change prevalence, we have to take posterior distribution. It means pl parameters which we estimated and we have to repeatedly generate those you know static coefficients repeatedly.
(1:37:08) There will be some randomness in them and then we regress those results those generated samples on our variable of interest to see if there is significant impact or not. That's how it is done. No, of course it is done via uh already uh via package. We don't have to program it. We simply we simply run. So this is by the way search key.
(1:37:36) we can obtain all kind of different uh multiple rounds of this STM and then we pull the results and then we can um estimate effect. This is what we what we do estimate effect. So it simple this simple line is doing the following. It tells me let me repeatedly basically sample theta from the distribution where parameters are whatever I estimated as my parameters and then let me regress afterwards regress my sample theta on coefficient of interest that's how it it does it and then we can see this results estimated coefficient and we can see
(1:38:19) this results here so now this is like exactly know basically how prevalence actual prevalence. So not expected one but actual prevalence is supposed to depend on my my my variables. For example, there is some kind of rating liberal and how it depends. We can see it is not significant in this case for example.
(1:38:47) So any questions about that? In our case we use this function and we would uh obtain these tables. Let me see. uh these tables and we will see marginal effect right now let me say gender effect for example you will see that difference between uh male female and male is going to be estimated as negative 01 and it is significant it means for this topic uh males get it more often and so on so that's how we can in practice use uh it comes from this package not to be precise we modified a little bit this package but it's already technicalities now uh any questions about this model
(1:39:36) okay let's see some examples so this is a introduction to STM package first of all we need to install it but by the way I want to mention that this is the only time basically you're going to use R. We're going to use R to only run STM because STM is not really available in Python. I saw some kind of, you know, attempts to reproduce it on some someone's GitHub, but I'm not sure how reliable it is. Basically, ours is very very reliable.
(1:40:10) Tons and tons of publications already uh obtained using this package, right? people use it like statisticians uh and also um uh social scientists, economists they use it. So it is it is very reliable package. Now it is a very reliable package. You install it this way in R and you can use it, right? So we can use use this package in R. After this assignment is finished, you don't have to use R any further.
(1:40:37) Now unless you want to take this pro this uh model as a sort of your final project, right? Then you can use R again. If you don't use it, you don't you can you may forget about that after this assignment. So now let me actually um show you some examples. I have slides in this case but I think I'll switch to maybe R and show you how it works.
(1:41:04) So it is the same code which I used to generate slides. So slides eight and I say let me generate my documents. No again I assume that uh uh STM is installed. I have to say install STM like literally install uh packages right. Then I say STM here. I hit enter. So it is finished. I have it. Then I say library.
(1:41:36) Then I say STM. So I load it. It will be available. So it is done. I installed it only once. Every time I start new session, I have to say library stem. So it it is it is loaded. I already have it I believe. Oh no. Okay. I I can see here load library.
(1:41:54) But I don't have to install it because you install it only once. Every time you start new session, you have to load it. So now let me create documents. In this case, in the case of R. No, this is basically equal sign. If you kind of don't like it, you can use equal sign. It's not important. Then C means no kind of concatenate, right? create like two documents.
(1:42:11) In my case, I assume that u actually not two even more than two because comma means every time new document I have a bunch of documents which was presumably created by outer number one and outer number one obviously likes to talk about cats right we have a number of documents outer number two likes to talk about dogs now this hashtag means it is just command as you can see different color it is command that is not executed Right? This line is just for me to remember what I what I did here. But it is not executed. So now if I highlight it and I can say run here or I can say
(1:42:51) as you can see it says controll enter. So if I control enter it will run it and then I have my documents. So my documents will will be exactly those documents. I can also click right here. Oh maybe it's not because it is kind of like a vector. No basically can see what happens. I have uh 16 documents. Now let me create a data frame.
(1:43:15) In this case I say first column will be author. It is metadata and column will be outer. I repeat outer first and outer second eight times. So I highlight I say control enter and I get my metadata which look now this way. If I click on it, you can see it here and it is my metadata.
(1:43:43) So basically my my my the only variable which I have in this case is outer name. No kind of outer first or second nothing else. Just to demonstrate how it works. Then I load my library again. I place my cursor anywhere on this line. I say control enter. I run it. I specify seat to to to have it reproducible. And this line is basically this line is basically what does text pre-processing right I say that my documents will be whatever my documents are my metadata will be whatever my metadata are and I use like a lower case it means I want to not differentiate between it's not case sensitive essentially I can also type
(1:44:23) here a question mark and say text uh process and see what it tells me. It tells me all these parameters. So basically remove stop words is true. It will remove it. Remove numbers true. It will remove numbers. You can change it if you want. But that's what it does by default. A lower case u I said lower case is true.
(1:44:51) Right? It is already true by default. Also uh sometimes even if after you remove everything if your document is empty basically document will have to be excluded from your purpose right no because it is empty so what to do with metadata that's why exactly why we supply metadata as well so it will basically know what to do with those missing documents it will remove them as well essentially so it will not only change your documents but also will change your metadata potentially because some of your documents could be empty. After removing stop words, after removing numbers, they could be empty. If you
(1:45:27) have a empty document, it will basically be removed. As a result, corresponding metadata entry will be removed as well. So, it is consistent. And you can see here what happens if I run my processing, it will do the steps. So remove punctuation, remove stop words, make it lower case, remove numbers, stemming, it does as well, right? So on the fly stemming is true, it does stemming as well.
(1:45:58) And we have this result processed. We click on it and we can see the subject has my documents first of all and it has metadata as well removed. You see document removed zero. So nothing was removed because nothing was really empty in my case. If some document was almost empty or it became empty after removal of numbers and stuff then actual metadata will be imported as well corresponding rows will be thrown away and it will tell me what was thrown away.
(1:46:28) So last line tells me what is not available. So let me say processed you can type like dollar sign and you can see metadata is basically my data frame. documents are my documents. Vocabulary is also there. You can see what vocabulary is. You can see what my metadata is. The metadata, by the way, is still data frame as it was before. I didn't change it, right? So, it is data frame, but I keep it together because if something was removed for consistency, it is quite important.
(1:47:05) You don't want to kind of manually go and change your metadata. That's why everything is stored in this basically object and uh that's why we supply metadata as well. And then I will prepare my documents. Let me run it once and I will get out. So you can see what it has my documents my data some words removed documents removed and so on. So what is out? Let me type out.
(1:47:39) So not out but it is uh prepared documents and you can see that uh it uh basically creates some kind of thresholds and it says if your document is very short for example it may kind of remove it right and so on. So this is kind of additional pre-processing and we get these documents. Now this object has documents, vocabulary and metadata.
(1:48:15) I can access it this way very similar to what we did before. So I can say there's my metadata. If I run this one, it is going to be vocabulary and so on. I will assign it to this variables for convenience. And now I'm ready to run my STM. So STM will accept my documents from here. will accept my vocabulary. I will specify how many topics I want to have. And this is important.
(1:48:36) This is actually the use of my metadata. This tilda sign means I'm going to use only single variable which is author name. So basically this means there is intercept plus outer name. This is typical for our tilda notation. Tilda means intercept plus another gamma time outer name.
(1:49:01) If I had something else, let's say if I had gender, I could say plus gender, for example, plus gender. If I have a date, I say plus date and so on. It is like equation. Basically, I type it this way. Equals this tilda and my equation. So now I can run it. There is also initialization. How I do initialization? Spectral means uh somewhat similar to um uh to this u um um nf right negative matrix vectorization there are alternative ways but stmm I think default is going to be spectral let me see here default initialization type so is spectral the first one is always default I can use LD as default so just first run LD basically and use this as
(1:49:53) an initialization of my all my parameters for some of the parameters. So I can use spectral which is basically somehat somewhat similar to NMF as we discussed maybe random and so on. So let me run it maximum number of iterations after 100 iterations it will stop. Let me run it and okay it stops quite quite fast.
(1:50:21) there is some kind of warning you have to pay attention to those because maybe camera didn't achieve right but let me pretend everything is fine and then I can say summary and I have two topics you see what happens first topic consist of dog energetic no it is stemmed remember it was stemmed that's why it is already just energetic bark enjoy and so on second topic is cat love climb pure spot and so which does make sense right now. We can say let me uh plot it.
(1:50:54) So I can simply take my object STM under underlined model my object of type STM and plot it and I will have this this chart. It simply shows me uh kind of cumulative proportion. It will show me how how often first topic occurs, how often second topic occurs. In my case since I have essentially somewhat balanced data set I have almost like similar proportions uh it could be different for example in this case we would obtain the distribution right that's exactly what we do know we did it differently so so it looks nice by default it looks this way you can run it and display this
(1:51:36) way the next one is exactly what I told you estimate effect itself one two means which topics I want to look at one two one column two means I take theta first for the first topic theta second for the second topic I specify on which I want to kind of regress it right I regress it only on on outer and nothing else I don't have anything else and I try to sample my theta from logistic normal distribution multiple times then I literally fit a line kind of plane in this case set will be regressed on outer name and I get my uh effects
(1:52:20) how how outer name impacts my impacts my uh status themselves I I cannot just take thetas because theta is for speaking what you see what you see up here is like expected theta but that have some variabilities it means I'll have to look at my posterior distribution and sample from here repeatedly multip multiple times and then regress my theta on whatever I want.
(1:52:47) Typically, we choose here whatever we want. We don't have to like plug everything here. Whatever you want. I can plug out for example. So, I run it. I estimate this effect and then I can I can click on it also. I can see parameters um formal which was used and so on. Let me better plot it. So my effects which is output from here.
(1:53:13) Then I say cover variable which I uses is author variable topics which I want to display first and second model is STM model method is difference. I want to look at the difference in this case. It means I want to see how to outers are different. Right? Then I say first value which I look at look look is outer second is going to be on the right.
(1:53:36) This convention coverance value first comes on the right. Second value is outer first and then I label somehow I label it this way again. Second will be on the right because it comes first and I specify main and so on and then I run it and I get my result. You see what happens. I'm sort of confident that topic one has prevalence around probably.
(1:54:04) 8 eight like 80% right 80%. Um and topic two is here. So basically uh uh I know that uh out of second outer like prevalence for for second outer mentioning topic one minus prevalence of for first outer mentioning topic one will be will be almost one even right it's almost like 100%. Now it should be probably the case.
(1:54:42) So I can see that topic one occurs more often in outer sec seconds second documents which is indeed the case because this was exactly generated by second author. That's how we can kind of interpret this results. Now imagine if you have like almost like millions of documents you can kind of run the same procedure and you can actually tell how differences are right.
(1:55:02) You can say who who talks about what for example how it changes over time. If your variable is not author but some kind of time for example the date or year you can also look at gender how it depends on gender gender of author or maybe different gender maybe about whom they speak and so on. So any questions about anything? Okay, you will have opportunity to exercise with this as well.
(1:55:37) Uh uh okay, let's now stop then. Yeah. Thank you. Thank you. Yeah.