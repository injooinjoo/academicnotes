(2) 89 day4 - YouTube
https://www.youtube.com/watch?v=N4-HP91yb9s

Transcript:
(00:01) Hello everyone. Hello Dr. So how how is everything? How was your assignment? Well, I was unable to resolve a technical problem. Although I was able to determine that it was not due to my uh local installation of uh of TensorFlow because I was able to get neural networks to run using other notebooks.
(00:33) So I was able to get the second third part to run but the first part continued to crash its kernel. So that's what happened there. And even for the second and third bit, I was unable to get my accuracy up to at least 75% using uh using uh using a recurrent network. So that's what happened on assignment two. Okay, I can tell you that the good news is that we will not have to do it kind of this way going forward.
(01:04) It was kind of exercise for you to see what happens if you try to use one encoding, but in reality you don't have to do it this way. you actually could could use embedding and going forward we'll we'll see how to do it. Basically last time we already considered example going forward we'll do it this way as well.
(01:20) uh and uh in that case if you do embedding you don't have to even transfer it to one hot encoding you can directly basically transfer your indexes to your representation which means you don't really have this this this issues later I'll consider some examples I try to give you some kind of you know goal so you don't just run like the very first model and uh you know and say it is perfect right I can submit it I want you to kind of have specific goal no of course first I check it so it is achievable then I give you specific goal And I want to see that you know spend
(01:52) some time learning learning right and uh maybe later I will give a little bit more margin so it is not like difficult to achieve but never this exercise was quite important you now get idea got idea how difficult it is if you use sparse representation know sparse means like long this long vectors which consist of many many zeros which is not really useful Right? It means we can find some kind of alternatives which means of course this spot representation is not the best way you will you will use different techniques going forward. You will see why it is much better. So now first of all
(02:34) if I could I wanted to make a comment. I was the person who mentioned on uh on pizzazz about the ludicrous loss that I got on my 10th epoch and you Yeah. What happened? I couldn't really understand why is it why is it Why is there this spike in loss? Yeah, so I did some research into it and so the last step uh of my model was a sigmoid function and then I did cross entropy of course for the loss function.
(03:03) So the problem is when the sigmoid function gets very close to zero or one for false positive or false negative the model is extremely sure of the answer but it's wrong. And then when you take the log function of that then the uh the loss blows up and that's what happens in so if if your if your if your output is like one basically right it means your in input to your neuron is very very I mean like your multip input like this linear combination is very very large like plus infinity and you get to one so your model is very very sure that your answer is one and then what you saying
(03:41) happens next it's When you get to a log of zero is a log of zero. Oh, because of you end up with a log of 0.00001 for your loss function and that blows up. Yeah. But uh that's why we sort of do it this way because we want to avoid such cases. We want to move away from those cases.
(04:06) The question is why you were okay and then there is a spikes all of a sudden. This this question which I still quite don't understand. Are you saying that like one of the points was uh so it was the model was very was very confident about a false positive or false like only in one case basically right right and this one case would spoil everything.
(04:31) So basically you're saying that for a very particular case your input to this last neuron would go to positive infinity and output will go to one and single case would basically input entire loss function right? entire cost function. I thought that uh you actually understood that and were trying to lead me towards an answer because in the in the problem you only ever asked for accuracy.
(04:59) You never asked for loss because if if the loss problem is on one single data point and it blows up the loss, then the loss doesn't matter. So initially I thought that you are you knew that and you were pushing us that direction. Then I realized no this is an anomaly. What Yeah. What I found was interesting, a couple side notes to it.
(05:18) The model that I published that had the uh the loss blow up on epoch 10 was the very first one I got to uh return an accuracy over 75%. So, it still actually returned good data. Also, after I started to rerun the model with various versions, it continued to blow up the loss function. It happened over and over and over again. And what I eventually got to fix the problem was a dropout layer and that fixed it. Okay. So yeah. Yeah.
(05:44) Yeah. Dropout layer is a good idea. Definitely. Yeah. Anyway, it was a it was a fascinating thing. I did a lot of research into it. I thought it was worth discussing. Yeah, it is interesting. Definitely. So yeah, basically you said that one single point, one single observation would produce almost one.
(06:04) And when you plug one to to this um logarithm logarithm of one minus one essentially you get like infinity right okay it kind it kind of makes sense yeah yeah so I have to maybe read about this I I I didn't experience this actually yeah okay got it so now any other comments now we have uh next quiz quiz number Okay, first question we're talking about tokenization.
(06:34) So what is tokenization? Now clearly we understand that tokenization means we split text into specific subunits, right? Specific units of information. It could be words, it could be even subwords or even characters. Now I can tell you that sometimes you want to break your text into words.
(06:54) If you do some some kind of classification of text, maybe you have a specific data set. you want to run your model on your spec specific data set appropriate for your problem. You can break into into words. So token will be worth sometimes especially if you kind of concerned about knowledge transfer from one kind of you know basically uh data set to different data set if you want to transfer your model maybe it's better to use like characters.
(07:16) So everything is actually used words are used characters are used as units of information. So basically B is correct answer converting to lower case is not reducing words to the root root form is not and so on everything is not the B is correct answer only B is correct answer in this case because we try to break sentence into subunits into some kind of units. Next question is about stemming.
(07:42) So in this case we reduce um reduce uh word to the root form by kind of removing prefixes and suffixes. That's all we do basically just kind of cut it kind of cannot kind of trim it right. If you read other options lower case splitting of text and so on nothing is really appropriate only the option is appropriate in this case we basically remove prefixes and suffixes.
(08:12) Now next option next question is about limit limitization. So what is limitization? In this case C is correct answer which tells me reduce the base base or root form which is also kind of valid valid word. Now basically you can find it in dictionary.
(08:33) It is not maybe as much important if you just want to do some kind of classification problem right or maybe sentiment analysis. Maybe you're not as much concerned about these tokens being actual words. Nevertheless, this technique is used a little bit more expensive than steming clearly, right? Sometimes if you want to do this limitization, first the model would have to understand what kind of part of speech it is, right? Is it verb? Is it like noun and so on before it can do it? It means of course it is more expensive. Clearly for stemming, it is like really really very
(09:04) cheap procedure. You just truncate it. You kind of trim you cut out your suffix and prefix and that's it. In case of limitization is a little bit more more computation expensive but if you're kind of concerned about this or especially if you believe it may improve your result and you see it then of course you want to imply it maybe right no it's not really guaranteed that on tasks such as sentiment analysis or classification it will be very useful.
(09:36) So now next one next one is question about disadvantage of stemming comparable to limitization basically doesn't create real words B is correct answer if you are concerned if you want to really get real words then stemming doesn't do it no you remember like octopus s will be kind of removed octop and that's it octopus is not really so octopus that's why uh maybe it is not uh Best approach if you are concerned about keeping actual words as your tokens.
(10:11) Again if you just want to produce some kind of output which is like classification problem is like 01 for example maybe a bunch of different indexes which correspond to classification then in that case maybe you don't not really much concerned if it is in the dictionary or not.
(10:30) But sometimes you may be concerned especially if you are going to maybe do some kind of generation of text maybe translation or maybe you want to create some kind of some kind of captioning. In that case of course you want to use tokens which correspond to real words. Maybe again in those cases maybe maybe character tokenization is a little bit more appropriate right just like basically character tokenization character tokenization is clearly much more expensive.
(10:58) It is clearly really um uh sort of u you have to have a lots of data to train it, right? But if you have it, you may have you may choose to use character tokenization because it will perform probably slightly better in a sense that you can also transfer knowledge and so on. That's what you would probably use if you have like a lots of data. But word embedding for real applications is probably the best choice.
(11:21) Actually, if you try to build your own model, your model doesn't have to be even neural network. It could be something like random forest. You can use your tokenized text which you later will transfer to numbers. It will be input to your neural to your model whatever it is like random forest for example and you can do your task.
(11:40) So it's very possible that you can use something else. In this case, it tells me, let me see how NLTK is doing. If I take this sentence in this case, by the way, if you really want, you can use uh artificial intelligence to check it, right? You can use CH GP clearly, right? If you want, obso not what I meant.
(12:05) You can use Python, run it there, and even ch will tell you the correct answer. I'm not sure to be honest, but it's better to use Python in this case because it says NLTK. You just run it on in Python and see what it does. So, it will be the the easiest way to solve this problem probably.
(12:26) And uh maybe you remember last time we discussed there were some examples or maybe you remember from the section file as well. You can just run it and see what happens. In this case, I can tell you that C is going to be correct answer. You can easily see why it is so because first of all if you are talking about a word tokenization then first one is just entire sentence is my token it is not correct I mean it is possible to do it this way but it is not correct it's not what it did so now uh next one it has in 1903 it is not correct it is not a single word it is not what it does next one now it has like J now it's interesting J is not
(13:03) really correct because NLTK is a little bit smarter. It will not just look at your you know spaces, look at your punctuations and you know extract words. No, it is a little bit smart. It will understand that J period this J dot this J is initial. It will understand it. So it will not say J is my talking.
(13:28) It actually is going to say J dot J period is my talking. So C is correct answer not D. The last one is also not correct because in 1903 is not not not single word not correct. This not not correct either clearly. So C is only option C and D are quite similar to each other. But if you care carefully look this J is initial. It will be like a token itself.
(13:54) So NLTK is smart enough to understand that this is initial. It will not throw away that. Any questions about this? Okay, this is quiz questions. So now today we are going to sort of continue talking about ways to represent text by numbers and let's see what we going to discuss. Now first of all bag of words we already saw some examples. Today we're going to talk about this a little bit more. Maybe also see some examples ways how to do it using different tools.
(14:30) So now let me u next we going to introduce engrams. So my token doesn't have to be single word. It can be for example two consecutive words. No it is called two gram or even three consequive words. If I ask you why do you think engrams could be more more useful than just single individual words? What do you think? Why engrams could be useful more useful than individual words? Let's say I'm talking about sentiment analysis.
(15:05) Any ideas? Does it help with context? Yeah, basically it's correct. You can say this way. It's not really going to capture like a lots of context, but you could say it this way because it takes like basically neighboring words. I think about very simple example, right? Um uh this movie was not funny, right? not interesting or just interesting.
(15:29) It is different. So if you use two grams, you will have this type of two grams such as not something right not funny. For example, if you do sentiment analysis, it could be a little bit more useful. It means in practice 2 g could be more useful.
(15:47) Even three gs could be more useful because it could be not too you know not not much interesting. For example, maybe three gs is important. That's why of course on grounds could be in practice more more useful than just single words. That's why we're going to talk about this and then later at the end we're going to introduce convolutional neural networks.
(16:07) So basically networks could be used in multiple ways in relation to natural language processing. First of all optical character recognition. It is nothing but convolutional network. You're going to basically try to supply image and you're going to extract what kind of character it is basically right no for for particular sub image you have to say what kind of character it is and second application if you think about also like applications such as such as like context in that case you can actually think about example where you sort of represent your sentence as a
(16:45) sequence of some kind kind of a vector, some kind of vectors. If you think about the sequence of vectors could be seen as a sort of array and conal networks, they exactly take arrays as input and they sort of locally locally take the signal like not only single signal at a time, but they locally take like multiple neighboring signals at a time.
(17:12) It means you again can capture kind of context from your sentence. We'll talk about this a little bit later. So now any questions so far? Okay, bag of words. Um uh in this case um we uh we remember example like when we say I'm going to take my document let's say my sentence I'm going to create vocabulary as a reference. Vocabulary comes from train data set. It's not like you take Oxford dictionary as we discussed last time.
(17:47) Typic typically vocabulary comes from training data set. You take entire training data set and you basically collect all unique words into your vocabulary. You store them somehow, right? No, typically we store it in the order they appear in your text and we take all unique words. We don't take the same word twice.
(18:08) Clearly, we don't need it. Once we see the word, we we take it in. If you see it again, we don't take it again clearly and we create some kind of for the reference some kind of vocabulary. Then your sentence will be basically of length of your vocabulary.
(18:27) I mean representation of your sentence of your document will be of the length of your vocabulary and you're going to start counting essentially counting number of times particular word or particular talk in general speaking occurs in your text in in your particular document and uh you basically have vector of length of your vocabulary on every location which corresponds to your vocabulary.
(18:51) you will place a number which is frequency of those words occuring in your text. So what is issue? The issue is that we going to lose time component clearly. Every time this word vector will be in the same order as your vocabulary, right? So it is a basic but yet powerful approach for text representation.
(19:14) If you have specific problem when you want to do some kind of let's say classification, right? It could be quite nice nice approach clearly. So it is not like something you want to never use. No, if you have specific task and you don't have much text, it could be quite efficient. Uh each document is considered as collection of words disregarding grammar and order clearly, right? It's just going to be vector.
(19:40) So applications of bag of words in this case I would say that main applications which I can think of is some kind of classification problem, right? some kind of sentiment analys an analysis because in that case we maybe not as much concerned about the order of words. If you do translation order matters if you do or if you generate text where you want to create captionings order matters and so on.
(20:02) If you do some kind of mapping from the text to a number such as 01 for example, right? Or you want to classify into many different classes you want to say what kind of text you have in that case it could be quite powerful. So I would not disregard it. So it is nice approach.
(20:22) If you have particular problem and if you have particular text then you can create your own dictionary or your own vocabulary basically then it is probably not not so bad approach. Um as I was thinking about bag of words and working on our project um yeah yes I'm sorry about the project you said. Yeah. Okay. Or you know our homework.
(20:44) One thing that I got frustrated with with bag of words I didn't really understand is if it repeats a word, it doesn't give it additional credit. So, if we're doing a sentiment analysis and let's take the word great, that' be a positive sentiment. If they use the word great 10 times in the review, that seems like really really positive, but bag of words doesn't recognize that. So, not not quite. Yes.
(21:07) It's a good point. So, there are two ways to do it. First of all, back of words just as you said, you basically can say is it there or it is not there. It means it will be like almost like one hot encoding. You say zero or one or you can use frequency. Why cannot you use frequency? You can you can count how many times it occurs.
(21:26) It means in the location of your representation which corresponds to great you will place 10. Not not one but 10. That makes perfect sense. Yeah. So you can do it. Yeah. Okay, so you understand how it works. You understand applications again applications you you could you could not hope to use it for translation clearly but for some kind of sentiment analysis why not easily especially if your token is not just word token could be it's called bag of words but in reality it doesn't have to be just words by word it could mean even some kind of program for example right so bag of words it is like
(22:06) official name in reality whatever you're token is even this program you can use bag of s photograms as well. Um useful for classifying documents, right? You can try to predict what kind of type of document you have. Uh so now let me basically give some examples uh steps in creating bag of words look this way. First of all we have to apply tokenization.
(22:36) In this case we typically say bag of words that means we talk about word tokenization unit of information will be single single word. So in my example Henry Ford and reduce model t the model t was revolutionary in this case we create first of all uh tokenization.
(23:00) So it means basically we split our sentence into this units Henry Ford and so on in the same order is just basically splitting this text into tokens as we discussed. Now next step will be creation of vocabulary. Vocabulary means we take all unique words and into the dictionary. Henry is first notice first because it first occurs in our in our corpus. It is like basically our corpus at this point.
(23:21) It is all we have essentially. Next forth, next introduced, next there and so on. But next time fourth occurs, we don't add it to the dictionary to the vocabulary. We don't need it clearly. Our dictionary or vocabulary doesn't have to have two times for two times. So we don't don't add it.
(23:41) The order in this case basically doesn't matter. If you think about applications when you supply this type of vector ultimately to neural network, neural network doesn't really care which one is which, right? There's no time component. you are talking about some kind of fully connected neural network maybe you can easily flip your neurons rotate them around that means the order doesn't matter that's why for simplicity all these algorithms will simply add them in the order they occur in your corpus that's why you see in the same order when you move to the next sentence if there is another Henry Henry
(24:13) will not be added second time clearly so that's how it works and finally we count frequency just as you said if fourth occurs two times. Why not to keep it two times? We do it we do it this way. We say two times and uh everything else is is going to be two except for model. M model occurs two times and t occurs two times. So we can do it this way.
(24:37) If something doesn't occur if my vocabulary has something else there will be zero. I will have one two and maybe zero somewhere else. If specific token is not in my vocabulary, no maybe I can use also out of vocabul vocabulary you know word in into my vocabulary or ov in that case or will be simply zero because everything in my vocabulary basically it is done this way if you move to new text for example it may have something new how to kind of represent something new okay or v will be basically representation of your new completely new token which brain data set doesn't
(25:15) have sometimes you want to reduce dimensionality of your vocabulary. In that case, OV is especially useful because you may you may have something in your training data set but you don't keep it in your vocabulary. It means OV will be counting those things which you don't basically don't take into your vocabulary.
(25:34) So now um let's look at some examples. SK S skarn first secret learn in this case I say this is my sentence which is basically my entire corpus in my example I can use count vectorzer first I have to somehow um somehow I have to somehow uh train it right so I can say my vectorzer is count vectorzer and I apply it to my corpus I say my vectorzer is called vectorizer vectorzer.
(26:17) feed feed transform on my corpus which is my text is going to be my result and then I say let me print names basically names which which are in my vocabulary in that case I say vizer get feature underscore names out and I get my unique tokens which I took into my corpus also I can print out uh basically counts right so I can print out counts and it will look this way.
(26:44) Now I can tell that maybe this is not the best way to do because it is not flexible. If you want to change something, if you want to do some kind of kind of interesting things, it may be a little bit less less flexible in that cases. If you want to maybe drop something, if you want to maybe change some parameters typically count vectorzer is not the maybe maybe best approach or nevertheless for simple problems you can use it. If you kind of not much concern about experimentation, you can use it as well.
(27:11) So this way uh now next one um next one is uh using NLTK natural toolkit and I can say that my text looks this way and then I first have to tokenize it. Well previously I didn't kind of have to do it. that is already kind of hidden in this vectorzer right in this case I have to explicitly tokenize it first I will have to apply tokenization then I have my tokens my tokens basically will consist of will be my vocabulary let's say this way my tokens will be my vocabulary right so I can print it out and see what I have so
(27:55) t is here t dot is here no it is because it's it thinks I believe It thinks it is like initial model T dot is it thinks it is like initial that's why it recognize it as as ST t dot T and T dot separately Henry for and reduce everything is there dot pro comes from here obviously and so on then I say what about my bag of word representation I can just essentially count right count how many times I have particular word in my text. That's so why do I take word from my vocabulary and I count how many times I
(28:37) have it, right? Uh that's it. That's how we can implement it. Quite straightforward. Now, it gives you a little bit more flexibility actually. Uh you can think you can think what you want to do here. You can introduce some kind of transformations of your choice kind of custom transformations and so on.
(28:56) You have a little bit more flexibility here. Now let me um let me um move to uh spacy. So in this case uh I can use spacy to do basically same same task. Again it may look slightly differently because we use different maybe uh uh different uh tokenization right but oh maybe I'm sorry maybe to tokenization is same actually to tokens is token text doc uh so we have text this way we apply this object create this object uh of type of of this specific type right doc type and we uh loop our tokens from the subject and create this a list of tokens that's what
(29:59) we do right using yeah this there's no stemming or anything else done right no there is no stemming yeah you can kind on the top of this if you want in the middle that's why I said is a little bit more flexible than that victorizer because you can actually incorporate inside of this also stemming for example right or limitization right now tokens are just tokens but somewhere inside you have this tokens right you can take these tokens actually and try to apply steming first before you actually move forward you can trans translate your tokens into kind of stemmed or liatized
(30:41) tokens. Yeah. So there's there is no steming no stemming noization. That's correct. Uh my vocabulary and I create my uh bag of word vector right in this case I just count how many times specific word it means specific token basically occurs in my text and I produce my back of word representation. Now you may think what to do with this bag of word representation.
(31:11) Basically you can use it as is if you want or you can apply some kind of normalization. Maybe you want to maybe normalize it, right? Or maybe you can actually say that whatever is zero is going to stay zero. Whatever is positive is going to be just one. In that case you don't count. You simply say let me say is it there or it isn't there.
(31:32) It becomes a little bit maybe cheaper kind of problem in terms of computations. But clearly it will be maybe less accurate. Maybe maybe not. Maybe yes because if there's less if it is like less uh expensive representation that means you can make maybe your own network more kind of interesting you kind of have flexibility to add more layers and so on because you're not going to have so many parameters. It is like tradeoff always.
(32:00) So now let me say that um a sort of alternative to this um to this representation will be basically representation by vector like sequence of vectors as we discussed last time as you did your assignment. Now let me first open limitations. Let me say limitations in this case we ignore the word order. This is huge problem.
(32:24) Sometimes if order matters it matters right? So it is obviously obvious limitation. Secondary, it may create sparse representations. You can say why sparse? I didn't see any zeros. It is like one and two. In my case, my vocabulary is based on my particular text, my corpus and nothing else. It means I don't have basically anything missing. Everything in my vocabulary presents in my text.
(32:54) But what if my purpose is huge? vocabulary is huge and I look at particular document no it has like 10 words 10 will be basically represented the other like thousands will be not represented that means I will have thousands zeros my representation will have like 10 numbers everything else like maybe thousands of different kind of locations will be will contain will have zeros there that's why it becomes sparse so generally speaking if you have text if you build this type of vocab using your text using your corpus
(33:30) your vocabulary will have thousands of specific tokens. It means of course your representation ultimately for every document is going to be sparse. No way around because you have a lots of unique words in your vocabulary. But then you say I focus on particular document which has only 10 words.
(33:50) Not 10 is at most what you you can have and also some repetitions maybe. No, it means you have a lots of zeros in your representation. That means it is clearly disadvantage. Now basically we treat words as indep independent features of this text. It means of course we lose semantic meaning right we kind of don't really uh understand sort of context right we understand the meaning of this particular word. That's why it is maybe potentially a problem.
(34:22) Clearly sometimes it may be important to know about context. So it could be a problem. So this model just is not capable of you know learning any kind of semantics right any kind of context. Everything is completely independent. Now of course u uh it is kind of basic model right? So we shouldn't expect that it will know uh context. It's not capable to do that.
(34:49) And um of course if you have a lots of a lots of text it means your vocabulary will be huge then your bay potentially will have storage problems and also computational requirements will be quite quite high because your network as a result of this will have multi-dimensional inputs.
(35:08) It means number of parameters will be huge computations will be expensive and so on. So if there is huge if there is large vocabulary inevitably we are going to have essentially this issue just to keep in mind we're going to have this issue that first of all storage secondary computational requirements. So what kind of alternatives we have now first of all we can use embedding. Let me say couple of words about embedding.
(35:31) So embedding you already kind of you know saw example last time also some of you tried to implement embedding on your assignment even though it wasn't part of assignment I ask you to do uh one hot and coin representations using sequence of vectors but some of you somehow tried to incorporate embeddings which is kind of correct way to do it actually even if it wasn't problemful kind of learning experience embedding is definitely the way to go in that case. Now let me say a couple of words about embedding. Uh like last time I discussed
(36:08) before the last time I met we also discussed I believe something. Let me talk more. So remark on embedding. Let's assume I have specific uh text specific text and uh first way would be to represent my text using sequence of vectors. I don't want to use like bag of words. I'm talking about embeddings. No, it means my text will be sequence of vectors.
(36:47) X first then X second and so on. Last time X capital T there was a question what capital T is. Capital T is number of time steps. When we introduce recurrent new network, capital T was number of time steps. Basically in this case, capital T represent number of tokens in my text, right? Essentially. So now um sequence of vectors every vector is basically representation of my specific word. Let's say first word is like cat. So cat is my first first token.
(37:25) Cat is my first word in my sentence. Second one is let's say set and so on and so on. Last one is math for example. Right? This way it is my sequence of words. Now last time as you did on your assignment in problem three of assignment two we discussed we actually could easily refer to the vocabulary and say let's say cat is second word in my vocabulary no okay it will be zero then one then 0 0 0 this way because second word in my vocabulary is cat for example now let's say first one is maybe out of vocabulary o then cat is my second word
(38:13) for example in my that it means cat will be vector of this type sparse not nice but that's how it will be second word let's say set will be zero maybe zero then one zero and so on zero this way and finally m will look something like 0 0 0 1 and then zero then this way or some kind of location which corresponds to math in my vocabulary. This one corresponds to math in my vocabulary.
(38:53) So this is nice from the radical point of view nice representation because we don't lose anything that time time is preserved time is not lost right we can easily recover what what it was if we refer to vable we can recover what this representation means it means my text is now sequence of vectors everything is nice almost perfect but there is one problem as you already faced that it is basically computational problem storage problem and so And the last time maybe you remember I think you did like two two 200 words.
(39:27) It means it was like x first because time is first and then first component of this vector. Last one is x first because time is first and 200 let's say have 200 most frequent words for example will be 200 dimensional. 200 is actually not not enough for typical applications.
(39:52) 200 is a very very very small number because 200 is what it is much smaller than any kind of dictionary right and this what you did last time and it is still still huge problem but even if you have 200 computation it is still huge problem so what you can do so basically as we discussed at some point last time you essentially can say let me take this vector I'm talking about like cat for example and my vector will be vector in this space of 200 dimensions.
(40:22) 200 dimensional space. This way 200 dimensional space. Now it has like 0 1 and so on. It means there is a vector somewhere in this space which corresponds to x first. Oh, it corresponds to cat. K is my x first vector in this space because it has lots of zeros and only one.
(40:49) That means this will be basically point on axis obviously right so this is my my cat I can say it is like 0 1 0 and so on zero will be vector of this type now I don't really want to use this representation and supplies as input to that work maybe there is a way to do it more efficiently and there is a neat way to do it more efficiently so called embedding basically kind of embedding right maybe what I do right now is not officially called embraing but it is completely equivalent to what is called embedding and we're going to discuss it like next. So basically I'm going to say let me
(41:24) introduce threedimensional space and try to somehow map my representation to threedimensional space will be some kind of point here. This is 200 dimensional space and this is only 300 th three dimensional space but nevertheless there is enough space for everyone right? It means we can easily do it.
(41:48) So this vector now it is not not on the axis. It will be representation of the very same word cat. It already will be not sparse and it will be actually quite nice representation because it will have already dense representation. Let's say this way 7.1 point 2 for example will be my representation. So this is more efficient basically more economical representation is this kind kind of embedding formally speaking it means I'm talking about function.
(42:25) So what is this guy? This guy is some kind of function which essentially accepts 200 dimensional vectors function which accepts vectors from 200 dimensional space. And this function itself is a vector because what I produce is three numbers. Now it means it will be some kind of f first component like first function which accepts my vector which is this one basically second component which accepts my vector.
(42:57) Next component which accepts my vector this way. So this will be my basically mapping three kind of functions I need to have which will accept vector of this type kind of sparse sparse sparse representation right sparse vector now there is actually a little bit better way to do it we don't have to like literally first represent it using our onen coding we can use directly indexes so it means my sentence head will be represented by index x first index x second index and so on x capital t index in first case it is second word in my
(43:42) vocabulary now it means it will be like two now if I start from zero maybe one right let's start from zero so let's say it is it is one we start from zero it is one 0 1 2 it is two index is So this one is 0 1 2 3 4 index is four. So basically my sequence or my sentence is now sequence of indexes.
(44:12) It is much much more economical basically way to do it 1 2 and so on 5 7 4 and so on. So but the problem is we we don't really want to supply this guy or this vector or this sequence of scalers as is as input to network. No, we never basically do it. If you have some kind of categorical variable which represents let's say location for example we don't want to say first location second location fifth location does much make sense to represent data this way we typically try to use one hot encoding basically right in case of words we say let us actually do soal embedding which effectively means we understand that it is number from one
(44:50) dimensional space but this number has a meaning of index essentially no it is like one-dimensional basically a kind of object and it is my number my cat in this case it is my cat which is like one in my example so cat is my number so basically the question is can I sort of similarly try to map this one directly to three dimensional space and the answer is yes I can I don't need to go through this representation and to be even more precise whenever we apply for example neural network to this kind of input
(45:25) We essentially switch only only one one of the connections only other neurons are all because of zeros. Now I can do it this way. Alternatively I can say one is going to be mapped to my space my representation will look this way which is also cat.
(45:49) No, it may be a bunch of numbers maybe different numbers doesn't have to be like exactly same right so will be representation because it is a little bit different maybe yeah question you can ask question uh yes sir so uh I I'm uh my understanding is we prefer the one hot encoding because it doesn't assume similarity between words but if we use the indexes like 1 2 and 3 4 in this example word x1 if it is represented by index X1 and word X2 if it is represented by X2 we may give the impression that they are close in the ukidian sense and also the embedding will be also closed but they are not related.
(46:26) Yeah. So that's correct. But there's always this problem. You are correct. But we are not going to supply this input like 1 2 and 4 and so on as is. We are going to first do embedding and it is actually going to be basically ultimately completely equivalent to the first representation. I will tell you why a little bit later. Maybe not now but later we'll talk about embedding again.
(46:51) I will tell you why it is completely cool. But basically you kind of your thinking is correct. It seems like we supply data where we say set is twice more twice stronger signal than than cat. But in reality there is all this issue simply because what we what we going to do here first we going to say I want to create function which is vector valid function which accepts my x which comes from my uh 200 uh dimensional space as before and I'm going to say basically vector weight function means I have three numbers f first implied to x f second
(47:31) applied to x f3 applied to x this way. So effectively specifically because my input in this case is of this form like 0 0 0 one only in one on on one one of the locations I have one it's going to be basically ultimately equivalent. Okay.
(48:00) So my understanding I'm a bit lost honestly but my understanding is that we will be applying three functions uh to to obtain the three coordinates of the new space that we're going to live in. Yeah exactly in this case also three functions but applied to vectors. In second case three functions but applied to scalar which is index. We are going to do it in a smart way essentially where we are going to say two means pick corresponding row from specific matrix.
(48:33) What it does 0 1 2 only one is going to be multiplied by something. It means effectively when we supply x second vector here essentially we say only second kind of entry from specific specific mapping will be used. nothing else is going to be used. That's why you kind of safely can say why not to simply supply two other than just the whole vector and this function will basically know that you are kind of talking about vector where we have 001. So there is a way to to do it completely equivalently.
(49:02) So it is it is really it's it's all computational tricks really computational tricks. Exactly. Yeah. Basically yes. Exactly. It's a way a good way to put it. It's not like mathematically not equivalent. Basically, it is equivalent. But because our vectors of this form, they have one only in one of the locations.
(49:27) And because our mapping will be basically uh basically linear transformations all we do we need to select specific specific signal from from this transformation like row from this transformation which we can do without even explicitly representing one encodings. We can use index and select from this matrix. That's why embedding is much kind of better than trying to implement your own transformation using some kind of network, right? Or like layer dense layer embedding does an efficient way essentially. Yeah. Question. Yeah. just just wondering to understand when you're talking about the bag of
(50:06) words you mentioned that you tokenize it and then you count it and get the frequency of the counts and then this is the input to the um to the neural network but um isn't it already implying that we're doing some kind of embedding without having to specifically call the embedding? No, in that in that case in that case there's much worse than embedding because in that case we first of all we like the difference right what is the difference we lose time component in case of bag of words we completely lost
(50:39) our time component. So what it means? It means if my vocabulary let's say example if my vocabulary tells me set and tells me a math and nothing else right then my example according to bag of words will look this way. It is a vector of length two. First one will be let me say it even differently.
(51:14) So let me say Matt and then second one is set it happens to be my vocabulary. Then I will say bag of word will correspond exactly to my vocabulary. Matt is it there? You know what even even different to make it more explicit. Let me say set and also I have uh dog for example right? Then bag of words in my example is going to be first math is represented.
(51:47) I place one set is represented. I place one dog is not represented in my particular text. Maybe it is the test data set dog came from training data set. Then I place zero 1 1 0 in this case doesn't reflect time time component anymore. There is no time at all. time is lost. It means u it cannot really be anyhow equivalent equivalent to embedding right because embedding is going to still keep sequence of vectors after embedding this will be first vector because I sketched only cat similarly I can sketch set it will be second vector and will be kind of you know sequence of threedimensional vectors in this space time is preserved
(52:35) Yeah. So what will be the input to the um to the uh neural network eventually for both of them? So in case of essentially in case of my embeddings input to the network will be if you do it efficiently and you use embeddings will be this sequence of sequence of my indexes because I know that on the fly it will sort of no kind of represent all this vectors and do embedding essentially without explicit explicitly representing why vector that will do embedding right away. That's why there is no frequency there right
(53:21) there is no frequency but there it is better than frequency because if my word occurs like multiple times if cat occurs multiple times I will see 7.1.2 to multiple times. So this information is not lost. More than that it will know when it occured, when cut occurred, after what before what? Okay.
(53:51) So maybe last time we had some let me see example there was a example where we used the recurrent new network. Yes. So recurren network to build some kind of classifier and we said let me represent my text this way. So basically this one is x1. What I highlight on the screen is exactly what we call x1 right 0 0 and so on because we don't have those words from vocabulary in our text 0 and so on. And this is my x first.
(54:23) No in this case actually this counts right. It is counts already. I'm I'm sorry. No, I have to take it back. It is It is like index. It is out of a out of the nine means corresponding index. 958 means corresponding index and so on. So basically the whole thing this one is my x first uh sequence of my x's. I didn't say it correctly. Sequence of my x's.
(54:55) So basically the first highlighted row is sequence of my X's which correspond to first document right I could have said 9 is this type of vector 958 is a type of vector it is not efficient that's why what I say simply I say let me build new will not work which will say I'm going to accept my I'm going to accept my sequence of indexes. In this case, you have to specify how many indexes you have.
(55:28) Now in my case plus one because of out of v vocabulary as well. So maximum number of features plus one basically represent how many different indexes I have and if I you say embedding then you specify how many unique indexes you have and then how many you want to get after then it will understand you supply index every time or maybe you supply like sequence of indexes every time right so basically in this case you specify not one D when you build embedding layer you specify 200 you still have to specify 200 but it will not go that route. I never represented my vectors by
(56:08) 100 codings. I supply it right away. Simply my indexes 9 958 8 and it will know what to do with this as long as I specify there are 200 unique indexes. I have to do it. I have to specify then it will do it this way without actually going through the 100 representations. I'll do it right away and map it to the space which I want.
(56:34) That is quite efficient and much much less expensive. Any questions about this. So now uh uh alternatives could be to use this type of embeddings right you can see examples what kind of embeddings we can use uh you can use question how can we get the uh 3D vector by the uh back propagation with integer representation input rather than the one hot input.
(57:28) How can we get three and so it is going to do it sort of um I mean we don't have three dimension vector as output it's going to be something which we pass further right our network we will have this in kind of kind of intermediate step which will take sequence of the indexes transform into three dimensional space into three dimensional vectors of this type and then we pass it further through not work.
(57:56) So in order to kind of optimize it, in order to get best representation basically you kind of solve your problem and because you minimize cost function on the fly it will know how to adjust parameters of embedding in order to make best possible representation. That's how but we don't really solve this problem independently.
(58:20) It is this problem with within the the network. Now let me go back to this um examples. Basically I say let me use embedding and then as always drop out LSTM drop out dance and output. So basically I'm not going to map or match my output from embedding to anything anything. I'm going to just pass it further. What is for? Yeah.
(58:47) So for this case uh the total parameters for the embedding there is only three. Right? If we want to output 3D vector in my case, in this case, uh, on the screen. Yes. Right. How many parameters? On the board. On the screen. On the on the board. On the board. In this case, it must be three parameters. Correct. Well, well, let me think. It must be No, it not 33.
(59:10) It must be 200 * 3. Still 200 * 3. Okay. But the input is actually the integer not the one hot right. If we it is not okay let me I know there are mathemat mathematically equivalent but I just want to know the practical implementation. Yeah. So implementation is going to be let me let me talk about this.
(59:40) So let me erase something. So basically you say how to implement it right? So let me do it implementation. Now let me say example I will maybe use different number of uh different number of parameters. Let me say my x is uh three dimensional right. It is like 1 0 0 0 for example three-dimensional then I say I want to map it as I discussed to specific kind of you know uh space let's say I want to use embedding X tilda which will be two dimensional X first X let me say U okay let me say U is my embedding which is U first and which is
(1:00:38) also U second this way and we'll output. So basically how to sort of represent this transformation. Now as we discussed u1 must be a function of those things. U1 is function of those things. I can say w11 * my 1 plus w u 21 * my 0 + w 31 * another zero. And that's how I get u1.
(1:01:14) What about U2? U2 will be W first 2 * my 1 plus W 2 * my 0 plus W3 2 * my 0. So basically my implementation because I I'm talking about linear transformation in this case embedding is specifically linear transformation no any bias no activation function essentially is going to be given by a matrix which has w11 w 21 w31 parameters and also w uh12 w22 w32 parameters.
(1:01:55) So all it does if you notice all it does it will extract the specific numbers because this is gone right this is gone because of zeros will extract W11 W12 it will simply extract my first row from this matrix which means I don't need one to do this computations all I need to say I need to say take the first one from the matrix that's what it tells take the first one okay let's take the first one it to know but in terms of number of parameters it must be in my case since I map three to two it must be 3 * * 2 so embedding in my example
(1:02:37) is going to be three which I'm going to map to two and that's how we implemented very straightforward very simple very transparent you can use one encoding but it doesn't make much sense if you just need to extract first row from your matrix That's why we say index is all we need to know. Does make sense now? Yeah. Yeah. Very clear. Thank you. Yeah.
(1:03:04) Let's make a break. So now next uh is going to be approach where we say token is not going to be
(1:12:48) simply word it's going to be engram kind of used when you say that maybe like as you said context matters maybe not context but neighboring word basically matters context is a little bit more difficult All right. But neighboring word kind of matters. So in GS basically means we we are talking about n consecutive words taken from text.
(1:13:13) If if you say two grams for example you take two consecutive words. Uh we can take one g in this case unig simply means we're talking about word tokenization. that is same biograms two consecutive words three gs or even higher higher maybe if you take like let's say you have a number of words and you take one gs no you have like your vocabulary of length of your number of words if you decide to move to two gs that means it is not only unique tokens but unique cow pals number of cowles clearly will be more than number of unique words it means of course vocab is going to grow.
(1:13:53) In this case, if you say I want to use two gs, number of two gs potentially is much much better potentially. It is not like every word with every word will be coupled with with every word. It's not going to happen obviously but nevertheless number of possible combinations number of possible to grams will be more than number of unique words not to mention three gs as well clearly.
(1:14:20) So it captures adjacent word relationships. That's how you should think about this not like context. So maybe to some extent you can think about context context as well. So yeah context. So you can say context is if it is like yes like to I wouldn't call it context. Um so now um let me maybe give you some examples.
(1:14:50) Let me move to some examples. Let's see how we can create it. So basically we have to create using count vectorzers. We have to create tokens of size two of two words. In this case there is parameter and and gram underscore range. And you can see I'm going to use only two gs. No it it it tells you two comma 2.
(1:15:12) 2 comma 2 means from two to two right you can say from two to five it will have 2 g three g four g 5 g you can specify boundaries in this case to two means only two gs otherwise basically everything else is going to be exactly same. I train my vectorzer on my corpus then I display my vocabulary. Now it has like all fourth introduced fourth model and so on.
(1:15:39) Then I will say my bag of fourth representation tells me fourth introduced is only one time. Fourth model is only one time and so on. So I can introduce in a very same way as before. My bag of words that is called bag of words even though I I refer to engrams. So any questions about that? I think it is quite obvious.
(1:16:06) Now again if you do some kind of classification or some kind of sematic analysis if you decide that maybe negation matters like not funny right and so on in that case maybe two gs is important even three gs could be important the more more n is you're going to take the more vocabulary is going to be which means in practice it may become already difficult to train the model right so now in this case we use nltk and I simply say that I'm going to uh to recognize my text and I create engrams.
(1:16:40) I I apply this function engrams and I create engrams of size two and then I do exactly same as before. Now they will be called bagrams for every bram. It is not word anymore. My vocabulary will consist of biograms. I count them and this is how it looks. every basically element of my vocabulary is going to be pupil was comma revolutionary ta forth.
(1:17:05) So every every specific entry to my vocabulary is not worth anymore is going to be exactly to g which is like pupil and I count how many times each pupil occurs in my text. Each to gram occurs in my text essentially it is representation. So now next um I'm going to use space as well.
(1:17:30) In this case I also specify that I want to have my biograms of size two and I display my results. So it looks quite similar to previous previous example. So I can do this way as well. Any questions about my uh uh construction of engrams? Which one is your preferred way of doing it? You know what? Actually, if you do specific problem like uh sentiment analysis and you decide to use n gs somewhat like two or three gs will be probably most useful.
(1:18:05) Maybe you can take a range from two to three. You can use at the same time 2 g and 3 g. If you do sentiment analysis, most likely your unigs will be not as useful. You can even not take them. So basically when you do some kind of standard analysis of movie kind of you look at movie reviews maybe classification of movie reviews if you take just simply awards it could be not effective algorithm not not effective in terms of like accuracy because you're going to lose information but is like funny not funny and so on.
(1:18:40) If you use two grams and also three grams which you can use right you can use like range of those you can use at the same time two grams and three grams probably it will perform be best. So if you do this type of exercise try two and three g together most likely will be the best performance.
(1:19:00) If you take more then there is a problem because your vocabulary becomes huge. You cannot really deal with this. That's why going further than three is not useful. One g maybe not useful. It means two and three. Maybe two and three together is going to be best approach. Professor when you two to three it goes through the words twice. Once to get the two words and once to get the three words.
(1:19:28) It is going to take all unique two gs and also all unique three gs and they could overlap. Even even if you if you you take like two GS they actually could overlap because we don't really know when it should kind of logically start that's why we just take overlapping ones right this movie is not fun this movie movie is right is not not fun all of them will be taken basically if you also include three gs all possible three gs even if they overlap with those will be included I'm sorry Was it was this your question?
(1:20:07) Like will it take both the two two words at a time and then three words at a time and put them together in one um dictionary and then use them. Yeah. Basically your vocabulary will be not of this form but it will have 2 g comma next one 3 g comma next one 3 g comma next one 2 g. It is like unit of information basically.
(1:20:33) All right. Okay. And we will count each of those for every two or three gram you will have fixed location your vocabulary. It means you will have fixed location and your ultimate vector representation in your bag of words. Now it is clear that if you use engrams number of things in your vocabulary grows right data becomes even more sparse in that case.
(1:21:09) obviously right also we can't really capture maybe like long range dependencies because we only talking about like local kind of context I'm not sure if you even want to call it context no kind of local you know kind of neighboring words basically not local let's say local context right neighboring words if n is like three four maybe it does already sound like context essentially that's why if you use engrams you still can't really go beyond your limits.
(1:21:36) If your n is like three at most, it means you take only three words at a time. At most three secretive words at a time, it means it is still local context. You can't capture long range dependencies. If you use some kind of representation of my indexes, then you use embedding and then use some kind of even recurrent network.
(1:21:57) In that case we of course know kind of you know assume that we capture even long dependencies because of longert like long memory for example that's why it could be potentially a little bit better especially given that we don't lose time component in that case but in case of engrams we still lose basically opportunity to capture long range dependencies in in a sense of context not to mention of course high storage and high Maybe there's no resources will be required.
(1:22:27) Alternatives will be recurrent networks right this way. No of course not not one hot encoding maybe some kind of representation by indexes and then embedding and also transformers. So now let me ask if you have any questions. So we have somewhat like 30 plus minutes. Let's introduce a special type of neural networks. Convolution networks. They are used while to actually deal with images.
(1:23:02) In reality it is not only image. It is any kind of array which we can accept and basically can basically um uh transform into the output. Typically it is image but not only image. You can use it also for text optical character.
(1:23:35) Optical recognition is is a kind of image processing and nevertheless which means in that case it is like naturally comes from image processing even though output is like a character but we're talking about still image processing which means in that case it is probably problem of image processing but if you talk about like application to natural language processing you can think about this type of example let me say note on the use of so-called convolutional neural networks uh in case of natural language processing in natural language processing.
(1:24:15) So let's recall that our text could be represented via some kind of sequence of vectors x first x second and so on x third this way. Now I can use like one hot encoding. In reality it could be already kind of embedding if you want. It doesn't have to be one hot encoding actually. Now let's say I'm talking about some kind of one hot encoding, right? It could be amazing.
(1:24:45) So what it means? It means we are talking about a sequence of vectors which equivalently could be seen. So it is basically equivalent to 0 1 0 0 0 1 and so on 0 and so on 0 0 0 1 and so on 0. So it equivalently can be seen as a sort of array right. So basically if you will you can think that we are talking about like almost like image essentially it doesn't have to be like image I mean because network basically except except array is input but it becomes almost like image and coral network essentially look by design at sort of neighboring pixels at neighboring signals. So it takes
(1:25:41) neighboring signals at a time and transfer it further. So basically convolutional network will at a time look at some kind of sub region in my image every time. That is how it is designed. We'll discuss today. That's why it is also way to capture context essentially. So we can think this way about application of conal networks. It doesn't have to be one.
(1:26:06) It could be already embeddings which you use in order to pass it to con convolutional neural network this way and then it can produce some kind of output classification or whatever you want essentially. So now convolution itself is some kind of u operation which is like you see on the screen it comes from idea that essentially whenever we see some kind of signal whenever we try to let's say measure signal which is like intensity for example we cannot possibly measure intensity at particular point in time um a particular point in space because in order to measure some kind of intensity
(1:26:47) you have cumulatively kind of locally collect you know v some kind of slit cumulative cumulative energy. It means uh formally speaking every device in some sense is going to measure only cumulative signal which passes through particular slit.
(1:27:11) In this case let's say f is going to be representation of my slit and this triangle is representation of signal itself. How do I compute my signal? How do you compute my intensity? Some kind of some kind of uh computer signal which corresponds to overlapping region. My signal and my slit overlapping region basically corresponds to what my detector is going to get.
(1:27:32) If it is like well aligned I'm going to get more signal. If it is away I'm going to get less signal. As a result essentially what my devices will see they will not see this triangle. they will see this kind of modified triangular because my slit always my device always has particular size right it doesn't have uh like zero d size not possible that's why this convolution which basically comes from signal processing is going to be naturally kind of introduced in in image processing in case of commercial networks so essentially it's going to say let me take at specific sub region
(1:28:09) and somehow material look at this signal 0 0 1 0 0 1 q motively basically and pass it further. This idea behind convolutional network formally this type of you know this type of function is called convolution. Basically equivalently in case of discrete discrete functions it is also similarly written by summation but that's how we define convolution.
(1:28:37) Now in case of um in case of um let me say note in case of fully connected neural network let's recall what we did fully connected neural network we are going to have some kind of signal let's say x first x second x 3rd x 4th x fifth some kind of signal x first X 2 and so on. X fifths and then I say let me introduce neuron over here.
(1:29:15) Let me introduce more neurons more neurons more neurons more neurons. Then what happens is in fully connected case my first neuron will accept signal from everyone from everyone because it is fully connected this way. So questions I see question. Yes, I have a question about the the the relation from the integral to the to the function.
(1:29:44) So can I is it would it be correct to assume that multiplication uh in the integral is um is is equivalent or like or means intersection in terms of the function graph here. So in the top it is it is not like if you're talking about like is it area it is not area it is specifically multiplication and we sum sum those results of multiplication together basically sum of symbols sum of multiplication sum of multi we sum together results of multiplication it is specific to current location. So this is also function of x as you can
(1:30:21) see it depends on how our particular you know uh slid and signal are aligned relative to each other. Yeah. What is g? What is g of x though? It would that be f * h of x? G of x is result f * h. Yes. Yes. Thank you very much. Yeah. But don't worry about this integrals. I will give you how it is done in case of neural networks.
(1:30:46) It doesn't use integral. It uses basically summation as we got used to. Basically we're talking about specifically w * x plus w * x plus w * x. That's what convolution is essentially. Think about this w * x plus another w * x plus another w * x is what convolution is essentially right uh no if you you will see in case of discrete disc discrete world in this case this is how it came from signal processing I just gave you this formal but in reality in case of networks there is no even any integral so don't worry about that so now my
(1:31:26) first neuron is going to accept all poss possible signals. If I look at the second neuron, let's say the last neuron, let's say this neuron, it also going to accept all possible signals. Everyone is going to accept all possible signals. More than that, parameters of first neuron will be different from parameters of this neuron.
(1:31:54) It means that fully network requires a lot of parameters, right? Obviously. So now in conversion neural network the idea is going to be the following. Let's say I have some kind of signal the signal will be basically image some kind of signal X first X second and so on X fifths then I will have neurons first second next next. So what happens is first of all my neuron let's say this neuron this neuron is going to look only at some sub region of my signals.
(1:32:37) No if you think that there's like special dimension here it will be connected only to first second third and nothing else. It will not accept signal from everyone. It's like as if there was like some kind of slit and through this slit I can see only three of them not more. It is going to be like called filter.
(1:32:58) So I basically have a filter through filter I can accept only signal from three of those. Now let me look at this neuron for example from this neuron very similarly I can say I'm going to accept signal from only three of those this way. And that's how my conal network is going to be designed.
(1:33:18) I'm going to at a time only look at sub region of my image sub region of my kind of input which is generally speaking array it is like one dimensional array like one dimensional image you can think this way but there is always like special component it's not like who is first who is second is not important now it is already important because we say we are looking at sub region sub region sub region every time it means we explicitly assume that there is some kind of you know organized data which we supply for example example image though there are some kind of correlation between those
(1:33:48) things if it is in terms of language there could be also correlations between those neighboring things that's why it is important that we are thinking about some kind of signal which has correlation within those those representations now more than that second thing which I want to mention in this case those connections which correspond to each other actually this one and also this one.
(1:34:15) They basically share the same weight share the same weight corresponding location will share the same weight. It means first of all we going to reduce tremendously number of parameters. Secondary we assume that the way to look at these three signals should not be much different or should not be at all different from the way to look at those three signals.
(1:34:50) Basically if you look at some kind of image whether you look at upper upper left corner or whether you look at lower right corner you use same eyes right you use same transformations you don't have to use different device why should it be different that's idea if you concentrate your attention on those six pixels or maybe on different six pixels because you don't really know where where my signal is located where my image my object and my image is located you kind of don't really care what location is.
(1:35:21) Every time your transformation is going to be exactly the same. There is no really need to you to use unique W's like in the case of network. So first of all not everyone is connected. We look only at sub region just behind us basically and secondary those corresponding connections will always have same parameters W's.
(1:35:41) It means we reduce number of parameters because we explicitly assume that the way to look at different sub regions is exactly same regardless of location. So basically in case of recurrent neural network we have a weight sharing across time time space across time. In this case we have a weight sharing across spatial dimension very similar idea essentially recurrent network we use we assume that regardless of time we have same parameters in this case regardless of space we have same parameters any questions about that so now uh that's why it becomes effectively convolution Because we say
(1:36:28) we say oh let me say note in this example we say this u if I call it u2 u2 is going to be u2 is going to be some kind of let's say w + w1 * x first w2 * x 2 + w3 * x3r and this is 1 2 3 4. Let me say U4 is going to be also very same W0 plus W first but already times by X3R a different location plus WC which is exactly same * W4 plus W3 * W5 so that's what happens and this is exactly basically what that means convolution regardless of location regardless of location we are going to have same transformation information right just like in this case in this
(1:37:28) case so basically in this case you can you can think that one of them is signal and another one is is like my w's so my w's depends on s right uh not on x and this is my signal this is my f for example my detector my detector is characterized by f so my detector is those w's if I move from position to position to position my w don't change. Basically, that's how you should think about this.
(1:38:04) Um that's how convolution is designed in this case. Any questions about the do not work? Okay, we'll see some examples. So now let me first maybe introduce some example which will clarify how exactly we're doing this in case of in case of arrays not one input but in case of arrays. Let me open different slide. This one filters.
(1:38:45) This is like visualization of those filters. Those W's which I wrote they are called filters. Right? So convolution let's talk about convolution. Convolutional neural network basically convolution is specific mapping which is key kind of component of convolutional network. Let me assume that I have image. Not even image. Let me say array. It doesn't have to be image formally speaking.
(1:39:16) Now you can think about image if you want because it is like more natural. Some kind of array. And I have signal 1 2 3 4 5 6 7 8 9. This way. This is my input. It is only basically going to have one color. It may have multiple colors. So it will generalize how to use multiple colors as input. In this case I have only one color.
(1:39:48) Then I say let me introduce soal filter. Now basically filter filter is a kind of you know like a kind of kind of way to store my parameters. W's those are W's which I store here. I will say some kind of W1 W2 and so on will be stored here. It is called filter. Let me as example take one one to two and also my bias W which is let's take like 100 for example this way.
(1:40:21) And let's now see how I convolute my image with this filter. Again filter means my matrix of W is essential nothing else. So let me say I'm going to convolute it. So for that I would need to place my filter. Let's say first I'm going to place it right here. So I'm going to extract four signals from my image or from my array whatever it is.
(1:40:49) It means I'm going to get uh output which is as you can see going to basically two dimensional because how many locations do I have? 1 2 3 4 output is two dimensional. Now my signal which I have 10 if I place filter above this four pixels my signal is going going to be exactly this one.
(1:41:13) So this is my signal right here and the question is how to compute it right how to compute it how to compute this signal let's do it so now the question is how to obtain this specific signal which is like number which I'm going to store here and it will be representation of my output from here some kind of activation function obviously multiplied to 100 which is my W 0 + W first is 1 * 1.
(1:41:50) So 1 * 1 + 1 * 2 + 2 * 4 and plus another 2 * 5 already. So that's how I can obtain my signal here. That's exactly what we call basically convolution. This guy is convolution. It is called convolution called convolution because when we move from location to location we don't change w that's why it is called convolution.
(1:42:20) This is w0 this is like some kind of w1 w2 w3 w4 if I compute how much do I get? Uh 10 + 8 + 2 is 20. 21 it means 121. Now let me say that my activation function f of z is going to be real. So maximum of z and zero. It means rectified linear unit I'm going to use here. I can use whatever I want basically but rectified unit is is often used as well.
(1:42:57) So this is how I get 128 right here. Similarly can compute every number for every location. That's how we basically introduce convolution. Now how to choose those filters? At some point it was not so easy to train parameters of filter and people would basically uh craft them manually. They would like manually design those filters of different kind.
(1:43:26) What you see on the screen is basically what people introduced somewhere in around 1998 I believe as filters which could do this this job. They were not trained at that time. They were sort of prespecified. Nowadays of course people train it. So people were able to find ways to train it. It is not so easy actually if you have deep network it is not so easy but ultimately people found a way to train it. Now it is not prescified.
(1:43:51) You can train basically using your data and find W's which best serve your kind of you know function your mapping that's what happens now any questions about convolution this is kind of same idea which is shown here you can notice that first of all my image becomes smaller so in This case we say that padding padding is valid.
(1:44:24) If padding is valid it becomes smaller. All right. If I want to preserve dimensionality I have to say padding is same and also I have to make so-called strides to be one. Now let me see what it means. Padding could be same or could be valid. In my case padding is valid. It means if I hit the boundaries I don't move further.
(1:44:47) It means dimensionality will be reduced. If you want you can kind of add fake zeros around your image and you can go beyond the boundary in order to preserve dimension of your input. Input is 3x3. Output will be also 3x3 because I can sort of grab zeros around my image. If I do it that way I preserve my dimension is called same padding. Same padding however doesn't guarantee that you get same image.
(1:45:14) It still could be smaller because there is second parameter. First parameter is padding. Same means you add zero surround. Same doesn't mean you get same same image. It means you get add zero surround where it means you don't have zero surround. But there's also second parameter which is called strides.
(1:45:32) So basically when I shift my filter across my image kind of uh along this two dimensions I'm actually able to also skip some of the pixels. In that case, I can specify strides to be different from one. Let's say strides is equal to to two. No, it means I skip one kind of pixel at a time. I move by two pixels every time. Not by one but but by two pixels.
(1:45:59) If I say str is equal to two, that's it. I'm not going to have image of the same size. Right? To get image of the same size, I have to say padding is same and strides is equal to one. In that case yes I get output of the same size. The only case and the only option to preserve dimensions strides is one pading is same otherwise we lose pixels.
(1:46:25) Any questions? So first question which you may ask what to do with this? Now first of all you can use it as input to the next layer and continue this procedure easily right you can say this is like image basically I can also use another filter transfer and further further and further so this is first thing secondary what if I don't like only single kind of you know so-called feature map but these filters are not the only filters I could potentially use here okay it turns out that you can actually use different
(1:47:03) filter filter. You can use two filters if you want. You can say let me take something like 3 three 44 or some kind of a W here which is 10 for example and you can apply this filter and then result will be second feature map. This way we are going to obtain so-called convolutional layer layer which consist of two feature maps.
(1:47:33) If I have two filters already, right? So I can do it that way now. You can ask a question. What to do next? If I want to apply next conventional layer, it is already going to have two two channels, two feature maps or if you will two colors, right? Turn that commercial networks actually can accept images which are colored images. Not a problem.
(1:47:54) If it has two channels, three channels, 700 channels, you can easily actually take filter which has two sub filters. It's not a problem. You apply it. Basically, first sub filter will be sliding above first channel. Second sub filter will be sliding above second channel. And it will have common W. Let's say W is 1,00 1,000 plus convolution of first sub filter.
(1:48:24) This first channel plus of second sub filter this second channel and you get like result which in this case is going to be like if it is like 2x2 it will be single pixel actually single pixel and also colors will be collapsed together because I will add everything together w0 plus convolution with this channel plus this channel that gives me 1,000 plus something plus another something from second sub filter and I apply my function I get single pixel here so basically I can do it this way right now if it is bigger it will be not one pixel it will be more pixels that's how we can do in case of multiple
(1:49:04) channels or equivalent in case of multiple colors my filter will have to have sub filters number of sub filters needs to match number of my channels always any questions about that. So sometimes u we have to know let's say resize image. We know we sometimes have to resize image. Same idea here. Sometimes if your image is large or if your array which represents something is large you apply your uh convolutions and you see that your image is still quite large. You may want at some point to resize it because you want to basically reduce number of parameters. Uh at some
(1:49:50) point during kind of developing this type of networks people would resize by taking averages. If you want to resize what it means just take 1 2 4 5 take average will be single pixel. Makes perfect sense why we resize it this way. But later people realized that resizing this way basically introduces no kind of image becomes blur regular kind of fuzzy in some sense.
(1:50:19) It means they realize that it is much better to actually take not average of those signals but take brightest pixel will be my basically signal which corresponds to this sub region if I decide to resize my image and they introduce soal max pooling. Max pooling simply means at some point I decide to take sub regions and for every sub region I will pick maximal signal and it pass it further.
(1:50:45) Now you can see here for example 532 on the screen I take this sub region 2x2 and becomes five. I take maximal signal. It is done this way in order to not not to introduce this um puzz right not to make it blur essentially because average works a little bit worse not maybe much but a little bit worse networks if you look at the history if you look at how they were introduced like those ones which were the winners of competitions basically best networks available this type would use average later people realized average is not the best approach they took maxima okay we can use maximum Any questions about this? So
(1:51:24) convolutional layer we understand convolutions look this way. This is basically what we what we mean by convolution. This type of operation is called convolution. So now uh next one is strides. If you don't want to lose dimensions you say str is equal to one. You can actually in this case say str is equal to two. Then they will not overlap. You can just skip some pixels.
(1:51:49) also padding in the if you want to preserve dimensions you have to say padding is basically same and strat is equal to one only option to preserve dimensions so now um let me look at some example I believe next slide will show me some example yes so there's my example let me sketch it now so let me sketch this specific example so we fully understand what's going on Sure.
(1:52:32) Example page number 31 lecture 4. So let's see what we have. You see here input shape 32x 32x3. So it is input input shape. Now it means I have three channels. Maybe it is image. So looks like it is colored image maybe. Right. Okay. I can sketch my input this way. First channel, second channel, next channel.
(1:53:16) Now you can think that we are talking about red, green, blue maybe, right? Maybe it is red, green, blue or maybe not. Maybe it is something else. So most likely it is like image. It depends on how we apply it right. What is application of this network? If it input is regular colored image, it means red, green, blue channels. So it is three-dimensional. I mean three channels means this those three dimensional and we have 32 pixels across this dimension and also 32 pixels across second dimension this way this is my input so what's next you can see that I tell convolutional layer then I say 3x3 3x3 means I'm I'm
(1:53:59) talking about filter so my filter is of size 3x3. So it means I have filter of size 3x3 in this case 3x3. But since I have three channels, it means my filter needs to have three sub filters. So there is second one and also third one because first one goes for blue next one will be used for green next one will be used for red that's why three sub filters this way and I'm going to convol of course we understand that there is also there is also bias right involved so this is what I call filter filter basically filter is my parameters
(1:54:50) if you will ws which are going to be same for every every position that's why we call it filter rather than just connections now there is also there is also number 32 what to do about 32 so this is 32 now it means actually I'm going to want to have 32 channels in the output let me say first of all I'm going to get 3x3 let me see 3x3 means 3x3 means uh means um I'm going to uh move it until the end and I hit boundary.
(1:55:34) So two are lost right? 3x3 means two are going to be lost. So ultimately the image will be of size no kind of output will be of size 30 by 30 already becomes slightly smaller 30 by 30 no because I don't I don't say that my padding is same strat by default is going to be one but padding is not same that's why I lose it even if strat is one I lose it so now this is how it is going to be done for the first channel but now it tells me commercial layer which consists of 32 two channels.
(1:56:12) Now it means I need to actually have second feature map which comes from another filter of the same size. I convute it and I get second one and then so on so on and then the last one will come from the very last filter which is 30 second filter. I convolute it and I get the very last channel this way.
(1:56:42) So now in this case you can see that here I'm going to have 32 already because I specify that way I say I want to have 32 in this case I'm going to have three sub filters because of that it comes directly from there let me maybe say convolutional 2D which has 32 feature maps filter of by 3x3 and then input shape and so on. We understand what it means. It means input.
(1:57:15) So basically this 32 means that we are going to have exactly 32 feature map at the output. It also corresponds to number of my my number of my filters first for that one second for next one and so on. 32 for the last feature map. So the 32 will define depth of my corrosional layer. What about 3x3? Now 3x3 basically specifies size of my filters, right? So it means this one will specify size of my filters this way. That's how it is done.
(1:57:56) So any questions about that? Everything is quite transparent I believe. Right. So next one. Max pooling. It says 2x2. Max pooling is 2x2. Let me apply max pooling. Max pulling which is 2x two. So what that means? It means basically every single a kind of channel will be reduced by two right because every sub region of 2x2 will be transferred into single pixel.
(1:58:27) It means we're going to have at the end one channel, second channel and so on. Still 32 channels. We're not going to change that. But 30 becomes 15 because of 2x2. Every every kind of location is 2x2. And in case of max pooling, they don't overlap. Doesn't make sense to make it overlap.
(1:58:51) If you want to reduce dimensionality, it means we're going to have 15 locations along each direction. That's why 15 by 15. So next one if you look here next one is again Mac uh again a commercial layer which consist of 60 64 feature maps and filter of size is 3x3. So it means it is one second and so on already 32 sub filters. Next one which also consist of 32 sub filters and so on.
(1:59:23) next on which also consist of 32 sub filters because it tells me there are 32 sub filters right I'm sorry 6 um 64 even right I'm sorry 32 means it comes from dimensionality of the input and 64 is number of those this way and filter is of size um 3x3 everywhere and number of those sub filters will be 32 which comes from dimensions from there.
(1:59:56) Now, let me emphasize if my input is going to have three colors, that's how I define how many sub filters I need to have. In this case, 32 will imply that I have to have 32 sub filters. But because my next layer is actually convolutional layer 2D, let me say convolutional 2D, which has 64 channels and filter of size 3x3. That's exactly why I'm going to have exactly 64 filters of this type.
(2:00:26) And that's why when I do convolution, I'm going to have first one, next one, and so on exactly 64 of those. This way, this way, second one, and so on, next one. And 64 comes exactly from my number of feature maps which I specify. 3x3 is ex exactly this way as before and I can continue this way right so any questions about that I have a question does the first number in the filter go to the number of rows first number of the field in my layer like 32 for example let's say 64 for example no in the in the in the in the second part there the 33
(2:01:16) so 33 is dimensions of by filter right 3x the first arrow shouldn't that go to the number of rows oh you mean which one is row which one is is column they correspond to each other basically so if you look dimensions here 32 by 32 first one correspond to first one you always use a square filter size no you don't have to no it don't have to more than that image doesn't have to be square image right I mean so it could be any rectangle That's why no we don't have to use square filter.
(2:01:51) If it was square filter, we would not specify two numbers, right? We will just specify one number. But we have we have option to specify two of them. So it means no. So that's how it is done. Now you can see how many how many we have. If it is 15, it means I have 13 locations. It is going to be 13 by 13 now. And then we continue this way.
(2:02:15) And only one part which you may not understand it is flatten. So what is flatten? Basically flatten means at some point I have this type of guy which is the type of array 13x3x 64 for example. How to pass it as input to full network. I need to reshape it. I need to I need to make it like a single vector. So flatten means basically reshaping my collitional layer. Nothing else.
(2:02:40) Reshaping means I just literally say reshape this array. It is not costly. There are no parameters which correspond to flatten. I simply make it single array and I pass it further as input to dense layer. So any questions? Yeah, I think I have a question which is um so how is it that we go from 32 uh to 64 though cuz 64 is larger than 32 or or so that's 64 layers is that is that what so we specify in this case I want to have 64 feature maps the whole thing is called layer each ind kind of sub layer is called feature up
(2:03:24) or channel 64 channels. If I say 64 channels, it means I say I want to have first filter I convert I get it. Second filter I get it and 64 filter. So first comes from this guy, second comes from that one and last one comes from the last one. That's how I get 64 because I want to have 64 filters. Every filter will have sub filters.
(2:03:52) Says 32 because I have 32 channels as input. Okay. And so um you you would not have been able to pick a number larger than like 15 and 15, right? Because it would have to be smaller than 15 by 15. So if you chose instead of three. No, no, it is different. Absolutely. So number of my you talking about 64 relative to 15 basic.
(2:04:19) It's like I'm talking about three three and three cuz if you had chosen like 15 comma 15 then am I right that like you wouldn't really be convoluting it basically cuz it's size so if you choose 15 by 15 yes you're basically going to collapse everything into single pixel it is still possible they still do it actually it is possible uh more than that I can tell you interesting thing people take filter of size one by one one by one you take one pixel at a time.
(2:04:45) Why would you want to take one pixel at a time? The answer is because you collapse colors together. Mhm. So one by one is widely used not widely but there is a specific um type of network so-called inception module Google Google introduced it was actually tremendous model.
(2:05:07) It was winner of competition somewhere in 2022 maybe and uh they use this idea that they used filter of size one by one. It seems like strange idea why you would wouldn't convolute just one pixel with itself but because you converted different colors into one into one signal. It makes perfect sense to reduce emissionity this way. Okay.
(2:05:32) And can you say one more time how you got the number 13? Um cuz it went to 15 x 15 and now it's 13 by3. So it is like you know what it is like let's say I have 1 2 3 4 5 for example so I have five as example right as example and my filter is of size is of size uh 3x3 3 4 5 let's assume my filter is of size 3x3 you simply need to count number of locations. First location, second location, then one, two, one, two, three, pendants at three locations.
(2:06:24) So, how do we get it? Basically, if you think maybe maybe it's not helpful. Let me do it differently. So basically if you think that you try to let me sketch it differently let's take image of size 5x5 and filter of size 3x3. So you can think that this is my first location which I have think about this corner right is right here and the last corner would be kind of over there but it's not allow it.
(2:06:54) I'm already out. So it means how many do I lose? So how many do I lose which are outside? I always lose basically the size of my filter three minus one. That's why I know what I lose is going to be size of my filter which is uh three minus one because it means I lose two at a time. First location, next location, next location until the end.
(2:07:23) If I don't care if I go beyond the boundary, we'll have two pixels out. That's why I say 15 minus two which are out. I have to push it back kind of. Does make sense? Yeah. Thank you. Yeah. So that's it. Yeah. Okay. Now let's now stop. Okay. Let's stop. Thank you. Professor, can I ask you a a general data science question related to Ding? Yeah.
(2:07:53) Okay. Yeah. So, this this isn't like really pertaining to natural language processing, but um but I've been a data scientist for a number of years, but I've never done deep learning ever and sort of only recently learned um like real real machine learning. So, uh so if you want to model some data as a data scientist, you would think to yourself, well, I wonder what data I'm sorry, what model I want to use.
(2:08:16) Maybe I want to use linear regression polomial, maybe XG boost. And so would the line of thinking be like you have a list of models that maybe you want to try and then deep learning would just go in that list of models you would want to try like does deep learning sort of fit into that mental framework like oh well you know I have a bunch of data here I want to make some predictions how about I just use deep learning instead you know it would you say that's correct yeah I mean if you have enough data because you know that deep learning with this neural network basically has lots of parameters it means you have to have
(2:08:46) sufficient sufficient number of data you know to be sure that your network will be trained like a certain number of features right yeah so basically yes it is nice way to try not work no questions often it actually performs better but the thing is um you have to have a lots of data at first thing second thing is actually if you have time right you can run whatever you want and then choose the best one according to test performance was a problem mhm yeah I mean yeah yeah I was kind of wondering if if like that was just the right way to think of Yeah, like there's also deep
(2:09:17) learning that you sort of try. Yeah. Well, you know, if you have like, you know, 200 observations, I mean, yeah, I wouldn't even ever consider deep learning because you want to run some kind of, you know, linear regression kind of econometric models and you have 200 observations, very typical scenario.
(2:09:36) By the way, deep learning is already out of questions. You don't even consider that. But if you have like thousands of observations, so better be even tens of thousands of observations at least, then deep learning is definitely something you want to consider. And the question is why not to try it and why not to basically look at test error and see what whatever performs better.
(2:09:54) Mhm. Test error, test accuracy, whatever you your problems. All right. Well, cool. Thank you very much. Yeah. Thank you, professor. Yeah. Thank you.