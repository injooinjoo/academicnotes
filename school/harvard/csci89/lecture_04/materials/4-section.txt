(2) 89day4 section - YouTube
https://www.youtube.com/watch?v=bxtFF_2ip_o

Transcript:
(00:01) Hello everyone. Hello Dr. K. So today we have a bag of representation. Let's see how we can do it using different tools. your assignment is also about that but you also need to do likeization steming maybe I will not do and steming I will see now probably I don't do it but you can modify whatever I have here for tokenization you can also on the top of this apply steming and alsoization in order to solve your homework exercises so now first of all bag of words. We remember what it means. We create dictionary in some order of those those
(01:03) words will be added to dictionary in any basically order. We don't care about that. It is back. It is not ordered basically. And typically we add in order they occur in the text and the corpus and we add as as soon as we see the new one we add it to the to the vocabulary. So we create vocabulary and with reference to the vocabulary we going to create for every single document we are going to create bag of words going to create representation.
(01:33) Now let's now let's now see how we can do it using tokenization from basically skarn learn so called count vectorzer it is quite quite ba basic actually tool but not there are some some flexibilities you can also use engrams and so on you can look at parameters never is considered to be like basic tool different from what space LTK can provide if you want to do quickly some kind analysis or maybe even build some model. You can use it.
(02:05) No questions. So now in this case I'm going to use my account vectorzer from sklearn and I going to create corpus. Corpus in this case means basically my text on which I'm going to train the model. Corpus in this case uh as you can see list of documents list of sentences actually.
(02:26) First sentence Henry Ford introduce the model T. Then next sentence for T was revolutionary and so on and so forth. So we have a number of documents. Now our task will be to first create a vocabulary and then feed this vectorzer. Now let's see how we can do it. Uh our instance count vectorzer.
(02:50) We're going to for convenience call it a vectorzer and then I'm going to fit it on my corpus. You can see corpus is basically my uh list of words list of documents and I feed it on my corpus vectorizer is my vectorzer from here and I say fit transform and I get my result. So basically this uh is procedure which is used to train this vectorzer which we can later use for example for also uh a planet to to new text. Let's see what we get. to get get feature names out.
(03:27) Let's see what we get as names from here. We get vocabulary which is automo by countries and so on and so forth. So basically all unique words will be added to this vocabulary and we say vocabulary. Now what if I want to actually see how my bag of words is going to look in this case. Now you can see I take my X and I say to array to display it and this will be my bag of code representation of my purpose.
(04:00) The very first document Henry Ford and so on is basically 0 0 0 and so on and one one so one one because it is like 1 2 3 4 5 6 7 1 2 3 4 5 6 7 for Henry. So we have them for Henry we have them that's why we say one one they correspond precisely to Fort Henry again order is lost we just simply say back of words it means later when we try to build a model we will not use anything like recurrent network of course it will be completely unordered un kind of you know unorganized uh vector we don't really care about the order it means some kind of fully
(04:39) connected network may be used in this case because back of words by by assumption by by construction means order is not preserved. It corresponds always to the dictionary. Second one second document tells me first word occurs one time. So this end is in the second document. Last one for example occurs two times was corresponds to two.
(05:08) It occurs two times was was that's how we obtain second bag of representation for the second document in my corpus and basically that's all we all all we did now there is a question what to do about new document right we don't want to add new tokens to our vocabulary it is not not not fair right it is not correct is not how in practice it's going to be if someone is going to install your application on on iPhone and uh It is already pre-trained basically that whatever they going to supply to this to this application may contain new words.
(05:44) It means during the testing sort of during the test right we have to understand that new document will consist of tokens which are not in the vocabulary. This is assumption that's how we build it. In this case we simply say okay if my new document let's say consist of only single documents then I say my vectorizer which is already pre-trained on my corpus can transform my new document as well then x new will be my basically new object which corresponds to new document I can also say let me display my bag of words and as you can see what happens I have a
(06:21) lots of lots of zeros it is normal because my new document doesn't of all those those tokens which we which we um I mean many of those uh new tokens are not in the vocabulary because many of the words are not in the vocabulary. You can see one you can see one like 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Let's see what it is. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 off.
(06:58) So it it tells me off occurs one time. Okay, off is one time, right? What about impact? Impact for example is not here. That's why no not this way. Let me say what about uh uh what about for example the very first one corresponds to zero. Why is it zero? Because there's no any any end here. That's why it is zero.
(07:23) Automo it doesn't have any automo. That's why there's also zero. So lots of zeros and new tokens like impact for example they are not in the vocabulary. We're not going to add it to vocabulary. Vocabul is fixed. It is based on train data set on the on the corpus. We're not going to change it.
(07:42) And I think that your assignment asks something similar, right? You have to take documents. One is for the training and second one is for the testing. You will be doing something similar but also you will have to do limitization and stemming. Whenever it is possible you have to do it right.
(07:59) If there is no way to do it you can say like I cannot do it or you can combine maybe different tools from different packages. It is also okay. If you cannot do some kind of like stemming for example from one package you can just say honestly it doesn't exist there or you can apply different stemming from different different package. you can just say it doesn't exist for example uh for this exercise it is okay so now uh any questions about this of course I didn't doization I didn't I didn't change my tokens yeah now what's the second one NLTK NLTK natural language toolkit is widely used by researchers it is a kind of already
(08:39) relatively old actually uh tool It it is widely used by researchers, it is slightly slower. That's why people don't really like to put it in production. All right. Space is most used for production. But nevertheless, for researchers, for academia, people like NLTK. Let's see how it works. We take our word tokenizer and we say corpose.
(09:06) It is the same corpus as before. Let me now sort of create my lists where I'm going to keep all my tokens and all my documents. As you can see now, it requires a little bit of manual work. On on one hand, it gives you flexibility. You can decide what you want to do with those. But on on the other hand, you actually have some additional manual work.
(09:26) Previously, it was quite straightforward. to just say let let me fit transformer and let me take new document and fit and transform my new document document that's all in this case you have to do something a little bit more than that you can say you should say for example that all tokens is my list where I'm going to keep all my unique tokens not even yeah all my unique tokens right uh basically right not even unique I'm sorry all tokens will consist of all tokens not even unique Once later we'll remove here
(10:01) you can see set when we create vocabulary we'll create set out of this tokens it means it will keep only unique unique ones later on this whole idea we create our list where we're going to store all tokens later we going to create set out of these tokens that means we are going to keep only only unique tokens vocabulary will consist of only unique tokens based entire corpus.
(10:32) Now how to extract my old tokens? Now I can say let's loop over documents from my corpus. That means first time document will be first sentence. Second time document will be second sentence. We are going to work with every single document from the corpus at a time. Then I say let me tokenize my document. That means for example let's tokenize first sentence. What do we get? We get all tokens from the first sentence.
(11:02) What to do with those? We are going to actually extend our tokens. It means we're going to create big big big list of all tokens which comes from corpus. All tokens do extend means we're going to add to the list. It's going to be long long like flat list of tokens. Ultimately we are going to loop over all documents in in this corpus.
(11:26) It means all tokens will consist consist of long long flat let list of tokens. What about actually tokenized documents themselves? We actually want to keep those as well because we want to kind of keep track of what result of tokenization is in every case. Then we're going to create tokenized documents. It's going to be empty list first and then we are going to append our tokens to this list.
(11:53) What it means? It means we are going to take our tokens and append it to this list. It means basically tokenize documents will be list of lists. First tokens is already this tokens is already a representation of first sentence. I'm going to append that. So now my tokenized documents will have as a first entry as a first element will have list of tokens which correspond to first sentence.
(12:21) Next next time my document will be second sentence. I will create my tokens. It will be representation by tokens of my second sentence. I will append it to the same list. Now my list consist of two already lists. First list is first document.
(12:42) Second list is representation of second document and my tokenized documents will be list of this representations via tokens. So now also when I create vocabulary out of this unique to out of this tokens I say set on purpose so I can remove this duplicates and then I say let me append special key like OV out of basically whenever something is going to occur and if I don't know what it is I'm going to basically send it to OV like if new text consist of something new I will say let it let me put it to this OV out of vocabulary.
(13:19) It's not like must to do but it may improve if if it improves accuracy. It may be a good idea to do it. That's why let me try to do it this way. So I just simply app to my list of uh unique tokens. The very last one will be ov. Now this function will actually create bag of words which also has ov just in case of there is something something new and it will use NLTK right NLTK using web of words in that case I need to supply my tokens tokens means basically list of my tokens which correspond to particular document like my document Henry for and so on is my document and
(14:06) tokens will be result of tokenization of this document. It means tokens will be Henry comma for comma introduce comma there and so on. That's what I say as input. I supply my tokens. It means like this list of tokens which represent particular document also I will supply my viable which I designed and I say let me create my bag of words.
(14:30) In this case, I simply say tokens. Let me count how many times it occurs. And I loop over my words from vocabulary. I ignore the last one because I don't want to loop over OV, right? So I don't expect that OV among my tokens. So I just don't don't use it.
(14:54) That's why I loop over actual tokens and count how many times they occur in representation of my document. My tokens will be list of these tokens, right? And I will say if my word which I currently look at from vocabulary is model I say first tokens consist of all the tokens and I count model occurs only once first means first entry will be one here. That's why we get like one here for example and so on.
(15:27) Then we say this is my function which ultimately will return my bag of word vector this way. Now also we have to check if something is not uh is not um uh is not in our vocabulary. In this case I just count how many times we have token in the list of our tokens which is not in vocabulary.
(15:52) not in the vocabulary and we sum those indicators together to get to obtain count of missing ones to count of extra ones those extra which are not represented in the vocabulary and we get this over count and we simply append this over count at the end to bag of words so basically last one zero means zero new words zero new tokens that's why zero now it is obvious why because we use the same text to train it and also same text to look at the results right in last case when it is new document I will have nine at the end nine will correspond to basically talking which is not in vocabulary occuring nine times nine different
(16:33) basically new new words will be not in the vocabulary that's why I call them to be nine for the new document so and that's it now I say let me simply print out my vocabulary for mass introduced revolutionary and so on. Then let me for a number of documents in my case it is like five of them let me print out my bag of words for each of those first document is represented this way zero at means no new words clearly because it is train document and so on.
(17:16) Now let me say if I look at new document this one for example then I apply tokenization to new document as well I'm going to get new tokens it will be new tokens will be like list of therea impact comma of comma so driving and so on maybe so driving will be split uh self driving oh no it will not be split space you will split ated.
(17:44) So s will come together and I say my new tokens. Now I apply my NLTK bag of for representation to this new tokens already using the very same vocabulary which came from train data set and I have my representation. So my new mega for representation is this vector right? It has some actually same words.
(18:08) 1 2 3 4 5 6 7 8 9 10 11 12 addition 12. 1 2 3 4 5 6 7 8 9 10. Oh, it is simply that. So simply that. So that wasn't my corpose. That's why essentially we have it was it was in my vocabulary and also we have that here that's why we have it that's what it tells me this is that you can see that is not at the end by no means it is just in the location which correspond to my vocabulary and what about nine as I mentioned nine means those which were appended uh at the end which corresponds to out of vocabulary tokens it means basically new tokens which are not represented on the corpus and it turns out that I have
(18:59) nine of those nine completely new tokens which were not represented in my corpus. Of course we hope that it's not going to be this way because we hope that our trend data set is large enough to capture most of the important tokens right now if not how it is how it is going to look. So now next one.
(19:26) So yeah, I didn't mention that on the top of this you can use also uh stemming and also or maybe alternatively uh limitization stemming or limitization whatever you like or whatever you asked right or maybe both of them for assignment I didn't do it it will be kind of your task I just gave you basic kind of structure of this of this approach but on the top of this when you look at tokens you can also apply limitization on the top of this or maybe stemming on the top of this.
(19:55) Now in case of space in case of space there is no stemming. So now in case of space we are going to do exactly the same actually exercise our corpus. We create all tokens where we're going to store all the tokens from the corpus tokenize documents where we're going to store list of our documents or to be precise list of our list of to list of tokens and then we say documents will loop over corpus and I apply my NLP right to get my representations.
(20:37) Then I say tokens will be token text in order to extract tokens token loop over uh elements in my doc this way and then I append that in order to get my tokenized documents in order to get all tokens I extended in order to get like flat flat single array kind of flat array of flat list of my tokens not list of list but simp list of tokens and that's it this is very similar this is very simple as you can see then I create function space space space which is used to create bag of words then I compute my vector I count number of times it occurs
(21:17) words count from vocabulary as before basically same same code I also append my out of vocabulary counts and I return bag of words and uh and uh I loop over tokens in my documents. No, it means tokens itself is like specific document represented by list of tokens. Tokens stands for specific document. Tokenized documents consist of first document represented by tokens.
(21:52) Second document represent tokens. When I say tokens, it means I extract first element of this list which itself is a list of tokens corresponding to first document and then I apply my function spacey bag of words and I append it to my bag of words vectors and then I load I mean print my vocabulary and I also print my bag of word representation every time and you can see what happens.
(22:23) My vocabulary looks slightly differently in this case. I don't have dash any uh I don't have like I have dash already as standing alone token tokens that means words will be split into sub pieces subwords and uh I have my representation slightly different. So if I take my new document, I apply it very similar as before. Then I apply my space spacey bag of words representation to this new tokens and I get my vector. You can see 10 already 10 new ones, right? 10 new ones.
(22:57) So that's how we can do it. Any questions? So now very similar we can do it actually using two gs. We just need to specify range two gs right and we do it by two gs. The logic is very very similar otherwise we can use notk and we can use we can uh also use backgrounds in this case and we get the result which look this way.
(23:34) Every basically element of vocabulary will be already tuple. Let me grab two con two consecutive words which occurred on the text in this particular order continues to they come next to each other basically was key they were observed in this order in the text was key that's why it is to ground and then I can represent that of course you can see that becomes already larger vocabulary grows in this case yes we sort of see some kind of let's say context around word or let's say we can sort of capture correlations between words this way but it comes at cost of having more tokens on the vocable it
(24:17) means representation is now even longer right and more sparse by the way then similarly we can take new document and see what happens okay we have even more zeros even no actually any single no single token is the same all of them are new because they were not represented and in the corpus and similar we can apply space shift to two gs and we get these results continuous too and so on also all all of them are new ones no because that's how because corpus is quite small just five documents and you don't new sentence
(24:56) doesn't really have any any of the same programs that's why we have it this way so any questions your task will be to apply clearly to applyation right and so limitization and stemming. So now let me uh see example on evolutionary networks. So in this case um you can see that we have a image of size 28x 28 by 1 that means one color 28 pixels by 28 pixels input shape is is that one we have 32 conversion layers and so on and so forth. Let me jump to second part.
(25:46) It is like second part where it explains I'm sorry this is a yeah right here it explains what we can do with this image after some time we can apply some kind of you know flattening kind of you know transformation let me explain what it means let me say basically part two of this model which you see on the screen part two consist of so-called flattening flattening means basically reshaping and the result of this reshaping we're going to get on one dimensional array.
(26:22) So it is just simply going to be uh no kind of you know vector of if you want numbers. So you can actually think about this vector as a representation of image. I mean if you take image and app shape it right away and just say let me stretch all these pixels along one line it will be like like image. In reality it is not actually applied to image right away.
(26:46) Even though you can if you want you probably I I mean you can easily do it sometimes it works but it could be result of applying this procedure to actually kind of intermediate conversional layer. It doesn't have to be applied directly to image. Now at this point let's say let's say let's say I took image I take my pixels and arrange them this way along one line and then I say there are 64 neurons 64 neurons and then next layer consist of 10 neurons so this is my second part clearly everything is connected to everything it is dense one very basic one dense one this way this way this way. Now, of course, um we also
(27:31) have to have bias. We have to have bias. We have to have bi bias not here. It means it's going to look that way. It's going to look that way as always. If I sketch bias, it will be there as well. So, this is second part. It allows you to do some kind of classification.
(27:49) If I want to classify into 10 different 10 different digits, for example, it will be y first, y second, and so on. Why had 10 10 different uh numbers will be like probabilities. Now you can see soft marks soft mark means will be probabilities which up to one and they are they strictly positive. Now what about first part? It is not really image.
(28:17) Okay, first part tells me let us actually not take image right away and not to stretch it that way. It maybe not the most efficient way because we right away lose credulations, right? there are some kind of relations then we can say okay in this case we can do it this way part one of my network all over this way 28x 28 by 1 is my input 28 by 28 by one so it tells me by one 28x 28 by 1 it is input shape then I have 32 filters 32 filters every filter is 3x3 Now it means basically I have one two and so on and 32 filters everyone is 3x3 I convoluted it and I get first feature map
(29:08) which will be slightly smaller will be like 26x 26. So result of this convolution will be let me save some space maybe. Result of this convolution will be first feature map which is 26x 26. Then second filter will result into second feature map and then so on. Last filter will have last feature map. Total number of feature maps is 32 because it tells me that right.
(29:42) And then basically I apply another max pooling I apply similarly next convolution convolutions max pooling next convolutions and I get at the end some kind of you know convolutional layer which is smaller clearly but it is already deeper it is 64x 64 it will be smaller because the size will reduce every time right maxing also will reduce size it will be smaller something like 3 by so it should say it actually something like 3x3.
(30:12) So it tells me 3x3. So it is 26 by 10 6 by by 32 and after some time it will be 3x3. You can imagine kind of small but very very deep. And then I say what to do with this? It is like image almost like image which has like 64 colors. You can think this way if you want.
(30:37) It is not already image of course it is already cover layer but it is organized in a way as image. And I say what to do with this? Maybe this point I'm already okay. I can say let me do classification. How to do classification? It requires you some kind of deep kind of dense layers. In that case we use so-called flattening. So flattening. Flatten simply means we reshape it. Nothing else.
(30:57) We take 64x 3x3 that is like how many? 64 by 10 640 - 64 somewhat like 500 76 right 576 numbers it means it will be like first and so first and soon 576 so this flattening doesn't have any parameters it is simply reshaping it comes at no cost essentially so fluttering is not not not expensive at all it does nothing really and then we use whatever we want after that.
(31:34) So you see what happens. We kind of could say let me take image and right away flatten it. There are some kind of you know models basic kind of models to examples which actually work nicely. Even for image classification of this type it works nicely. But if you try to apply to real problems it's not going to work nicely. I can tell you why.
(32:00) If you take like two example and you say my images look like like two maybe this way. Second one looks like four this way. Next one looks like five this way. The thing is those numbers those digits are quite nicely centered quite nicely placed. They're not small not large nicely placed.
(32:26) That's why if you take those type of images and right away you flatten it your classifier most likely will work nicely. It's not going to happen if you have real world application where your your digital one time is small maybe one time it is at the left upper corner next time at different location if you flatten it because of those important correlations you're going to actually not be able to solve this problem nicely performance will be poor.
(32:52) So if you flatten it means every time this corner is is there for example that corner is somewhere else it means you kind of lose correlation right but next time you supply the image from this data set but digit is located at different location it means those kind of you know neurons which are trained to capture particular location will not be able to understand where two is because two moved to different location that's why trying to simply shape it right away and build build your network is okay if your data set is clean and nice like in the in case of uh those hundreds and digits. If it is not the case, it is
(33:23) much more difficult problem. That's why those convolutions are quite important. Those convolutions will basically allow you to uh kind of regardless of position sort of regardless of position construct your signal and then later on you can flatten it will be much better approach. So any questions Dr.
(33:48) Koskin I think you already covered this before but I'm not remember remembering the do the 26x 26 dimensions are they related in any way to the initial image that's 28 to 28. Yeah 28 and 3x3 filter if filter is of it is like filter filter is of size 3x3 filter right it means essentially that we are going to lose two pixels it is like like example again it is like if I have like one 2 3 four let's say five and nothing else right and then I say let me try to place filter of size 3x3. How to count how many times it can fit there? I typically think this way. I'm
(34:42) going to place filter of size 3x3 and I look at this, you know, left corner. Then I slide it to the right and I hit boundary. Let me forget about the boundary and say, let me move it all the way. I mean, until the last pixel. So, it will be this way. and my left corner will be placed against the last one. This way I would I would recover all of my pixels.
(35:08) But the problem is how many already outside of my boundary? Two precisely right or to be specific three which is size of my filter minus one because one still stays in that's why we always lose basically says a filter minus one. Does make sense? Yeah, professor I have a question follow-up question.
(35:34) Uh you said uh earlier that uh usually people take one by one filter uh in most scenario right uh so in that case there will be no loss of uh any any metrics right any quick pixel uh it is no not quite quite it it's not like usually people use one by one u there's another approach so-called inception module so the idea is actually quite quite quite obvious you may wonder why do we take like 3x3 and everyone is 3x3.
(36:08) What if what if one you know object is large and requires different filter another object is small and require a smaller filter maybe and people came up with idea that maybe we should actually use filters of different sizes when we move to next layer. If you use like 3x3 it means we always going to be looking through like 3x3 you know window it means we kind of can cannot capture large distances in our image right and they say okay let's try to use 3x3 then next time 4x4 next time 10x 10 for the same transformation we can use different windows because what happens right now everything will come through this window of 3x3 it means going forward signal is
(36:47) already kind of lost we cannot really recover it anymore or no I mean large those large distances kind of not going to be nice nicely represented that's why it makes sense to use 3x3 next time 5x 5 next time 10x 10 and people say okay let's do it and then they realize that it is quite expensive very difficult to train and they started to to kind of think what to do and the idea was to try to use filters of different sizes but uh uh collapse colors, right? So basically one of the filters which they they would
(37:24) use would be one by one but could be potentially very very deep filter. No, because if I apply it here like one by one. So I can apply like one by one but it will have like 64 sub filters and I will sort of collapse my colors. But it's not the only thing which I apply. I also apply different size of filters. It will be it will be kind of one amount of those filters kind of.
(37:49) Yeah. I just have a I'm trying to understand the using variable filter sizes. So we can in the same exact layer we're going to use 135 78 1 nine. Is that is that the idea? Yeah. Idea is a so-called naive inception model is idea is why why do we always apply 3x3 3x3 3x3? How do we know that objects in the image will be of similar size? What if something is huge right somewhere in different case isn't on the contrast small is this kind of zoomed in zoomed out and so on if you apply filter of 3x3 it means we're not
(38:28) able to look at image we not able to capture like pixels from left corner and right lower corner together we always take only one of those at a time it may be a problem if you have large objects kind of you want to see it right away sort of right I mean you want to take filter which captures everything maybe maybe performance would be better that's why the idea was let's use maybe filters of different sizes for transformation to next layer but in the idea it didn't really work because it means that you
(39:00) have to have a lots of parameters much more I believe that the edges will be blurry because in order for us to get the same exact size to the next level we have to use uh padding to be the same so we If I have a if I have a filter that is too small and then a filter that is too big that what beneath the the big filter the big filter will produce lots of zeros around the boundaries.
(39:28) Right? So whenever we take the the depth wise convolution uh it will be adding many uh many zeros especially around the boundaries. So that's why I think this model may not work very well. You can look at this um sort of at this uh you know uh sketch. I just found it online. I have one slides for different class but I won't I not open it.
(39:54) I'll just simply show you from online. So they say let us try filter 5x5. So this this filter will result into um blue one this part filter of size one by one. it will result into this orange part and they sort of stack it together into one single convolutional layer. So basically in my case every layer is a result of convolution of this filter of 3x3.
(40:20) First layer uses 3x3 second layer us second feature map uses 3x3 filter. Next feature map uses 3x3 filter. everyone uses the same kind of filter and they say let us try to actually apply filters of different sizes which kind of makes sense with this natural idea right but the problem is it didn't really work nicely because it was because it was too expensive to train and they ultimately came up with idea that you can let me find or maybe this one.
(41:10) They came up with this idea that they would actually try to always, as you can see, always use filter of size one by one, one by one, one by one, one, one by one. It is basically used to only collapse your colors together. sort of they would collapse colors together or in case of commercial layer it means they would collapse feature maps together one way or another but after that they would still use filters of size 3x3 of size 5x5 different kind of kind of you know filters and basically they came up was I think Google introduced it they came up with with this type of inception module which you
(41:43) see on the screen which uses convolutions of different sizes but at some point we also have convolution of size one by one. Coition of size one by one doesn't doesn't really convert anything. It is just essentially just um just if it was like my case one by one would be identity right one by one would result into the same in the same feature map as original image but if I have different colors colors will be collapsed together if I convert this filter of size one by one that's why we kind of have it because we don't destroy kind of special kind of characteristics but we just we collapse colors together
(42:24) or collapse feature maps together. That's why one by one is sort of used everywhere. No kind of empirically. They found that using one by one filter doesn't really damage much in terms of accuracy but it reduces dimensionality of the problem. It reduces tremendously number of parameters because one thing is to handle 64 fish maps.
(42:46) Another thing is to apply filter of size one by one. It means collapsing colors together. It's a weighted average very much per pixel. Weighted average per pixel. Exactly. Yeah. Right. If my image originally has only a single color, it means basically does nothing. Right.
(43:06) Basically, it's only done in the case it's only used in the case where you have multiple color colors. Later on, we have multiple kind of colors and quotes, multiple feature maps. That's why it is useful to kind of reduce dimensionality this way. But spatial resolution is still there. And we use filters of different sizes 3x3, 5x 5. Right? Again they use only 3x3 and 5x5.
(43:26) It is a kind of official inception model case of Google model. So any questions? Will this notebook be available after class? It is already available I believe. Right. Uh let me check. I only saw the let me. So left um left representation sort of what does make sense one by one 3x3 5x 5 everything is nice we keep all of them together we stuck all these feature maps together into deep deep conversion layer but it doesn't really work but they sometimes refer to this case as naive inception module and they say let us actually apply one by one every everywhere to sort of collapse colors together.
(44:12) So, any questions? Did you find it in the notebook? No, I only see the intro to NLP4. I don't see this notebook unless it's not in the in the sections for today. It's it's NP4. I see it. Or yeah, it should be. Okay. Uh, actually, we finished everything. If you don't have any questions, we can stop now. Where do you see it? Under files files sections.
(44:53) I see it is uh it is published. Yeah, I see that intro NLP, but I don't see this 5.1 introduction to con. Oh, it is part of your assignment, I believe. Right. Oh, that's where I've seen it. Thank you. I didn't pause because it was part of your assignment, right? It looked familiar and I was just wondering where have I seen that. Yeah, it is part of your assignment. That's the mystery. Thank you.
(45:17) Yeah, this this right here. So, example example which is posted under assignment. I have a quick question. Uh I I received an email that my homework too was graded but I I I don't see the grade. Uh yeah. So for consistency we try to you know hide the grades grade grade everything like use the second round third round like check everything so everything is consistent and nice that's why we didn't post it yet no I mean yeah so you just have to wait but typically how it is it is done maybe you know about this that people typically create first and then post for
(45:52) everyone I prefer to post like uh as soon as it as it is available but in this case we have multiple We have many students for consistency. We have to double check everything. That's fine. Thank you, sir. Yeah. Just to be consistent. Yeah. Because different people grade different assignments. I want to be consistent. That's fine. Thank you. Yeah. Yeah. Okay.
(46:17) Then let's now question. Yeah. As well. Uh going back to the bag of words. Um what do we do like what's convention around stock words? Is it something that we would emit stock words and just focus on other vocabulary or would you always include stock words for a large text maybe removing stock words would make it more efficient? Yeah.
(46:41) So basically if question is how to some kind of um how to reduce dimensionality of the problem right and people say maybe things like and is not really useful right there they is not really useful that's why we basically apply a technique which is called uh no kind of exclus stop stop words there are dictionaries for that right official kind of different packages may use different dictionaries but um but u MIT I think has famous uh list of stop words and if you basically apply it uh before even doing tokenization it will simply throw away there will throw it away and we'll throw away I think even she they right so there's so we do
(47:28) this you can do it but um account vtorizer doesn't do it by default right you can See it keeps it uh this is there they there. Yeah. No, we did some um analysis of of student relations. By the way, it is published. You can read it online. Um student relations uh at Harvard over like 10 years or so 11 years I believe and we applied this techniques when we had to throw away this wars because we wanted to reduce dimensionality of problem. So this is typical procedure actually when you apply this type of techniques in
(48:06) practice you say I don't believe that and the air will be useful in this case we just keep everything but in practice of course you want to remove stop words most likely again you can try to experiment and see what is better but most likely if you remove stop words it will reduce the mality of your problem it means you may gain something from removing them because more parameters means more difficult to train.
(48:34) Yeah. Yeah. Okay. So, any further questions? Have a good night, sir. Yeah. Okay. You too. Let's stop.