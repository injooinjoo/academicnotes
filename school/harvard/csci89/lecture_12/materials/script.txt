(4) 89 day 12 - YouTube
https://www.youtube.com/watch?v=0WJ793st4R8

Transcript:
(00:01) Hello everyone, welcome to lecture 12. Today we are going to talk about attention mechanism and also transformers. But first let's review the previous quiz. What is advantage of using conditional random field compared to hidden mark of models? Last time we discussed that in that case in case of condition random fields we can actually use more complex dependencies right we can also look at join distributions with future tokens and so on. That means in this case we can actually uh handle much more complicated cases
(00:53) where semantic is also sort of uh reflecting dependence on not only past past tokens but also future tokens. So this is correct answer. Second question in this case we have a key benefits of combining LSTM together with conditional run of field and you can see B we enhance models ability to capture both past and future context right because conditional runoff field basically allows you to take context from the past context from the future the result combin combined in this way comparable to LSTM allows you to capture this um context Next um
(01:37) now next one uh question three suppose we use regular autoenccoder not variational one to encode passages of text written by shakesphere and now we say let's look at those uh encoding and let's shift them manually one way or another and see what happens with the output. So last time we discussed the space of in encodings in case of regular autoenccoder is not structured.
(02:08) It means if you shift one one way or another your codings you may completely destroy your output or destroy in a sense that your output will not make any sense it means output will not meaningful text. Even if you slightly shift your encodings, you actually may get already completely destroyed like image in case of convolutional networks or text in case of text.
(02:28) That's why I know it will not be meaningful anymore. Next question. And let's say rational autoenccoder and we train it and we say like we have mus is like uh basically those uh mean codings as they sort of refer been referred and those mus are going to be exactly functions of functions of our inputs.
(02:52) It means every single input will have its own mu. Now let's uh look at previous maybe sketch previous visualization of out encoder in this case mu is a function of input. You can see this part it is basically a regular neural network. Mu is deterministic function of input. It means if you change input of course you're going to get different m obviously. So mu is function of inputs.
(03:34) Uh but later we are going to add some stoasticity but it's going to happen later plus some kind of noise is going to happen later up until mu we're going to get deterministic mu. It means every single output even if you like take cat and an image of different cat slightly different cat will have already different me so clearly because it is deterministic function just give me one second one second I'll be So now you can hear me right? So now next question uh question five in this case we have various autoenccoder in this case uh
(04:45) given this weights whatever we have in particular input the network output is not deterministic. This is absolutely correct because in this case we add noise. As you can see this block will add noise. That means output is not deterministic. Even even if you supply the very same input next time you get maybe different output simply because you add different noise.
(05:09) Any questions about this quiz? Uh just one question Dr. Krushkin. Yeah. In question one is option B also true just not a primary advantage or is it not true? are more effective for tasks with discrete input variables. No, it is not advantage of conditional random fields, right? U it is not about being discrete or not.
(05:40) If uh let's say there is a structure which allows you to apply hidden mark of models, why not to apply it? It is not because of discrete not discrete. It is because of actual context around your words and condition fields allow you to uh build connections between your token and future and past tokens.
(06:01) That's why it is not about being discrete or not. It is about kind of time flow and hidden mark of model explicitly assumes that basically current state depends only on previous state and nothing else. That's why hidden mark of model is less no kind of accurate and realistic cases such as language processing because it's not like next word depends only what what you set previously right or on what state previously was which would generate words and also next basically state this is how hidden mark of model is designed and uh it is not it won't be
(06:35) about being discrete or not I mean this is completely completely different question Right? Both of them could be used for discrete, could be used for continuous variables, but it's not about that. Condition fields allows you to build this connections between your token for example past and future tokens also with your states from the past.
(06:56) You may remember connections with double with wise as well. So that's why it is not it's not even about that right. So in this case uh hit mark of model could could be applicable in discrete case as long as it is indeed uh the case where there is no like depends on future for example that's why it is not correct answer yeah so now any further questions uh I have a question about the deadline for assignment 10 assignment 10 something not correct you imply yeah because uh you said I think it's u uh November
(07:43) November 23rd right but in the syllabus you said it's around December 4th or something like that yeah let me double check let me double check uh maybe maybe we can change it because we have a deadline for the proposal you probably mean right do we have uh more assignments for the following lectures or not? Um we we will have more assignments, right? Yes. So let me see.
(08:12) Uh okay. Because syllable said that uh we just have Yeah, we will adjust. If syllus says we will we will adjust. So it says like uh assignments, right? Um let me check. Assignment uh nine is due. Assignment 10 is due. And in this case, we have final project selection. Okay. This is important. Yeah. Okay. I'll I'll shift it.
(08:47) It's okay. I'll shift it. So it means assignment 10. And now you have assignment 10, right? It's like the last one. Okay. I see. I see. Okay. Yeah. I will shift it. So syllabus this we will shift it. Uh somehow I thought we have extra extra week but it's okay. So let's see uh what I have in my schedule. Um so I have assignment 10. So okay I'll I'll I'll adjust it.
(09:19) So okay says uh 10. We'll make it 10. Um yeah we'll synchronize with syllabus. Thank you for letting me know. I will change it uh later after the class I will change it. So any further questions. So the assignment is not going to be due this Sunday. So you see syllabus says that next time we have final project selection and there is no assignment mentioned.
(09:51) We will have I will adjust it as syllabus says it. So it means December 7 is the last assignment. Okay. Sounds good. Thank you. So now uh yeah, thank you for letting me know. I didn't know I forgot about uh leaving only project selection for this week. So now let me u start talking about today's uh slides. So today we going to talk about attention mechanism, right? And as a result we are going to move uh to transformers. We're going to introduce transformers today as well.
(10:29) Um let me maybe first uh uh remind that we have this recurrent networks, right? Recurrent could be not simple recurrent networks. It could be LSTM for example or JU get a recurrent unit whatever but they all kind of designed this way.
(10:58) Here we have recurren networks where we have time flow and basically we construct this sort of hidden representation hidden kind of vector hidden state y or whatever whatever it is called y in case of attention mechanisms we will call it h kind of historically happened this way they call it h. So h or y in this case will be this uh representation representation basically captures everything from the past because in this case we assume time flow and y second if you remember like formals which we at some point would write down y2 depends on the past it depends on previous y basically and also on x2 y2 depends on previous y and also on on x2 in the case
(11:39) of LSTM it is not only y it is long and short memories is right instead of y which will be will be used as inputs inputs to the cell. So when there is like time flow and the result you have kind of issues you maybe remember yourself and you have to try to use let's say auto encoder and you try to reproduce sentence it was quite difficult task no surprise because we try to basically take entire sequence and shrink it into single vector formally speaking that's kind of possible may maybe right because if this
(12:14) representation is let's say 128 dimensional we 128 dimensions. There is enough space for every single sentence. Clearly this huge space we live like in three dimensional four dimensional space and sentences easily could live in 128 dimensional spaces. But in practice it is quite difficult to train by multiple reasons.
(12:37) First of all so-called vanishing gradients right vanishing gradients. It is difficult to train such networks. Now of course if you introduce long memory this issue is slightly mitigated but not no not not not completely eliminated. That means it is still quite difficult to train such networks. Not to mention s networks quite difficult to run in parallel because this sort of consecutive structure of data flow in this case right.
(13:10) It means uh it is difficult to drain if it takes long to drain and also we have vanish ingredients. It means in practice it becomes almost not not impossible at some point it would be dominant technique to for the translations right for example but it was not not not not so so so great still so and as you remember yourself you tried at one point on the assignment to try to reproduce sentence in one language use bottleneck and then second language and it was quite difficult you can say okay what's the problem let's take a sentence and move it to different let's to the sentence as output from this layer. Let's not create bottleneck. Let's use
(13:45) it as is. So this type of architecture could be used for speaking. There is a problem with this type of architecture. You have a lots of lots of parameters. It is also quite difficult to train. Not to mention the structure is is done in a way that y3 for example depends on y2 and also x3. It also y2 depends on y first y0 x x0 x1 but those are sort of already um no kind of weaker dependencies right at it is quite difficult quite in practice difficult to capture long sort of longterm dependencies in practice if your let's say token depends on words which occured
(14:29) like five five tokens before it is already quite difficult for such network to capture it right that is main main disadvantage not to mention what if future words matter you say y2 is my recovered token or maybe my translation but y2 can easily depend on x3 can easily depend on x4 that's why we sort of say maybe in coder decoders design is better because we take entire sequence shrink it into one dimensional representation this is like bottleneck here and my my sentence has been represented by one single vector. Everything seems to be okay at this
(15:10) point because we capture everything before we try to reconstruct. So future is already there, right? Basically if this x first and when I construct y0 x first is already kind of introduced into this vector and I use it to reconstruct y0. It means this kind of bottleneck does make sense but again practically it turns out to be not the best approach.
(15:37) Um first of all we have a fixed fixed dimension of this representation let's say 128 but one sentence could be long long second one could be short and so on. So maybe it is not uh not the best idea to kind of you know use the same representation. Maybe you should do it like in case of remember convolutional neural networks we use like filters.
(15:56) We can slide it over the image no matter how large image is. Filter could be of size like 5x5. It is applicable to all kind of kind of images basically doesn't matter what size is you can even borrow pre-trained layers. That means basically with filters you can specify what your image is going to be and you can use those preand filters for your specific images works pretty pretty nicely right in that case.
(16:28) In case of language it is much more difficult also not to mention when we try to model it we kind of assume that this vector will represent semantic will represent kind of dependencies within the sentence but all of this is very very much dependent on the corpus which you are using. If you switch from one type of text to different type of text everything is already completely different right.
(16:53) So your representation might not work well. So transfer of of knowledge in some sense of learning is very very difficult if you move from one corpus to different corpus from for two different types types of text basically that's why um this model is even though kind of capable to reproduce maybe your sentence or even to do translation but it is designed to handle specific right specific maybe couples from specific corpus let's say this way maybe you try to reproduce same sentence but it is still same same kind F corpus. So it means it is um
(17:28) designed to shrink it and then reproduce it, right? It is designed to represent your input via this vector and then reproduce it. But it is not like something which kind of nicely learns about uh neighboring words about semantics and so on and then they introduce ID which is kind of in in a sense intermediate.
(17:52) It is not like like trying to take all the vectors and shrink into one right away. At the same time, it is not like an in original uh uh design where every time I will output my vector and pass it further. It means only partial sort of matter. My y2 better be function of not only x0, x1, x2, y2 better be function also of x3, x4 of future ones as well.
(18:16) If I do translations, of course, I understand that maybe future words also matter in order to understand how to translate. We typically when we do translation, we read like entire sentence and then we do translation. It may be quite important for our understanding of what is being said. Same idea here.
(18:35) Y2 may be better be function of all of them kind of at once. This is sort of done in a way this way in case of encoder decoder in case of autoenccoder. But this representation is kind of rigid kind of you know kind of very very specific and also not flexible because it doesn't have many parameters. It is like done in a very specific way.
(18:58) No matter what your input is it means it is not flexible basically like rigid representation even len of this vector is basically predetermined right. Yes, this vector this bottleneck captures everything kind of but in a sort of specific predetermined way. Once we have trained it, we have this mapping and it turns out to be not working like nicely for all kinds of inputs.
(19:20) Maybe one requires one type of representation I mean type of mapping. Basically another sentence from slightly different corpus or slightly different uh context will require a different mapping but we have this parameters and we just use the same mapping. The result essentially what we are doing is we are saying let us try to make y2 which is going to be function of all of x's y3 which is going to be function of all of x's as well how so a sort of as a sort of weighted weighted sum of those inputs basically right so
(19:57) let's see how how it has been done sort of you can think about like intermediate case between original architecture and autoenccoder so this representation will be not single representation What this representation will be designed for every single time step basically and this representation will be at point Z will be one representation at point one will be second representation but also based on all of this inputs including future ones and so on. It is already more flexible. It allows you to capture semantic also context around your words
(20:28) future words and so on. So let's see how uh we can do it. Now you can discuss like challenges which we face when we build uh translate or build uh let's say autoenccoder even using recurrent networks. uh we clearly cannot um capture long uh let's say we cannot um capture context from long sequences because in case of in case of our recurrent networks uh as further we move as more and more and more transformations we apply remember how recurrent networks is designed and we kind of struggle to capture long-term dependencies is basically even though LSTM sort of was designed to handle this issue to
(21:16) mitigate this issue at least that's why alone memory but that's not still didn't eliminate the problem and also sort of related to the same issue so called vanishing graded problem it means we cannot update parameters related to this long dependencies basically it means we can't really learn long-term dependencies second problem is related to kind of computational issues I would say it is already related to nature of recurren networks where we kind of consequently pass information forward. That means we cannot efficiently run it in parallel. So this is a limitation clearly because
(21:53) if you want to make it practical, we have to train it long more and more and more and more. If we cannot do it in parallel, this is a huge limitation actually. This is this is a problem. This is practical problem. Fixed size representation bottleneck.
(22:14) It means this representation if you use auto encoder has like length of 128 for example it is fixed fixed size representation which is not always optimal because you may have different types of sentences longer shorter and so on. Maybe one sentence requires uh one type of vector second sentence may require a longer vector and so on. How can we kind of sure that we capture every time it input into the same vector? No, it means this vector must be like large enough but it already means we have to have a lots of parameters and so on. That's why it is quite quite quite also a problem in this case and
(22:52) now we can create so-called attention mechanism. Originally it was introduced in 2014 then they published in 2015 this this this paper right and they suggested the following basically they say let's create this new kind kind of representation that is called context representation vector context representation vector for every time we are going to have its own context representation which is based on all of those all of those um uh let's say outputs from recurrent neural network right let me try to sketch something here let me maybe this is picture from
(23:34) the original publication which was published in 2015 let me make it maybe even a little bit simpler in this case you can see h and h it means we're talking about birectional approach h is what is basically denoted by y in this case y so h is one of postwise and second type of H with arrows pointing to the right is already um uh RNN or kind of LSTM for example where I have a different direction this is birectional approach Y where sequence goes from right to left and I'm sorry not Y but H where a sequence goes from right to left and also H where a
(24:21) sequence goes from left to right I concatenate into a long vector as usually we do it in case of birectional stamp example and then I view this vector as input which I pass further let me take basically simple recurrent network let's say recurrent network with attention mechanism right with attention with attention so this is what what we have to what we can look at if you want to just an attention and I don't even have to use birectional typically typically it is birectional because as as we mentioned future matters and so on which way it flows let me take like simple recurren
(25:06) network basically and say I assume that there is some kind of input x1 this way at 101 x1 and then I have there is output which I call h1 so this is my output h1 vector Now I'm going to use regular error because I'm moving to the right. So this is my signal which propagates further. Xc is input output is h second.
(25:38) And I continue this way. I have more and more and more time steps. So x3 I move that way. Next one. Next one. Next one and so on. Right. Let me let me say dot dot dot. So this is h three. I move uh further. Then the last one is x. No let me say capital t. We typically denote it by capital t is last time step. H capital t is last one.
(26:19) So we have a lay layer of simple recurrent neurons. No birectional anything like that. If you have birectional you simply have to concatenate like glue together those parts right. One h comes from direct input and second one will come from as output when input is reversed and we have those hes but in reverse they will be different hes because input is in reverse.
(26:44) We concatenate those glue them together into one long vector and will be our hes right in my case I just call it h because I basically use simple recurren network with no other direction just to illustrate how it works then I say let me uh use some kind of weights I will tell you how to design those but let me use some kind of weights alphas which will tell me how to combine things together and then I say let's say this is like u this is like u in this case some kind of time time t let's say st is is time of rel to already output output right output so it is like yt and then I say
(27:32) let me not maybe before there was st minus one this way and of course there was something before that there was something before. So they are connected and uh there was also something in front of me. So this is my u kind of a kind of a uh uh layer which is sort of almost like you know next layer which I would typically use if I wanted to add more layers.
(28:04) But in this case I say it's not like H1 goes to first H2 second goes to second. No I'm I'm saying it differently now. I say let me take all of them. Let's say I take first one second one third one all of them together this way and I combine them. So I say this plus which means literally I take a weighted sum of those things and I create a vector which I call CT.
(28:35) So CT is essentially going to be a context a context vector in this case at time t right which is going to capture all of those not even access but outputs from my recurrent network. Basically CT is like weighted weighted sum of outputs from recurrent at work. That's what I mean by CT. In this case, we understand that we are talking about time t. So what do I do here? I simply add it to add it there.
(29:06) Right? So it is going to be used as input here as well this way. And then I say alpha t first because it is related to h1. Alpha t second is related to h2. Y t because it is going to st right at time t and alpha t3 and so on. Alpha t capital t. And I do it clearly for every single time step.
(29:37) Here is also something going to enter from every of from all of those something going to enter and so on. And that's how it is designed. So in this case you can see that ST will accept signal from previous ST minus one and also will accept signal from all of them as a sort of weighted sum. Now let's see now how exactly it is designed.
(29:58) H is concatenation of uh this forward hidden state and backward hidden state typically because we have like birectional then we're going to introduce this C vector which is so this every C is basically vector let me specify uh that context vector context vector CT is going to look this way. No CT enters uh this um ST clearly but it also enters not only ST it also enters first one second one for every time.
(30:30) That means I can compute C1 vector with C1. I would use uh alpha 1 1 H1 plus alpha 22 H alpha 2 alpha alpha one 2 H2 and so on. and last one will be alpha uh capital t right I'm sorry first one is first and then capital t just like in this case t1 t and so on capital t and then h capital t this way this is c first vector you don't see it I didn't sketch it but if I move forward in time it will be vector s t which is alpha t first exactly as I have it over there time h first very I'm H clearly right plus on alpha T capital T H capital T this way
(31:27) and I also move to the last one let me say C capital T would be alpha capital T first H + alpha capital T capital T H capital T this way so this is how we create context vector context context vector is time dependent first of pull every H in this case generally speaking is multi-dimensional in my case if you say this is just simple neuron then I H is one dimensional this is correct if you kind of look at my sketch you can argue that H is scalar then okay then C will be scalar generally speaking it is not scalar generally speaking H is like multi-dimensional because I can
(32:12) potentially have multiple neurons right now let me let me say it is not It is not even let me say some kind of box right or like two neurons inside for example maybe if I sketched this I will be already clear why it is multi-dimensional maybe inside I have like two neurons for example or I have LSTM inside whatever I want basically I can apply this techniques to a layer of recurren neurons in this case like two neurons for example then H is two dimensional if every H is two dimensional and C is also two dimensional clearly
(32:51) that's how we can do it. Uh uh so now these alphas are exactly what we say attentions right sort of attention mechanism in this case we try to see what alphas should be how essentially how let's say h first impacts c first or how h first impacts ct if alpha t1 is very very very large it means we understand that x first basically which which produces h1 is very much related to the output at time one. So it is sort of a way to uh estimate attention.
(33:32) Now what is alpha? How is it designed? We don't really use it as you know parameter. Alpha is not a parameter. Alpha is soft max applied to this e things. Those e are um so-called alignment scores. All right. They computed as a function. You can see a is some kind of function. applied to ST minus one and also HJ by design they use a a represent some kind of feed forward network which uses those inputs. So that's how they they compute alphas.
(34:07) So alphas is not like parameters which you have to estimate on their own. No, they are sort of u outputs from softmax we apply to those uh e those alignment scored a function is some kind of network which we have to train s i minus one and hj will be inputs in order to get e i j. Now let me show you how it is done.
(34:33) So this function a is basically assumed to be or at least in the original paper it was this way hyperbolic tangent which is this way. As you can see it is just a fit forward network. So a is some kind of function basically of those inputs and we assume that those alphas will be soft max applied to those a because a is what I call e ultimately apply soft max to get alphas alphas will be like probability distribution sort of they sum up to one let's take 1% from first 50% from second 5% for next means 50% for second means second is quite quite important for
(35:11) current time t whatever it is. So this is how it is designed and that's why uh that's why it is actually something which is going to outperform regular recurrent network because we also take future into account and we do it for every time step. Uh so this referred as dynamic contextual contextual uh information.
(35:39) Basically we have this context vector which is now dynamic. It is not just single bottleneck. is dynamic. Also, it uses as you can see some kind of alphas. And you may argue, okay, capital T is also fixed. Not quite because alphas alphas are not actually parameters. Al alphas are some kind of functions based on.
(36:03) It means if you want next time to supply a longer longer vector, you can actually do it using the very same sort of setup, you can do it. you can apply to different uh to to sentence of different different lengths for example. So you can do it and that's how we can uh design it. Now here let me show maybe original publication. So it is uh right here and they propose this attention mechanism.
(36:34) It was published in 2015 neural machine translation by jointly learning to align and translate. So in this case um they basically say annotations I think they use what what the same as I did and then they say that we're going to do it this way. uh we create C vector which is like dynamic vector dynamic context contextual representation not like single rigid bottle bottleneck designed for everyone like to represent entire sentence.
(37:08) know it is going to be kind of dynamic now and it is also using past and future just like in case of out encoder basically you can see it is similar to out encoder right now but the beauty is that CT is not only for current time T like for basically not not in case of out encoder it would be for capital time T basically if I use C capital T it would be like out encoder almost but in this case no I say let's do it for every time and I do it this Okay.
(37:34) So now uh you can see that si is going to be function of previous s yt minus one and c. So now this how we do it. Let me see if I have something to else to tell from here. So you can read about this. Oh yeah this results. Interesting. Let me move to slides. Now we have uh these results which tell us u how it works in practice. This uh square this visualization is basically visualization of those alphas of those alphas.
(38:11) You can see alpha depends on two parameters t and capital t like t and t is a index of input and second t is index which corresponds to next layer. It means basically I can talk about questions like how third word or third token in my input relates with second token on my input.
(38:39) If I look at alpha third I can talk about second one in the input and third one in the output. Alpha alpha second for example let's say alpha third will correspond to third in the output and second in the input that's what they kind of what they what they represent they say the agreement on the European economic area was blah blah blah blah blah and then they translated to French and they say so in this case for example area is 1 2 3 4 5 6 7 words in the input you want to translate that.
(39:15) So if you have the same number of words in the same order, it must be basically alpha 55 is is maximal, right? But that's not the case. You can see that 1 2 3 4 uh maybe it is um it is five let me see 1 2 3 4 5 6 7 okay 7 in this case 1 2 3 4 5 zone is actually fifths that means alpha 75 must be must must be large right alpha 75 relates area to zone it allows you to already relate because this attention mechanism to relate for example fifths output to the seventh because you have this alphas. You have this probability to relate whatever you want with whatever you want. Basically
(40:02) that's why we have this you know kind of highlighted square here. It represents alpha 5, 7 area and zone they are related. That's why when we build translator using this type of approach we are able to better translate. We don't have to shrink into single vector. We have this bunch of alphas which are not by the way parameters. I want to mention alphas are not parameters.
(40:27) They designed as soft marks and those inputs already kind of uh uh networks outputs from networks. Remember hyperbolic tangent and we can see here also what happens like zone and your opinion then they have nothing to do with with each with each other. So this space is like almost empty like zero almost this alpha is not not interesting European zone or diagonal.
(41:00) Yeah, this diagonal 1 2 3 4 5 European 1 2 3 4 5 zone. They not related. That's why this alpha is zero. It allows you to sort of essentially capture semantics can capture this dependencies which are not not truly not truly uh in the same location in terms of time right different time locations. Now any questions about this this model attention mechanism? I I do just have a question about the implementation.
(41:33) So it looks like RNN was was an attention it has to be like um a separate layer because it does use the intermediate from from the previous layer. So you mean like this input? Yes. So it is not not just layer. It is actually two layers. Typically we use like a forward hidden state from from from well when we supply sequence of inputs in the same order and then h this different error means backward instead which basically means we supply input in reverse order remember by birectional this is idea then we simply concatenate those we
(42:11) combine them like glue together and we get this h is output from this birectional recurren to work. Okay. Yeah. So in my in my case it is simply two dimensional vector from simple layer of simple recurren neurons. Gen speaking it is not the case.
(42:35) We actually can use birectional which is much better probably and this H is basically concatenation of forward and and backward hidden states. Thank you. Yeah. So yeah. So in this order we get one h and different order to sort of different similar but different kind of layer of recurrent neurons and we obtain those hes and we call them differently they use error pointing to the left then we concatenate for every time step we take h which comes from this network and h which comes from second network where I use it in different order and I concatenate those.
(43:11) So it allows you to basically not worry much about also time flow in this case right. So we use birectional as before gives you more flexibility in case you your current network is kind of because H2 depends on the previous only right you can say what's the problem I also take weight applied to H3 weight applied to H4 so maybe it is okay you still have future but every age is function of previous ones only that's why we want to kind of have more flexibility and we say let's also sort of in parallel with this run a layer of recurrent neurons but input will be reversed together. It is called birectional.
(43:53) Then we concatenate and then based on those already long hes which is concatenation of those two types of hes we create context vector for every time t we can do it right now it uses index i. So i means like not time for the output in my case at the scope t on the board.
(44:18) So those alphas are soft max applied to E is output from neural network where I take S I minus one S tus one and also H J as input rate and then I get this E from where I can compute my alphas that's how it is done um any questions it was actually huge kind of advancement because we would be be able to enhance our recurrent network and it will be able to after this would be able to actually look sort of um uh at the sentence at the sequence uh let's say as a whole at once right almost like almost like our encoders do it but our encoders have this disadvantage that we have to
(45:05) basically shrink into single vector kind of rigid vector which is always same in this case it is dynamic it is much better it is already able to do it depending on your current location, right? Not to mention you can also actually supply different types of inputs. Could be longer, could be shorter, for example.
(45:30) So now let me see uh uh so this is my uh this is my uh representation. Yes. So this is my representation and uh let me uh this is just you know just kind of two example basically but this is how it is done this is how it is done so in this case we build this type of model you can see actually muscle on the board right it means it's going to be exactly same we going to create this attention layer right later we going to supply attention layer afterm we take LSTM supply attention layer and then after that this how it is done. Attention layer is done exactly as we
(46:09) sketched. So we do it this way. You can see what exactly we are going to do in this case. We're going to compute those alphas. For example, we apply soft max to vu. Vu is exactly my a functions. This v * this one. And you can see that um u is hyperbolic tangent just as we can see it over there. And um so this is a V transpose strate uh V and then also this U and we get basically this product of V * hyperbolic tangent and we compute alphas and then we get output and then we pass it further and we create this attention layer which we add to the LSTM. And so
(47:05) this is how it is done. Um now of course you don't have to do it like that simple. you can use maybe even something more interesting before right so possible but so the whole the point the point is we don't want to just take current uh h depend on the past we kind of do it in a way that we capture linear combination like with the sum of all the hes and it becomes already more flexible more powerful that's why we can use this architecture so any Any questions about this? If we if we use the this attention layer, this mean this this layer will be a quadratic because because it will
(47:51) compute for every step a combination with with all the steps. Quadratic you mean like uh in terms of number of parameters? I mean I mean in terms of number of steps. So uh um uh um so the way that we calculate these alphas um so for every for every step for every vector in the so we we're going to calculate the the um the intermediate output of the LSTM right so for every step we have a vector okay and then the attention the way the attention works we're going to multiply every vector with all other vectors and
(48:30) we we're going to wait these multip multiplications by by alphas right um CT is I'm not sure what you mean by other vectors CT is is weighted weighted sum of hes from all time steps if I look at particular time t my CT is linear combination of all the hes basically weighted sum of all the hes yes um it's like the way attention works I'm Mega I don't understand this attention layer.
(49:07) I assume that this is the same one we use for for transformers that where we take every vector and we multiply it by the by all the context is the same. Exactly. So basically this idea would be used in order to create transformers. Exactly. But transformers would get rid of recurrent network. They would say attention is all we need. Yes.
(49:30) And they would say we don't even need any any recurren network anymore. We have this recurrent network forward backward we concatenate and then the question is what's the point? If you use this attention mechanism let's just take those tokens and apply this attention. This basically idea behind transformers. Exactly.
(49:51) But they they create they apply some modifications to this idea. But the idea is exactly that. So in case of transformers we basically get rid of uh this recurrent network because it seems to be not as much helpful. They published in 201 17 the paper is called attention is all you need. Right now now I understand where the name come from. Yeah. Which means let's get rid of recurrent recurrent layer completely.
(50:18) Right now we have actually birectional recurrent layer. In my case, I have sketched only one. But you understand that we have birectional. I would have to sketch next to it another layer where a will be in reverse order. Then corresponding edges will be concatenated and I pass it further.
(50:37) But they say no no we don't even need those recurrent layers. They don't help much especially because we still have all this issues such as vanishing credit and so on. We still have to train those those pieces. Turns out that it is not actually um useful to have this uh recurrent layers. Yeah. My my initial concern is those attention layers usually are slow. So they will take long time to calculate.
(51:01) They should. Yeah. Because there lots of parameters. You can imagine that we can create now a context vector for every time single time step, right? So of course it is already something to okay to estimate in addition to what it was before. Yeah. Even though alphas are not exactly parameters. This is not like trainable parameters.
(51:25) You may it may look like alphas are trainable parameters. No they are not. There are soft marks applied to those is to this uh is which is basically output from this neural network which takes s and corresponding h. This is how it is designed. So now let me this is summary. We have like 116,000 parameters attention layer here. Okay.
(51:56) Okay. And then we say uh benefits we dynamically focus on relevant uh basically a part of our input right clearly we also wait all this inputs no we don't really focus on inputs on access because we still use recurrent network we focus on those uh representation those hidden representations hes forward and backward concatenated together But those alphas basically allow us to select what we want. That's why this this kind of mapping here.
(52:30) So it allows us to select if I want to know how to create a zone as a translation. It will know that it is very much related to area not only maybe to economic also somehow but area is important and that's why this alpha is large. So they visualize this results. This results do make sense right.
(52:56) So in N and and I don't know French but N is something response to in most likely right? Does anyone know French? Yes you're correct. Yeah 1992 corresponds to 192. So makes perfect sense right? That's how it is. Agreement a court and so on. So now we can focus on particular part and the input it doesn't have to be at the same time step clearly because we have this now capability alpha J.
(53:23) So we can do it even if it is of diagonal element. So that's why we can do it um and we can also uh take care of now long sequences because if there are some dependencies those alphas could actually capture this dependence. So we don't lose information as before in case of even LSTM when there is longterm memory even in that case we always lose information right if it is far away from current location um we can actually also apply to vectors of varian lengths formally speaking because those alphas are not just fixed parameters they results results like outputs from a subn network
(54:09) that the hyperbolic tangent And it means if we have to supply like longer sentence we can do it. We can basically apply same kind of architecture to different type of inputs essentially which also is clearly advantage right. Um because we can sort of translate this to different task. Uh well they also found that stability improved right.
(54:39) uh we don't have to have a single static vector anymore because right now we have not like out encoder where we have static single vector which is like rigid and specific single vector now it is time dependent context vector which is already much more helpful because it dynamically captures what you have in your text which is is better represented I believe in this in this uh chart so that's what we have And uh so now it is 9:07. So I want to start transformers.
(55:19) But maybe uh we'll do it after the break, right? Um uh let's maybe start transformers after the break. Uh so you understand the the idea behind attention mechanism I hope right? So this is how it is done and uh advantages are kind of clear you know basically this more flexible we can capture longterm dependencies and we can already for example perform translation might much much better.
(55:48) So next is going to be our transformers. Let's talk about transformers after the break. I'll introduce it after the break. So now we going to try to somehow get rid of recurrent part and whatever is
(1:05:04) left is going to be called transformer but transformer doesn't look like you you may think it is not like the the remaining part of course transformer is different we have to do some modifications to the remaining part in order to make transformers. it would be too simple to just remove it.
(1:05:26) Right? So let's see how we uh we can do it limitations of uh recurrent networks with attentions related to first of all let me say let me let me kind of read it through in the same order. Let me say that um um um oh let me say this way. So if you have a recurrent network uh with attention uh first of all we can't really transfer knowledge as as as easily right still so there is still a problem because when you try to transfer knowledge in that case it is still kind of kind of problem we can do some kind of tuning afterward but it is still not as as easy secondary
(1:06:08) because if you have recurrent part it is still quite difficult to run in in parallel it is In other in in other limitation we cannot really run it in parallel because of recurrent part means it is much much longer uh much more more difficult to train such networks.
(1:06:31) Secondary because of your current part we still have issue related to vanishing gradients the very same problem right it's difficult m gradient flow basically because of this recurrent part we still kind of still in inherit inherit all the issues related to recurrent networks uh so that's basically paral computations the problem gradient gradient vanishing gradient is a problem and transfer transfer knowledge is still a somewhat of a problem.
(1:07:04) Right? So uh so now uh we going to introduce a transformer which will sort of basically get rid of get rid of uh recurrent network part. So this is architecture of transformer exact architecture original transformer. We have inputs. In this case, we introduce embeddings as always. Input means like sequence. For example, we introduce embeddings which becomes sequence of vectors.
(1:07:34) And then we say there is soal positional positional encoding. Let's forget about this for a while. Later I will explain how that means. Then we have specific transformation which is called multi head retention. It is somewhat related to what you see on this on the on my boards but not exactly that.
(1:08:00) So we have some kind of transformation to create to implement attention mechanism. Right? Then we also have shortcut not connections in this case. Going to ask you did I did I explain what shortcut cut connections mean? Did we spend some time on shortcut connections in this course? I honestly do not remember. Okay, basically we take this input and add it just literally add it to the output.
(1:08:38) Turns out that it allows you to avoid issues related to W's being bin on very very different scales. The result this simple idea when we when we create shortcut we skip skip a number of layers and we take the signal and add it to the output as a result your network is now trying to actually model not the output as function of input but effectively it tries to model difference between your output and input as a function of input sort of residual that's why it is called residual connection so shortcut cut connections so called residual networks if you have time I can talk about shortcut
(1:09:15) connections and at the end let's not focus on shortcut connections I think you understand what this means we just connect it this way add it to the output and move forward now there is fit forward uh part of this network also shortcut connections in order to improve conversions and proof basically improve our ways W's are designed they will be more more or less on simal scale if introduce shortcut connections and this part is what's called encoder right encoder of transformer encoder we have to fill in pieces we have to discuss what is multi attention what is positional encoding but that's how it
(1:09:56) looks input embedding it means we create sequence of vectors embeddings we add something to the sequence of vectors to sort of capture position first position second position in a sentence third position in a sentence. It is just kind of kind of you know almost like uh arbitrary created you know indexes or vectors which are going to be added to our vectors in order to make them different from each other.
(1:10:27) So model can differentiate if it was like first position or second position or third position just to help it to help it with differentiating which position it was first talking second or so on we'll talk about this and multi head attention is quite important part you can see it has like three inputs one input we'll be we'll be creating three inputs to this multi attention then shortcut connection and hit forward part and then this signal goes further to is a decoder already and decoder will accept basically what is called output not because in reality it is input but shifted by one
(1:11:03) token we shifted and effectively we are trying to effectively we are trying to predict next to token that's why it is shifted in this case output is going to be soft max but we are trying to kind of say what is going to be next token And the one which is kind of coming next right and this part seems looks very similar to the encoder part.
(1:11:33) We have position encoding multi attention but in this case it is masked. I will tell why masked then the shortcut connection again multi attention fit forward linear soft max. So this is architecture of transformer quite actually simple in some sense right now I can tell you so you um kind of keep in mind at some point I remember reading a book they mentioned that um co costs to train this type of model is estimated to be around $4.
(1:12:00) 6 6 million of dollars around 5 millions of millions of dollars right so just to keep in mind it means of course in this case we assume and we hope that knowledge will be transferable and it is indeed transferable if it start taking pieces from this network we can already create so-called be or GPT right exactly it is exactly part parts of this network it is almost like out ander basically as you can see input and output kind of try to what it tries to do it tries to predict the next talking.
(1:12:33) Essentially it means you can train it on large amounts of texts and then you can tear it apart and you can take first part for example it is encoder and you create beard birectional encoder representation from transformer. So from first part this first part will be able to sort of uh understand your input it like humans like we understand input it will be able to understand what it is about basically and it can be used for tasks such as classification for example or something like something like u uh uh
(1:13:08) no for example it we can recognize what part of speech it is part of speech recognition and so on. And second piece if you again tear it like a part and take only second piece it will be part of GPT because second part can be used to generate text. How do we generate text? Basically we predict next.
(1:13:38) That's all we do right every time we predict next and uh this second part this second part is going to be part of GPT generative pre-trained transformer. GPT is nothing but generative pre-trained transformer. It means if you look at this out encoder and you take encoder it will be B and decoder will be part of GPT generative PR transformer you supply your input for example question of some kind and it will then generate for you like next tokens as answer essentially to your question. That's how it works.
(1:14:11) So you can see if you this cut it cut it there is also some input from your encoder input is not going to be used just cut it and say there is no input you don't don't need it if you use this part for GPT then input will not be needed the whole thing is needed to train it so we can train all parameters and then we can take pieces from this transformer to create different models such as beard and GPT so now uh let Let's talk about some pieces.
(1:14:41) Multi-headed tension is important important piece. This is how multi-headed tension looks. If you try to take it apart, so essentially it looks this way. We have Q, K and V as inputs and we get this type of network, right? And then multihood attention means we take it we take many such networks like in parallel can see one two three for example and we take output here and concatenate and get finally output from this network. This is what multihood attention is. Now let's talk about Q K and V.
(1:15:21) These are quite important concepts Q K and V. So Q K and V. Q is like quy K is for key and V is for value. So let's now let me let me let me think how to present it. I think I will just look at some examples. So it it does make sense for you. So this important part which is attention itself how it is done in case of transformers.
(1:15:54) So we're talking about transformers, right? Let me take some example maybe um let's say well any example clouds drift across blue sky for example this way by By way in case of uh our transformer we use tokenization we don't really use like stemming or this kind of things limitization we just use tokenization but tokenization is is based on sub words for example if you have running there will be two pieces something like run and in running so it kind of knows that this is kind of a continuous form righting running so it takes typically
(1:17:01) take subwords cut it into subwords and subword is going to be token and we don't apply any kind of not just cut it there is specific mechanism which allows you to kind of effectively cut it into sub pieces but it is not like for example just words and limitization no we cut it and we try to keep information about what format was like running so we want to keep in run and also ink for example That's how it is typically done. So in this case, unit of information is basically sub word. So so we keep that
(1:17:36) in mind. In my case, every token is basically word. Now I'm going to ask a question. If I focus on word sky, for example, I want to ask a question u which word is important for sky kind of right. So in this case I can say uh if I if I look at sky this is my my let's say sky is my my query query right now.
(1:18:09) So it means I look at sky let's say I look at word sky right now. So I want to know what's what sort of important what information is important for word sky. Maybe first you know what let me even even say differently. So typically in case of natural language processing we use embeddings right.
(1:18:41) So every token will be will be embeddings of some sort will be let's say one 7 and so on or 1.2 it is like embedding of word sky. This way also I'm going to have embedding of of word blue. and so on. Now let me ask you a question. If it happens that I have the same words in different contexts and I use embedding the way we use it up until now is embedding going to be same for word bank for example let's say have example where I have like let's say let's say bank bank account for example bank account bank account and second document is going to be river bank. Remember we discussed how nice it is to represent tokens by
(1:19:40) embeddings. We spend time on this. You also did assignments. Now if I ask you if I focus now on bank and also on bank over there can I apply tokenization will tokenization be different from bank in this context and then in that context or not? No, they'll be the same. They will be same. Exactly. This is limitation. So embeddings assume that we have the same representation.
(1:20:08) But we know that it is not the case. Context matters, right? Bank account and river bank those banks should be different. But in my case, they will be exactly the same vectors. So basically this is issue. If I now say bank is like.3 and so on 1.8 8 in this case and this is.3 and so on 1.8 in that case and they exactly same.
(1:20:39) So they are same same means it is actually bad thing because we want to also kind of you know incorporate context in some way. It means what we are trying to do now we are sort of trying to let's say uh simply we are trying to slightly adjust embedding that's what we are doing in this case how to just embedding we look at sky in this case we basically say query is going to be let's say Q is going to be about sky right we are talking about sky so what is of sky it is some kind of representation we don't take embedding as is Q is going to be so
(1:21:19) typically Q is going to be let me see Q is going to be my embeddings. How did did we denote the embeddings? Like let me see if I have some formals. Um the embeddings embeddings. So maybe maybe I can look at original publication so I can later refer to it. So in this case we have uh embeddings which are I think we don't have a letter for embeddings.
(1:22:25) Uh now we can introduce some letter for embedding. Let's say H for example H. This H vector is embedding basically right and then we multiply it by the WQ specific matrix of parameters related to my transformation from embeddings to my Q. In this case it is in matrix form. Let me say it is in matrix form. So H is my embeddings. H is basically matrix based on this embeddings.
(1:22:52) And then I multiply it by matrix W index Q to get Q. Q Q will be like alternation to embeddings will be different type of embeddings right Q could be square matrix I think in the original in the original format in the original paper let's say original original transformer would have uh let's say H was I mean embeddings let's say embeddings were 1 28 8 dimensional embeddings and uh as a result my cues uh would have to be maybe different maybe not depends on how I choose W matrix right I can shrink size a little
(1:23:41) bit nowadays they shrink but originally it was actually square matrix it means my WQ was also 1 to 228 by 1 to 228 it means Q is essentially the same lens. If in this case I have 12 12 12,000 dimensions Q which now kind of new representation of my sky is going to be is going to have also 12,000 dimensions.
(1:24:11) I do it this way because I want to ultimately adjust my representation. Right? So in this case across is also something and so on 18 and drift is also something and clouds is also some kind of embeddings. So in this case this is related to embeddings. Those vectors are related to embeddings. which I can introduce to represent my tokens and then I say I ultimate goal is going to be let's try to adjust my embeddings based on the context which I have in my case right so and I say let's look at what sky I fix sky and I say my goal is to adjust embedding what should I do essentially I'm doing like before I'm
(1:25:05) I'm looking I'm I'm I'm trying to estimate attentions right so sky attends to clouds they are related we kind of know and we are trying to see if there is relation let me sketch it this way and that's how I'm going to do it I'm going to say is there a relation or not so in that case I'm going to uh introduce another matrix which is key let's say key matrix is going to be my embeddings multiply by W already K is K matrix.
(1:25:45) Now it is in again in original transformer case it was also 12 to28 by 12 to 28 doesn't have to be this way but how it was. So and I say if I now look at sky and also I look at word cloud cloud clouds has some embeddings but I already used embeddings multiply by some kind of ws and I have key query is related to the word at which I look currently sky it tends to work clouds and I look at key which corresponds to clouds and I simply compute that product right now it is like k related to already clouds this way. This way. Not to be precise.
(1:26:27) It is that row to be precise. It is going to be like in this case column in this case in this case row in this case also row in order to get appropriate like dot product. No they apply like transformation basically in this case right Q * K and we get we get a scalar as a result.
(1:26:56) So Q is again Q is like not original embedding but some kind of transport embedding Q is also some kind of vector in my case even of the same length because I assume that W square doesn't have to be same W can be not square can be rectangular it means Q would be shorter representation it is often also used and it becomes new representation it allows you kind of flexibility because while W you can decide how to choose Q is kind of related to the current toward you are looking at it is like what exactly are you thinking about okay I'm thinking about like sky and I want to represent my sky the security basically
(1:27:29) and then I say let me loop over all kinds of all over all kinds of over all kinds of words and then I say let me look at drift as well and I get by the way u this quantities which again is like a vector a new vector which represents my my my token and this is also a new vector which represents my cloud but if I look at sky I use Q for sky because I'm I'm talking about sky right now and cloud is only to which are sort of I kind of refer right sky its to clouds in this case I use K for skies that's why two kind of matrices two representations Q related to the one about which I'm talking about right now
(1:28:17) which one I want to transform and create upgraded upgraded representation and K key is letter to the word which sort of helps me adjust my representation. That's that's how I do it. And I create this kind of score, right? Then I also say let me take Q related to drift and times K I will say transpose.
(1:28:45) So it is clear why it is like that product related to drifts drifts. Then I continue and say Q I'm sorry this is Q sky I focus on sky. Q is always related to sky this way. Q sky multiplied by K transpose relative to across this way then Q sky and I multiply using that product by K which is related to already my blue word this way and I'm going to have a number of different different kinds of number of different kinds of scores which I can use and decide which word is more important which is less important in order to understand how to adjust my representation of sky how to adjust my embedding to be precise I have to also
(1:29:43) use skew of sky which attends to itself so I say sky tends to blue okay sky can attends to itself as well sky can attends to sky this way and I compute those scores. So everyone it we're doing it this way and also sky tends to sky this way we compute all the scores.
(1:30:16) Now ne next we are going to take basically this scores and compute soft max out of the scores like it is like 10050 uh something else something else and we based on those compute compute uh using soft max compute probabilities and we will know that actually clouds for example contributes ultimately is going to contribute when we apply softmax is going to contribute like maybe 50% to sky right drift will be only let's say 10% across will be only 5% and blue will be no let me say blue will be like 50% and maybe clouds will be let's say 30% and sky itself will be remaining 5% for example maybe let's say
(1:31:11) 20 then 30 that is already 60 80 plus something from here and let's say let's say 10 as well. So ultimately we we are trying to recover this probabilities from from this type of quantities which we call scores. We create those scores on those matrices.
(1:31:36) Again how do we create matrices? We simply take embeddings H multiplied by vector matrix of something of W's. We create Q. Similarly we create K which is like old key. And because this is used when we want to get information from the words which inputs current words and then let me maybe open slides. Then we create this quantity which is which is going to be designed this way. We also create a value.
(1:32:07) So value is going to be also my embeddings multiply by W subV which is W subV again could be square matrix could be known but could be square matrix let me as example use square matrix this is like original dimensions what they use nowadays is not known that is not disclosed now you can assume that is somewhat similar maybe more a little bit right so but second dimension dimension of actually the representations Q, via V could be less because W has W's have potential to be like rectangle matrix. We can shrink a little bit dimensions when we do this procedure and we do it
(1:32:49) this way and then we say let's apply soft max right but before we do it we actually in every case divide by dimension square root of dimensions of K matrix in every case you can ask why do we do it this way so basically we don't want to be sensitive to what dimension of K matrix is similar what Q matrix is and that's why we divide is related to kind of probability theory.
(1:33:23) Remember like average like related to variance, right? Variance of random variables is going to be like sum of random variables essentially. And if you assume that u there is like sum of specific number of random variables in order not to not not to get large variance when we get more and more random variables we want to kind of rescale it. That's how we rescale it. And then I say I apply soft max.
(1:33:43) No, it means soft max basically is applied as usual to this quantities. So let me say apply soft max to every case this way soft max soft max and then I multiply by v. So in this case I know that clouds somewhat contributes to my sky. it will tell me how much I should adjust my embeddings to make it more kind of uh reasonable in this context. That's why I multiply by V of clouds.
(1:34:19) Clouds we don't actually know what W's will be but we kind of keep in mind that it is presumably going to be in a way that V is like what a value of clouds is. So it is kind of value which I should take from my clouds. I should multiply it by this weight and then I know how much it will contribute to the to the change which I need to impose on sky.
(1:34:50) So same I do it here soft max then I multiply by v of already drifts and so on soft max then I multiply by v which comes from across and last one is next one is same soft max three which comes from blue and next one is soft max and then V which comes from sky itself because in this case sky tends to sky and that's how we how we create all this basically so this 030 for example in first case so it is clear this specific uh quantity ultimately is going to be exactly as I sketched 30 right because clouds uh will kind of input in quotes my sky
(1:35:56) in this case uh with weight 30%. That's why it is 30 and other ones also sketched in this case and then I say let me take essentially sum of all these things well no kind of weighted sum effect actually of of this and it will be how much I need to change my embeddings uh any questions about this so this is what's called attention yeah if I understand it correctly so we have three uh different embeddings for the same word is that an more So, so you can think this way. Yes.
(1:36:33) Essentially we have embeddings H and then we pre multiply those embeddings by different W's to get as you say different embeddings. Right? We don't call them embeddings. They are called cur and values but they are sort of different representations of very same tokens. Exactly. So, so the HQ and HK and HV those are fixed matrices.
(1:37:10) We multiply every word by or every embedded by or do we have them uh do we have them per word? um we we so so what we're doing is first of all they are not fixed one they will be trainable during during optimization right those will be trainable but but we design work in a way that we say uh when I want to know how much what representation of sky to take when I want to adjust representation of sky it is Q I'm talking about Q right now query but when I say which different token is important in this case or you say sky tends to clouds okay sky tends to clouds in that case from clouds I don't want to say take take again I want to take some different matrix which is coming from key from k it gives you
(1:37:53) flexibility when I refer to sky in order to adjust representation of sky I use when I refer to clouds to see how much information from there I should take I refer to key because it it comes from clouds already yeah but let's say that blue is my is going to be my my word of interest. Would I would I multiply it by the same WQ that I will that I applied or that I multiplied for sky? So is is WQ fixed uh through sky and blue if both of them were were so so Q sky is basically fixed.
(1:38:30) So what happens is yeah Q sky is fixed. What happens is you can think that Q sky is basically different representation of my sky. So it's a different embedding very much different embedding kind of people don't say it this way but yeah you can think this way because essentially we multiply by different matrix will be different potential of different size because W doesn't have to be square matrix exactly I have I have another question why do we have three why not what would happen if we just say only we're going to keep the query and the key why do we need query key and value
(1:39:01) yeah we kind of assume that we need to provide network some flexibility right we say in In this case we one thing is when we talk about like a word which we want to kind of uh improve uh representation of so in this case when improve representation of sky that's why we use skew for in that case when we talk about sky but we want to give some flexibility and say if I use different words what kind of representations of those words should I take same cues or same embedding looks like note I want to give some
(1:39:39) flexibility that's why I introduce new they call key right key which is like different representation of the very same tokens basically right but they are different because they serve different purpose they serve a purpose of telling you how much you should kind of see how they are related to how clouds impact your sky or how much sky tends to cloud so it is different one yeah have so again I'm I'm quite sure um maybe there would be no um no um no specific answer.
(1:40:16) So why not two and why not four? So A and V maybe I will also use Z to give it to give it more flexibility. Uh which one to use? I'm sorry I didn't hear that. I'm just I'm just really wondering why do we have specifically why the authors uh just came with with the three matrices K Q and V4 for example let's think this way we have like a task of trying to adjust representation so we understand it we look at sky now we say we have like different players in this case one player is sky the one who who I'm trying to to adjust. Second player
(1:41:01) is clouds who helps me to adjust it. Now you are saying why not to use same representation of sky when I say I want to adjust sky and representation of clouds when I when I use it which who helps me to adjust and they say let's use different different representations if clouds is used in a way that helps to adjust representation of sky maybe representation of clouds should be different they call it key that's all I appreciate this part because this part also clarify if I if I if if cloud is going to the main word and sky is going to be the the associated word. So now this way you
(1:41:42) you will end up with with two different representation. So I understand I do appreciate using uh I do I do appreciate what you just explained and makes makes perfect sense. But my question is why do I need V um in addition to Q and K? And why not why not if if V if if V is is is good addition why not use one more like a like a Z for example like one more embedding one more matrix so you concern why we use V in this case now right? Yeah.
(1:42:21) Why not to use what? Embedding itself maybe. No, no, no. I mean I meant what happens if we drop V entirely. No, then we have probabilities. We want to the thing is we want to actually create ultimately you want to adjust embedding, right? It means we want to add some kind of values. Those soft marks will be just probabilities.
(1:42:45) You don't want to add probabilities to this vector. V is like vector. V is like almost like embedding basically of different kind already. So we and and this is just a scalar 30 ultimately this multiplication will mean that product and I get just a scalar and in order to know what kind of vector to add to my sky ultimately I need to take a weighted sum of some kind of representations of all the words.
(1:43:10) Yes sir. And I take V. You could take maybe embeddings but they take V. Now there was a point that actually original paper uses 5 5.12. I think I missed it but what I was keeping in mind that this is actually not original paper. This is related to GPT GPT3. So please check I believe 12 to28 is GPT3. That's why that is is different from the publication.
(1:43:38) Yes, publication could be 512. I I don't know. You can check yourself. But uh GPD3 I'm kind of sure was using 12 to28 going forward they would not release what they used right that's why 12 228 now as a result we can take some of the things again uh so I I hope it is clear that H is one representation Q is some kind of different representation of potentially same lens or maybe even different lens K is different representation.
(1:44:15) V is different representation because we have different kind of players in this case. Who is going to be adjusted? Who is going to help and what kind of representation of that helper to use? That's what it is representation of the helper. Key is basically uh how much from this helper should be contributing to the probability and Q is my current uh current representation of current token which I'm trying to adjust. That's how it is.
(1:44:46) They called cur key and value. Now we say let us take some of those and we are going to get uh attention right. So well everyone is attention basically everyone is is attention. Let me say this way. So I'm going to take some of of those right some of those and I'm going to get attention. So the result is attention. Can you see what I'm writing? It's a bit cut out.
(1:45:19) Okay. So this is like basically fire them together. What this is what we call attention, right? Attention. Attention. Um no maybe uh so okay let me say attention it is like Q matrix no matrix because if you take like all the similarities becomes matrix similar if you take like all Q representations becomes matrix that's why Q key and value and this is going to going to help us adjust our embedding now how to adjust embedding let's see how to adjust embedding we compute uh so-called attention which is also called head in this case right and we um let me
(1:46:09) let me be careful in this case so this is attention and we we actually in case of single head can use attention as is and we are going to say the following so we are going to say the following my representation of sky let me Say next my representation of sky. H is like my embedding.
(1:46:39) You understand my representation of sky is going to be essentially original representation of sky. No why I update it right updated this way representation of sky. And then I say plus uh attention. So plus attention. So attention of of what dimensions? It doesn't have the same dimensions as V because V is like a vector in every case is some kind of vector, right? Whatever it is, it means uh essentially my attention is a vector.
(1:47:10) It is very possible that my vector my attention is different from H because V doesn't have to be of same dimensions dimension as H because V is H multiply by W which potentially is not square matrix. In this case we take attention essentially take attention let me say attention and we multiply it by some kind of different matrix W they call W.
(1:47:35) So this is square matrix if like in my example generally speaking this W is not square matrix to kind of make sure that your dimensions match and we get updated representation of sky. So you see what happens my new representation of sky is original representation of sky is original one.
(1:47:56) So there's original one uh which is 0.1 7 and so on 1 1 2 then plus attention which is sum of all the things which means basically it is like 30 * v representation of clouds right the view of clouds and so on and so on and so on v is going to be vector but it could be of different length than my h that's why multiplied by a matrix to make sure my dimensions match and this way I get basically new embedding which is sort of already context specific right that's all that's all it does very actually simple model in some sense as you can see I have another question if or I can I
(1:48:42) can wait for other times yeah yeah you can ask what happens if in in attention instead of passing Q so whatever you wrote on the board here if we if we pass just instead of passing Q KV V we can just pass any of them like QQQ or KKK or or VV. No, clearly it means you have less flexibility, right? So again we we have we have like for example sky uh as a different types of you know players in this game.
(1:49:17) The one who is being adjusted and the one who is helping to adjust the other ones. Okay. And also a value which we use here. value. This value could be embedding maybe or maybe not. So we use value which is another kind of representation of the same of the same token. It it gives you idea that you have like they call it query they call it you know key and so on.
(1:49:41) Just think that they play different roles. The same same token may play different roles. It helps you adjust your representation of your current token or maybe in this case we're talking about current token that's why we take Q because we're talking about this token and so on. So this is this is how it is done. You can see on the screen that there is also multi attention.
(1:50:05) So multi attention means if you look at this sketch that there should be multiple different attentions because maybe we are not happy with one way to do it. Right? Even though we have a lots of flexibility already, we have like www. we have different kinds of you know representation of the same tokens.
(1:50:25) You understand that H is embedding. Q in every case is another representation. K is another representation. V is another representation. But it is still like not completely flexible because they are sort of uh interrelated by matrix W. So it is not like completely flexible. And they say let us try to actually use maybe different type of uh different type of attention mechanism in this case and adjust it slightly differently.
(1:50:54) And they say let us in parallel design multiple attention mechanisms right multiple attentions like in parallel we can even train them in parallel. That's the beauty of this approach. And uh if you want to do multiple of those then there is a question how to make them different from each other. And solution is simple.
(1:51:13) They say let us here not take attention of Q K V but let's take attention of Q some kind of W K some W V some W specific to your attend to to your head head is I multi head attention you have number of heads they are in parallel over there and we say plug not Q KV I take take even more matrices is over there take Q * another W which is specific to your head K another W specific to your head V * another W speed to your head and you get different attentions Q * W case the time W V * W's gives you even more flexibility.
(1:51:50) You can run multiple attentions in parallel and then you concatenate those things together, right? You multiply by W to kind of make again match to your dimensions which you have to have to have ultimately because you ultimately want to adjust your dimension your embedding representation simply concatenate that's why they say concatenate from one from second from third in order to make them different you plug different inputs here multiply by different W's specific to head any questions about this.
(1:52:31) Okay. So now uh this just example. Let me let me uh mention one more thing. So this is just example. Okay. This just example. Uh you can maybe take a look later this example. You have transformer block. You have all this u uh uh dropout layer. You have this uh uh normalization which was uh sketched over there.
(1:53:06) This normalization you have this um uh let me see uh dense part is related to specifically this part. a dense part u multi head attention we just take multi head attention from this uh layers and it means we don't have to actually actually uh code what's happening here we can just take it from there that's why it doesn't look like crowded and we have this model but let me actually move back and say there is one interesting also thing which uh is related to this um positional encoding.
(1:53:47) Let me ask you a question. In this design, if I switch towards drift and clouds, how will it impact my attention? Nothing. Nothing. So, it's going to be same. So, this model is nice, but it doesn't know about the order. So, order is kind of not important, which is not nice because order clearly must be important, right? when we try to specially generate text.
(1:54:14) That's why since this model doesn't have any it doesn't doesn't take into account the order we have to somehow incorporate the order. This is what positioning encoding is about. And it tells me essentially tells me let us um tells the following. Let us for the first position add one type of vector to the second position.
(1:54:35) Let's add second type of vector for the next position next type of vector. It's important that they are different. So they sort of help model differentiate between first, second and third position. This is what is happening. There are different types of positioning positional encodings. If you move back to the what's happening publication you can see that there is a positional encoding which is done this way.
(1:55:10) So they tell let us use this sign and cosine. Now what it means position this position means first position second position third position I means entry to my vector. So it means basically if I want to adjust with position coding what happens with the first one I have to say plus plus and let's see how much I should add.
(1:55:35) um I should say if I let's say if position is equal to first one let's start from zero so if position is zero what do we have position is zero let's say I is supposed to be the first one right so I starts also from zero and that means the first position of my vector which I'm trying to add we'll use position zero I is equal to zero it means sine of zero okay it will be first entry is zero I will just before doing all the things I will I will adjust my embeddings. Now if my position is zero but I is already
(1:56:12) one it means I mean it means I have to this second case this cosine case I is equal to is not important what it is because position is zero cosine applied applied to zero gives me one in the second position and so on and last one is probably zero and I add this vector.
(1:56:38) So I kind of artificially add some kind of vector which depends on position. Turns out that it helps the model differentiate which position it is. In second case I will add something different. No it will be if you compute like s cosine it will be already different things right. You can compute and see what those positional embeddings are and I can add those.
(1:57:00) I just literally add those things to my current representation. Y embeddings and they make them different and somehow model will feel much more confident because it will already be able to somewhat differentiate between positions and this will be like new embedding. Basically my age now becomes new embedding and the mass the rest is exactly same.
(1:57:24) I add this positional encoding positional encoding, right? And I treat this original embedding plus positional decoding as new embedding essentially and mass is same. Uh any questions about this? Now there is also mask. So mask. So what happens is actually in this case uh input is going to be uh transformed to embeddings I add positional encoding I use multi head attention mechanism it means essentially I'm going to adjust my representations then I use fit forward and so on this is kind of end of my encoder this way what
(1:58:06) about decoder now in case of decoder it is kind of assumed to be used for prediction of next token basically. So in that case uh my output which I supply as input actually to this part my output which I call output is going to be simply shifted to the right shifted to the right shifted to the right don't know why to the right kind of want to shift it to the left um oh no to the right to the right so I want to kind of predict next one right so if I shoot it to the right kind of point to the left. Cloud drift. Next one better be in the third position
(1:58:49) better be blue because I want to place clouds drift across blue and then I want to predict it. If I want to predict it, I want to shift it okay to the right. So it is not visible yet. Probably. Yes. So shift it to the right. So sky is not visible yet. I'm going to predict it. So shift it to the right.
(1:59:13) Basically I shift it in order to have corresponding kind of uh tokens uh if it is missing I want to predict it uh for example and there is this kind of masked masked point what is marked point so essentially what happens is I'm saying whenever I try to predict next talking let's say I want to predict a cross drift I want to predict a cross I don't want actually a cross is potentially being influenced by clouds, drift by cros, by blue, by sky because all of them will impact it. I I use all this attentions, right? A cross will attend to clouds, will it tend to drift?
(1:59:52) Will it to cross? Will it tend to blue? Will it tend to sky? It is okay for my input on in the left part for the encoder. But it is not okay for the right part because in this case I kind of presume that is used for predictions. It means in that case I want to introduce mask. That means I basically place zeros over there. Zeros.
(2:00:17) Now what does that mean? It means when I construct Q * K those K which comes from whatever comes in front of me and from the future should be made made zero. That's what mask means. They should be made zero. So a cross does not attend to blue. A cross does not attend to sky whenever I apply it as input to my decoder. That's all mask means in this case. Any questions about that? Dr.
(2:00:41) Kroskin, if you're doing if you're adding masks for every sequence, that's not computationally expensive. No, it is just zeros, right? You just basically zero them out. It shouldn't be. You cannot do it on the fly, right? Now when you do this computations you say basically if you talk about let's let's say sky right Q of sky when you compute these things you loop until sky occurs and then you don't go further if it wasn't sky if it was a cross you will do this procedure for across clouds across drift across across and then you stop you ignore the last two that's what mask means Okay, that sounds sensible. Okay, thank
(2:01:31) you. Yeah, we don't need last two if it is like across across across across across the last one which you want because a cross may attend to itself. It is okay but a cross should not attend to blue in that case. So we drop it drop one before last one drop the last one add three together and we do it this way because we actually we don't just drop last two lines actually because if we drop plus two lines they will probabilities will not add up to one.
(2:01:58) So it means we actually drop before we apply soft max clearly right so probabilities must be adding up to to one it means we first we drop and then we apply soft max. So total probability will be like one. Uh okay. So this is how this is designed. Uh we can run things in parallel multi head attention is is is really really helpful.
(2:02:25) We can handle long range dependencies is because of this approach because we if there is some kind of dependence between sky and clouds even even if they far far apart then nothing prevents transformer from learning this dependence and using it right in practice even if it is like far far away if it is like long text you still can use it. So of course it is very expensive to train and so on.
(2:02:48) And from this transformer we can use parts. One part will be called beard. Second part is called GPT. Generative train transformer. Beard comes from encoder. GPT is basically decoder. Uh and uh let me ask if you have any questions. So I hope it's clear how it is designed. Uh oh yeah this is foundations for whatever we use nowadays such as ch and so on. Uh very simple actually.
(2:03:22) So okay let's let's now stop. Thank you. Good night.