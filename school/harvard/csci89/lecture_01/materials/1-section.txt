(2) 89 day1 section - YouTube
https://www.youtube.com/watch?v=1b3fjrDHFRc

Transcript:
(00:01) Hello everyone. Hello. Hello. Hello everyone. Hello. Hello. So first of all I would say that usually on Fridays we're going to consider some
(01:13) kind of question examples but today since we didn't uh not quite finish one of the part of the lecture I want to spend 15 maybe to 20 minutes talking about cost function. Let's basically finish what we didn't finish last time.
(01:34) I kind of didn't want to really go deep into this details, but nevertheless, let me try to briefly mention what I mean by cost loss function and also how typically algorithms will minimize cost function. So cost function, loss function, cost function. First of all, let's talk about a loss function. Let's assume that we have some kind of data set. So we talking about a loss function.
(02:06) Assume if you have some kind of data set it means input maybe multi-dimensional and also output let me sketch it this way as if input was basically one-dimensional in reality X is multi-dimensional we understand it output formally speaking could be also multi-dimensional generally speaking sometimes it is 1D sometimes more than 1D so it could be whatever ever basically and we have those couples right in reality it's not even couples because x is multi-dimensional y is multi-dimensional potentially so we have
(02:39) this points in this multi-dimensional space and our goal will be to fit some kind of model which means to fit some kind of dependence let me say I want to fit something here this is model in our case if you use for example neural networks it will be basically exactly representation of neural networks neural networks is what it is some kind of mapping right it means this curve which I sketched is neural network for example it it doesn't have to be neural network by the way not to mention natural language processing doesn't have to be
(03:11) only used with neural networks it could be used whatever with with whatever model you want that's why it doesn't have to be only neural network let me pretend for you know in this example that this is neural network for some for some specific choice of parame parameters W's I have some W's. It means I can basically load my mapping from X to Y.
(03:36) Now in this case you can see notations on the screen. In this case I have uh basically I have a a lowerase M number of points. Right? So I I assume that I runs from 1 through lowerase M. It means I have M observations and then I say let me take particular point X I. I runs from one through M. That means I have M observations.
(04:12) Let me take particular point. Let's say this one. Let me take uh this one. X I X super index I again I could X itself could be multi-dimensional X could be vector then point on my curve which I sketched is basically prediction. Clearly my network will produce this type of output. Prediction is denoted by Y hat.
(04:38) So this is Y hat it is of course function of parameters function of input as well. Now I can say y super index I to emphasize I'm talking about particular point fixes ws are fixed so I know that they are fixed yhat of course also depends on w as you can see on the screen now what about actual observation let's assume this is actual observation which I can denote by y i so that's what I have actual observation and also prediction based on my model whatever it is new network for example then I and compute some kind of distance here. So this is some kind of distance. This distance is exactly what I call loss
(05:21) function for particle observation X I. I have output I have predicted output distance between those two is what I call a loss function. So this is exactly what I call loss function right a loss is based on particular input XI. So I'm picking what particular input capital L super index I of course it depends on W's as well that's what how we do it right let me say formally let my output yi as a function of w's w's potentially could be a vector right be defined this way yhat when I plug my x i and Also I'm I'm I'm
(06:11) given some kind of parameters W's. So this is my output. So this is what I what what I what I denote by Yhead. Yhat is output. Y as a function of X and W is exactly my curve basically right. Output will be also denoted by Yhat with super index I. That's what we have as output. Now loss is going to be defined as some kind of deviations some kind of distance.
(06:41) It means li I as a function of my W's as well. Now if I twe W's I have different loss. Maybe we I get worse result better result. So it's going to be by definition some kind of function. So this is some kind of function. Basically we have to study what to choose for the loss function. This is something we have to decide up front. Basically depends on the problem.
(07:07) If you do like classification, you have to choose one function some kind of cross entropy. If you do prediction, you choose different function maybe some kind of mean square error for example, right? Square error function. So some kind of function we will talk about this like in a second which is a function of my first of all predicted value as a function of W and also it is a function of actual observation yi this way.
(07:37) So that's what what we mean by loss function just exactly as we sketched some kind of distance in this output space right between pred values and actual observation some kind of distance you can think about examples what kind of examples do we know so we have uh first of all squar error let me say first example squared error Let's assume I have capital M.
(08:14) So capital M is basically dimensions of my output space. Y in my case is one dimensional. So it means M is one potentially Y could be two dimensional, threedimensional, multi-dimensional, right? If I do classification into thousands of types of images. For example, m is y is already th00and dimensional. In this case, I say as an example, let's assume that m m is one.
(08:41) It means my y is one dimensional. Now, let me say it means by itself output is simply 1 d. That's what I mean by m is equal to 1. So, how do we define squared error loss function in this case? Now we seen that we say that L this function which we have to choose is a function of Y hat so it is Y hat I don't have to use any kind of indexes as W's so we understand it depends on those but in this case I just want to simply define my function so yhat and second one is y in that case it is if it is squared error I simply say yhat minus y squar squared deviation
(09:25) basically So popular choice squared error. Second example which is using uh so-called um absolute error right this one is defined as follows absolute error let me say capital M is one as well it means exactly same case y is one dimensional in that case l is defined and simply as absolute value between yhat absolute value of the difference between yhat and y this way so that's how we define it any questions about this so that's how we define loss functions I think I got a question if you so I just want to make sure I understand that m equals 1 it says that the output is a
(10:20) scalar so yes yes output is a scalar exactly so the output could be like 10 for example example or you know cuz cuz 10 is also a scaler like am I am I yes if your output is just a number 10 15 20 52 it is one dimensional case 1D 1D is not like single number 1D means line right basically 1D means like literally this type of line for wise you can get 50 you can get 100 you can get uh 10 right So this is a one-dimensional case. It still has infinitely many numbers in it, right? It is one D case, but it is not a
(11:06) vector. That's what I mean. Oh, okay. So M is like Yeah. number of dimensions and stuff. Number of dimensions. Yeah. So yeah, next example. Let me say example three and also three and also four squared error when m is equal to two. It means y is already two dimensional. We want to predict vector.
(11:32) We sometimes want to predict vector because what because maybe we want to uh for example do classification problem. We have two dimensional output maybe. So in that case uh not maybe classification this is a bad choice actually to be precise but maybe you want to do some kind of prediction of a vector.
(11:51) So very possible we want to make prediction of two numbers right away. So in that case you for example you're talking about like uh not just stock price your stock return you're trying to predict you're talking about stock return of some kind of portfolio maybe in that case I mean you want to talk about the return of every particular stock and your portfolio you have portfolio you have 100 stocks in that case your y could be 100 dimensional vector let's say two stocks you want you want to predict two stocks at the same time using same neural network so very possible Well, in that
(12:25) case, we basically say that you want to have you want to have uh uh just as you can see it here, right? You want to have this type of uh uh definition, right? So, it is like a distance between uh two vectors squared. Now if you want to write it using component wise approach it means using pythagorian theorem it means yhat first minus y first component squar plus y 2 minus y 2 squ again as as you mentioned that means it means y in this case is y first y second right two dimensional this y is not a scalar it is already two dimensional So basically output is two numbers. I want
(13:19) to predict maybe two two two stock returns at the same time using same neural network. I can easily do it using the same neural network. Right professor. Yeah. Sorry. How do you draw that in uh instead of like this uh uh y and zed that we have right now? How do you draw it to visualize it? to visualize it now pretty much uh same but you have to um say that output is two dimensional right it's like maybe if you know about complex numbers for example it is difficult to visualize it right but you can kind of try to imagine that what's happening in this case you have like um
(14:02) basically you're saying what if I have mapping from let's say two dimensional space to two dimensional space Right? Uh people usually don't try to visualize it but if you want. So basically you have two outputs at the same time and you have two inputs let's say X first X second.
(14:26) What happens is for every single point in this space X1 X2 and two dimensional space for example you have to produce Y first and also Y second. You can try to visualize that there is like point which first of all corresponds to height along y1 and also corresponds to height along y2. In this case I basically sketch sort of point in four dimensional space.
(14:53) Now you can equivalently say if it is confusing you can say basically I'm talking about two functions. Maybe this is the best way to visualize x first x second and also x first x second will produce y second. Probably you should visualize every component independently, right? So y first and the same network for the same input will produce me y second. It means one surface here and another surface over there. This is probably the best way to visualize it.
(15:25) But if you if you're able to switch on like imagination, you can actually even present like this mapping as a point in four dimensional space. If if you can do it, if you cannot, you can do it component wise. It might be something similar to a cube but with their crooked surface or something like that.
(15:42) Um I'm not sure if it is cube or not because it depends on on the well well if if you're talking about like um lines parallel to the axis. Yes, it will be like some kind of four dimensional like cube or probably pipet. That's true. Yeah, that's true. Okay, thanks.
(16:07) Yeah, we don't try to visualize that's why I sort of collapse it into one line but in reality one line could be generous speak in multiple lines. It is better to split it by split it by components. Even though I understand that manual network can have so manual network can have two outputs from from it. It is manual network and I have y first y second. It means basically what I produce is two numbers.
(16:36) There was some kind of neural network before and I produce my two numbers y first y second. How to visualize maybe independently. Right? So now in this case we have um uh absolute error loss it means first minus first absolute value then I I'm supposed to square it right basically I'm talking about distance in multi-dimensional space not distance square it anymore but distance it means I'm talking about this type of uh quantity using P pythagorian Pythagorian uh theorem I can compute it this way right so this is just it basically distance squared is my squared error
(17:25) distance in the small dimensional space of wise is going to be absolute error any questions about that there is one question which you you may want to ask you can say what's the difference if one is minimal second one is minimal as well. If this one is minimal, it means they should match. First one is minimal as well.
(17:53) You may say, what's the problem? I mean, what's the difference? Why do we need to have two of them? Let me say note. Let's say in the one dimensional case, for example, if I'm talking about a loss, which is squared error, right? So this is square root error loss looks like parabola basically this is what I'm trying to minimize in second case if I'm talking about absolute error loss it looks like line and line this way so basically minimum occurs at the very same location right you may say what's the difference but the difference is basically conversion properties of my algorithms remember in order to compute my minimum I would have to somehow
(18:41) numerically uh move downhill in this case and in the second case derivative is different that's why in practice my convergence will be happening differently now if I'm talking about basically very very complicated problem right in that case let's say I'm talking about already realistic example where I have uh some kind of loss as a function of my parameters already in this case it is like a function of of basically my y first and my y first let's say in one dimensional case simply as a function of y then I know that there is minimum
(19:27) somewhere but minimum in the first case corresponds to minimum in the second case then there is a logical question why do we need two matrix to loss If minimum occurs at the very same location so basically because shape of my function is different that means if I plot my loss as a function of my W's let's say W's right it will look already much more complicated it means in order to get this minimum which I want I would need to use different techniques also introduce the high and so on it means potentially I can stuck at some local
(20:04) minimum so this is my local minimum as a result choice of my loss function matters because even this case also you can see it matters converse in this case is different from converse in second from in the second case no simply because derivative is different that means if I'm talking about depends of on the parameters already in case of very complicated like you know network where I have like millions of parameters choice may have impact on my convers that means you have to have sort of ability to maybe to switch loss from one
(20:36) type to different type to see if you get better result or not and so on. That's why when you do this optimization, when you try to find best parameters, people interchangeably sort of use two losses.
(20:55) Even though it looks like they produce same results, but it is only if you know where a minimum occurs in real in reality, you don't know. In reality is a function of W. It is highly nonconvex function. It means you better have this kind of opportunity to switch from one loss to different loss to see what performs better ultimately. So any questions about that? So good question professor. So looking at the the equations of like the two loss functions, right? Um they look to me similar.
(21:24) I I don't see the value of absolute there absolute value because they are already squared and they are positive. So the difference is only the square root. One loss is second loss squared. Correct. square root error is simply absolute error square root because of square root right yeah how is the the shape like the V shape we we drew for the error that's from the absolute function right right so but we don't have so minimum occurs at the very same location if you know the answer then you solve the problem using first loss and I use I solve the problem using second loss. If you both correctly get global minimum we get this exactly
(22:16) same result because one loss is sec square loss. So this loss is square loss. It means uh if we both get correct result if we both get absolute minimum we get same result. The problem is not that the problem is that we don't neither you nor me we we both don't get minimum we get some kind of a local minimum you get one local minimum I got different local minimum in that sense shape of this function really really matters if you plot your loss as a function of parameters already then you are going to stuck at some local minimum
(22:56) and I will stuck at some local minimum but because we use different basically losses because you use the first one I use square root of of your loss. We basically in practice get different results right when when taking the gradient wouldn't you have a problem with the absolute error when you reach the minimum is the gradient defined there gradient is in that case well it is undefined yes if you hit this point gradient is not defined it's not differentiable yeah so in such cases we just have to have some kind of rule what to do first of all is highly unlikely to hit this
(23:39) point. Right? But if you for example don't get derivative by default it will be set something like at zero for example. Yes, you're correct. It's not differentiable. In second case it is not differentiable. But again in practice we will not really hit exactly this point. We'll approach that point but probably we'll never hit it.
(24:06) Now if we hit we can define derivative somehow if derivative doesn't exist let's define derivative to be zero right typically that's what happens professor our assumption here we're not using the same w's like we're using different w's that's why we might get different uh local minimums right um we get different local minimum because we move downhill according to algorithms such as gradient descent And we basically look at every point. We look what derivative is and we move downhill move downhill downhill downhill
(24:42) until it converges. Since we use specific like step it is finite step clearly and it is defined by the value of the derivative. Derivative in first case is clearly different from derivative in the second case with respect to parameters. It means um even even if you use same parameters like same uh same parameters of so-called learning rate right and even if you use same parameters such as learning rate you're going to move at different different speed let's say using different steps that's why if you use
(25:19) loss function which is square error you stuck at one local min if I use absolute error I will stuck at different local manual. That's what happens because we use different shapes or different forms of loss function even though optimal result at the same location. So, so, so convergency in terms of the rate of convergency maybe the first one is faster converging to the correct maybe faster.
(25:49) Yeah, that's exactly the case. Derivative is probably higher, right? Well, it it depends actually. So if you're far away, it is high. If we close already, it is slower. So in in the first case, it has tendency to to slow down as we approach the minimum. In second case, rate is constant.
(26:15) No matter where we are, we just move move using same basically step every time because derivative is same every time. It is like constant basically, right? Uh but it's more complicated because we talking about derivative respect to like y itself. But there is also by chain rule derivative of y with respect to w right that's why it is uh not really constant to be precise but but your intuition is right probably it is faster in first case but as long as you are far away but if you are close to the minimum already your step becomes smaller smaller and smaller because derivative gets smaller it is kind of kind of
(26:50) convenient actually way to do it because it is self adjusting in some sense as you approach minimum it will be you know taking more precise and precise steps until you find this minimum in that case if your al rate is no kind of if your step is kind of large you can be even bouncing you know around your minimum right in that case your step becomes smaller that's why we want to kind of this option to choose which we want to want to use we don't know Which one is best like for all cases but we should keep in mind that they look differently
(27:28) even though formally minimum occurs at the very same location but we never will get in practice minimum all of us will stuck at some local minimum that's why you want to have this different tools different cost functions like loss functions which we ultimately going to minimize and then we'll look at some performance of neural network on some test data and see who wins basically Any further questions? Okay. Now there is um another type of problems. So if it is prediction basically two types of um uh loss losses
(28:05) squared error and absolute error is all all you need to basically use in practice. Most likely we'll never need to use anything else. But there is another type of um problems which is classification. Let me talk about classification. Clification is when you want to say this image contains dog, this image contains cat or maybe this text describes cars, this text describes animals.
(28:38) That is called classification problem. Basically, let me say classification problem. Classification problem. Let me pretend that M in this case is going to be two. No, it means two classes. Two classes. My Y is two dimensional. I have two classes. If I have two classes, maybe Y could be two dimensional.
(29:12) Right? So what do we do in this case? In this case we can say that what we observe is basically first class or second class. So what we observe is y which is some kind of vector basically right it belongs to the following set first class second class. Now I could say I could say dog cat it it it doesn't sound right because how can we compute distance from dog to you know to probability of being dog whatever that's why typically when we have classes dog head we try to use one hot encoding that's what we going to do we're going to say okay it means my vector my output is maybe 1 zero or
(30:00) maybe 01 it is called one hot encoding right if it is one zero I say it is dog if it is 01 I say it is cat let me let me switch let me say cat and dog typically we start with cat and dog it's not important so cat and dog one 0 01 so this is my output whenever you have this type of data set you say output as cat output as dog output as cat dog what do we want to kind of quantify this output and we say let me introduce one hot encoding whenever I see cat in my data set I will say one zero dog will be 0 1
(30:43) typically right typically represented using one hot encoding if you have more classes if you have cow it will be 1 0 0 1 0 0 0 1 if m is equal to three so now when we do prediction what what do we say we want to say that this probability of being cat or probability of being Look.
(31:09) So how prediction works in that case? Prediction is some kind of y head which is already basically two probabilities. First one is y first hat and second one is y second head this way. So my output is going to be vector of probabilities like 0.9 and.1 for example 7.32 probabilities.9 means probability of being cat is 0.9 probability of being dog is complimentary.
(31:46) 1 in this case I have to put some constraint I have to say that my y heads must be actually non negative let me say non negative in practice they are strictly positive actually often we use soft max it means they're strictly positive also y first head plus y second head must be equal to one so this is my my problem again you can see that my output is two dimensional right I have 0 1 0 0 1 as output and my prediction which basically is produced by your network is going to be vector of two probabilities you can you can think about like example
(32:29) You can say uh I'm going to say that y hat for example is going to be 99 comma 01. It is my prediction. What does it mean? It means that essentially I say that probability of being cat is 99% probability of being dog is only 1%. I can produce it for example from some kind of neural network using softmax function I can produce two probabilities this way right and let me say well true observation would be actually y vector which is 1 z that is my true observation 1 0 is true observation I will predict it to be 0.99.01. That's how in practice we quantify this
(33:24) type of this type of data. Cat will be one zero vector. Dog would be 01 vector. But what network produces? It produces probabilities. Now there is a question how to compute how to compute distance essentially remember loss is kind of distance between yhat and y.
(33:49) How to compute distance between yhat and y? Any suggestions? What would you suggest? Want to use the distance formula since you could treat them both as points. So now since we represent everything as a point in two dimensional space you can say why not to use mean square error basically right? So this is wrong approach. I want to emphasize that's not how people should do it. This is not how people do it. If you do it this way, most likely you will not get nice result.
(34:22) But if you use MSE in this case, MSE formally speaking looks at the following distance. So MSSE looks at the following distance. It tells me for the cat example for the cat I would have to use this one minus one squared. No, it means y had first - 1^ 2 + y had 2 - 0 squared and this is for cat because cat is 1 zero. This is cat.
(35:04) For dog, it would do it different. I mean it would it would do it similarly. It would say y first head minus 0 because it is dog squar + y 2 hat - 1^ 2 for dog. Now intuitively you can say okay it seems like if it is one you want the first probability to be close to one. So what's the problem? That seems to be to be working nicely.
(35:30) That's right. Kind of weird though. Weird. Okay. What what is weird? Yeah. Well, because you're subtracting a probability from something that could not be a probability because you chose one and zero. So, it happened. You could have chose like 10 and zero and 0 and 10. Exactly. Yes.
(35:49) So, if you if you kind of um know the answer again, if you know the answer, this metric is supposed to be quite small. So, if I correctly design my neural network and I choose my parameters, this metric indeed will be quite small. Minimum of this metric will be quite small.
(36:12) But what happens is as you mentioned because we take difference between probability and just discrete value 1 zero. In reality what happens is this type of metric will be highly highly I mean this type of cost function ultimately will be highly highly nonlinear. So this is global mineral. This is actually okay. So if if you can find it it is okay.
(36:31) But the problem is you will never in practice be able to find it if you use this type of this type of metric this type of loss function. So in practice because it is highly highly nonlinear exactly because we take difference between kind of continuous number essentially probability and discrete number squared. As a result as a function of parameter this metric is highly nonlinear.
(36:55) It means in practice you're stuck somewhere over there and that's it. You will never find what you want. This point would be okay if you know the answer. This metric tells you okay your result is good. But if you use this metric to sort of find your parameters, you will stuck somewhere and that's it.
(37:14) You can kind of intuitively understand it if you sketch your you know points in two dimensional space. One 0 means this point. 01 means this point. So this is my y vector. It takes values here and also over there. If I'm talking about yhat, yhat has two numbers. They add up to one. It means they live on this line. Right? So my yhat values live on this line.
(37:48) And now we are taking kind of distances from those points to points on this line. They sort of belong to different spaces in some in some sense. No actually in very precise sense they belong to different spaces. These two points are kind of discrete points in this space and vector of probabilities is point on this line which is sort of continuously moving along this line and we take distance and we hope to minimize it. So in practice it's not going to work nicely.
(38:17) That's why ultimately we're going to just stuck somewhere. We'll never be able to find the minimum. Again if you know the answer this metric will tell you your answer is good. But if you don't know the answer, you have to use this metric to somehow find the answer. And this metric will not let you find it because you're stuck somewhere. That's why this method is not really used.
(38:36) So many examples when students would use it. They would forget about you know problem being of different type and they say let me just minimize you know mean square error mean square error as if it was just prediction problem and they would not get nice conversions. The answer is obvious. metric is not the best. It's not the best choice. If you do prediction, it is okay. If you do classification, it is not okay.
(38:59) So, what to do for what to do for classification? For the classification, we use so-called cross entropy, right? This is different type of function cross entropy loss entropy loss. You can see it on the screen. So basically L as a function of Y hat and Y is defined negative Y first M is equal to two remember that means I have first and second component Y first * ln of Y first head minus Y 2 * ln of Y 2 hat so this is how cross entropy is defined this way let's Say I want to take example let me assume that I have
(39:54) my y to be again cat as before so to be 1 zero it means I'm talking about cat this is y first essential this is y second so this is cat well this is cat one and zero is cat so let me say first is if my y head which is which which is prediction looks this way like.99 and also 01.
(40:35) So if that then what I get for my loss function is going to look this way. So it's going to look as negative y first which is 1 * ln of 99 minus y 2 which is zero time ln of y 2 hat which is over there right this one 01 what do we get -1 * ln of something which is close to one it means I get almost zero value basically right almost zero value.
(41:15) You remember how my algorithm looks? Algarithm of P hat as a function of P hat looks this way passes through point one. At 0.1 it is almost zero. It means -1 * almost 0. I got almost zero value. Second case let's compare what if I take another extreme case. My predicted value is going to be on the cont 01.99. Then in this case L is equal to negative true value is still same.
(41:45) I'm talking about cat one but I predict cat to be almost like dog basically. So -1 * ln of 01 minus 0 * ln of.99 how much do I get? Second term doesn't contribute. First term is -1 * a of small value it is going to negative infinity. This is not quite not infinity. Of course a l of 001 is negative I don't know five maybe or something you can compute.
(42:18) So this negative something like five maybe but if you take like let me say approximately negative infinity right it means result is approximately infinity. Yeah, I want just kind of emphasize that there is going to be large potentially if probability of being cat is very very small loss function becomes huge and positive minus one * negative infinity becomes huge and positive minus infinity from from here clearly.
(42:45) So it goes to negative infinity and I get something large. It means minimum occurs in this case indeed over there. So this is minimum. That means I'm going to have to basically uh have my result look like that. First probability is large close to one. Second priority is small if I have cat. That's exactly what I want.
(43:07) That's why this laws basically when it is minimized will give you same sort of same result. First priority must be close to one. Second priority must be close to zero as I want. But because of structure of this loss function, it doesn't have this kind of issue anymore. At least it is mitigated.
(43:25) As a result, minimization of cross entropy is much much easier problem. So it is it is doable now. That's why people prefer cross entropy in case of classifications. Any questions? Yes, I have question. Yeah. For the binary cross entropy if if the mis if there's mclassification the the error would be will be huge, right? Yeah.
(43:54) Uh but if if I have like 10,000 points and just one point is giving me infinity, isn't it really just misleading the entire optimization process just because I got one point wrong and I got this huge number. So you say that you have like uh thousand points in one in one key this term basically will give you infinity and you say that's not what you want and you kind of concerned because you say I don't care about one point 9 um 999 points are okay and one is off you kind of say I like it right okay what I'm trying to say is usually you look at the average right so the the optimization happens based on the average value across the 100 across the
(44:47) the southern points or whatever the entire uh training data so if one point is infinity the average of infinity would be also infinity and it was just really okay I should probably be a little bit more careful let me say five if it makes you more comfortable it's not really infinity it's not going to be like like truly infinity infinity is like almost zero, right? If you completely mclassify and if you are confident it is it is that way probably it's not it's not what you want anyway but if you sign like 1%
(45:24) and you are m mistaking then it's going to be not as huge right I see what you're saying what infinity is just I took extreme case hopefully your network will not produce your results confidently and not correctly but confidently Right. But for that professor, do we have to uh worry about like numerical stability? Like if the number is getting very high, is there a way to make it like bounded so that it doesn't go to infinity? So you what you're saying is so so basically when I say infinity to be precise to get infinity, you you have to
(46:03) plug zero over there, right? We don't have a option to plug zero because we use soft max. soft max remember it doesn't produce zero it always produces some kind of number which is strictly positive it means I just give you this sort of extreme example but maybe it wasn't really good idea to to present extreme example because zero will not be the case probability equal to zero is not something neural network can ever produce okay it gives you 1% it is cat one point it tells you no it looks like it is dog I'm pretty confident about this but it is only point for example
(46:43) one chance that it is still cat then my loss function is five as you say take average over thousands of points and this five doesn't contribute as much so infinity was was by the idea infinity is is not is not going to happen so to answer your question infinity is not going to happen I just didn't want to compute what it is that's why I told infinity in reality it is never infinity it is some kind of number and your network always have opportunity to kind of you know shift the weights a little bit to make it no at least like 2% 3%
(47:17) right so infinity is not an issue right does does make sense infinity will never happen I just didn't want to compute ln of 01 but how much is it by the I think it's 4.6. Okay. So, five is five. So, it gives you five. Five is okay. So, you you are fine with five, right? Yes. It's a it's a manageable number. Yeah.
(47:57) So, if you take thousands of observations, you average all this losses over entire data set, you get so-called cost. life will not contribute as much. You are completely wrong about this case. You are so much cover it is it is actually different animal is like a dog but it doesn't give you infinity. It will never give you infinity basically because we use soft max function. Soft max function doesn't have a capability to produce zero.
(48:21) It is always positive number. So now cost function cost function as we already basically discussed means we are talking about average of losses over entire data set. When we minimize when we optimize neural network we want to minimize all losses at once.
(48:46) It means we want to minimize cost function which is average of losses. So basically I'm talking about let's say cost function. So this is cost function and I say let me again sketch X as input Y is output and I say that I have some kind of neural network neural network then I have some deviations of my true observations from neural work. This deviation is L first.
(49:32) That deviation is L2. This deviation is L3 and so on. I have many many different deviations. L let me say M because I have M observations. So I have different different deviations. I compute a losses for every deviation. Then what is my cost? So I have m observations right. So remember m observations.
(50:05) So that my cost which is j function as a function of parameters is defined. So by definition is going to be simply average of individual losses. J let's say I runs from 1 through M M observations and then I take losses individual losses losses depend on parameters and that's it so this is is called uh cost that's it called cost but sometimes people take sum of losses it's not as important right average is better so you don't have to rescale learning rate.
(50:52) So any questions about cost function? Uh just to make sure we're only talking about uh one function though, right? So we're not mixing like mean squared error with absolute error or anything. We're just so so l is whatever you decide. Okay. Square. You don't mix and match though for that, right? Just making sure. You mean you don't take different losses for different points? Yeah.
(51:19) No, this is kind of a dumb question. Sorry. No, no, no. You have to take you kind of stick to particular notion of basically distance that is same L. L is kind of same letter. It means the same type of distance, right? All right. Thanks. Yeah. Yeah. Yeah. So you don't want to do it. Yeah. So that doesn't make much sense. I don't know actually. It's interesting.
(51:42) But if you do it that way, it means you put more weight on some points and less weight on different points. You never Yeah, I I was kind of thinking like take the like let's say mean squed error of all of them and then also take like an absolute error of all of them, you know, and then like maybe maybe what was trying to be communicated was taking an average of each set of loss functions. But I I guess I'm just overthinking it a little.
(52:04) Sometimes when you see the math like that, you might think it's just a different Yeah. So basically same letter L denotes same type of loss. So now if L is squared error it would be called mean square error because min J what is this mean average of mean L I'm sorry mean L average of L's is mean L. If L is square error we get mean square error. If L is absolute error we get mean absolute error.
(52:31) If we have cross entropy we get we don't say mean cross entropy. People simply say still cross entropy. Now you can kind of plug all this formulas and into one and you get average of our else average of our else. So clear right it's going to be called cost function this kind of terminology people sometimes use it interchangeably sometimes people uh when they may be confusing what is loss what is cost typically this is convention right like when you say I want to specify loss function for example when you build your network basically you specify what each individual deviation is right but when
(53:12) you optimize your not work it will optimize average of those losses which means cost. So this is how basically typical cost function is going to look as a function of parameters. So it has many many local minima. It is not nice case not convex case. It may look this way easily.
(53:36) It is a still projection clearly because we have many parameters like millions of parameters potentially at least hundreds of thousands of parameters in order to present it visually we have to take projection onto like three threedimensional space for example and this is how it looks.
(53:55) So we keep in mind this picture that minimum is not going to be unique I mean local minimum is not unique. It means to get global minimum is going to be quite difficult problem in practice. you have to try different techniques maybe different parameters and so on and so forth. So now the next thing is actually how do we this is how we can specify uh loss function we can say losses categorical cross entropy for example also there is a way to specify so-called metric just keep in mind a loss means what we minimize if I say loss is cross categorical gross entropy it will minimize average of those cross entropies it means it will minimize cost
(54:36) which is categorical gross entropy but gross entropy itself is not really convenient metric it has some negative lens and stuff like that it is difficult to interpret what we get that's why there is also tool to to basically specify so-called metric metric is what is going to be computed on the way for you so you can actually see how nice your model is so metric for example is going to be accuracy proportion of things classified correctly is called accuracy.
(55:09) Then what is going to minimize is cross entropy but on the way it will compute from time to time will compute metric for you. So you can take a look at performance. It will use metric only to compute basically performance and display for you. It will not minimize metric.
(55:28) Metric is only to kind of take a look at your result. A loss is what my network will minimize. metric is what it will compute from time to time and display. Now optimizer is how to do it. Now optimizer is quite important important step how to actually minimize function in practice. Here is a list of different uh loss functions.
(55:52) Let me move to basically uh ways to minimize my minimize my uh cost function. So this is slide which I want to discuss. So it is called graded descent. First of all algorithm is called graded descent. Grad let's assume I have function of parameters. Now, of course, I have many, many, many, many parameters, right? Like literally millions of parameters I can have.
(56:47) So now uh W first, W second and so on. The WD parameters, right? my cost which is J is a function of W. Let me pretend that I have nice cost. It's not going to be the case but let me sketch it for now. This is my cost as a function of parameters. Basically my question now is to find this location. This is my question mark.
(57:18) I'm located somewhere else at this point. For example, currently I chose somehow my W. I'm here. I want to move towards question mark. How to do it? In this case, we have to recall calculus. If you don't know this multivaried calculus, don't worry. Basically, we have to find direction in space of parameters which corresponds to fastest fastest uh growth of my function.
(57:49) It's going to be something predictable to levels. It is called gradient. So basically it is like derivative of my J but it is a multi derivative which is defined exactly as vector of of derivatives. J with respect to first one second one J with respect to last one just vector of derivatives. If I move according to derivative according to my gradient of J I will go I will move away from question mark.
(58:20) It means quite obviously I need to move the opposite right. So it means I have to move towards negative alpha* gradient. Again gradient means a vector in space of parameters w's which shows where a function grows fastest. There is a way to get it. The simple vector of derivatives will give you exactly what you want. So you can uh basically look at some simple example.
(58:50) Let's say J is a function of W first W second. You can choose any function you want like W1 2 + W you know to power five example. Then I can compute gradient. What is gradient? It will show me where function grows fastest. It will be derative of J with respect to first W. It means this one differentiated respect to first W.
(59:25) I get 2 W first plus this one differentiated with respect to W first I get plus 0 + 0 and similarly I can get second der second entry of my gradient. So the d with respect to w 0 from this term plus 5 w2 ^ 4 and I get this vector which will tell me essentially where my function grows fastest if I'm at particular location w1 v2 I plug it and I see vector what vector right away is so this is how my gradient is computed if I specify to python that I want to mean square error.
(1:00:10) It knows how to get this derivative analytically just like I did it in this case. Python will know how to get it analytically and will plug basically everything into this expression and eval it for you. So it will know how to move and where to move. Quite straightforward ID. Now grad descent. So this is graded descent algorithm.
(1:00:31) Grad descent tells me W which you're going to get after the update is going to be just like we sketched next W is going to be previous W plus this vector that means minus alpha * grad of J. This is old location gradient of J attached to the old location. This is old location right and that one is actually going to be new location where we're going to move new location that's why this is old location but this is something which I call new location you can use sub indexes and n + one n if you want if you don't use sub indexes it's better to use like column like programmers do it column equal it means we sort of define
(1:01:22) in that way right there are different W's. So this is graded descent. Now issue with graded descent there are few issues. Can you name some issues with grad descent algorithm? Odo. Yeah, it's so now issues with this with this approach. You can overshoot if you don't have like the right alpha maybe. So yeah, it's a good point.
(1:01:57) If alpha is large, we can overshoot. So if alpha is large, algorithm may diverge. Absolutely correct. Uh we have to decrease alpha. In case of convex function, you can easily prove that was officially small alpha algorithm will converge, right? Uh will not diverge. But you're right. If alpha is large it it will diverge.
(1:02:23) So other issues from kind of you know computational point of view at least. Okay. There are two what saddle points like you can arrive at a saddle point which is not the global minima. It's like um so settle point that's interesting. So it's a low point derivative could be probably um so what's point I think that partial derivatives could be different from zero it depends on how you point looks but point means u so sle point means uh something like something like that so basically sle point means something like this or sle point right so in this case You say what
(1:03:15) is my my vector of uh what is my vector of partial derivatives? Okay, maybe so if you get this gradient and this gradient it is like 0 0. So you may stuck there basically. Yeah, that's interesting. You may stuck. But the thing is it's very difficult to hit precisely set point and stuck there forever.
(1:03:42) If you're somewhere around you probably will not move towards settle point. It's it's not attracting you. It's not attracting your algorithm. I mean, it's not it's not attraction basically, right? That's why point is not an issue only if you kind of completely unlike and hit beside this point and you stuck there basically forever because derivative is zero. You're you're right.
(1:04:00) But it's not typically a problem because you will not move towards the point and hitting it precisely in practice is basically impossible. Zero chances. So any other issues? Um if you have a large data set that's a lot of computation, lots of computations. Yes, it is doing it on every single data point. This is my data point.
(1:04:21) I show my function using like levels of J just like when we talk about mountains, we show levels, right? Constant J constant J constant J. And my negative alpha negative alpha time gradient shows where to move. And you're right. It is a lots of computations. Let me say issues issues. It is uh a lots of computations.
(1:05:00) So basically to make one single step and remember you may have like millions of observations in order to get average of losses or in order to get basically gradient of your J which is average of gradients of losses you need to get average of those gradients in loss of computations right loss of computations and there is second issue it is related to actually issue that my cost function is typically not convex what it means not convex it means maybe I don't want to know precisely my gradient maybe I want to introduce some stoasticity maybe it is okay to to kind of move not according to negative alpha gradient sometimes because I don't want to stuck
(1:05:42) at local minimum and basically this algorithm be being not stoastic it is not stastic becomes an issue becomes a problem right that's what happens so in this case my gradient is essentially gradient of my average of losses so it is gradient of 1 / m the sum of my uh losses i is from 1 through m which essentially means I'm talking about average of no gradient is like derivative but is a linear linear operator it means I can say that way average of my gradients.
(1:06:28) I can compute gradient. It is not a problem. But if you have a loss of points, it becomes computationally problem. So this is average of my gradients of losses is what I need to compute. Okay. They introduced alternative algorithm which is called stcastic graded descent. the highest gradient S GD now it is same like levels of my cost function I want to move towards question mark and they say let us do the following let's not try to compute gradient of J precisely because we actually ultimately want to introduce some kind of statistics in order to get out of local minimum and the best way would be to not
(1:07:19) not to even try to comput it precisely. We sort of say resource you don't want to comput it and then disturb but doesn't make sense compute precisely and then disturb but instead we're going to comput roughly and we say basically gradient of J is going to be approximately gradient of my loss I take one point at a time based on single point based on single loss I can compute gradient of my loss and I say gradient of cost is approximately that Right? What it means? It means in first case I'm going to move not perpendicular to level but somewhere else. Then I move
(1:07:59) somewhere else, somewhere else, somewhere else, somewhere else. But on average I will sort of approach my question mark. That's how stoastic graded descent looks. I will be bouncing around but on average sooner or later I will sort of approach my point. ic descent I can say my W is basically going to be previous W minus alpha it is supposed to be gradient of J but I say no that's not what I want I don't want to do computations which are so expensive right I want to simply say gradient of lossence that based on
(1:08:41) single point I can make update right away this way any questions about the grad descent So issues with this algorithm what about is it stastic converg it will take long time to converge. Yeah, we take one point at a time. We typically basically take random point from the data set. We shuffle observations and we move from through data set from the start to the end.
(1:09:16) But every time we shuffle it, it means it is stic. But the problem is now it is too stic. I tried to demonstrate that it is too stastic, right? It is too stastic now. So yes, calculations are not so expensive anymore. Every step is is very cheap. In this case, one step is very very expensive.
(1:09:41) Potentially one step is very cheap, but it it requires you to make many steps in order to get like towards question mark two stic. It guarantees that you will not stuck at local minimum. You get out of of it easily, but it is still too stastic. And second issue, it is quite difficult to run this thing in parallel. Let's say difficult difficult to run in parallel. Why? So in that case when you compute average of gradient of losses basically you remember we at particular point at this one old point we compute loss for it means we take one observation comput loss second observation compute loss based on the very same W next
(1:10:25) observation compute loss based on the very same W. It means we can run all these computations in parallel. Essentially in this case we don't have this opportunity anymore because before we move to the next loss to the next observation we have to take a step we may take a step we are different location now we compute we have to compute different loss already different w it means we sort of sequentially consequently should update parameter right it means we cannot do computations in parallel only sometimes
(1:10:58) maybe but not always and then in order to fix these two issues we actually introduce so-called mini B mini batch mini batch graded descent which uses the following idea gradient of J will be approximately not average of all losses not kind of average of all gradients of losses not gradient of single loss it will be average of a number of gradients Let's say average of 64 gradients.
(1:11:35) I take like 64 observations at a time. Compute this gradients and estimate my gradient of loss. Why 64 points? It becomes what? It becomes much less stoastic potentially. It means it is already much better approximation to my true gradient. So it is still stoastic but not so much stoastic as before. I can get convergence much faster now.
(1:12:10) So it means my W for example is going to be W minus alpha gradient of J. I don't like grad of J. I replace it with average. Let's assume I took 64 observations at a time. it means mini batch of size 64 and I compute those things to estimate my gradient of J. So it is so now it is quite stoastic right I mean it is still stastic I have like parameter like the 64 which I can use to control stoasticity right but at the same time it is not too stastic so first it is still stoastic still stoastic but not as much anymore.
(1:13:04) more not as much as my stoastic graded descent. Secondary can I compute my gradients in parallel? Yes, I can easily compute because they all all 64 attached to the same location. I can perform 64 computations at the same time using using parallel computations. Yeah, question.
(1:13:30) Yeah, with a mini batch gradient descent can you still get stuck in local minimums? You you well you always can stuck you will stuck at local minimum but because of the high if it is not like too deep local minimum you can get out right it is it is still stastic so it means you so you you may get out out of local min. So basically uh in this case if I have my cost function then I will be kind of bouncing around around around maybe I stuck there right maybe it is it is too high to jump but this will be my result if I'm talking about like this case I have my cost function then since it is
(1:14:24) deterministic I I got to my local minimum and I stuck there forever. In this case, I stuck there. In stoastic case, if I have my loss function, I will be kind of bouncing around, right? Bouncing around will be very very stastic. I have to think about ways to cool system down. You may have to maybe reduce alpha over time.
(1:14:48) It is called learning rate scheduling. I can do it. So, I will not stuck anywhere. Basically, I will be bouncing around like forever. In this case, you're stuck at the nearest local minimum basically forever. In that case, it is still stastic. You will be bouncing around, but you you may actually end up at some minimum which has like high walls around it, right? And maybe not global min, but it is already much better result.
(1:15:18) That's why min is preferred. So, any questions about anything? It's it's more of a comment than than a question. So th this is the reason why using uh like a moderately small batch usually produces better results for training a deep neural network.
(1:15:42) So for example, if I use a batch of maybe 32 um items or 32 points or 64, that's usually much better than using like 10,000 items. I I believe this is because of the stoastic property of the algorithm. Yeah. So if you if it happens that your cost function looks this way you needs the hosticity in most cases your cost function looks somewhat like that. If you need theicity then using large mini batch size or graded descent which uses basically all observations for every update of W's is not the best idea because you will stuck somewhere right away. the nearest local minimum is your your basically destination you need
(1:16:23) first what comes to mind let's introduce some kind of stoasticity artificially basically there are such algorithms in numerical computations when you don't get this surface from data if you just minimize some kind of function numerically you say let me use descent but introduce the hosticity like can randomly kick it out a little bit right and you will be not already you will not you will not stuck at local minimum forever.
(1:16:56) But in this case in this surface comes from data it is much much better would be to you know just use mini b gradient descent approach in order to speed up computations and also effectively introduce the highity. So now back to question if you have like a large mini batch size. Yes in some cases if you have function which look this way it is not good idea maybe you want to reduce if you reduce size of your mini batch you get most theasticity right.
(1:17:27) If you increase size of your mini b you get less the hosticity. The question becomes which you want which you need for your problem depends on the shape of your cost function. Most likely it is highly nonconvex. That means yes your mini B size should be not very large. 32 observations is a very good choice. 64 is a very good choice.
(1:17:46) So what is default? Default is probably 64, right? Or 32. Do you remember? 32. So there is a reason why they chose 32, right? Think about this. Why they chose 32 as a default? So for for most problems, for typical problems, they found it to be optimal. But so but but usually with training deep network, let's say that I have like 10 million uh uh 10 10 million samples on my data set.
(1:18:17) So what I may think I may think if I if I use a a batch size of 10,000 that's not too uh not that's not too big. Uh but but actually it's it's still um using a smaller one usually produces better results. Yes. So, so it's not about it's probably it is not about uh um relative to the to the data set size. It is actually it's interesting question but it is probably related to already.
(1:18:44) So it is already a question about statistics. If you take if you live in United States you want to take sample right to estimate some kind of population parameters. If you take sample of 100 people, you may believe that if you live in uh Luxmbourg, you need to have sample of much much less people in order to get same accuracy. This is not correct.
(1:19:13) Basically, accuracy of your estimator doesn't depend on population size. It is slightly counterintuitive. But if you estimate some parameters of population sample size is is is what you need to know in order to quantify quantify accuracy of your estimator basically to quantify which you introduce. It's not about your not about size of your population.
(1:19:38) Basically it is about about minim size in our context. Uh if if you okay if you let me differently suppose you you you cook soup right? Okay. And I and I cook soup and I want to know if it is salty enough or not salty enough. So if you cook much much much bigger pot do you need much bigger spoon to to to taste how much salt you have or you can take very same small small spoon same same spoon will do same spoon same same idea here.
(1:20:14) If you have like huge data set mini batch is like size of spoon. You take a spoon and you cannot test you know how how how your gradient is. If your data set consists of millions of observations and my the data set consist of only thousands of observations it it doesn't mean that you have to take many size of thousand times bigger size. Does make sense? Yes. Makes sense. Makes sense. Yeah.
(1:20:38) Because theity which you introduce is basically function of your mini B size not function of your population size. Yes. Yes. Okay. So now um I have some examples which I have posted online. You can take a look at this. It is interesting. Probably maybe next time we'll talk about this a little bit. It is section one file.
(1:21:01) I will not hold you any longer today. But you can look at this uh file. Maybe next time maybe even on Tuesday we can talk about this examples a little bit. You can take a look what I did. We need to go through this to be able to solve the program. No, not quite.
(1:21:26) You actually need to go through the through the example which is posted on canvas and I didn't plan to go over that example anyway. You have to read that example and you have to do some kind of I believe modifications right to that example which I provide on canvas in order to solve the problem. It is just example which I wanted to de to you to to use to demonstrate how we can classify text using so-called TF IDF frequency inverse document frequency approach I will cover it much in detail later anyway so I just sort of start I wanted to give you some kind of example which shows how we can use TF IDF to to classify to to to uh classify text,
(1:22:07) classify text. In this case, I use different models. Can neighbors support vector machines, random forest, neural network, and I show you some kind of test accuracy which one which one performs performs how how each of those performs, right? But it's not something which is required to understand for the assignment. It's just sort of interesting example.
(1:22:34) Maybe later we'll talk. So where's the case that you posted you mentioned? Say it again. Where's the case that you posted and that will help us with the assignment? So assignment is based on I believe some kind of a chapter don't remember okay let let me see three chapter three right and that particular file is also posted and on canvas you have to make sure that you're able to run it.
(1:23:04) Once you're able to run it, you just need we'll need to make some modifications in order to solve it. Let me see. So, in this class, we don't usually go through examples of uh codes that will eventually be don't worry. So now um you can always ask T we'll have two TS sess sections for that reason right so we can see examples on Fridays we'll also go through examples today I wanted to cover this stuff it is important before you move forward it's important to understand how it works under hood so in this case we have example which is classification of use wires, right? And we have um this example which you need to first of all
(1:24:04) be able to run and secondary you have to modify it and pull some kind of training and validation accuracy and report test accuracy and then you also have to experiment with different optimizers. All right, different optimizers and uh in order to solve it, you have to take this specific uh file, this one, and should and and you should be able to run it. Once you're able to run it, modification is quite straightforward.
(1:24:34) Uh any questions? Any of the topics in the assignment we haven't covered so far? Any of the topics? I don't think it is correct. Did we cover? Yeah, I mean this part but the last part last questions. Is there anything we didn't cover? So which ones? Uh um like experimenting with different optimizers theor because in our theory did we we didn't see examples of different optimizers.
(1:25:20) Are we supposed to see that or just like try different variables and see how it works? So uh we need to uh use different optimizers. It means you need to basically use uh so descent min descent is what's called optimizer. These are different optimizers right there are some modification to those optimizers. For example, optimizers is equal to SGD stoastic descent. It is a first optimizer. This optimizer has parameters.
(1:25:52) For example, alpha parameter is learning rate. When you you ask to play with parameters, you can play with alpha rate. For example, second parameter is mini batch size like 64 in this case. We say batch size which means mini batch size is equal to 128 in this case is second parameter. So you can say optimizer is stoastic descent.
(1:26:17) There is also alpha parameter by default it is 01 and also mini batch size those are parameters you have to play with this parameters and get different results. Now there are some modifications which for example if alpha is equal to 005 you want you can specify it explicitly and say I want to change my parameter alpha right.
(1:26:41) So next one is modification which is um called ADM optimizer. You can open for example this case optimizer is ADM optimizer. Then you can change optimizer from SGD to Adam will be different optimizer. You can change mini B size to something else. It means you play it with with hyperparameter mini batch size and you get different result. So again, optimizer means what you specify up here.
(1:27:08) Parameters means whatever you specify in addition to the type of optimizer. Does it make sense? Yeah. No, it makes sense. So this is called optimizer. Adam is most advanced I would say by default. If you don't specify it will be Adam, right? Yeah. So you can you can keep Adam.
(1:27:35) You can also know basically read documentation and see how to change parameters of Adam optimizer or see examples online right it's not like u something you don't want to do clearly you can you can always and you should always read documentation also examples if you want to change parameters of optimizers you can you you can you can find it but my point is it's important to understand what optimizer means optimizer means what kind of updates we are going to use this update or that update. What is update? Hyperparameters is something which we specify up front.
(1:28:07) It's not like W's. W is not hyperparameter. W's parameter which is trainable parameter. Alpha is hyperparameter. For example, 64 is hyperparameter. That's terminology. When we when we say play with optimizer, it means change approach, right? Change it to different optimizers.
(1:28:34) When we say play with hyperparameters, change parameters such as alpha because alpha is not trainable parameter. Mini batch size is not trainable parameter. You can also change SGD to add them. You can use different optimizers. I will not actually spend time on this course on details on of optim on optimizers. All right? But you can always read about options. No, let me say uh let me say I can open extra slides since you are given flexibility with what you you can use here.
(1:29:15) You can choose from those which we we considered. But if you want to know a little bit more, you can find online or you can look at my slides from different class. Let me show you. I will show you I will show you a kind of list of optimizers. It is not something which you cannot find online. You can easily find it clearly but so adder.
(1:30:29) So now optimizers. So first of all is stastic graded descent. One extreme case is graded descent. In that case mini batch size must be equal to size of your data set descent means mini batch size is equal to one. You can specify that will be descent or you can choose whatever you want from one to the data set size. It will be mini graded descent.
(1:30:59) Here is what you can do sd and you specify size of your basically uh mini b. Now there are some modifications momentum optimization I will I will not cover it but if you really want there is twist to this optimizer which allows you to speed up convergence a little bit because we sort of accumulate this vectors from the past and we compute current vector along which you want to move is sort of average of previous shifts.
(1:31:30) we use kind of momentum from the past that's what it does effectively and we get optimiz which is momentum so it is also SGD but momentum is switched off point 9 that means we sort of uh compute things from the past and uh average those things from the past and to current and obtain current vector which we're going to move so it is momentum optimization another one neated gradient And in that case you can say nest true adapted descent which allows you to use local curvature a kind of curvature of your cost function to adjust vector which you're going to move then you can use this parameters but my point wasn't
(1:32:19) uh I didn't want to ask you to play with all these parameters because I didn't even plan to cover all those optimizers basically need to know Adam right which is default optimizer just see what parameters it has and play with those parameters.
(1:32:39) I understand that you may not know what those parameters mean but still you have to be able to play with those parameters and see if it it is helpful or not helpful and also optimizer which is SGD which is choosing one of those depending on the mi batch size also play with this parameter and also rate is quite important if rate is large you may not get any conversions clearly it will diverge if rate is very small you may converge very very slowly it's not nice case either it means rate is quite important parameter learning rate as well.
(1:33:10) So there are over here learning rate some other parameters if you want but you don't have to know that I would focus maybe on Adam just as we covered it last time in slides Adam is well you can see optimiz and mini batch size is probably what you want again question is quite fored quite you know quite u broadly it means you and do whatever you want.
(1:33:42) Essentially, you can, you know, learn about what kind of optimizers are available and play with those. Try four optimizers. Try different for optimizers and pull your results. Any questions? Tomorrow you're going to have section and I believe that those optimizers will be actually covered. So tomo
(1:34:16) rrow is going to be at 9:00 a.m. right? So you you can you can view as you can view uh recording so you can join at 9:00 a.m. and see optimizers. I'm pretty sure that optimizers will be covered tomorrow as well. Any further questions? Okay, let's stop. Thank you.