(2) 89 day1 - YouTube
https://www.youtube.com/watch?v=-QtqoT4dA38

Transcript:
(00:01) Hey everybody, how are we doing? Fine. How are you? H school. Thanks so much. Doing well. Looking forward to the class. Hello everyone. Hello Dr. Krokin. Hello. Hello. Hello.
(01:07) Hello. You should be able to see my screen, right? Yeah, we can see. Yes, we see it. Okay. Nice. So today we have first of all we have to talk about organiz organization of this course now it means we'll talk about deadlines what is required to submit and also who we have in this course I mean TAS support teaching stuff and second part will be introduction to neural networks so basically I don't assume that you know neural networks which means we'll have to spend some time talking about neural networks
(01:53) So you can see on canvas there is a tab announcements. I encourage you to kind of you know pay attention to those uh emails from this announcements or maybe from time to time to check what is posted there.
(02:11) Basically whenever we have some kind of information we post it under announcements so everyone can get email and also can have opportunity to sort of uh login and check what what is going on. First say that lectures are going to be basically available via zoom and also later you can watch it if you want. You can see here that uh there is a link where you can find lectures. No clear it is under class recordings.
(02:35) So next one is is about the files where you can find all files which are going to be basically used in this course. There's going to be a lecture slides. It's going to be some kind of examples also. TS will will post some kind of files. Whatever you want to whatever you're going to be presented basically will be shared with you afterwards under files.
(02:53) Here is a link and also you can basically click on files itself. It is shared with you with you and you can find um a corresponding folder and you can actually download it and use it for your for your purposes. You can also if you want uh essentially borrow the code which is provided to you it is allowed and it is not against the policy or maybe you can uh sort of site and say it was you know example which was presented in class or pres presented by a TA.
(03:22) So basically whatever I submit is going to be kind of modification of what was presented. So this is totally okay. That's not a problem. And also next announcement was about uh WhatsApp group. I just want to mention that WhatsApp is basically some kind of place where you all can communicate. We don't have like classroom where we can sit and and and discuss um these topics, right? Which means uh it is nice for you to have some kind of place.
(03:50) So in such courses as deep learning is natural language processing. I usually create this type of uh WhatsApp group so you can communicate. We not necessarily going to observe what is what is what is happening. If you have questions to us, you probably will have to maybe communicate those questions to us differently. Maybe using email or actually better using patia. We are going to create forum which is called patza.
(04:14) On pat you can post your questions and we sort of feel that we have to answer. It's not like we obligated to answer. Sometimes students ask uh the questions which are sort of off topic. We may not answer all questions but we try to answer all of the questions.
(04:34) Basically the swip will be a place where you can ask questions to this questions to me and WhatsApp will be place where you can basically communicate if I decide I will from time to also check it if if I see that I can help with some kind of you know references I also will post my answers on WhatsApp but basically WhatsApp is for you to to communicate if you want to communicate to TS to me is best place but you can also always send email directly clearly right even though if you have specific questions which maybe other students also would get would want to get answers to it is better to to post it on pata on the forum so we can answer once for everyone
(05:11) also if you know answers to the questions which are posted on on forum pata you can you actually encourage to answer those questions it will be very nice and helpful for us so now uh this is about uh communications. Second part I would say first part on the slides is about um uh our TAs.
(05:36) We have five TAs in this course. Uh Andrea is going to be our head TA. She's very experienced. I think she has been TA in deep learning and also natural language processing for for about 5 years already. So Andre is if you have some kind of maybe logistical questions or some kind of questions which you don't want don't know whom to to ask please email Andrea first she will help you. So let me see what uh uh questions. So hello everyone.
(06:09) Yes hello everyone. So now um no yes so these are our very experienced please don't hesitate to contact them. And by the way, since we have large classes here, I'm going to ask uh I'll have asked already our TAs to hold two sessions a week. You will have two TA sessions a week in this course.
(06:34) By the way, you don't have to basically attend anything in person in this course because this course is sort of uh has option which is no kind of a online option which means it's not like a conference. It has on one option which means you always can skip sessions, lectures and watch later. This is allow it just to be clear. So two T sessions per per week. Now they will tell you when they will have it.
(07:01) Usually it will be around Wednesday, Thursday and then around Saturday Sunday two sessions per week. Also I'm going to have my own sessions on Fridays. During those uh dur during the that one hour on Friday, I will actually try to cover some examples. I will show some Python implementations which we can discuss also a little bit maybe theory but usually I prefer to discuss theory on during the lecture like on Tuesdays in this case and then implementations will be discussed mostly on Fridays.
(07:34) In addition to this you will have two TA sessions. I encourage you to if you have time I encourage you to join and ask questions. This is kind of the purpose of that. So let me see if you have any questions. Um looks like not. So by the way if you have any questions it is okay to interrupt.
(07:56) Please be a little bit like careful when you interrupt when someone talks. Don't just interrupt in the middle of the sentence because other students will not be happy about this. Right? So we had this type of this type of cases when students would interrupt like in the middle of a sentence. People would complain, students would complain.
(08:16) So be kind of respectful, right? Don't interrupt in the middle of a sentence. If you see some kind of I took some pose, I kind of, you know, want to switch the topic. You can ask your questions. Totally okay. Don't like interrupt in the middle of sentence. But interruptions generally speaking are okay. So it is encouraged. It is fine. So now any questions about uh uh whatever we have been discussed so far.
(08:40) Okay. Communication T alls are here. Of course all of you probably know that you can connect directly via canvas. On canvas you can see inbox right and you can you can click on this inbox and you can find our course and also where you want to contact TA. So me or whoever right? No, maybe not students. Maybe students are hidden from you.
(09:05) To be honest, I don't remember. I think student names are hidden from you. You will not be able to to contact students. That's why we have WhatsApp. If you want to contact someone, please use WhatsApp. If they share their contact informations, it means they are kind of willing open to communications.
(09:28) It is okay, right? If they don't want, so we don't want to just share names and contacts. By the way, I think I hear some noise. Maybe I'm mistaking. Always switch off your mic, please. So, if you forgot then this is Oh, no. Everyone is off. I don't know what I maybe it is some kind of Oh, everyone is off. Yeah. So, thank you for that.
(10:01) Which days will be will be hosted by TAS? So I don't know we kind of working on the schedule. We will not be able to fit every everyone's schedule clearly but we'll be posting those um announcements as we go. Basically I'm going to have we're going to have a lecture on Tuesday. We are going to have my session on Friday. It means T sessions will be somewhere on Wednesday, Thursday the first one and also on Saturday, Sunday the second one.
(10:32) This way Wednesday, Thursday first, Friday mine, Saturday, Sunday another T session everything will be recorded. So if you don't have time to attend it is okay. Now if you want to ask questions then you have to find time to attend one of those sessions. Yes, you can ask questions. Uh, so the the TA sessions uh are they going to cover different material than the lecture? No, no, same same.
(10:58) Usually on Tuesdays I will talk about some kind of um T theory behind you know some kind of models uh from theoretical point of view. On Fridays I will show some implementations. NTA will also show some implementations but they cover pretty much same stuff pretty similar topics right there is there is some flexibility. T can choose what they want to present.
(11:26) It is like um not like I tell them what to do but we have list of topics they follow the schedule and they cover those topics but the way they present and what they presented is already up to them. They are pretty advanced, right? Some of them have PhDs. They are pretty advanced. They will decide themselves what they want to talk about, what they want to present.
(11:45) This is how it works. TT is is is a kind quite quite advanced uh instructor, right? In some sense, because they have their own sessions. They're not just graders. They teach they lead sessions. They will decide what to present. Yeah. I'm sorry. What is your next next question? No, I was just going to say that I was going to ask if the two sessions are are duplicate from each other. So if attend one of them that No, no, it's not like that.
(12:17) We're not not trying to to to duplicate to kind of I know what what what you're thinking. You're thinking like what if I cannot attend on Wednesday? Can I attend on Saturday and discuss the same topics? Not quite. They're going to basically break topics into two parts. One part will be covered on Wednesday, Thursday. Second part will be covered on Saturday Sunday but it is not no not not not the same stuff.
(12:44) No there will there will be some overlaps clearly right but the intent is to cover different topics or I would say subtopics of given week. Okay. Thank you. Yeah. So now uh there is a syllabus you can click here the syllabus if you have any questions uh regarding or organization I always encourage you to look at the syllabus it has information right it has detailed schedule right here what is due when right what we going to cover and and so on and so forth pay attention to this final deadline final project is going to be due on the last day of class. Final project is a big deal. If you miss it,
(13:28) it is a problem, right? So basically if you miss it, you will have to uh contact the school and get approval. I remember at some point around 10 maybe not 10 uh let me think how how many 7 years ago I was taking a class. I approached professor and told that my baby is due like next day after the final.
(13:51) Can I take final during the last day of the class in class? It was in class at Harvard campus on the Harvard campus. He told me if you want to no because what if baby is born like 2 days before then I'm in trouble, right? And he told me you have to basically contact extension school.
(14:11) So he was like very very strict about that. So basically later I learned that it is basically rule at Harvard Extension School. At that time I didn't teach it at Harvard Extension School. I thought it is like flexible. I can tell you, okay, you missed the deadline. That was a problem. Submit it. Submitted two days later. No, it's not how it works.
(14:31) Even when my baby was due, professor told me that it must be must be done by special paperwork through the Harvard extension school. So keep keep it in mind. Okay. So this is final project um which is strict deadline. Other deadlines are slightly different. Basically, I can tell you that quizzes also going to be quite strict. If you miss deadline on quizzes, I will not have opportunity.
(15:02) I will simply not have opportunity to let you retake it because questions will be discussed next time. Quizzes will be due before next lecture you submit the quiz. Next time we meet and I basically answer all these questions. It means if you miss that miss deadline please don't ask me to you know reopen it for you don't ask me to create another alternative quiz for you I always have students who tell who tell me please create alternative quiz for me now first of all it is huge work right to create alternative quiz I hope it is understandable and secondary it is not
(15:37) even clear how to do it because if questions already answered and I am presumably supposed to give you similar questions questions to be fair. But if I give you similar questions given that they already answered, it will be also not quite fair because you already know how to solve this type of questions.
(15:56) Which means if you miss deadline on the quizzes, it is quite a problem for me. On the other hand, you're going to have like 7 days to solve the quiz. You can open at any time. You can submit it any time. There is no time limitation. There is no proctoring. You can basically solve it from anywhere you want.
(16:16) It means please just don't don't miss it during the seven day time you know window you should be able to find the you know 15 minutes to answer those questions. That's my recommendation because I will not be able to reopen it. So I want to just emphasize that I will not be able to reopen quizzes. Don't ask me please. It is policy. Formally speaking it is policy.
(16:41) on the syllabus it tells you that no late quiz will be allowed so keep it keep it in mind it is logistically quite difficult to do second assignments no assignments will be due on each Sunday no except for coming Sunday because you don't have full week right now each Sunday or almost almost maybe each Sunday you're going to have assignment you actually have some flexibility you may submit slightly later but you may get penalty if If you submit it like 3 days later you get like 30% penalty and TS will just you know have to take points. So
(17:16) because we have to follow policy right this is our policy. So please keep in mind that is better not to delay assignments as well but in case of quizzes it is quite important because even if you miss like by you know few hours everything is already basically solved and posted. So, any questions about assignments and quizzes? Final project it will be due on the last day of class. I'm sorry. It is Monday. Uh maybe it is not correct.
(17:49) It is Monday. This should be Tuesday. It is a table. Tuesday. So, it is a typo. Yeah. So, it came from the last year when it was Monday. Classes were on Mondays. It is December 16. December 16th. So not not Monday. So homework contributes 65% quizzes 20% final project 15%.
(18:16) For the final project you you will have to choose what what you're going to do. And there is special week dedicated to this project. Final project selection. It will be uh due on November 25. Okay, November 25, just right before Thanksgiving, you have to select your project. I will post list of kind of suggested topics for the final project on November November 9th and you will have some time couple of weeks, right? to make your selections. You can suggest your own project if you want. It is totally okay. You don't have to select whatever from
(19:01) the list which is offered. You can suggest your own topic. It is okay. If you work in particular field and you feel that it is it is better for you to work on particular problem. Yeah. Totally okay. It is even encouraged actually. So you can select your uh if you took your deploring during summer it is very similar to that to that case but in this case final project must be explicitly dedicated to natural language processing right of example such thing as structural topic modeling which we didn't cover during summer which is not even neural network. We understand that natural language
(19:39) processing is not only about neural networks, right? There are also other techniques to work with language, which means it could be different from what we did during the summer. So, so if you want to use some kind of artificial intelligence, generative tools, right, generative AI, it is totally okay.
(20:10) lecture in this course it would be kind of strange not to use it if you want to study it right and I can tell you that basically it is inevitable that in workplace you're going to use this tools 100% um uh you know that everyone is going to use these tools uh when they quote even when they write emails letters whatever right so it is ine already clearly inevitable because efficiency is going to is going to increase if you use this tools.
(20:40) But at the same at the same time, please make sure that you always site that you use it and always understand that whatever you submit is basically considered to be your ultimate work. You are responsible for this result, right? You can say it wasn't me, it was AI, so I'm not responsible. So now this is a technology policy. This course encourages students to explore the use of AI, right? So you can use it, right? But every every time you have to site it, you have to indicate that you actually used particular AI.
(21:15) You can take your code and you can actually try to fix it. You can can try to improve it. you can try to ask um uh to make some specific modifications and AI will be able to in most cases will be able to help you. It's not going to work right away. So you will have you will have to learn how to do it. Basically it's not going to work right away.
(21:42) You have to uh you know it is kind of iterative process. You have to be able to clearly explain what you want. You have to be able to supply maybe working example which you're trying to modify and so on and so forth. But this is actually good skill. This is very nice skill. Um nowadays it is encouraged in this course.
(22:03) So I hope it answers your question right this part about AI technologies. So now uh what else? No let let me switch back to to the slides. Um uh textbooks. We have actually plenty textbooks which you could use for this course. I couldn't use I couldn't use like 10 books at the same time. I couldn't choose just one book because it is difficult to find one book which covers everything especially if you want wanted to be covered in a particular order you know if you want it to be covered in particular way.
(22:36) It is basically impossible to like to find a book which you like. I mean I never like any particular book to be honest. No I like books about like mathematics right? I mean if you want to find a perfect book about a book about calculus it is not a problem about you know linear algebra not a problem but if you want to find book about for example natural language processing for me it is almost impossible.
(23:05) So I list some popular books which you can see here by bird right very very popular book then next one is national exposing with transformers we're going to slowly introduce transformers maybe we're not going to spend too much time on transformers in this course because it is not only about transformers right transformers is a very specific type of models which we will cover sort of brief Hopefully right but we will not spend like all time on transformers.
(23:40) Uh so so now the question is about chapters. So basically uh if you learn something like uh uh data science for example maybe the data science is bit little bit easier but if you learn something like natural language processing essentially as I mentioned already you don't want to just follow particular book maybe no I mean we we can follow particular book if you really like this book uh then we can meet and discuss chapters just like you know they do it like in reading classes they read particular you know pages and
(24:11) then they meet and discuss It's not how we are going to do it. We're going to discuss topics and I encourage you to actually shift your thinking from following a book to following list of topics. Why? So if you want to understand particular topic now for example you want to understand uh let's say recurrent neural network you can you can um use different sources to learn about recurrent neural networks. There's no reason to stick to particular book.
(24:43) you you know that today we have to cover the current network that works okay you read it from different sources and by the way uh different styles I mean not let's say you may like one style writing style I may like different style I like when people like more like moriented I really like when I see formulas in the text some people on the contrary don't like it they hate it they just want to read it and they don't care about must behind they perceive information this way much better that's why for different students different books I mean if different students use different books it is totally totally okay that's why we
(25:24) going to follow topics rather than we are going to follow particular chapters in the book and if you ask me what chapter to read from this book for the next class it is quite uh I mean it's not ready I mean I can try to match but it is not ready I don't have this list available anywhere which means I will not be able to answer this question right away.
(25:49) But if you open the syllabus and say next time we are going to talk about uh for example uh LSTM okay we're going to introduce some kind of regard networks and you can read about this from any book you like from online resources as well you can implement it as well even before you come to class you can try to implement it is totally okay so you don't have to stick to a particular book you on the contrary try to understand the topic this ultimately this is ultimately ultimately what you want. I remember when I was like undergraduate student, I
(26:20) was taking many classes on quantum field theory and books would have like 800 pages. It was impossible to just read one book because you you you going to get lost right away. You would have to switch from one book to different book to different book to different book to understand particular topic.
(26:44) So you have to start already thinking about how to understand particular topic not not particular chapter of a book that's what my recommendation would be okay for example recurren networks read everywhere you can about recardial networks even research publications books specific chapters whatever you like if you don't like my books which I listed here you can find maybe different books more more useful for you it is totally okay Right.
(27:10) So next book is about that is called text as data. It is designed for social scientists. Um but there is one important chapter about structural topic modeling. In this course we are going to cover so-called STM structural topic modeling. Now it allows you to basically identify topics within particular corpus.
(27:33) Now you have text and you can you can identify particular topics and you can sort of almost like clustering in some sense if you will you can you can identify what kind of topics contribute to particular piece of text it is called STM uh structural topic modeling and particular chapter in this book which is chapter 13 discusses exactly that so this book is not I believe available anywhere you don't have to buy it alter Al alternative to this chapter would be publication itself.
(28:06) If you open this publication, you can actually get same information from the publication. You will be able to understand without reading particular chapter from this book. But just original research publication. They kind of nicely presented this information for the readers, right? Keeping in mind that people could be from social science, but original publication may be a little bit more difficult to read but readable, right? So you don't have to buy this book if you don't want it is not kind of required because we have alternative.
(28:37) So any uh yes by the way structural topic modeling is implemented basically already packages available in R. It means for this particular um I would say class of models threshold topic modeling we are going to use R. This is the only place where you have to use R. We'll slowly introduce R. You don't have to know R because we are going to only basically use R for this specific example this specific type of models structural topic modeling.
(29:16) Uh I remember some students took uh final projects as well. So maybe some of you will take final project on threshold topic modeling then you encouraged to use R you cannot really use Python it is not available I mean you you can maybe incorporate R but you cannot use a package in Python right that's why R so yes in this case we have to use R and there is another book which is listed as optional textbook because this is a little bit maybe more difficult to read if you're not kind of prepared Right. Italy has a little bit more more mass.
(29:50) It means it is a little bit difficult more difficult to read if you uh didn't get used to this. It is optional. I like it. But if you don't like it, it is okay. You can use previous ones or maybe something else as well. Again, as I mentioned, you choose particular topic structural topic modeling and you read from different sources from different publications.
(30:17) Even some people publish results and they discuss what model is you may find someone's explanation maybe better right even than original one it is possible if you find it this way it is okay you can read it from some research publication maybe so any questions so uh professor. Yeah. Yeah. Uh since uh none of the books are mandatory for us to go through, uh would there be a case where you'll ask us to read some pages and then solve the assignments based on that or it's whatever we explain in class. You know what the thing is? I I never ask students to read anything in
(31:06) advance. We are going to cover new stuff and then there will be assignment. If you feel that a lecture wasn't enough, if you feel like examples which we provide during T sessions, during sess section on Friday, they are not enough. You can read, right? Some students may not even for specific topics have to read anything. We'll just implement it because everything was clear.
(31:35) If not clear you can open particular you know uh readings on particular topics on particular models and what you open is totally completely up to you right so yeah so I'm not asking you to read anything in advance no of course if you want you can read it in advance especially like I know that in months for example it is nice to to come prepared because otherwise you may get lost it basically never happens right and nobody comes prepare that only few students but those who come prepared they become really really proficient right so you may try to read in advance it is it is encouraged but it is not
(32:11) required it means answering your question will I ask you to read anything I will never ask you to read anything I will ask you to solve problems and how you approach it is up to you let me by the way show some example examples of the of the assignments [Music] Well, maybe first of all quiz. So, this is uh example of the quiz.
(32:43) It has five questions. It is going to be your first uh quiz, right? You will have to look at this particular network and your question is about so-called over propagation. We'll have to compute output from this neural network. Whatever you see here is called neural network. Today we'll discuss it.
(33:07) Second part is about some kind of uh neural network and your task is to compute number of parameters. If you understand what it means this particular implementation in Python we can easily compute and so for and so forth right. So this will be kind of kind of example of the quiz. Similarly, we are going to have assignments which look this way and you are going to be asked a number of questions or typically about like three questions.
(33:45) In this case, you will have to consider particular neural network and you have to compute some kind of validation accuracy and so on and so forth and then you provide your solution. So in this case you can see that I tell you solution solution here. You may wonder how is it possible to you know to present your code basically the solution here.
(34:05) So basically what we are going to going to want to have we going to to want to to want to have some kind of report from you and it means you will of course submit your implementation like maybe Jupiter notebook or whatever you like maybe you just you know use Python files and then important results and also maybe key part of your code you just take snapshots and basically insert here to present your results or maybe some discussion as Well, so you submit your Jupiter notebook for example and also your solution. Now why do I do it this way? First of all, it is nice reference
(34:43) for you for later. If you if you want to understand what you did, you will have code and also you'll have some kind of report which is going to look this way. First of all, you have problem and then you have solution over there, right? Not at least when you do the problem and you remember what you did, it is easy for you to take snapshots of important like key parts, right? And also of results and also write couple of sentences about your results and also about implementation of your uh of of your solution. Uh while you are doing this, you remember how to basically do it
(35:20) right, it's not a problem at all. But later when you decide to sort of come back and you decide decide to kind of refresh, it's going to be very handy for you. That's why we do it this way. Secondary, if you use like Jupiter notebook, okay, you can you can say what's the problem? I can't print out my PDF file out of Jupiter notebook.
(35:41) Yes, you can. And basically, this is allow it. You don't have to use this template. If you want to use Jupiter notebook, it is totally okay. You can just basically create PDF out of Jupiter notebook and submit it. It is okay. So you will have to submit Jupiter notebook itself. So TAS can run it and also report which you produce by saving PDF file.
(36:04) It is one approach. A second approach if you for example don't want to use the PD notebook and you kind of you know don't know how to present your results then just take snapshots and insert in this file. That's how it is going to be. Later we'll talk about this a little bit more. So now let's see next prerequisites in this case you can see calculus.
(36:32) Well in case of classes like math statistics calculus is quite quite important. We need to know how to take specific types of integrals. In in case of deep learning I typically ask students to sort of implement implement back propagation. Now it means it is important to understand how it is derived maybe right in this case I will not ask you to implement back propagation it means in this case calculus is probably least important nevertheless I'm asking you to sort of u pay attention to it because I want you to kind of understand the formals which are presented in the books
(37:12) also sometimes maybe we'll present some simple formals Right? That's why I put it as a sort of prerequisite. Formally speaking, calculus in this case is not as much important. You don't have to integrate. You don't have to differentiate yourself. No, basically will never have have to do it because we are going to focus on particular implementations of this type. This these are going to be our problems.
(37:39) If you took my other courses, I have like different part of assignment as well. on those courses where you you are going students are asked to do to do some kind of theoretical solution kind of analytical solution in this case not in this case we we are going to focus on basically Python implementation of this problems it means calculus is no important so you understand what's what's going on if you see some formulas but not critical so you don't have to worry much about calculus even though formulate is like prerequisite right when I say derivative And I hope you understand what I'm talking about just
(38:16) so you know terminology basically so we can communicate only for these reasons. Second one is a probability theory. When we talk about generative models, generative adversarial networks for example, in that case uh we generate output as a result of uh input which is random input to network which is like random variable which comes from normal distribution.
(38:48) For example, when I say such things as random variable, such things as normal distribution, I assume you know what it means, right? No, at least intuitively normal distribution. So I don't have to spend time explaining what random variable means. At the same time, it's not like we have to know what what definition of random variable is. No, it's not the case.
(39:06) You just need to have an intuitive understanding what random variable means, what normal distribution means, what normal random variable means. basically right that's what what we need to know about relity theory is just basic stuff so we can communicate essentially and Python Python is more important we are not going to introduce basics in this course you you you're expected to already know how to code it means you should be able to understand what we are doing in in during sections typically so we assume that you famili familiar with uh types of uh
(39:48) uh problems covered in this uh E7 course, right? Introduction to Python. So now again we have uh assignments which will be due on Sundays, weekly assignments due on Sundays. 65% of your grade first one is due on September 14. So you have at least one full week, right? Quizzes will be due on September 9. It means next Tuesday.
(40:17) Literally next Tuesday by the uh by uh 810 by by 8 10 p.m. So we can talk about those problems in class and final project will be like 15%. Yeah. Question. Yes. When is the homework is released and also when the is is a quiz is released? So quiz is going to be released right after after the class as as for the homework. Nope.
(40:50) You know what? Uh you will have one week to solve your assignments, right? It means somewhat like on Sundays, but the first one will be released a little bit earlier, right? But later it will be released after you submit your previous assignment. Okay. on Sunday. No, maybe I can res it slightly earlier. But the problem is if I do it, uh there are always students who submit uh assignments to the wrong books and it becomes messy.
(41:25) So I prefer then the line is already passed and then I publish next assignment. So we can do it a little bit earlier but I'm not sure if it's going to be helpful. Anyway, you have to finish previous one first. Also, if you think about this, you're like working on the assignment, you still have like one day and you get next assignment published.
(41:42) It's not really nice feeling probably, right? You want to get rid of previous assignment and then you get next one. So, I don't like posting assignments before the previous one is due to be honest. So, I'm not sure if you want to have like three assignments, you know, posted for you. Maybe some students like it.
(42:02) I don't know. That's why I probably will post it on Sundays. Yeah. One by one. One by one. Yes. Which means first one can be can be posted basically today but next one will will be posted on September 14 right here. Does make sense? Yeah. So now Yeah. assignments. Um it is due every Sunday.
(42:32) understandable time as Boston time. It is clear right that we are talking of Boston time. Doesn't matter. It doesn't matter where you live. It's going to be Boston time every time by the end of day in Boston here in Cambridge. So solutions which are posted late will be basically penalized. Submit key parts of your code, results, plots, tables, brief discussions as Microsoft Word or PDF, whatever you prefer.
(43:05) So basically you can include your solution here which is like key parts of your of your solution right and result and also discussions or you can create PDF. So it doesn't tell you that you have to use my my template. It means if you want you can easily solve the entire assignment using Jupiter notebook and create PDF out of Jupiter notebook. In that case you meet requirements. PDF is submitted.
(43:31) At the same time you also it tell it it tells you you may generate this using J notebook if you want. All right in case of R r mark markdown. So uh if you also in addition to this you have to submit your original code.
(43:50) Now if you see that something is wrong we want to check you want to improve it maybe you want to help you maybe to find a mistake know it is difficult sometimes what's if you have time if have time to also correct your code they will want to definitely will want to try to run it right so in that case uh we need to have your code it means even if you use Jupiter notebook to create PDF file please submit your JP notebook as So basically code is required not only snapshots. Snapshots is not a solution.
(44:21) Snapshots is going to be some kind of report right which you can read and see what you did and how you did it and what results are. But also solution itself which means your code itself should be submitted. If you have like multiple files we don't want to have like 15 files on on canvas and then download one by one one by one one by one and then remember who who submitted what in that case or by this reason we ask you to submit a zip file which will contain your uh your code. Now if you submit just one J notebook it is okay just submit one J
(44:58) notebook. If you if you submit like two or more files then please zip it and submit zip file plus in addition to that please submit report so we can quickly see what you did or if you did at all right and also if you have to kind of you know dig deeper if you want to see what what exactly is happening we'll be able to download your code if one file we can download it if multiple file files please zip it into one file and upload to canvas.
(45:33) So report is is report and zip file or your code is separate file. Uh report means something. Report means doc or PDF. That's what I mean by report. All right. Report means regular Microsoft Word file or PDF file. You can create this file PDF file using Jupebook for example or you can use template which you see here and just insert important parts of your solution and results and the discussions. This is what I mean by report.
(46:12) This is your report. By the way, I have example. Let me show it. So under files you can open assignment assignment number one. I know assignment and then you can open example. Professor we can see the quiz only. Yeah. Yeah know. Yeah. Thank you. Yeah. Yeah. I just didn't want to show too many intermediate files not to get you confused.
(46:56) Right now this is what you can see under files. If you click on files on assignments on example you can see here files assignments example you can find example over there it is some kind of solution. I think it was for deep learning class but basically requirements is is going to be same. So in this case they did some kind of um assignment on neural networks. They presented some results.
(47:26) Second problem whatever it was next problem and also they implemented as appendix they added quotes to this solution. You don't have to you don't have to do it this way. can actually continuously in insert your code and then some kind of results maybe plot or table or whatever and then discussion right they did it differently they discuss and they show results and code is at the end that is also okay you can do it this way so you can find this under files assignments example and there is example a I will also share final project
(48:11) example. Right now you can see only whatever is published. You can see this example. [Music] So two files report similar to what you just saw PDF or or micros Microsoft Word and also source file whatever you use to generate it. It could be Jupyter notebook or it could be like files which you can zip and submit it to canvas.
(48:41) Any questions? Um so make sure that the code is submitted. If you have multiple files please submit them as single zip file. Also in addition to this submit your report which means PDF file just as you saw uh assignment counts as 65 as 65% of your final grade. So now there will be uh solutions. there will be uh solutions uh no sort of I would say not quite not not not key as as you understand it I'm not going to post like key to kind of key to the to to to the assignment right it's not not going to be like literally like key to assignment to maybe sometimes I will post my own solution but uh we going to
(49:45) pick a number of solutions from you kind of randomly Yeah. Or maybe you'll see which are nice one, right? Typically, it's not like it's we don't not necessarily choose the best ones. We just choose the ones which are correct, right? And we post those solutions on canvas in particular folder. I will share a link to this folder with you.
(50:12) Basically what whatever is supposed to be found under can be found under files and you can see how those five students or so did it how they solved it right so if you don't feel comfortable sharing your solution by some reason please contact Andrea and discuss what options we have right basically I think that we never had this issues right but if you really feel not comfortable comfortable by some reasons.
(50:43) If you didn't do it correctly, we will not choose it. Right? If you didn't submit like entire solution, we will not choose it. If you did everything correctly, but you don't feel comfortable sharing your solution by some reason, please contact us because every week we are going to share around five solutions and I will explain you why we do it this way, why we sh showcase solutions to each assignment rather than just posting a key.
(51:09) Right? So let me let me uh tell you what's happening in case of basically deep learning right in case of deep learning when we use neural networks we are going to solve problem of type where we minimize specific function so-called cost function. So we minimize uh cost function. So this is what we do in case of optimization of neural networks.
(51:46) You remember maybe like in linear regression for example you fit a line which best basically fits your data set. There is unique solution. No unless there is a let's say not enough points maybe some kind of singularity there is no solution at all. But if there is a solution there is always unique solution you can analytically find solution position of your line in case of linear regressions in case of neural networks problem becomes much much more complicated.
(52:14) So basically in practice we have to minimize some kind of so-called cost function right some kind of sum of squared deviations for example and because our network can consist of multiple multiple parameters typically many layers many neurons function in practice is going to look somewhat like like this highly highly nonconvex function convex means this one this is This is called convex.
(52:42) In case of neural networks, you don't not going to have convex function. We're going to have something like this. Now, if you ask a question, how to minimize this type of function numerically without even taking like numerical optimization courses, you can understand that this problem is quite difficult, very different from that one.
(53:06) So my cost function as a function of parameters looks practically this way which is highly highly non-convex and you want to somehow minimize screen is frozen homework assignments. Are you sharing something else? Uh you should be able to see my board white board. Can you see it? I don't.
(53:39) You should be able to toggle it somehow to switch you basically minimize my screen and maximize my my board. Can you do it? Yeah, I see it. Thanks. Yeah, we can. Yeah. So, yeah. So basically uh if you want to try to minimize this type of function numerically we have to somehow go downhill basically using numerical numerical algorithms and one students will go downhill and we'll stuck here right so it'll be one solution this probably probably not the best solution best is somewhere over there maybe right another student will get different solution there's like one student gets one solution, right? Second
(54:23) student will choose second set of parameters. Completely different solution, different student will choose this set of parameters. Why? So because it is stocastic process, right? We start somewhere, we optimize it and we stuck at some local minimum maybe potentially. So basically neural networks which you're going to get going to be quite different even if you use same architecture same data set even if you split your data into no so-called training data set validation data set the same way using some kind of seed we all may have different results
(55:00) right that's why we want to actually uh share solutions so we can see what could be what else could be Why did they get different result? Why this result is better? For example, we try to optimize the same function. Maybe they initialize weights differently. Maybe they use different types of u activation functions.
(55:25) Some if they use different types of activation functions, by the way, it means we already change this function, right? You forget changing changing so-called activation functions means we change architecture. It means we change cost function. Even if you don't change architecture still you may get different results.
(55:43) That's why it is quite important useful to see what other students obtained. That's why we going to post showcase solutions so you can learn from each other basically. So now uh this is yeah this is what this about assignments. Any questions? So now final project we already discussed. I have a question. Yeah.
(56:16) Uh in your previous classes um you used to drop one of the quizzes because you know it's probably um one of the quizzes you you used to drop and you also used to drop one of the um assignments and take an average of that uh remaining remaining assignments. Are you not going to adopt that? No, it's not not the case in this course. Syllabus doesn't say it. We're not going to do it this in this course.
(56:38) Uh well, there is a reason for that. If you talk about maybe mathematical statistics for example, right? It is more difficult glass and uh it is u not difficult to get everything like correctly right in this case especially since you are given opportunity to use generative AI it is easy to actually get almost perfect all results almost perfect that's why we are not going to do it yeah that's a good question but we're not going to drop lowest quiz we're not going to drop.
(57:17) Well, basically not drop, but as you mentioned, we somewhat drop lowest assignment score. No, we're not going to do that in this course. It's a good question, but no, it's not the case. Please check the syllabus. Syllabus doesn't say it, so we're not going to drop it. Uh, any questions? By the way, there is one important detail.
(57:51) Sometimes I like to put kind of goal for you. I'm not sure if I want to do it on the first assignment, but sometimes I tell you please make sure that you achieve particular performance, right? So I want to kind of make sure that you not get result which is very very bad bad result. I give you particular performance that is above like the best performance. So everyone should be able to get it.
(58:17) But it may take some time for you to experiment with different maybe architecture with different maybe different uh ways to initialize your for example your parameters and so on. Right? So it it takes some time. Sometimes on some assignments there there will be specific goal. It is probably going to be the most challenging part when you are going to be asked to achieve specific performance.
(58:44) I'm going to tell you please achieve accuracy of you know 85.85 85 all better right it will be maybe difficult otherwise it shouldn't be shouldn't be very very difficult that's why we don't really drop anything so any questions okay I'm going to open um um I have one question from uh for the final project um uh the working demo and also similar to what you had said previously we have to submit the um the repo not the repoint everything the same zip format is that correct so in this case it is slightly different um uh actually in this case you have to
(59:36) submit uh report it tells you here seven to seven or more pages right up to 15 pages of your report also slides which you're going to use to produce video you're going to produce presentation 10 to 15 minutes presentation and this is what you need to submit and also work demo means basically code which we can run and see it is working that's what it means so there's a requirement it is a little bit more than assignments in this case report also slides and also video presentation now it is 9:10 let us make a break and we'll Discuss a little bit
(1:00:16) more. So now uh later we will talk about this. I will post detail detailed instruction
(1:09:20) on the final project. We'll provide some templates. We'll provide examples. Yeah. Questions. Hi there. I was wondering were you trying to say that the test error is somehow three-dimensional by saying that it's convex just kind of noticed. So in this case I'm talking about specifically train error not cost function right maybe there's not even error but no okay this some kind of error which is one dimensional j is one dimensional and uh so this is like some kind of uh you know deviation sum of square deviations for example but w is a set of parameters in this case of
(1:10:11) set of parameters is sometimes million dimensional right so it is r in dimensional space it depends on how many parameters you have d is number of parameters so basically I would have to sketch it this way many many many many w's w first w second and so on wd then my j which is one 1D it will be surface in this multi-dimensional space surface this way. That's what I meant. I didn't sketch it that way.
(1:10:53) I sort of pretend that W vector this line represents all other dimensions for W's. And convex means uh well looks this way, right? It has definition. I will not introduce definition. But that's how convex looks. Basically, convex has only one minimum. one local minimum which is global as well. In this case we have many many local minima there very difficult case to work with questions.
(1:11:20) Oh thank you. Yeah yeah yeah so this is how it is. Yeah I didn't explain but W vector this line represents many dimensions because W is a set of par kind of represents set of set of parameters which means many W's we have over there and one J. So okay this is about final project. You will select a pro uh project for you.
(1:11:48) You will specify that time you will have to specify what data set you're going to use what your inputs to network what your outputs and so on. So basically you'll have to specify what you're going to work on. And by the way I have one more file which I keep it to myself but I can show you it as well. So this is my basic schedule. On Tuesdays we meet lecture then my section on Friday lecture quiz is due section Friday assignment is due and so on. This is how it is going to be.
(1:12:23) Later on we'll have some kind of u [Music] uh we'll have project selection. And there will be no lecture at that time. It will be dedicated. This time is like midterm basically dedicated for you to select the project. Now of course it is not only during those two hours you have to select the project.
(1:12:49) You will have to select it during you have to select it maybe during the previous week. That's why you don't have assignment over there. You will have to work on your project selection. And then last assignment will be on December 7 and then final project is due on December 16. So basically you will have nine more days to finalize your final project.
(1:13:19) And the sections in this case will be on Fridays from 9:10 to 10 10:10 p.m. I think it is also posted on on Canvas. If you check syllabus you can find it there as well. Yes, you can have a copy of this of this uh the schedule. But I can tell you that actually this is is already is already on Canvas because it tells you lecture section quiz is due, assignment is due. It is same but using different format, more detailed format, right? And so on.
(1:14:00) Oh yes, I can pause this one as well. If if you want, I can post it somewhere. Uh and also it tells you sections will be from 9 10 to 10 on Fridays plus last two sections uh from days. Okay. Now let's uh let's move on uh days of interest. Please check this uh slide later.
(1:14:34) uh basically you have to keep in mind when final is due when assignments are due quizzes are due we already discussed. So now let's talk about um uh basically neural networks right. So first of all let's consider example which doesn't have any neural networks. It's a simple example where I'm trying to fit linear regression. Basically it is called linear regression.
(1:14:58) If you don't know this, it is called linear linear regression because it is a linear in parameters. That's why it is called linear regression. X is like data and we say X itself is U u1. U1 is like feature which we mainly reconstruct. U2 is X is second feature. U3 is X cubed is next feature.
(1:15:24) And based on those features U1, U2, U3, you can basically construct your output in a a simple linear fashion you can think about like Excel spreadsheet where you have X column, X column, X cubed column and you may even forget about where this data came from. You just have X X2 X cubed and you fit linear model. That's why it is called linear regression because it's same basically mechanics just in just like in case where there are no transformations.
(1:15:50) very same model. Now there is drawback of this approach. You have to manually design this features. You can see manually design features. How to do it manually? Now we have to plot it somehow. You have to analyze this data set visually. No, sometimes you can do it right. Visualization is is is powerful tool in such cases.
(1:16:16) But what if we are talking about multi-dimensional input even like you know image for example has multiple signals every pixel for example has three numbers right uh three colors three numbers in case of language it's even more difficult to have sequence of vectors every word essentially is going to be like a vector in multi-dimensional space no word is what word is a word from the diction ary right that is kind of vector basically sentence means sequence of vectors your input is going to be sequence of vectors not even x not even x vector but sequence of vectors it means you cannot possibly visualize it to suggest nice model it means you have to design algorithms
(1:16:59) which allow you to introduce this type of features uh no kind of automatically sort of you want to automate the process that's exactly what we are going to When we going to build neural networks, we are going to basically design a model where those features will be designed automatically. You can say what's the problem? Put power here. P1, P2, P3.
(1:17:27) No, it's not going to work. P1, P2, P3 will be very very difficult problem because it is highly nonlinear even more than what I sketched, right? It is not work. It is nice case. But this will be much worse if you say power is parameter power is parameter power is parameter I have to optimize with respect to W's and also with respect to powers will be very difficult problem basically impossible to solve numerically highly highly nonlinear problem that's why people came up with this very simple idea actually they say let us introduce nonlinearities but there will be some kind of
(1:18:05) intermediate linearities in some sense Right? And they say let us design it work this way. First feature you want will be some kind of function applied to a linear combination of X's. Next you next feature will be function applied to linear combination of X's.
(1:18:26) So in intermediate uh a linear combination is there. It is it turns out that it is very helpful when you try to optimize this type of network. It means when you try to find this minimum numerically having your functions in this form is very useful to be precise it is because you can easily analytically differentiate this type of function.
(1:18:51) Okay, there is some nonlinearity f but remember chain rule from calculus. If I want to differentiate u1 with respect to w, it is going to be f prime times that frime at at this point times derivative of linear function. Derivative of a linear function very easy. That's why basically neural network became so popular.
(1:19:18) You may wonder why this specific form because we can easily differentiate basically because we can easy apply chain rule. That's why we want to have some ultimately some nonlinearities but we want to design this model using intermediate linear transformations just as you can see here u1 u2 f yes f which is not linear function applied to linear function of x's just like in case of linear model basically you may remember like logistic regression for example looks exactly this way some kind of function applied to linear combination why linear because it is more convenient essentially Then what about next yhat which is which is output f applied to linear combination of use in that case it was
(1:20:00) nonlinear transformations and then yhat is a linear function of those linear function with respect to those use in this case we do it not that way we say f applied to linear combination which is going to give us more flexibility to reproduce some kind of nonlinear dependence Clearly if you have many many you know this type of intermediate features use also many so-called layers you ultimately may get basically no any kind of any nonlinear function which you want with any accuracy which you want basically given any nonlinear function
(1:20:42) given any kind of tolerance which you want to achieve kind of accuracy you want to achieve there is a given number of neurons given number of uh uh layers, right? Which allows you to achieve this accuracy. If you are willing to to do it differently, if you're willing to take more and more use and maybe more and more layers, you're going to achieve any nonlinearity you want ultimately, but every time intermediately you you're going to have linearities, which is very helpful. So now this is kind of you know
(1:21:15) visual representation of what what is happening in this case. In this case we have um image as input and we say let's construct some kind of intermediate features. It is like U1 U2 and so on. Basically there will be some kind of ways of looking at on input. Okay this is some kind of lines which we select some kind of dots which we select and so on. Right? different representations of our input.
(1:21:43) Then we take different uh or next layer. Each feature is like u which is a linear some kind of function of inputs from the previous layer and so on. Then we get new network. It is just you know visual representation. If you if you want I can go back to this example and try to sketch it how it would look.
(1:22:11) So basically what I'm saying is if I have this transformation which is not not network not not your network but just example which we have on page 17. So in this case I say my output which is yhat is going to be some kind of parameter plus parameter * x which I call u1 plus parameter * x which I called u2 second feature plus parameter * x cubed which is u3 okay how to sketch it so basically in case How it works? People say let us sketch it this way. No, there is like X basically which is input.
(1:23:03) I'm going to use some kind of you know representations different from neurons. Neurons are represented by circles to differentiate. I'm going to sketch box maybe right. So maybe box then I say let me construct you first which is simply X. So this information goes in and I construct you first which is x.
(1:23:31) Then this information also goes into next box and I get u2 which is x next box information goes in and I get u3 which is x cubed. So basically that's how we can sketch this type of transformation right then output will be yhat. How do we get ahead based on those inputs already this way and we get sort of visual representation of this transformation you may say why do we need it right so basically we need it because we this way can visualize what's happening especially if we have much much much more more intermediate transformations and we can make some
(1:24:10) interesting suggestions some improvements for example you can say why not to connect it that way for example why not to connect it that way will be called shortcut connection turns out to be very powerful tool to introduce shortcut connections it means plus x itself for example and so on so people try to visualize this type of transformations now in case of neural networks it is not so we don't want to introduce x x cubed we have some kind of f applied to linear combination that's what we're doing in of neural networks. So now in this example right exactly our
(1:24:56) neural network now we can sketch it as well we can sketch it easily right maybe I will not write down the formula so maybe I I can write down some of those let me say page 19 is second example which is already neural network this is already neural not work. We are going to say there is some kind of input X first and also X second.
(1:25:31) We understand that there is output Y. Then there is clearly data set and we say let us try to somehow fit surface here essentially. So we have to fit some kind of surface. Maybe it is not nonlinear dependence. you want to introduce introduce on linear dependence.
(1:25:58) Now the question is how to do it if you want to still have some kind of intermediate linearities and the approach will be just as you can see on the screen. First step introduce first feature f which is w0. All right. Plus w * x + w 2 * 2 x. My first feature. Now notice it is not just x. U1 is not just x. Not just x cubed. Not even x to power p. It would be difficult but it is still not linear not linear fun nonlinear function applied to still linear function of x's www are going to be trainable parameters during optimization we're going to obtain best set of parameters w that's how that's exactly how we by definition
(1:26:49) introduce new network we say u1 this feature is going to be constructed this way www must be obtained by minimization of cost function we don't just you know kind of manually say it is like x cubed no we don't want to do that we want to give some flexibility so neural network will decide how to choose w's but class of functions will be exactly this way f applied to linear function because we want to have some intermediate linearities when we differentiate it's going to be easier to differentiate u very similarly f applied to w0 0 second
(1:27:32) W first second W second this way very similar on different W's clearly but very similar idea and finally this is like first layer finally yhat is going to be also some kind of f potentially could be different f for yhat f could be different doesn't have to be same F is called activation function.
(1:28:05) Then we say W 0 + W1 already multiplied by U plus WC multiplied by UC. Okay. This way that's what it means, right? Yeah. questions is just the f of yhat called the activation function or are the other fs called f f itself is called this f which we apply here f okay so not the other fs and those are also activation functions oh okay okay yeah for each layer we have we have specific activation functions typically as we move from neuron to neuron within same layer we use same activation function so basically this could be first activation function first activation function but this is already
(1:28:47) For second layer for the output layer could be different activation function generally speaking right people don't make them different they keep it same within same layer so basically my network becomes so you can understand that it is like f in this case fi w second + w multiplied by f again apply to w1 1 W11 X first + W 2 1 first X 2 and next one is W second this one multiplied by UC which is up here it means F first applied to W 0 plus W first 2 * x1 + w from here second multiply by x seconds. So why did I
(1:29:58) write it this way? I just wanted to emphasize that neural network is basically always going to look this way. Almost always it is some kind of nested functions, right? Function applies to linear combination of functions apply to linear combination of functions and so on. Similar to what we did before, we can sketch it. Actually, we can say there is a signal. Let me now use circles. They are so-called neurons.
(1:30:28) We use circles to represent this this uh data x1 x2. more than that to represent U1. on U2 we also use circles in this way it will be U1 it will be UC what is U1 U1 is F applied to linear combination of X that's why we connect it this way we think that information goes that way right also there is bias there is this kind of w0 people typically sketch it as well basically it means w multiplied by one and the signal also goes in even though one is not real signal it is like fake signal just to to sketch my bias. What about you to very
(1:31:16) similarly right? It means I connected that way. So it is like visual representation of manual network which allows you to sort of modify it to improve it to suggest something better. Maybe that's how people sketch it manual network. That's how it looks. You can generalize it already. You can take more layers if you want.
(1:31:41) It's going to be function applied to linear re combination of functions which are applied to line recomination of other functions or maybe inputs like in my case it is not work. Any questions about this? So important question how to choose activation function. Turns out that if you have many many many layers, many many neurons on every layer, it is not as much important how you choose activation function.
(1:32:08) To be precise, it is not much important if you want to reproduce particle dependence. From optimization point of view, from practical implementation, it could be quite important. So it means in practice, it is quite important to play with those activation functions. In practice, you may get completely different results.
(1:32:27) But from the theoretical point of view, let me say note. So basically if I choose any activation functions which are not linear functions those fs and I have particular dependence let me say y depends on x in a very very specific way right let's say this is true dependence turns out that if I have sufficiently many layers sufficiently many neurons I can always reproduce this dependence with any accuracy I And so there is a theorem which basically demonstrates exactly that I can reproduce my dependence with any accuracy I want
(1:33:15) right so deviations will be basically approaching to zero as number of neurons number of layers go to zero this is my neural network that's how how I can do it no matter what activation functions f you choose you can always achieve basically any dependence you want.
(1:33:40) That's why so it's not important what you choose here from theical point of view from practical implementation can be quite important because first of all it may impact number of neurons number of layers you have to choose secondary it may input conversions remember many local minima where you're going to stack next and so on right so it matters what you choose for your activation functions yeah questions So the question is why they are called activation functions. Interesting question. Let me let me talk about this.
(1:34:14) So this is how we can represent it. Basically every connection connection is associated with particular W. Let's say W11. It connects first X to first U. You see first X to first U. W1. So this connection is W11 basically. Now why are they called activation functions? Right? Now you can uh think about ways we can or we should choose our activation functions. Right? Let me keep it this way.
(1:35:01) Why are they called activations? This was a question. So it's interesting but if you think about chain rule it becomes obvious why we want to choose f applied to linear combination because we want to differentiate with no kind of you know difficulties. It is easy to differentiate f applied to linear combination.
(1:35:32) But historically it wasn't actually introduced because of that. It was introduced because people discovered how brain works at least at least approximately how brain works. If you take biological neuron, let me say in quotes because maybe more complicated, but at least ID is similar to to what I'm going to tell you now.
(1:36:05) So this way and let me say in biological neurons basically what happens is you have neuron it is specific type of cell right and it produces some signal neuron it produces some signal and then you have output here and you get result. So this way some kind of signal some kind of signal and you you get output from here.
(1:36:39) So it turns out that in case of biological neurons they have specific kind of you know extensions which are attached to different neurons and those extensions sort of carry the charge like literally charge. If uh cumulative charge which enters next neuron is above threshold basically above threshold the next neuron gets activated. If it gets activated it will pass this signal further.
(1:37:08) If it is not above threshold if cumulotive charge is not above threshold neuron is not activated it's not going to pass charge further. That's why basically terminology activation is neuron activated or not activated. And initially people thought that maybe you want to choose activation function which looks this way not basically zero for negatives one for positives Z is my linear combination specifically right and f of Z look this way heavy side function so-called heavy side function what is Z in this case is clearly some kind of bias S
(1:37:51) + W * X1 + W 2 * X2. Now there was a question why there is a bias. Remember cumulative result maybe weighted by W's is above threshold below threshold. It means my threshold must be not at zero. To make it more convenient, I will say there is like parameter W which I add to shift this basically location where a zero occurs. Now you can sketch it this way.
(1:38:25) That's why it is called bias because you want to shift threshold above which this combination must be right. This is my combination of X's and this represents threshold. Basically if I incorporate my bias my activation function can be located at zero otherwise it must be located somewhere else. That's why there is a bias. So now do you have any idea why this activation function is not practical? So it is not practical because if you try to build network using such activation functions you can imagine that my cost function will always looks this way will be peacewise constant
(1:39:08) function this is my j function as a function of w's the question is if I'm staying right here then the question is where to move to the left or to the right typically we compute gradients Remember I told you compute derivatives to get gradient. Gradient tells you where to move downhill. You want to go downhill.
(1:39:32) In this case, if you stay here on this plate, there is no downhill. You're going to basically uh say that I don't want to move anywhere. So such activation function is not really useful. But it is truly activation because neuron gets activated, right? But it is not useful. The next idea would be to introduce maybe so-called rectified linear unit.
(1:39:57) Let me say artificial neuron. Artificial neuron. In this case, we introduce rectified linear unit is going to look this way. F of Z as a function of Z is zero for negatives and simply Z for positives. right this way. Oh, you see is again linear recombination clearly. So this is called rectified linear unit and f of z is essentially maximum between z and zero which means zero for negatives and z for positives.
(1:40:42) So this is okay this is widely used nowadays it is widely used so you can use it right now there is one problem if you try to plot your cost function it also will have some kind of places where it has plate not not always but sometimes right we'll have it so basically if I stay over there I still don't know where to move so there is some kind of problem related to this choice of activation functions but it is not so critical anymore for the most part I know where to move from sometimes I don't but because we introduce some kind of stoastic to the optimization algorithms I will not stuck
(1:41:24) there forever on average I will know okay downhill downhill then I don't know where to move but because of sty I will kind of push out of this location and then I will know where to move again which means this is w is okay actually so this trade is okay now people try to avoid this issue with local constants and they say okay we can choose something like soal leu it will be z over there and it will be some kind of line for negatives with different slopes so we can do it that way so this is going going to be called a lick so this
(1:42:09) is licky Okay, rectified linear units. So basically f of z in this case is maximum between z and also some kind of alpha * z. Alpha is parameter like 0.1 for example right example you can choose this parameter it is hyperparameter but this line is not z anymore. this line f is equal to z and this line f is equal to alpha * z.
(1:42:42) This slope which is less than one that's why we don't have this constants anymore. Basically it's going to be much better. Now if you load your activation function it will be much much better if you have here then you if you stay here you know that you you have to move to the left for example that is my activation fun my cost function for this choice of activation functions I know that in this case I have to move to the left for the most part I know where to move because derivative will tell me where to move. So any questions about exion functions? So theoretically the leaky is the best
(1:43:22) one but in practice god knows what's going to happen. Yes. So leu is quite popular. It is indeed correct. Yes you can choose it. But there is one drawback. You have to choose hyperparameter. This alpha is not trainable during optimization. You have to choose it.
(1:43:40) Now there are modifications where you can also choose this type of parameters. But lary itself by definition uses hyperparameter. You have to know what it is. Maybe it gives you kind of opportunity to play with your parameter and maybe potentially get the better results. Your question. Um sorry professor I have two questions. First about the leaky relu.
(1:44:07) So does it mean that we have to try multiple times with multiple alpha values in order to what what will be results? Yeah. So this this question is generalizable. What is hyperparameter? Hyperparameter means you have to choose it before you actually try to optimize network. It means as you said you have to try different alpha parameters. Basically you will have to ultimately plot your performance let's say some kind of test error right as a function of your parameter alpha. You can plot it, plot it, plot it, plot it and then say okay this is best result right.
(1:44:43) So this is best result. So formally speaking you have to actually write different alphas. Turns out that in case of activation function alpha is maybe not so important parameter. People don't even bother. They just choose one and they you know they try to find performance best performance because it is not so important.
(1:45:10) But you write for hyperparameter you have to write different various of hyperparameter and see what you get for each of those and choose the best performance. That is the case. Yeah. That's why it is kind of drawback because you still have to choose this hyperparameter. Yeah. More questions. Yeah. So Oh, sorry. Please go on. I will go after. Go ahead. It's okay. Yeah. Thank you. Um so for the um for the value and the B neuron does it mean that we do not really have to um defy a specific alpha value in order to to make the model learn and try to see the optimized um alpha and the optimized results. You're talking about
(1:45:57) the B neuron on the left. In case of root there is no any hyperparameter but there are other hyperparameters for example learning rate right which is also alpha by the way but this is different alpha this is not learning rate if you think about learning rate this alpha is just you know value of the slope of left piece it is not learning rate but you have to specify different parameters but not parameter of activation function this case this activation function doesn't have any parameters whatsoever which means you don't have to specify it all to put it differently if you will. Essentially it
(1:46:32) is when alpha is chosen to be zero. You kind of did specify it already, right? You see alpha is zero. Essentially slope of left piece is zero. That's why you don't have to to do anything else. It is already zero. So the range of the parameter starts from zero to the positive side or the negative side. Well, well, well, well.
(1:47:01) Formally speaking, legally uses alpha which is strictly positive and below one, right? But there is a generalization when you can actually have two pieces connected anywhere you want, right? So, it is called max out. So, there is another example which is called max out. In that case, you can choose basically any lines you like. this way this way f of z and no kind of uh okay let me let me leave it this way so two lines more than that slopes of those lines are basically trainable in that case it has extra parameters at this kind of problem but it gives you more flexibility if you kind of believe that your particular example would would
(1:47:48) perform kind of perform better for different activation function maybe max out is the way to go but it has extra parameters It may not give you may not give you anything but but formally ly uses alpha which is strictly positive.
(1:48:11) I'm just saying that basically is case where alpha is just zero right the format is called differently but yeah for for the B neuron on the left side we don't have we don't have to specify the alpha value there there's no nothing right just two constants zero and one but but but this fusion function is not used you you never will want to use it there's no even this type of option here site I because you don't know how to optimize it, right? If you want to minimize numerically, you have to rely on your uh derivatives. In this case, derivative is zero. That's why it is just you know kind of
(1:48:49) as a starting point of thinking about this type of models and people realize that this is not how we should do it. So people don't do it that way. Uh so our our goal is to find what would be the lowest test error. So that is why you specify y W with the arrow on top of it. Right.
(1:49:18) What would the lowest test error? Yeah. I'm just asking about the main goal of all these practices. So main goal when you basically say I want to uh minimize specific cost it means your goal is to minimize so-called train cost function train train one but ultimately because it may result in overfeitting right ultimately you use as a reference different data set and you see if performance on that specific data set which is test data set which is used only to test performance is not nice.
(1:49:55) You don't like it. So basically you have to fight every fitting. There are different techniques. We'll talk about this. But essentially ultimately goal is to minimize so-called test error. Error on the data set which was basically some somewhat put aside was not used only at the end after the training you you use it and see how it performs.
(1:50:21) But a sort of intermediate goal is to minimize your train error. This J for the train data set. Once you minimize it, you can now look at performance of your model for the test observations. Is it good or not? Right. Maybe something else is better. But when you run this minimization problem, you minimize train error on the train data set. So you have more questions? Yeah. Uh that's all.
(1:50:54) Thank you for Yeah. So now in this case you have um basically different types of networks and we have uh different choices of this uh uh activation function, right? It could map uh your data to m dimensional space. It will be some kind of some kind of regression problem basically, right? You can you can output output m number.
(1:51:21) So network f super index l means it is your yhat if your yhat is m dimensional you can produce m dimensional vector as output from here. Now there are cases if your capital m is like one and basically you're talking about classical regression where you want to produce scalar as output from your neural network. Second case this is how you can do it.
(1:51:51) You can say activation function as for example and you have single neuron. Single neuron means remember each neuron means you produce feature in this case single neuron means you produce scalar intermediate layer will have 16 neurons. It's like U1, U2, U6. Input is n dimensional 900 axis activation function is relu like we discussed and you have 16 neurons which means U1 U2 through U6 you have 16 use and then you have next layer which consist of only single neuron it means output is one dimensional just like in this example M is equal to one it means one dimensional output and you get your result to be basically number from 0 to
(1:52:34) infinity why so because W maps everything to 0 to infinity. It mean this network will be able to produce result which is non- negative. You have to always pay attention to range of your actuation functions. If you use it means the only result which you can produce is from 0 to plus infinity.
(1:52:55) So that's how we can specify it in in kas right that's how we can build this type of network. Uh any questions? It is used for regression. Uh, professor. Yeah, just um back to the uh board when you're saying you're trying to uh you don't the like let's say uh we don't know where to go. Um isn't the goal as you said to minimize the error? So don't we always have to go downwards or is it programmatically we don't know how where to go? Yes, but programmatically we don't know how to to go because we need to to rely on gradient. Gradient tells us how to move. In this case, gradient you see it
(1:53:36) is sloping. It mean there is a gradient which tells you that you have to move to the left. Gradient shows gradient points to the right. It means you have to move to the left. In this case gradient is zero in every case. It means you don't know where to move. I mean if you look at this function you know where a minium occurs but you only kind of locally stay here. You have set of parameters. You also have derivative.
(1:54:06) You can compute analytically and evaluate your derivative. But derivative is always zero. No, at least often zero. In this case, always in this case often zero. Not often, but let's say sometimes zero. Yeah. Okay. Re is fine. Re is used. Okay. And in this slide now presented input shape and is what is this exactly? This one? No, the one with the previous one you input shape. So in this case when we build network we have to specify how many axis we have.
(1:54:40) In my example I have two axis. Two axis x1 x2. Remember linear combination of x1 x2. If I if I want to build this network I would have to say n is equal to two. Input shape is two two dimensional. Okay. There is also comma comma comma and basically number of observations you have which you don't have to specify how many observations you have is not as important.
(1:55:07) Okay. Thank you. So now you can also uh solve problem where you produce um where you want to run some kind of classification right you want to produce basically a bunch of probabilities. You want to say not just number from 0 to infinity, not just number from negative infinity to infinity but maybe vector of probabilities because I want to do I want to run classification problem.
(1:55:33) Is it cat or not cat for example is it positive statement or negative statement. So you want to have like two probabilities for example maybe generally speaking like capital M probabilities in that case your activation function must satisfy the following property. Each output is non negative because it should represent probability basically and also sum of those entries to your f must be equal to one. It is used for classification. Very popular choice nowadays is soft max.
(1:56:07) So soft max is used in such cases. If you want to run classification basically you can say soft activation function is soft max and we can use it for classification. Right? So now let me uh uh erase and say soft max as well. So in this case we uh going to introduce some let's say y had first right and y had second let's assume that my capital m is equal to two and I want to sort of define myself max then I can take e to power z first what is the first let me say where the First is basically
(1:57:17) whatever enters first neuron W first plus W0 let me say W first W first U first whatever it was before some kind of U first plus W second first U second and this second very similarly is linear combination W + W first plus W second. This way I'm talking about two dimensional case, right? No, you you may have extra to be precise. You may have more than that.
(1:57:58) It's not as important. You you may have any as many use as you want. Let me say W capital H. Uh W capital H first. U capital H plus W capital H second U capital H. So it means I'm talking about a neuron which has many many many inputs which is hd dimensional inputs right but output will be two dimensional so this is my y first head y second head and my goal will be to introduce probabilities how to do it out of those z's and this is now now it is widely widely used it is called soft marks which is simply this way and this is E to power this second E Z1 + E Z2
(1:58:56) that's how we define soft max any questions so soft max basically is a sort of convenient way to produce two probabilities if signal Z1 is strong let's say example Like example if signal Z1 is like let's say 11 Z is 10 you can compute soft max which should be somewhat like 7 maybe and by head second will be.
(1:59:42) 3 it will assign probabilities stronger signal means more probability that's how we make transformation to get probabilities If you run classification problem, let's say some kind of sentiment analysis, soft max is a nice choice. Uh that's how we specify it. And here is a slide which shows you activation functions which you can choose.
(2:00:07) Right? We already discussed relu sigmo it can be chosen hyperbolic tangent can be chosen also exponential linear unit it makes it slightly smaller there is like this uh this um point of non- differentiability but you can make it smooth like in that case max out you see one line second line you take maximum so basically two lines just like I sketch it over error two lines you take maximum two lines but whoever wins is going to be our result whoever is higher basically that's how we do it and this how to specify your activation functions in kas elar
(2:00:52) unit and so on so now last next part is about activation functions you know what let's talk about activation functions next I'm sorry about loss function next time you Look at those slides. I can tell you that I will not inter I will not derive whatever is presented on those slides.
(2:01:12) I mean I will not derive back propagation. In this case I'm not asking you to implement back propagation. It means I will not even have to derive it. Maybe that is useful actually exercise but in this course I will not ask you to to implement it. It means I will not spend time trying to derive it. But I will nevertheless discuss how cost function is introduced and so on.
(2:01:36) So now any questions about anything next time we'll start and talk about loss functions how it is introduced how code is introduced and also mention how algorithms are designed. Yeah questions. Yeah. Uh professor I have a question. Yeah. Yeah. Why can we define the soft max as a probability function? Uh it's just a coincident or just uh any zero behind you know probably there are many ways to um define similar function right like the sum is one. Yeah.
(2:02:16) And each term and each term is larger in between zero and one. So, so why could we um I should say um how can we map the soft m soft max function to the probability function? Are they strictly equivalent or not? So our goal is no not equivalent no not equivalent our goal is to introduce to introduce some kind of transformation which will produce vector of probabilities basically.
(2:02:56) So initially when people suggested such transformations there was like competition on on classification of images I think it was 1998 they introduced new network it would have um different activation functions right not softmax softmax wasn't kind of suggested yet but later softmax became much much more popular there's also related to so-called uh gradient policy techniques and reinforcement learning as well. So this is quite popular approach.
(2:03:28) Now if you ask a question why we do it this specific way it is same question why do we do this transformation this specific way why we choose lick by no any reason we just need to have some kind of transformation which gives you basically desired result in this case vector of probabilities the way to choose it will be dictated by those parameters right which we try to obtain here if there is different transformation you will not get exactly same result by no means you get different result right because there's no one on one correspondence but you can sort of if you know like
(2:04:10) about for example uh probit let's say about u logistic regression and also robbit model right so in that case you have different functions why do we use different functions because we need to produce some kind of probability but how to produce probability and this is kind of matter of modeling.
(2:04:36) You can choose one type of function second type of functions if you have enough flexibility because of because of many parameters because of many features before it may be not as important what you choose essentially ultimately for the performance but it by no means it's going to be equivalent. No, it's not going to not going to be current.
(2:04:59) Uh, does make sense. Thank you. Yeah. Yeah. Yeah. So, because you have many neurons before those W's, you can kind of, you know, obtain what you want ultimately. But it's not not going to be equivalent to different choice.
(2:05:19) If you decide something different, let's say decide to I don't know if you multiply by two, it is equivalent because multiplication by two means you scale W's. Not interesting. What what else can you do? If you take like e to power z squ, we don't want that because square means it means already we essentially introduce moniarity then we have to introduce okay let's now stop professor just one last question.
(2:05:47) Um here you mentioned softmax is preferred for the classification types of outputs. So is there like other um models that are usually used for classification and ones specifically used for non-classification? So if you do classification, if you have like more than two classes, soft max is basically first what comes to mind people use it is just the dominant approach.
(2:06:13) If you have only two classes in that case you can you can use sigmoid. If you have only two classes, sigmo it produces number from 0 to one. It means you can actually handle handle that problem using only single single neuron and output will be number from 0 to one. You kind of produce probability of success.
(2:06:38) Basically whatever is different, whatever is second class will be failure. Probability of success will be what what you will be producing. But in case of uh more than two classes people typically choose soft max if two classes 10 is again two classes what I'm sorry I'm sorry what did you say 10 h is it also two classes can we use it for two classes minus one and one um hyperbolic tangent doesn't give you probability probability must be from 0 to one and this one is from -1 to No. If you rescale it, yes, you can use it.
(2:07:17) If you rescale it, okay, if you like shift it up and divide by two, you can use it. Yes. It's not like people use it, but you you could you could do it definitely. Yeah. Okay. Basically, whatever has like predetermined range, right, is going to be okay for probability. Uh yeah. Okay, let's stop.