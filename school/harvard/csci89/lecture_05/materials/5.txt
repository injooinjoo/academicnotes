(2) 89 day5 - YouTube
https://www.youtube.com/watch?v=xz5d4xdCGoQ

Transcript:
(00:01) Hello everyone. So you should be able to see my slides, right? Yes. Yes. So how was your assignment last one? I think it was a little bit easier, right? You just need to practice. You have to see how we can use those techniques. Otherwise there wasn't really kind of goal like we did last time which means it wasn't so difficult I hope I think right is that correct you are right it was indeed somewhat easier than the previous one okay so we are going to cover a little bit of more stuff today which is going to appear on your next assignment so let's make next assignment you not this Sunday
(00:50) but the following Sunday basically we'll have around maybe almost like two weeks to work on it I already posted your assignment But it will be due not this Sunday but the following Sunday. So we can can in detail cover cover everything in class and also during this sections right? So it will be due next the following Sunday.
(01:16) Uh is it only for the for this next assignment or for every assignment? Uh no it is I mean it's all going to be accumulated. We're going to continue the same way using like one assignment per per week. And I also posted this updated schedule. It's not going to change number of assignments. So basically it will just shift them.
(01:35) So we kind of with no rush can cover stuff and you don't have to I mean if you cover it today and you have to send it on Sunday. I didn't feel comfortable doing that. That's why I want you to give you like at least one week before the deadline because because for example turn frequency and word document frequency is going to occur on your assignment and we are covering it today only even though we discussed already like we considered some examples but formally we are covering this today which means I decided to move it forward. It's not like it's going to shift anything. Let me show you.
(02:33) It's on that uh Excel document that you posted in announcements. Correct. Yes. Yes. So basically what I did is you can see here is a project selection right uh and assignment like right before that no it means everything will be pushed forward just so to give some kind of gap between lectures and also deadlines.
(02:52) I didn't remove any assignments. I simply pushed this one forward. And the result I pull this one forward as well and then project project selection somewhere in in the neighborhood of assignment 10. So it means we simply push it forward to give extra time for every assignment. It's not like they're going to accumulate. It's not like next week you have to submit like two or anything.
(03:12) They're just going to be shifted forward. So it's clear, right? Yes sir. Thank you. Yes. So now um let me open uh the quiz itself. Quiz number four. First question. In this case, first question is about where weight sharing occurs in which of these type of neural networks. Let me briefly kind of remind you what we mean mean by weight sharing and when it occurs in which kind of models.
(03:46) It is basically quiz number one, right? And we are talking about let's say fully connected neural network. First type of networks which we learned. Then second one is recurrent type. Recurrent neural network it could also include LSTM for example right and last one is convolutional neural network.
(04:10) So first network is fully connected no basically fit forward dense well dense network fully connected one. It means we have some kind of neurons here signal maybe coming from previous layers. Understandably also bias which is simply means signal is one. It is fixed signal one. Then we have next layer right this way. And we connect basically every everything to everything first of all and secondary we explicitly assign W to each connection.
(04:43) It means every connection will have its own W. Every W is unique. Let me say wigj all of them unique. So basically no weight sharing in this case clearly because we are talking about fully connected network. Second type recurrent network. Now I will not explain how it how it is designed. You already know I have some kind of XT vector right? It may represent XT vector.
(05:16) this single line maybe actually not single line it maybe maybe vector output maybe not vector but it may be actually just a number if I'm talking about single neuron right now I can actually make it already layer of recurrent neurons that means will be multiple neurons or I can generalize it and make some kind of memory cell it means xt vector is in and already y vector t is out this Okay, you can think that inside I have some kind of neurons.
(05:48) Maybe it is layer of regard neurons or maybe it is LSTM for example. It means I have sub networks inside and then I I'm going to push it back this way. So now this pushing back what is that in reality as we discussed it means that essentially we are talking about neural network where so what is this neuron? So this neuron means one neuron second neuron and so on going forward.
(06:13) number of those corresponds to number of time steps capital t then we say I'm going to take input second input next input next input but output will be pushed into second neuron second output will be pushed into third neuron and so on this way and this is how basically my recurrent layer of recurrent neurons is designed now what what what's important here it's important that we not just design it this Right? It would be just full network.
(06:46) Basically, we design this way and say those connections will have exactly the same identical parameters W's. It means they actually share the same weights. Right? Let me say those connections basically they share they share the same W share W's. That's what's happening. That's what we mean by weight sharing.
(07:11) In this case it is done this way on purpose because we believe that there is some kind of you know uh independent of time dependence of my signal on previous signals we slide over time and we assume that dependence stays same that's why we assume that weight shing occurs it is a way to reduce dimensionality clearly so you can say what's wrong with this network absolutely nothing if you know how to choose this type of network from theoretical point of view that's it we can basically to solve any problem.
(07:42) We have enough parameters, billions of parameters. We can basically find any dependence we want. We can handle any kind of input. You want time C is not a problem. But the problem is practical problem is how to find those parameters. Basically, how to train this type of network to make sure that problem which we are trying to solve like prediction for example is solved.
(08:04) So that's why it is basically some sometimes is not practical. Sometimes it is okay actually but sometimes it is not practical. That's why we start trying to reduce dimensionality and we say let's introduce weight kind of weight sharing and this is idea behind recurrent network which means recurren network is actually example where weight sharing occurs definitely not fully connected one.
(08:29) What about convolutional neural network? Let me sketch it this way as if it was just basically if it if it were neurons and then I say one neuron, one neuron, one neuron, one neuron this way. Then I say some kind of input to those neurons, right? Then I say let me connect my neuron only to previous three neurons which are sort of in front of current neuron. This is idea behind conversion at work.
(08:59) If I look at different neuron, I will connect it to corresponding three neurons very similarly. More than that, corresponding connections will actually be exactly same. I will say those share same W's. No, people don't don't sketch it this way. People sketch it differently. People say let us essentially let us introduce so-called filter and we pretend that we slide filters over our inputs and we translate signal into our feature maps. So that's how it is sort of visualized but in reality you can visualize this way if
(09:35) you want as well. Our neuron is sort of connected to previous neurons only and more than that those corresponding connections will have same W's. When we say we slide filter over what it means we don't slide here anything right if if we were sliding it would mean basically effective that w slide filter over it is exactly case where we say weight sharing occurs that's why in this case a and b cases where weight sharing occurs for connected one is not where weight sharing occurs any questions about that
(10:11) so now second one in this case we use bag of words but we use two grams for tokenization every token is basically only uh is going to be two g two consecutive words from the text you have to keep in mind that sometimes we remove stop words there and so on they could be removed in this case we don't remove anything there is nothing really to remove I think right but sometimes we we remove by default we will remove Actually uh no it depends on what tool we tools we use to be precise.
(10:47) Now in this case the question is what is representation of this sentence of this document if you use two grams for tokenization. Now we clearly understand that line is not g first second is not appropriate. Then I say ports assembly line one reduced and we are missing assembly line right it means we are missing the one which correspond to overlapping grams in case of two gs we discussed we actually use overlapping ones because we don't really know where it starts so we use all of them it means next one third one
(11:26) is not appropriate either and finally last one says costs it is not appropriate either you may believe that costs means since there is I think be after that you can say maybe it is only single word. No it is not how it works. Car costs is going to be last last two gram and we stop there. It means the only option is first one port assembly assembly line.
(11:51) So basically they overlapping GS that's how it is it is going to to look in this case. Any questions about that? So I assume that text consist of five consecutive tokens. If all tokens are unique, how many agrams are there? Now let's solve this problem. I think you able to just look at some example, right? So we have five consecutive words.
(12:20) Question three. So in this case we have five consecutive words. Let me say text is going to be a b c d e and nothing else. Now I have to compute two gs. First g second g again they're overlapping. It means second g is bc. Next one is cd. Next one is de. So this what we're going to have four g 2 g number of let's say 2 g.
(12:56) Number of two grams will be basically like length which is five minus no minus one because if we move all the way until the end extra kind of you know word is out no kind of extra space is out it means minus extra space one and we get four so that's how we get it now let me ask you do you think that number of grams is going to be always smaller or always greater than number of words we have in this case.
(13:33) Number of two grams seems to be smaller. Why? So well you're you're kind of taking the number of words and dividing it by two or at least that's the most the number of biograms that could exist, right? Or no or no, you'd have to the most number of two grams which exists is length minus one, right? basically. Yeah. Yeah. Yeah. But the question is I mean I I just want to tell you that in practice when I work with this I always see that number of two grams is much much much more than number of one gs.
(14:12) Now you can imagine that if you take some words right computer what comes next to computer right could be multiple options. It means we have only single computer is one g but two gs will be much more right computer is fast computer does like doesn't respond for example computer does and so on so you can come up with many which correspond to first word computer for example which means in practice it seems to be kind of reasonable to assume that number of toss is much more that's why of course if you run some kind of algorithm you have to Keep in mind that number of tograms is much more in practice. It means you're
(14:54) going to have much more difficult computational not only computation also in terms of optimization of network problem because your number of parameters will be much more in case of two gs rather than in case of simple words. Now this example doesn't convince you probably that number of two gs is more.
(15:17) You can say how so if it is less number of words is five but number of two gs is is only four. So tell me why why it is so what what it differently why do we have less number of programs than number of words in this case because we hit boundary basically right so this kind of boundary effect in some sense it means what we lose is not as much we lose because we hit boundary if you have if you have like three gs no we can lose at most at most like two of them right uh but in reality the text doesn't consist list of unique words in reality text could be could be different. Now let's say my text looks this way. A B and then A a
(16:04) let me think um maybe I should say it differently. AB and then AC for example. A B A C and maybe I want to have more to GS the number of one GS more to GS the number of one gs no maybe they should alternate right let's alternate this way the number of two gs is 1 2 3 4 5 as before so number of two gs is Exactly as before they still unique clearly because I alternate a b a c all unique so it means five minus one is not just let's say four in this case but if I compute number of let's say 1 g in this case you can see a b c d a b c d
(17:11) a b c D it's also four I think right A B C D 2 G is is five right yeah I think you meant to write five for the 2 G for 2 G I meant to write five yes okay then it is it is now already more convincing that number of two gs has more so basically in practice when you work with text keep in mind that number of two grams will be potentially much much much more than number of one grams if you decide to move that code.
(17:42) Yes, you keep kind of local context. It is nice but it is much more computationally expensive. It means model itself will be much more more difficult, much more complex. Training is more difficult. You need more data and so on and so forth. You maybe saw last time sparse representation case of two grams.
(18:02) It is exactly because two gs in my in my corpus were basically unique most of the of the time. That's why so like in this case AB occurs only once in this text right well A occurs one two three times that's why so now next one professor I mentioned just now about the text the test being sparse um because the vocabulary might include more words than the or more words than the uh test right So then we'll have more zeros in the test case of the bag of that's correct. Yeah.
(18:47) Because of out of cases. Yeah. So question is can we for example when we're representing the bag of words of test can we eliminate the cases that are uh not available in the vocabulary uh by we sort of do it but the question is what we what you mean by eliminating cases. we sort of s store it into like out of vocabulary for example token right I mean the other way sorry I mean the other way like if it's available in the vocabulary and not in the test can we um eliminate the cases and just keep the words that are in the test so it would be frequency words in the test um
(19:27) and um even if it's not in the sorry even if it's in the vocabulary but it's not in the test we just eliminate it. So I can tell you that by multiple reasons we cannot do it. First of all, we cannot take any information from test data set into our training.
(19:49) Right? What you are saying already means that you took basically information from test data set and you assume that you know it well you sort of used the test data set to test how it it would perform on completely new text. So kind of kind of rule which is applicable to any kind of data science problem.
(20:12) Why do we split into train and test data set? Because test is something which you kind of use as a sort of kind of a a a real application scenario, right? If your goal is to only sort of apply your model to this particular test data set, it is explicitly different kind of task. But we use test test data set to see how model would perform on completely new text. We don't don't only design the model for this particular test data set. Test is used to kind of see how it would perform under new circumstances.
(20:40) Which means taking any information from test data set is basically not allow it. I'm not taking it from the test. I'm basically comparing it to the vocabulary. But if it's in the vocabulary, I see. Sorry. If it's not if it's in the vocabulary and not in the test, then I'll eliminate it.
(21:03) So basically when you say you sort of do it on on the fly every time kind of right so you pre-train your your model you have particular task when you want to do some kind of let's say classification and you say right now I don't really need this particular words because it is not in my no my basically uh is there yeah yes my words for each I'm trying to to do prediction for example So let me think um so you said that I you want to sort of on the on the fly every time sort of customly right it will be custom sort of process every time you want to remove it so it means uh your so I'm not sure what
(21:43) what we mean by remove for example I built a fully connected neural network right it means my vocabulary consist of 200 words for example it means my network will have 200 inputs and that's all it will have because I know that my vocabulary consist of 200 words. Now if you say I don't need any I don't need one of those words anymore.
(22:05) What are we going to do with particular neuron which accepts signal from this location? If you don't do it, it will accept signal zero essentially, right? Mhm. What are you going to do if you say I'm going to remove it? No. Effectively this kind of removal in some sense because if it is not on the vocabulary my input at the corresponding location will have zero my neuron will accept zero and we'll propagate it further. So it is kind of as you say removal in some sense. Now if you want to kind of literally remove it then it means you
(22:36) have to change architecture of neural network because network will have now to accept only 199 inputs which it does doesn't it accepts all 200. You see the problem right? Okay. Yeah. Yeah. Okay. Yeah. So, but if you actually like what you say is actually exactly what we are doing, but we don't say remove.
(22:58) We say uh keep it as out of vocabulary or just keep it as zero. Right? Well, out of vocabulary is like new ones and you send is you send about something which is not going to occur in your particular text which you're trying to classify. You say let's remove those from the vocabulary.
(23:15) But the problem is removing from a cable means change of changing of architecture of network which we not doing which we are not doing right. Okay. I I thought you wanted to sort of change architecture to design a specific point. Yeah. If you want to design your network specifically for this particular test data set and you say since I will never use it let me remove it.
(23:37) it would be kind of okay but your model would be only designed to handle this particular test data set. Okay. Yeah. Yeah. It is it is interesting because what if you say I want to solve specific problem or you want to maybe do some kind of classification of text from test data set where you don't know how it is classified. It's not like completely completely bad idea. Maybe from this point of view it may be applied.
(24:04) You can say since I only want to use my network only once on this particular test data set which I want to classify somehow maybe you can actually you can think about this idea right uh how to do it you can basically throw away indeed all this words which will not occur on your test data set maybe it's not so bad idea maybe actually but the problem is the problem is uh I'm not sure you have to think about this if you have like particular word in your test data set, right? It is not like is going to be like in every document. If it if this word was like in every document in your train data set
(24:44) but never occurs in your test data set, in that case you may say it's not really useful because it's not going to differentiate anything. But what if you are talking about word which is going to be from time to time occur in some documents but is not not going to occur in other documents.
(25:09) It means this word could be quite nice discriminator of some kind and then you say my text let's think about example my text that tells this movie is terrible I will never never go to this movie theater again so this this text document doesn't say word excellent should you really remove excellent from the train data set because excellent and funny and nice was nice differentiator kind of discriminator it was nice word Because some of the reviews have it, some of it, some of the reviews don't have it. And now you have your text which you're trying to classify and it doesn't have it. It doesn't mean you
(25:40) want to throw it away. It is a lot of information basically because your network will know okay this is the text which doesn't have excellent. It means it is probably not excellent. You see my point right? Yes. Yeah. So you don't want to throw this away only because it doesn't occur in the text data set.
(25:58) you want to throw it away maybe because it is something which occurs everywhere and you say how can I use it okay clear right so basically even from this point of view I would not do it so now next question in this case we have u padding is same and we say explicitly that strides don't have to be one right evolutional network padding is grading is same. Let me take some example.
(27:03) Maybe I'll take one, two, three and so on. And I'm going to use filter of size uh let's say 2x2 this way. Then the question is what's going to happen if I convolute my my image with this filter. So I'm going to convolute it. If I say ping is same, basically I want to sort of try to preserve my dimensionality, right? It means I'm trying to place zeros around my image. Now, let me do it this way.
(27:46) You kind of can do it even on the right side if you want. But it turns out that if we kind of already cannot move forward with our filter, we throw away the rest. Basically, don't need the rest, right? Because you don't want to increase the dimensionality.
(28:05) At some point by the way when networks were introduced people would sort of artificially add this type of zeros they would pre-process images it was like 28x 28 and they would say let us add zeros around artificially let us preprocess it it will be 32x 32 ultimately and they use filter 5x5 I believe and it would be basically they would preserve dimensions nowadays we don't reprocess images we simply say ping is name and it will basically will know what to do.
(28:33) It will know that you want to add zeros around first location then second location next location. In this case when I say that I clearly assume I assume that strides will be equal to one one and what do I get as output from here? So because it is like three positions along each of these directions, I'm going to have 3x3. So 0 * 1 + 0 * 1 + 0 * 0 + 1 * 0.
(29:12) I'm going to get 0 here. It happens to be because my lower row is 0 0 here. Next one is also zero. Next one is also zero. Next position here. No, next position. You understand? I have to move down because th is one it will be this position. Then I say 0 * 1 1 * 1 already 0 * 0 4 * 0 and I'm going to have one.
(29:42) In this case I assume that activation function is a linear linear activation function if you want you can assume as well because for positive numbers if I apply redu to positive number it is effectively like linear. So next one 1 * 1 2 * 1 4 * 0 5 * 0 gives me 1 + 2 essential 3. Next one 2 + 3 5. Next one is going to give me 4 + 5 9 uh I'm sorry 0 + 4 0 + 4 is going to be 4.
(30:17) 4 + 5 9 5 + 6 11. So this is my result of this convolution. If I say padding is same but important in this case I have str is equal to one. What if strats are not equal to one? What's going to happen? In that case I have to skip basically pixels. It means in that case I'm going to lose my dimensions. This is the only way to preserve basically the dimensions. I want to have padding same.
(30:40) It means I want to have zero surround. Also I want to have strides equals to 1 one. Then I will be able to preserve dimensions. If I say strides two, it means I will skip like pixels at a time. The result of course I will not be able to achieve uh same same dimensions of the output. That's why the answer is no. In this case, the answer is no.
(31:02) What in case of valid right we will not work is one one valid. No valid means essentially I'm going to uh not add any zeros. It means of course it will be reduced. I will I will slide it over my image and I will get 2x two instead of 3x3. That's why no it's not going to reduce my it's not going to preserve my dimensions either. In order to preserve I need to have same ping and str one.
(31:30) Any questions about that? You may wonder why we sort of need this type of know uh kind of you know dimensions being of being of the same kind. Now typically if you try to build some kind of so-called auto encoder which means you take image construct conditional layer take image construct conditional layer sorry image layer commercial layer maybe max and so on and then you start recovering it sort of back you use so-called deconvolutions you try to take your commercial layer and you on the contrary try to increase dimensions of your commercial layer and
(32:08) no I mean size no not like depth but size and then you can reconstruct your image. No, it means you can design a network which will take image as input and will reproduce the same image as output. So let me let me see if I can find some nice representation. For example, this one we can look at this one.
(32:36) So we take image and then we construct this type of network. First part is simple network. Second part is so-called deconvolutions. We on the contrary try to move kind of backward and try to increase size of this conditional layers and we reconstruct the image. So the the whole point here is try to reconstruct the very same image. This so soal out encoder you can say why is that useful? First of all we in as you notice we introduce some kind of bottleneck in the middle.
(33:06) It means our information will be somewhat efficiently represented by those neurons. We have to keep only most important information. It turns out to be very powerful tool. First of all, you can introduce some kind of damage to your image like scratches and try to reproduce original image during training.
(33:29) It means because my network will have to somewhat throw away not important things and will learn how to throw away scratches it becomes denoising den noising out encoder for example it means I can train it to remove noise of scratches of some kind for example and and the point is which I'm trying to make here the point is sometimes we for convenience in order to kind of recover the original size of image you want to kind of on the way of doing this pro procedures you want to preserve size of our image. If you don't do it, it is okay. We can still kind of revert it, invert it in some sense, but
(34:04) probably becomes a little bit more difficult. It is much better in such case in case of commercial autoenccoders at least to try on the way to preserve your size, right? As you can see like one commercial layer, second one, next one, they all have the same size.
(34:21) It is much much easier later on to recover size. That is called sandwich sort of sandwich architecture. We use like the in symmetric in symmetric way and on the way we try to use a few commercial layers of the same size. That's how it is typically done. That's why padding same strat as one is widely used in such architectures. Does the location of the padding make certain pixels more influential like like if you cuz right now you did it on the top and the left if you did it on the bottom right.
(34:54) You know what? You know what it is actually convention of commercial networks. It is always like that. Let me try to open uh slides where we introduced it. I will show you even more interesting result without any pings. It looks uh not what you probably would expect. Uh so basically yes it does it in a sort of asymmetric sense. symmetric a symmetric wave that's what you're asking right and this is how it is designed let me say um my my maxing in this case basically maxing works very similar to uh convolutions in case when strat is equal to two it will look exactly the same you see what happens we have 1 2 3 4 5 6 7 8 9 pixels
(35:51) and we have max pooling of size 2x2. It means stride is equal to two. In case of max pooling by default strat is equal to two because when we take max pooling we want to reduce dimensions. They don't overlap. In case of um convolutions by default they do do overlap. Strat is equal to one by default but in case of max pooling they do not overlap. You can change it but they do do not overlap.
(36:14) You can see first location second location of filter and so on. Last location then there is single row left. You see what happens with the last row of pixels is going to be thrown away. Just as simple as that. Even without without any paddings, there is a type of not symmetric behavior kind of right. Yeah.
(36:37) So yes, it is not not symmetric even without zeros. Zeros is like oh yeah even that is already kind of some kind of isymmetry. But in this case if you don't fit a whole number of filters whatever is left is going to going going to go away. Uh so now let me see what we doing next. Okay today we have uh uh ter frequency inverse document frequency.
(37:12) So now let's let's look at this uh representations. Basically last time we introduced bag of words. Bag of words essentially would mean that we are talking about frequency for the most part, right? I mean not frequency but counts. Let's say counts. We create vocabulary. Every location is fixed right just in alphabetical order for example.
(37:36) And then we look at document and say how many times this word occurs, how many times this this word occurs and we count every of those and we have representation so-called bag of words. Clearly we lose time component. We lose we lose um the order of the words but that's how we represent it using bag of words.
(38:01) Now a little bit better approach would be so-called TF IF which is by the way widely used even nowadays right it seems to be maybe counterintuitive but it is not so bad approach actually and TF turn frequency inverse document frequency is doing the following let's say you want to look at local news for example kind of Boston kind of you know Boston local news and you want to build some kind of classifier first what comes to mind is words which are frequent ones are useful.
(38:32) So we can use them because they are useful, right? But maybe words such as Boston for example is going to be not so useful because it occurs everywhere. As a result they came up with idea that first of all we have to count number of times it occurs in the text first of all and secondary we have to count number of basically documents where it occurs.
(38:51) If it occurs everywhere it is useless. I I cannot possibly use Boston as a as a sort of um discriminator simply because it occurs everywhere. It means I have to discount such terms. That's the whole idea behind this type of approach. First of all, we can compute 10 frequency and secondly we can compute so-called inverse document frequency.
(39:15) If my Boston occurs everywhere inverse document frequency will be simply zero. Then I multiply them out. term frequency times inverse document frequency. Let me formally term frequency means number of times term occurs in a particular document. I'm talking about document. I'm talking about particular term term occurs on this document five times. Okay.
(39:41) My numerator for this document for this term will be five five and denominator will be simply length of my my document know how many terms I have on my document. So basically this frequency not count anymore and back of course we will say let's use only number of times it occurs now now it is proportion number of times over the length of the document no it is done this way because if it happens that document is longer right number of times my term occurs is going to be more maybe I don't want that I want simply frequency one of relative to the kind of length of the document that means like
(40:13) frequency how often it occurs in my document that's how it is defined term frequency. So very transparent. Second one, what to do about uh discounting those words which occur everywhere and that's how they did it. They say it is by the way basically empirical empirical stuff.
(40:38) There are some kind of ideas which they borrowed from information theory but it is not really justified anyhow. It is basically basically empirical stuff. If you try to understand why so it's almost like impossible. They just experimented and they come came up with this approach inverse document frequency. Total number of documents in your corpus which you have read over number of documents which contain particular term.
(41:02) If my term Boston is everywhere it will be total number of documents over again total number of documents ratio will be one. I take logarith and I get zero. So this inverment frequency is some kind of discounting. If it is zero, it means that my term like Boston is not very useful.
(41:24) I multiply by IDF and my term frequency even though it may be high actually maybe it is everywhere like often occurs in my text but I discounted by inverse document frequency this way and I get even zero maybe potentially for term frequency inverse document frequency. Now any questions about that? You may ask a question.
(41:48) How do we divide by number of documents containing the term? If this is possibly zero, right? But that we are talking about terms which come from our corpus. It means they do occur somewhere. Number of documents which contain this this term is not zero by definition because they come from the corpus. No people kind of you know use modifications. They say let to be safe at plus one in the denominator.
(42:13) We will see modifications later example secret learn what what they use we can see the formulas but this is basic idea and it is okay because we kind of create idea based on train data set it means denominator is never zero. Now second second question what if I move from train data set to test data set. What do you think how to compute inverse document frequency? Okay frequency is clear.
(42:37) I see my next going to complete a new sentence. Then term frequency will be number of times particular term occurs in my new document over time over length of this new document. Term frequency is okay. Then frequency will be computed based on test test document based on test data data set.
(43:01) What about inverse document frequency? You see number of documents containing the term. So it means uh it seems to be already kind of strange because how many documents do I have in my test data set and the answer is what what would you suggest? I would use a training. Yes, definitely IDF inver frequency is measure of the related to the term not to the document to the term itself.
(43:27) So basically IDF is premputed based on train data set and is going to be always used no matter what you supply as test data set we don't care which does make sense because what if your test data set consist of single document you supply some kind of you know movie review and you want to do classification we don't want to based on single movie review compute IDF IDF is premputed so essentially IDF is going to be computed based on train data set but this frequency for the test data will be computed based on test data set will be kind of mixture TF comes from test and
(43:58) IDF still comes from training data set. So now uh here I have example you can take a look at this example four documents first document consist of three words three terms that's why when I compute term frequency of cat I say it occurs two times one two three one two times that's why two over three because lens is one lens is one to three that's by 2/3. So 2/3 will be the frequency of a word cat from the first document.
(44:38) Now if I move to let's say word dog lens is same I'm talking about first document lens is same I always divide by three everywhere if I if I refer to first document but word dog occurs only once that's why one out of three mouse occurs zero times zero out of three for the first document two times dog two times cat one time dog and zero mouse 2/3 1/3 zero out of three. So that's how we compute frequency.
(45:17) Now if you move to example third document, it consists of two terms. It means dog cat occurs from here zero times. So 0 / two is for cat for the third document. One out of two is for dog and one out of two is for mouse for the third third document and so on. So now this is how we compute the frequency. This is exactly how we discuss it at some point right when we did classification of those patent data.
(45:50) Now what about inverse document frequency? Let's look at cat. I'm not talking about any kind of document. I'm talking about entire corpus basically. And I say cat occurs in one two documents out of four. It means logarithm of total number four over two where my cat occurs. It becomes logarithm of 42 which is 69. It is quite strong.
(46:18) So think think about this cat occurs in two out of four documents. It means cat is a very nice discriminator. I really like term cat. Sometimes it does occure, sometimes it doesn't. I really like it. Right? So back to the question about test data set where we don't have this cat. It's not important.
(46:43) If you don't have that cat, we will know that maybe this document of type types similar to second one or third one. So that new document doesn't have cat. But cat is a nice discriminator. I want to keep it. On the contrary, I want to keep it. And I want to say okay it doesn't have that means my new document is of this type it talks about talks about dog maybe about mouse if it doesn't have cat that's what we have IDF is strongest for the cat what about dog occurs in first and second and third and fourth it means in four documents dog occurs logarithm of four total over four where it occurs
(47:17) which means logarithm of one becomes zero it means when we multiply by IF this type of TF which corresponds to dog will be zero. So dog will be useless essentially. So we don't really pay attention to dog if it is something like like stop word there such things we actually want to just drop them we can drop those stop words mouse occurs in one to three documents of four out of three is 29.
(47:52) So this is for the mouse and then we construct uh turn frequency document frequency for cat as we discussed signal will be maybe strongest. Yes frequency times and document frequency becomes almost 0.5. You can see it is the strongest signal for dog it is useless for mouse it is going to be zero because I don't have mouse in the first document now second document cat is not there so zero dog is zero mouse is there but in first document frequency is not so large so we get somewhat small and so on that's how we construct our TF IDFs any questions about But
(48:39) so now there are some kind of you know uh tweaks to this people implement some different twists. They say let's first of all maybe modify the way we comput it just to be kind of on the safe side. Let's take here maybe plus one in the the denominator.
(49:04) So we don't really divide by zero even though we never will will but to be on safe side let's say not not divide by zero. Secondary let's also maybe add plus one in the numerator. By the way logarith means if there is something which is really really rare it means we divide by some small small count of this term. We don't really want to assign a huge IDF to this term. know some kind of reward occurs in one out of millions of documents.
(49:31) That's why we sort of apply logarithm this kind of you know decay and how to say um derivative decays right it is like um um diminishing return how this is called no basically uh it is becomes flat flat and flat it means if you apply logarithma we are going to little bit punish a little bit more those which are large one that's why we apply logarithm also kind of empirical stuff so we apply logarithm So if term is very very very rare term we don't want to sound like crazy huge number that's why logarithm and uh next people sometimes say plus
(50:10) one here logarithma and plus one why plus one because you may wonder why dog is completely useless so dog becomes zero everywhere maybe you don't want that maybe you want to say dog is steal some information let's assign something at least to one maybe that's why People say let's say plus one over there.
(50:32) No, it will be one will be one one everywhere. Basically everyone will be shooted by one. Dog will be dog will be one as well. No, not one to precise it is like zero becomes one. 1/3 * 1 will be 1/3 2/3 * 1 will be 2/3 and so on. That's how we we do it because we should buy one not result.
(50:55) We should buy one only IDF and still there is the frequency part. So now there is second actually idea. People say what if we have a lots of small numbers. So maybe on the contrary those are large numbers. My document consist of many many words maybe and maybe those TFs are large and they actually usually on the top of this they try to normalize this vector.
(51:26) Now you understand that doc one means vector 4 6 0 0. It is my vector which represents my document. Now let's say example example my document document one becomes vector of type let's say.5 I'm going to round it zero. Oh maybe maybe not this one. Let me choose document number four. Number four will be 2 and then zero and then also one this way.
(52:09) So it is okay we can use it as is but typically people say let us compute essentially length of this vector right the length of this vector you remember so called L2 norm essentially like ukian distance will be 2^ 2 + 0 2 +.1 squar will be my lens how much is that let me let me See? So in this case square root 2 squar plus.1 squared and I get 22. So I get 22. So basically what people do they say let us introduce slightly different
(53:21) representation. Let me call it dog for tilda. So slightly different representation. So what I'm saying is when you run your algorithms, you will not maybe get exactly same result as what I tell you, right? You may be getting a slightly different results. Don't be surprised because some modifications first of all and also typically we apply normalization and we going to say first component must be divided by my my lens.
(53:51) Second component must be divided by mile length. Next component must be divided by my length. And we're going to get a result which is let me say 2 over.2 maybe even differently let me say this way.2 over my square root is.9. So it becomes.9 right 0.9 then zero and last one becomes 04 so 0 4 this way this will be my result which I'm going to use as input to my network ultimately so people typically normalize it whatever you see on the screen is before normalization but this is representation is like what we use right this is
(54:47) what we use with neural networks. Any questions about this approach of computing their frequency inverse document frequency? Maybe some questions maybe something is not clear. Yeah. So when we normalize it with the magnitude of the vector it will become a unit vector, right? A unit becomes unit vector. Exactly. Correct.
(55:14) Yes. So you can visualize it as a sort of points on a sphere basically right. So then the the similarity we're comparing now it be will it become like cosine similarity it doesn't consider the magnitude of the vector instead. Yes. Yes. Ba basically basically that's a good point.
(55:39) So basically we kind of explicitly say let us try to normalize it. That means we are talking about points on this circle basically sphere in multimedial space and we have one document document first then we have second document document second I can tell you more actually no it is like two dimensional case right I sketched it kind of you can you can think about three dimensional if you want so basically what happens is similarity in case typically means exactly cosine between those angles.
(56:14) All right? If there is alpha then cosine of alpha is similarity. That's how we introduce the distance via cosine. If alpha is zero, it is it means they sort of identical, right? Alpha is zero means cosine is equal to one. So we get exactly same vector. Does make sense? Yeah. One question.
(56:49) So why do we need to do the normalization? Well, we kind of want to um That's a good question actually. Let me think. I think I I never thought about this. Um so does that mean like maybe we don't care about the frequency the actual frequency of the term we care about the existence of the term when we normalize it we care about if the term exist or not instead of how many times it exist.
(57:23) So we first of all we introduce frequency and what we do there we actually divide by divide by number of terms in our document right so if we normalize in some sense we don't even need to maybe let me think do do we even need to divide by length of document probably not right so let me it's a good question let let me think about this what I'm saying is since it's is a question I'm sorry it's a question about normalizing in that this very last step whenever we divide by very last step yes we can do it we may not do it so we kind of can choose what what to do
(58:05) yeah I I think just if whenever we do it we we deal with like a um a unitless or like one a unit vector so we really focus on the direction uh of the vector so if you want to think about it the way I see it is every document will have a meaning and then this this document is going to be encoded in the direction of the vector the magnitude we're get we got rid of the magnitude uh by dividing by by by the size that's the way I see it that's correct way to see it yes since we say that similarity means angle
(58:38) length is not important anymore it is one kind of one one thing and second thing is since we already sort of took care of the length of the document Right. Let me let me see here. We already took care of the length of the document. We say let us divide by number of terms in our document. So basically length of the documents is not the issue anymore, right? Because we already divided by and we kind of on the top of this normalize again.
(59:11) Let me say example my document is going to be repres be represented as something like let me say this way uh let me say this way sort of frequency of first term over a length of my document let's say n frequency over length of my document and so on frequency of the last term over length of my document and then I say every time I want to also multiply by uh inverse document frequency which is ln of total number of documents right let me say number of documents will be like capital d for example over uh number of documents which contain this particular term term is first it
(1:00:03) means d which contain this particular term Um right this this way then I say f second one over length len of d second term and so on. So basically as you can see we already kind of normalized by n it can be pulled out we already normalized by n and the question is which is kind of correct question why do we do it again on the top of this? So it is like either first way or either this normalization or maybe second normalization but nevertheless um people actually on the top of this employ this normalization. So I would say that if you apply normalization at the end why do we need to divide by n we already
(1:00:50) kind of normalized. So it is uh something you have to think about but does make much sense to divide by n if you're going to normalize anywhere at the end. Does anyone knows? Does anyone anyone know answer this question? So could it be like uh professor like for example if you have large text and the summary of that text so the two should be equal right in TF if we normalize it then the large text and the summary will be the same right because now yes then volume that's correct yes that's correct by the way n refers to refers to this
(1:01:31) particular document right for second document I'm going to have different and if I say this document of type A for example it will be A it means when I'm talking about second document B it will be some kind of term whatever it is doesn't have to be F first it is like referring to my vocabulary let's say 99 or number of documents number of terms in my document B len length of my corp what kind of corpus D99 F00 / NB ln of D / D 100 and so on.
(1:02:18) So basically when we normalize it this way we divide by different NB and there is no of course guarantee that we are going to have the same length. So at this point without normalization my documents will not be on this on this unit circle clearly they will be all over the place. One will be maybe this one short one.
(1:02:45) All right and second one maybe very very long. So basically we say that we don't like it. We want to normalize it because we want to uh somehow make sure that it is not really sensitive to maybe length of document right since normalization happens differently in this case in first case and the second case in first case we're talking about very short document maybe let's say if vector is long and in this case is is somewhat like maybe 259 words right and second documented is long specific typically because it is very short documents. But wouldn't um but I I I
(1:03:26) actually think that the larger the document is though it must be directly proportional to the term frequency in general because you know like the the more the term it must be it must be so so it's almost like a mood question I think so if it is you're saying you're saying that you saying that numerator also grows correctly but yeah so and if it didn't if the numerator were the same I mean that just speaks even more to how unimportant And it it probably was. That's correct. That's correct. Yeah.
(1:03:55) So it is not so trivial because f also grow. That's a good point. Which means in this case um now I would say that there is two different types of normalizations. I I don't see uh how to kind of justify why we need this normalization other than basically empirical kind of you know consideration.
(1:04:13) We can try to read about this. Please u maybe provide some feedback if you find something. But I kind of never thought about this systemization. kind of make sense to place everyone on the same scale because maybe by some kind of reason okay if if by some reason one of the vectors is very short how can we justify why it is so you see that frequency also grows right or if yeah or if it didn't then that would be significant because it would imply that it's actually not so important I I I don't know the way I see it is that TF in a way is the normalization in some yes it is I I agree Yes, I agree. It is
(1:04:52) already kind of memorization because we're talking about frequency, right? Specifically. So, what what what we saying is this one uh is maybe like uh 15 and this is only like two because it is short, right? That's what we are saying. Uh this is correct here. That's why maybe it is not in this same relation and the denominator not in the reverse relation denominator.
(1:05:18) So it is not obvious why we need to do it other than justification by kind of empirical consideration. I don't I don't see a kind of other way to justify it. Even though we can try to think why let's think why one vector should be much shorter than second vector. Let's think this way.
(1:05:35) If you understand why this vector in my example is much shorter than second one, we will understand why we want to push it back maybe. So how may it happen that document one has much shorter vector. Any ideas? It is not maybe because of length of the document. Maybe because the document had a lot of the same terms in it, you know, like it had the word dog too many times and uh those less and the document the document could just not be as significant maybe.
(1:06:05) I don't know. So it may have an explanation. It can make let me just just give you one kind of hypothesis. It may be the case that document A consists of only rare terms basically right terms. Yeah that would yeah just summary of it and maybe the larger document will have redundant information in it.
(1:06:31) So it could get larger and uh when we we take summary of it will keep only the relevant terms and relevant information. Right? So it will it will not be but but but the problem is if if it consists of rare terms D1 it means number of documents where it occurs is small as a result large it will push it up it's not just rare terms I don't think so because D1 number of documents where this term occurs will be small we divide by small we get huge number here it means not just rare terms you see they're kind of competing right so the question is how Can we possibly get short short vector? It means how is
(1:07:08) it possible that every every kind of entry here is zero. So basically every entry is not interesting discriminator right maybe on the contrary consist of very common words for example okay I can tell you this um maybe it's not only about we have this space right uh maybe it is not as important what length is going to be ultimately yeah actually I I think probably um in the neuronet network um the input is better to be normalized for for example if we do the uh classification right if the input is not normalized probably yeah we couldn't train any coefficient
(1:07:57) well no it's not the no it is not the same when we normalize input it is different we take all of them and normalize it similarly right we talking about different things I know I know what you're saying you say we typically normalize input but what we do in that case We find common sort of normalization factor for everyone and we say let's divide this by 10 and this by 10 as well it is different. We don't individually normalize data points.
(1:08:27) Yeah. Right. So it is not same. Okay. Yeah. So I I have to think about this. It's interesting question. Okay. It's okay. Yeah. To save time. Let's just move forward. Thank you. Yeah. Uh so uh let's actually make a break. So now let's continue. Did did anyone find anything interesting online?
(1:15:07) So I can assume that you know what I can assume that maybe sometimes if you don't have this restrictions sometimes vector becomes very very large for example right or maybe very very small. It is possible maybe that in some cases if you have some kind of you know exotics case or kind of extreme case maybe you get very large output if you do some kind of prediction or maybe you get something like related to stability basically of your output may maybe that's why we normalize it. No, we have to we have to read about this. We have
(1:15:41) to see what people people people say about this. Uh maybe related to some kind of stability because output can be in some cases very very strange for some documents if you don't normalize it. But nevertheless you can think think that this is like empirical maybe stuff. So now let us um um move forward like clearly bag of words.
(1:16:08) We'll have disadvantage because it is simply counts nothing else. we don't really differentiate between terms. One term is more useful, second term is less useful. But we don't care. We simply count them. So it means force will have this type of limitation. In case of TF, we kind of adjusted by weights which are related to inverse document frequency already which is much much better. Maybe in practice at least intuitive we understand it.
(1:16:32) We have we cannot really kind of justify that is like almost like empirical basically consideration in this case. Now notice like summary of what we discussed right we apply normalization often right we apply often normalization as we discussed um and we also no kind of balance by multiplying by inverse frequency as we discussed that's what we do effectively and uh this is widely used in keyword extraction you can take it take a look at text and try to extract keywords from there right classific classification can be used for classification easily and also apply with search engine results.
(1:17:13) And uh in this case if you apply it may be important to introduce similarity of words as we discussed similarity means basically casine between those angles. Now it is by the way also related to idea why we want to why we want to sort of normalize it. I I I I don't like I can't really justify why we have to do it when we enter to the neural network.
(1:17:38) But when we introduce similarity, we have to sort of define the metric this way. Now remember how cosine is is introduced, right? So it means my similarity or my cosine ang of between two two vectors must be defined simply as as product right as my document. Now let's say I * document uh J which is that product here. This is that product and in this case we definitely have to divide by norm of the first one and also by norm of the second one.
(1:18:18) That's how we introduce cosine between two documents I and J. That's how we do it. Uh now if we already normalized as we did it over here it means cosine will be simply that product cosine nicely becomes so similarity between between two documents becomes simply that product of two documents of two representations of my documents.
(1:18:45) So now next modified modified kind of you know uh variations of this TF. You can you can find different variations of this um TF uh a metric representation. First of all you can simply compute row counts. Now it means bag of words essentially this lower case represent frequent count simple count simple count.
(1:19:11) So you can use it as well actually you can use simple count you can also uh use sort of 01 representation if you want occurs doesn't occure so also possible like boolean term frequency you can do it as well tf will be defined to one if the occur zero if doesn't so let me see what you so now um Term frequency as we defined is simply count over number of documents in my part number of terms in my particular document.
(1:19:48) This as we defined capital T capital FTF term frequency. So this is what basically is used by default in secret learn right TF TF IDF vectorzer is going to use specifically this type of TF frequency simple ratio. Now you can try to use logarithmic you apply to counts you also add one and you get this type of you know modification scales turn frequency you can also use different approach you can say count over maximum value of my counts in my vector.
(1:20:29) So kind of count you look at particular when you say it occurs two times over maximal basically maximal count which you found in your document. So you can do it this way as well. But there is also K + 1 minus K where K is some kind of weight.5 for example people choose then you can use this type of double normalization representation no kind of variations.
(1:20:54) Now what about id of what we introduced is ln of n over kind of ln over ln of n which is number of documents in my corpus over df where is uh count of my term in my document maybe I should say not frequency it is not really frequency it is like uh document frequency it is like number of documents essentially right this confusing because frequency is like ratio.
(1:21:25) So capital D, capital F means DF means number of documents which can contain this term. Then we can apply smooth variation of this IDF. No people simply say plus one just to kind of be on safe side division by zero basically plus one over there and also plus one to the result plus one to the result in order to make sure that whenever IDF is equal to zero because what is too too common right we don't want to kind of completely disregard it we say plus one so we want to make it to be positive essentially that's what's used this is this one smooth IDF is default in secret learn if you run TF
(1:22:02) vectorizer basically you will use what I've introduced today frequency over lens of documents times this most variation of ID IDF that is also so called probabilistic IDF it comes from probability theory actually it is already a little bit more maybe even justified to be honest I don't know the derivation I don't don't know derivation of this particular idea probably there's no derivation there is some kind of justification maybe based on probability theory but the idea that you take logarithm number of documents where your term doesn't occur
(1:22:39) or number of documents where your term does occure. So this is this a modification not not all documents not all documents over uh number of those where it occurs but it is a ratio where it doesn't occur or where it does occur this way. Any questions? No. The next one is maximal IDF. You have this variation as well.
(1:23:09) So this uh matrix which I used and also on the top of this basically on the top of this if you run TTF IDF vectorizer you're going to take frequency times smooth IDF and also at the end it will normalize it for us. It should use L to normal should use metric basically. So any questions? So some examples I think we did it already at some point actually I remember I demonstrated classification of a patent data and I used TF vectorizer.
(1:23:45) We take it from secret learn. We have documents. This vectorizer feed it on my documents and we get representations. By the way let me check. Uh I think I never checked uh what is length of this vector it is similar to mine right? Oh we basically obtain it already I think so it is 4.08 08 24 squared it is not one8 and point four is not one.
(1:24:42) So it means um by default it probably doesn't do doesn't do normalization right. So first one squar plus second one squar doesn't reduce one let's say third I think for the last row actually the amplitude is uh less than a half maybe here probably yeah so it is almost one. So this is what you see on the screen.
(1:25:23) I took it exactly from the from the uh Jupiter notebook where I run this example to prepare the slides. It means it is actual numbers. It is not like fake numbers to demonstrate it is actual numbers. Okay. And we take maybe square root of this as well because lens is what lenses square root of such quantity. So on you see it doesn't it means it so basically it means it doesn't reormalize it by default uh actually now curious maybe give me one second I want to check something um GF IDF pariz in such cases you actually have to look at the documentation if you see normalization
(1:26:10) Just let me know. Stop words. None vocabulary max of features. Remove none means no character normalization. It is different normalization. Norm. Okay. So norm uh default is L2. It tells me right default is L2. Each output row will have units norm. L2 is by default ucidian distance.
(1:27:00) L1 means uh right means absolute value plus absolute value plus absolute value. Uh and none means no normalization. So by default we are going to have L2 norm. So if I go back why don't I have it here? It's kind of strange. So any ideas why we don't have it here? I'll check it later. Maybe we can run this base report in Python to check it.
(1:27:29) Um I just don't like when it is inconsistent with documentation. So norm in this case is telling me it must be must be default must be L2. Okay, we have to play with this. I don't see any other options. Even though I didn't type this numbers, I took them. I copied it from Jupiter notebook. Which means it is not just kind of illustration.
(1:28:35) It is actual representation via this vectorzer. professor. Yeah, I get different numbers when I do it. I get 8944 and 4472.89 and what else? And then 447. Okay, so it means I did a typo. Okay, that explains. So basically when I cop it, I probably made a typo. Okay, it it's good. Thank you.
(1:29:17) So, you just ran it, right? Yeah. Okay. Thank you. Yeah, I thought it was correct. So, you see I put the matrix in the chat. Okay. Which matrix? This one. The bottom one. Yeah. But did you run did did you run this uh code in Python? Yeah. Okay. I ran in Jupyter notebook. Yeah. And you obtained like different result. That means when I copied it, I just typed it somehow wrong not correctly.
(1:29:45) So it means so so conclusion conclusion it does normalization by default right I have to fix my result. Uh any questions? So thank you. So it kind of explains because documentation tells it does normalization all normalization specifically like norm it better be the case. Okay. So now word embedding right now at some point we already discussed couple of times word embedding embedding idea. Let me first show this uh representation.
(1:30:26) We see that every word in in in our uh document may be represented by one hot and coying right. Now it means that this sparse representation of this white box means like signal one otherwise signal will be zero. It is sparse representation one encoding we refer to the dictionary.
(1:30:51) If first word is fifths for example word from my diction dictionary it means one will be in the in the fifth location and zero otherwise. This is representation which is kind of nice. We didn't lose anything yet but it is um not u most efficient one in terms of computations and otherwise because we don't really capture any kind of you know relations with different words. We didn't really care much about um uh semantics right we didn't care much about how different words are close to each other.
(1:31:24) We just place all this terms into vocabulary any cow anyhow in any random order basically. And this word can be very similar to last to the third word for example but location where one occurs is quite different even though they could be almost like synonyms for example it is not nice. We want to do something about this and there is a way to basically implement so-called embedding which means we kind of reduce dimensionality of this one the one had encoding but at the same time we sort of do it in a smart way because we want to maybe on the way solve particular problem on the way of trying to solve
(1:31:59) that problem we implement embedding essentially and representation will be the result of this embedding. Let me let me again talk about embedding. Today is like official time and we discuss embeddings. So embedded. We have uh for example we have uh some kind of uh uh text and we have let's say vocabulary of size three.
(1:32:46) Let me say example we have vocabulary of size like three words only in vocabulary. It means I will have potentially 1 0 0 or maybe I have 0 1 0 or maybe 0 0 1. It is not my sentence. It is my no kind of all I have right all possible words I have. It is not my sentence. It is simply representation of my vocabulary basically. and I say now my specific word is one of those it means it is formally speaking threedimensional vector I'm going to say let me try to somehow map three dimensional vector to let's say embedding of lens two to two dimensional space this way now it means
(1:33:37) I have some kind of x vector which comes from three dimensional space I want to map it to u vector which leaves in two dimensional space as we already basically discussed it at some point. In this case, if you wonder how to do it, you can simply suggest new network and it will basically do it for us.
(1:34:09) You can say one input, second input, third input, x first, x second, x third is what I have. But the thing is in that case every x is only either zero or one. This is kind of feature of this type of one encoding. It is only zero or one basically right now I assume that w is equal to zero. In that case I don't need it. So by assumption and also I assume that my activation is a linear.
(1:34:35) So it is linear activation function because I don't really need any kind of any any kind of transformations. I simply want to use a linear transformation and that's how we connect it and we obtain U first first entry of my U then this way and we obtain UC of my U vector as output.
(1:34:56) So basically embedding is nothing but the step of the network essentially. Now let me note if I compute let's say some kind of a output let's say you first if I want to compute you first what does it mean? It means I have to take w uh plus w let's say first first x first plus w second first x 2 plus w3r first x 3r this way very similarly I can compute u2 W 0 + W sec um let's say 1 2 1 2 X first plus W 2 X 2 second plus W3 second X third this way that's all we do that's what what's what embedding is doing in this specific case right it is a kind of
(1:36:06) kind of custom embedding if you want it is not the best way to do it there are alternatives but this is simplest way to do it now if I assume that my signal is of type let's say 1 0 0. So let's say that my vector is basically 1 0 0. What it means? It means that everything else is going to be gone. Second one is gone. This one is gone.
(1:36:38) This is zero by assumption and this is one. And it means all I need to obtain as a result those W's. So it means in my example if I plug 1 0 0 U1 will be simply W11 and U2 will be simply W12. As simple as that. It means embedding effectively means trying to take my parameters W's and they will be exactly my embeddings.
(1:37:04) So basically this matrix of transformations is exactly matric embedding matrix. It is it is even called this way embedding matrix. So those values are basically embeddings. You can visually represent it this way as a sort of network which is fully connected only linear expression functions no bias. You can also try to represent it differently.
(1:37:23) You can say I want to essentially consider three cases. No I is is one if it is vector 1 0 0 I is equal to 2. If vector is 0 1 0 I is equal to 3. in the last case. So basically I'm trying to find in this case it means in this case it means I is equal to one right and you see what happens one it's not occasionally so is one and this is also one not occasionally if I say first which means 1 0 0 all I need to do I need to take corresponding W's from my embedding matrix and you can also visually represent it this way if I
(1:38:04) say let's say my first vector is is going to be is going to be uh 1 0 0 it means if I consider a case where I is equal to 1 let's say this way I want to obtain a result which is basically my w's which is my u vector which is going to be simply w11 and also w12 this If I plug uh as input my one encoding 1 0 0 uh in second case very similar you can guess it's going to be u which is vector of those no for the second position right of those w first w second 2 second and finally if i is equal to 3 my u is going to be simply
(1:39:08) W3 first W3 second. So basically you can think this way if you want that my W so my embedding matrix is entries of my use for every single input from my space of those 100 encodings I have to get it own W's it is possible only because space of my 100 encodings is not entire R3 it is only three three points from the entire space that's why I can do it I can kind of alternatively refer to indexes rather than this to this one encoding things and now it becomes sort of matrix which I essentially have to look at which is
(1:39:49) W11 W12. How to get my U itself? I need to extract first row from my embedding matrix. In order to get it, I input I first and I output my first row from embedding matrix. very similarly I do it in the second case and similarly I do it in the last case. It means embedding actually in practice doesn't even require you to represent your words by one hot encoding.
(1:40:28) Yes, it is formally this way but nobody would compute this simple quantities while this type of linear transformations. Now what if there are kind kind of you know thousands and thousands of dimensions why you have to multiply zeros by the signal doesn't make much sense by ws like zeros by ws you just know it is not there it means what we need to say we need to say if I know that my vector is like first vector from my dictionary I need to supply index i essentially and extract particular row from my embedding matrix that's how it will efficiently comput it that's why it will be not the same as you did it last time remember you would
(1:41:00) represent a vector y 200 dimensional vector we don't have to do it in practice we can simply we can simply kind of you know use basically different piece of software which handles this not this computations computations means I construct fully kind of network it is equivalent kind of but in terms of computations it is much better to use specifically embedding layer right which would extract this matrix and we ultimately design representation of every one encoding vector via this corresponding row from
(1:41:31) this embeding matrix. Now how to train it? No, essentially you can build build it for a particular problem. When you do some kind of classification or whatever prediction or whatever you do sentiment analysis, you can have this embedding layer during minimization of cost function.
(1:41:55) It will find a way a best way to represent your higher dimensional vectors. Basically why lower dimensional vectors it maybe not like from 3 to two dimensions. It could be like from thousand dimensional space to maybe 10 dimensional space. That's how we do it. Any questions about that? Yes.
(1:42:19) Uh I was just wondering if there is any rules on the the the size of the vocabulary how how we map it to the dimension of the embedding space. Any rules um on the vocabulary? What do you mean? So for example, if we have like 1,000 vocabulary, which mean that we will have that our input vectors will be of size 1,000 because it's one hot encoding. Yeah.
(1:42:43) And I was just wondering how do I determine the embedding space size? Is it 10? Is it 20? Is it 100? Oh, you mean this is Yeah. So this is pure experimentation. This is like hyperparameter. Yeah. So this is if you if you do it you have to uh understand that uh it is there is no like rule but if you use like 200 dimensional space we can kind of understand that it probably it is probably sufficient right I mean because in this case it is dense it means we can use every entry from the vector you see this signals it is uh one point only three points from my space but in that case I have basically opportunity to take any point from my
(1:43:22) resulting space. It means if you take like 200 dimensional vector, it would be probably okay. It would be probably sufficient to represent um your words in your like practical application. But there's no way to say what is optimal. Right? Now you understand that two is not sufficient. Five isn't sufficient.
(1:43:38) Maybe 200 is already nice. 1 million is already probably not required because if your original consist of millions of words and you or maybe thousands of words and you decide to to keep million is not doesn't make much sense.
(1:43:58) It means you keep like at the level of hundreds essentially that's how people typically do it like 200 maybe dimensional space but it is empirical. Okay. Thank you. Yeah. So now let me also say that actually if you do it in a sort of smart way I will talk a little bit more about this. So but it maybe it may be the case that actually your representations will even have will even make some sense.
(1:44:26) It will be not just some kind of vectors arbitary placed on this you know resulting embedded into this resulting space. they start making even some sense in this case if I move from man to woman it will be some shift so basically woman minus man is shift if I say king plus the difference woman minus man I'm going to get some kind of results this result really could be something meaningful something related to queen you can say what is queen how is it no basically we find location in this space and try to find the closest word from this space what it corresponds to and it may corresponds to something like queen for example right so it will be literally
(1:45:08) closest vector and closest in terms of the metric which you introduced in terms of cosine will be queen in this case we also introduce metric same coine as before it will be queen so it means a woman minus man or queen minus king will be same so woman minus man queen minus king will be same this shift will kind of make sense Now typically it uh it can be used to even kind of generate new word basically right when minus one plus 10 gives you queen quite interesting of course in order to achieve it you have to do something a little bit smarter
(1:45:43) than this type of embasing but yeah you you will actually do it yourself you will see it some examples they are kind of impressive so now um let me let me say the application of port embeddings is basically everywhere We can we can represent text without a loss of time component without loss of the order.
(1:46:08) It means even like in translations it is exactly what people would would prefer to use. Maybe it depends sometimes it is character to organization but so if you're talking about words so basically is going to be used widely in many applications. Now um now I can tell you one thing maybe if you want to do something like for example classification right and you don't really care about the order remember trying to deal with sequence of uh vectors is still difficult now you can say well my example now is the following I say cat set on the mat is going to look this way first vector 2.1
(1:46:56) 9 next vector 7 2.59 and so on. last vector 0.1 9.5 it is okay if you are willing to keep this type of this type of you know time component if you are willing to work with some kind of recurrent networks for example it is okay you can do it it is one approach and there is also second approach now it is a little bit maybe less intuitive but people use it actually nevertheless you can take for example if you don't worry about time complaint you don't worry about order of the words you're doing some kind of classification maybe and you say maybe for me it is better to reduce dimensionality and build a little bit
(1:47:41) simpler model since I don't have much text maybe performance will be better people compute some kind of averages right so you can say let me simply take 2 plus 7 + so on plus.1 over 1 2 3 4 5 over 5 and so one and the last one.9 +.5 +.5 over 5. So it will be kind of representation where average of those embeddings you you lost time definitely but it could be a little bit better in some cases than frequency and frequency could be better than bag of words that's why you can do it that way for example if you don't worry if you don't want to have like time component if it is too
(1:48:32) expensive for you to train this type of models you can use this type of representation of entire document so it is also used any questions about that okay so that's what I wanted to mention like average right so average in this caseh you have to keep in mind that average means essentially that different kind of you know documents may look similar to each other but this is kind of not an issue in practice because we rarely have like kakai incidents right they rarely will coincide basically that's why maybe it is not not huge deal.
(1:49:12) So now um I have a quick question. So is it is there any chance that some of these common words coincide? So um the mapping is not unique is it is not unique. Absolutely. It is not unique. Yeah. If you I mean I'm sorry you're talking about embedding or you talking about average about the embedding? I'm sorry. Embedding. So embedding is not unique.
(1:49:36) I think I don't think it's going to happen actually. Uh not unique. So you say that basically for two different inputs you're going to get same W's. I don't think it's going to happen. So it is very likely to be unique. It is likely very likely to be unique simply because your output in the training data set will be different.
(1:49:55) So what you're saying is they could be same, right? But it means your training data training data set would have to have same output essentially but it doesn't have same output typically, right? Typically it means um yeah there's no basically probably a way to get it. Now maybe at least practically it is impossible right even theoretically I don't see how it's going to happen if your output is different when you move from document to document from word to word essentially you're going to have different type of outputs it means it will not want to use same w essentially because you train train data set have different outputs
(1:50:36) so now applications yeah set of applications is huge obviously right because it is already quite advanced representation and uh no limitations which I have to mention. First of all, out of vocab vocabulary cases, right? It is in this case, as you can see, it is designed to handle only those one encodings. Whatever is in our dictionary will be represented.
(1:51:01) But what if it is not in the dictionary? We have no idea how to deal with this. It means out of a words will not be represented. They simply will not not be represented. Now, next one. Um um this embedding is sort of pre-trained typically, right? And if you move to the next kind of problem and it is possible that maybe some kind of you know use of words has changed but embeddings are sort of static it is related to pre previous kind of you know uh training data set it means it will be not so efficient maybe you have to sort of retrain it essentially next time again this is clear issue now in this case when I tell
(1:51:41) you you can implement as part of your network it means like you're kind of custom embedding It is not an issue because it is your your current your own embing. If you borrowed from someone and this issue so it is static essentially right bias reflection what is bias reflection in some cases u because embedding is kind of rely on the text right and if text has uh let's say some kind of biased you know let's say uh ideas or biased u uh um representations. So let's let's take example uh if driver is often often
(1:52:20) combined with he for example right now maybe in old days when there were like old cars people maybe more often would be drivers more often would be maybe males and females. If you use this type of text then there there would be essentially some kind of bias and your embeddings will believe that driver and men are sort of closely related in some sense.
(1:52:47) So becomes already issue you have to somehow reduce this bias which is already different and difficult problem but of course bias will be reflected in your embeddings. If there is a bias in your text which you use, it will be reflected in your embeddings which may to be not the best um not the best uh I mean could could be kind of a kind of uh uh issue which prevents you from nice performance because embedding also as you understand has kind of meaning right behind it.
(1:53:20) uses like semantic and stuff which means if if there is a bias it means it is possible that it will prevent the model from performing at the best like if you do translation for example and so on if your model already a priority sort of believes that driver is hidden it is issue clearly right and some languages don't really have much text available like in electronic format in that case it is difficult to train embedding clearly right in in those cases Um no um I can tell you that u also semantic which means also language changes over time which is sort of repetition. So now uh there are two very
(1:54:02) uh popular embeddings words to vector and also glove. a word to vector and also glove. They're different. Um ID is similar but the design is somewhat diff somewhat u different. Let me say that Google to vector basically uh I'm sorry words to vector it is designed by Google basically is doing the following.
(1:54:33) It is trying to take sliding window right sliding window and looks at your uh chunk of text as input to your network and try to make prediction of your current word. Now it is so called uh continuous bag of words and also the opposite skip gram is doing the opposite. It is taking your word and tries to predict entire chunk of your text surrounding your word. So this is literally like neural network which takes your word cut piece of your text and tries to predict your chunk of your text. At the start we don't have any embeddings because embeddings is what we're trying to develop here. It means
(1:55:07) we have to essentially try to represent our words by one hot encodings essentially. It means at this point we have to kind of represent words by one encoding effectively and we say we take a word as input and we try to predict entire chunk around this word and we slide it through the text and that's how the things are being trained word to vector and this is actually exactly what produces quite interesting results because in this case we already capture meaning meaning of a word we capture some semantic we capture some relations to different words. That's why this type
(1:55:44) of embedding is is much better than what I presented on the board. What I present on the board is the easiest one. It doesn't use any kind of you know correlations between words and so on. That's why of course words to to to vector embedding is much much better. So significantly improved efficiency of of models.
(1:56:09) Right now um let me no this is exactly what what I meant when I demonstrated that example where you say king minus man plus woman will be queen. It is actually related to this type of advanced embedding such as word to vector. You all will work with this. You will see how how how nice it may produce results by similarity between words.
(1:56:31) We always will mean actually cosine of angle between those representations. So now um finally the very last one is glove also quite quite popular uh embedding developed by developed by Stanford. In this case we actually doing somewhat different. We actually say let us represent u uh it is related to let's say uh let me say this way kind of the easiest way to understand uh you see probability of co occurrence.
(1:57:02) We introduce some kind of probability of co occurrence which we measure from text. We create this huge matrix which is like correlations basically between different words and then based on those correlations we introduce that product using this logarithm of probability of co occurrence. We define that product this way. That's how we implement embedding.
(1:57:25) So those cosigns will be basically reflecting nothing but algorithm probability of co occurrence. As a result the space becomes kind of structured and that's why when we say let us take one vector and then let's take similar vector the one which has let's say smallest cosine right smallest angle then similar vector will be indeed the closest one kind of right will be like synonym maybe if angle is quite small will be kind of synonyms will be clustered together essentially and so on that's how we do it and again that
(1:57:57) product between vectors is by design is going to be logarithm of probability of co occurrence. That's how we we do it in this case. So glove uh this is already of course much more advanced because we essentially look at the entire corpus right we create this huge matrix we compute those those co occurrences and we also can expect something similar to what we had in previous case king minus one plus woman will be somewhat similar to queen specifically because we say that minimal angle means words are similar to each
(1:58:29) other. In this case, we don't care about angle. We just do it anyway. We want basically we just say there is space for everyone. Let's do it that way. Not the best not the best approach on the board which you see is not the best approach. But in this case we say let us try to incorporate sort of you know a metric in this space which is basically cosine right it is exactly what I mean by metric right and kasan will reflect algorithm probability of coherences and uh uh no you can compare those results right uh word to vector and glove right word to
(1:59:09) vector remember there is like a word as input and context around all kind of a bunch of words around. We are going to make to try to predict and glove is much more difficult because it is going to use this huge matrix of this probabilities of co occurrences and try to incorporate it into the results in a way that elevation of probability of coccurrences will be coign of angle angle between those words.
(1:59:34) So just keep in mind how it is designed and here is like the last example which shows how warto vector works and also how glove works right. So we can run this examples and see what we get. Now on Friday I will show you some examples also I will plot some words and space of maybe two dimensions.
(1:59:53) I will show you how this angles nicely you know going to be small if words are similar to each other large if they are different from each other. So it is very nice and exciting stuff to kind of look at especially at those visual representations. Any questions? Just out of curiosity, do they are the are the um operations considered intuitive if they occur in pairs? For example, just intuitively thinking to me um king minus man would equal queen without you having to add woman.
(2:00:31) So, it's odd that the operation is actually king minus man plus woman equals queen because it just seems like if a king has the man subtracted from it that the outcome would be would be queen. It could be a jester, you know, or maybe a a priest. Be a lot of things. No, but you're still starting as a king though. Can be a prince.
(2:00:53) Can you say it please again? So, you say what is your concern? Oh. So, um, so I'm wondering if they always work in pairs cuz for example, if I think of a king and then I subtract manhood from him, it would seem to me that the outcome would be queen. So, so it seems odd that you have to add a woman to that to that equation to have the outcome be a queen.
(2:01:19) I mean, it's just an arbitrary interpretation. I'm just I just don't know. It didn't seem intuitive to me that it that it involves four terms, right? Yes. But but you say interesting. It's interesting, right? You said manhood, but maybe maybe we should understand man minus woman as as manhood, right? You said interesting words. You said manhood.
(2:01:46) So manhood is kind of shift from one gender to different gender, right? Does make sense? That's how I see it at least. So just adjust your thinking to how how it's how it's being done. Yeah, that's fair. Because uh yeah, because uh it's not manhood. Man is not manhood, right? We have to kind of subtract shift from one gender to different gender. And shift is what? Manus woman.
(2:02:18) Does make sense? You will play with this. It's interesting. You will play with this yourself. Next time you will see. You have like time until following Sunday. So you will play with this. And out of curiosity, do you find any um uh difficult like uh in applying a lot of these concepts to different languages.
(2:02:44) So just arbitrarily, English is very compact and concise. Um there are no genders for objects. The verbs are very repetitive like he sat, she sat, they sat. But one place where it's very inefficient is an adjective. So like if I say Dr. Karashkin, please pet the yellow dog. Yellow comes first. So you have to think about everything that's associated with yellow. While in romance languages, it's you know, Dr.
(2:03:10) Russian, please pet that dog yellow. So immediately you're focused on the object. You know the thing is I mean kind of from kind of you know uh modern point of view I think it's not a problem at all because we have this capacity to handle order and stuff like that.
(2:03:29) The problem is as you mentioned it becomes more complex becomes maybe more dimensions right uh no because in such cases when we want to do translations and stuff of course we don't do any kind of stemming or any kind of limitization uh it means it becomes more complex on one hand and secondary typically in other languages we have much less data to train models on there's only problem it is not like not like conceptual problem but but practical problem yes definitely definitely you can try maybe different um uh chart GPT in different language. You know what like in English it works like amazingly like like as if
(2:04:03) you talk to human right but in different languages you may find it a little bit more difficult right I mean a little bit less accurate I would say less realistic sometimes more artificial um any of you would you be able to differentiate actual speech from speech produced by charging in English.
(2:04:34) What what is your feeling? I think it depends on the the quality of it, you know, cuz cuz you can get various um outputs, you know, like maybe with certain topics or something it would be sort of a bad output and we'd be able to identify, but maybe um so so maybe the question is what about in like 20 years when this stuff gets incredibly incredibly good? Who knows? But uh at least grammar grammar is always correct or not.
(2:05:01) Do you see any um like errors? Well, then also well you're also presuming that we speakers use correct grammar, you know, all the time too. So I think it just has to match the way people typically talk. Well, there's one thing matching is one thing, right? But um I mean yes there could be like errors on the text which is used to train it on but typically at least if you kind of go kind of you know look at different languages maybe you will find that it is not even reflection of the text yet right so you you made a good point yes it is also a feature of the text which is used to train it on which means there could be some
(2:05:39) intrinsic errors which would have to be reflected in the chity clearly responses but That's interesting that uh if um no sometimes at least you can see that maybe errors are more often than in the text which you would kind of used to to train it on. I don't know. So it is interesting.
(2:06:05) Anyone any ideas how you feel about the text which is produced by Charles Gp does it look realistic to you? Very realistic. I actually um but I can I can tell if I if I'm looking at the passage that generated by one of these large language models, there are certain vocabularies that they will always use uh like collectively or um uh taken together.
(2:06:29) So there are really certain things that I can recognize in whatever the the the read. Uh and did anyone try different languages? I did. How how did it look to you? Well, unfortunately I I was learning Polish and I use JGBT but I don't understand Polish that so I can't really assess like the quality but but also but also I speak also I speak Arabic and I can actually even direct it to um to a specific dialect and it can and it works very well. So you ask the question basically how it is in different languages that's why I'm asking everyone. So if you have this
(2:07:06) experience like trying to generate for example text in different languages and what can you tell us about you know your experience if it if it is as good as English or if it is a little bit worse. No in in my in as I said it I can I can direct I can ask it to generate Arabic in specific dialects.
(2:07:31) It is pretty good right? I think the only difficulty in Arabic is differentiating sometimes between male and female in certain words because it's a matter of changing one letter. So it's a bit more challenging for it. Okay. Any anyone else any experiences with different languages? I have tried uh Hindi language uh and it does pretty well job.
(2:08:01) The only thing is that uh uh mean we don't use that professional Hindi ourself like Chip comes up with very polished uh language polished Hindi. So it it is pretty obvious that uh either some professor is talking or I see I see. Yeah.
(2:08:27) Yeah. Why why why do you think it happened? Is it like uh the text which was used to train it is polished or Yeah, I think so because it's because it's textual. It's like from the books. It's not actually uh like Facebook text or some other text. Did you try to ask to say like tell me like you know in normal language like not this academic you know you know? Uh no I did not actually that's a good idea. I will ask I will try that.
(2:08:55) Um next uh next time when I'm talking uh my family back in India uses a lot chat GPT and uh they are very happy heavy about what I'm curious do they solve kind of problems like yeah they don't have to now search uh Google search they oh they replace Google search with charg basically right yeah kind of thing yeah it makes perfect sense yeah it makes perfect sense as a search engine basically, right? Yeah. The only problem is it is always has threshold that is strain on particular data set up up until kind of time,
(2:09:33) right? And then it doesn't know about future. We already in future basically for the CHP. Yeah. There there's also the element of how good are you at asking the the magic machine questions and so uh m maybe in years to come there will be classes on how to ask like the best possible questions to the there are there are there are such classes actually there are classes how to use charge dpt uh to um create a code right to program wow yeah I I mean yeah yeah engineering yes so there are such process. Yeah. Cool. The most interesting um suggestion that
(2:10:13) I've seen that is very effective is to after every um after every bit of information it sends you to ask it, okay, what mistakes did you make or what incorrect information did you provide? Cuz then it revises what it gave you and it actually points out the mistakes that it made.
(2:10:33) It's very um it's a like almost a a necessary step now. Yeah. Okay. What you know what what I noticed whenever you ask a charge, it tries to level with you. It it tries to kind of first give you for example general answer and you say no no give me like exact definitional definition and it like switches right away and tries to basically use different sources.
(2:10:58) It's not always uh I mean it's kind of kind of wants to know what you want kind of which is kind of scary in some sense even right? Yeah. Yeah. To improve your prompts, you can always ask Chad GPG to tell you how to improve your prompts. Well, there is an infinite loop. Amazing. Yeah. What is the ultimate question? Okay, let's now finish. Uh have a nice weekend.
(2:11:25) Maybe this weekend is going to be easier for you. Get, you know, some rest. Also, uh we have what? Thursday's holiday, right? So basically um uh assignment is due in two weeks. Yeah. Keep it in mind. Thank you. Thank you. Good night. You too. You're right.