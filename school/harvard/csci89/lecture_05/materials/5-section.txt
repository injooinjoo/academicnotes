(2) 89 day5 section - YouTube
https://www.youtube.com/watch?v=GFy9VsTOA8k

Transcript:
(00:01) Hello everyone. Hello. So you see my notebook, right? Yes, we can. Section five and let's see what's happening here. So last time we discussed the frequency frequency. Today I'm going to basically um compute those TFS using secret learn TF vectorizer.
(00:43) Let's first consider a document which consists of a text which consists of uh four documents and let's see how we can apply this TF vectorzer to it. So we have TF TF idea factorizer and I say first of all let let's kind of initialize this instance TF vectorizer from uh from secret learn right and we have a bunch of things which we have to kind of specify and if you don't do it it will be exactly same by default because whatever you see on the screen is default those are default values right so lower case true not true and yes it will convert that to lower case and grams which ones do we use only
(01:19) one gram It means just just words stop words will be nothing will be removed. So by default we are not going to remove anything even though you could specify here what you want to remove and also token pattern.
(01:37) Basically when we say word you want to ask a question what you mean by word and this is basically how we're going to do it right. This is how we're going to do it. By default it's going to do it this way. You can see here for example this W means word character. It means uh W and then W plus means we're going to get first word character and then maybe one or more than the one following word characters after that it means the words will be of length two or more characters essentially.
(02:04) So what it tells you it tells you that you have two or more characters by default. It means like things like a for example will be dropped because they are too short right and uh yeah so that's that's what we have. So now uh you can modify it clearly but this is default behavior. It will also uh it is also not nice idea to always read documentation in such cases to kind of get convinced that you understand what's happening. You see maybe more options over there.
(02:35) You can uh see it from see learn and you can see what what else we have. You see kind of my pattern which I use here mine uh gs and so on. So uh can you can see what they mean right? So it is not nice I need to refer to this documentation. So now let me simply take my corpus my documents is list of list list of those uh documents right my corpus I apply my vectorzer I fitt it basically on my documents and I obtain this result from this result I can my this TF matrix from this result I can extract names for example I also can extract my frequencies this way if I
(03:21) want to pull it I say to array and I plot So let's see what what names I get. Get feature underscore names out and I lo my vocabulary. Now it means basically I extract from this text uh this unique words in some order. I create vocabulary. This list is going to be my vocabulary also is happens to be first.
(03:44) They by the way seem to be arranged alphabetically as I can see it right. So also and pocket and so on also is there and is there and so on. A is not there because a is just a very short word right? So that's why we don't have it because w plus means we have at least two of them. You can of course change this behavior. You can also specify what you want to drop.
(04:08) Sometimes you may want to drop something like English stop words for example right? So sometimes you may even want to drop something like if you're talking about movie reviews maybe you don't care about movie maybe word movie is not so interesting for you and you try to classify it is positive review negative review maybe using word movie is not so interesting they all talk about movies basically you can add to your stop words like word movie for example it will drop it so with this puzzles you you may want to implement such particular uh scenario then Let's look at my representation and you can
(04:43) see what happens. Let me look at my vocabulary. So this first document is represented via this specific vector of one vector of its first position corresponds to clearly first word in my vocabulary to also also is there. That's why I have it. TF is relatively high I would say because also occurs not not everywhere that's why it is relatively high ne next second TF corresponds to end this already slightly lower it is like end is here not here and is here end is here and is also here this way lower but but is not in the
(05:30) first document that's why I get zero cat is in first document that's why I get something gets in the first document I get something and so on. So I have this representation almost like bag of words. I don't have any like um order preserved. It is just the same order as my vocabulary every time.
(05:51) It means if I want to do for example translation is a very good bad idea maybe to do it this way. But sometimes if you do for example classification problem right it may be actually sometimes even better performing than trying to represent Y vectors Y sequence of vectors.
(06:10) Maybe you remember last time you tried to do classification you used sequence of vectors it was quite difficult to achieve 75 performance 75 accuracy this time you're going to do somewhat similar but you're going to achieve 90 accuracy your next assignment when you classify movies using ideas but there is a sort of catch last time I told you let's use only 200 most frequent words that means we already kind of lost in information.
(06:36) Now, we're not going to to do it that way. If you open next assignment, let me let me see it. We are going to use this type of representation. It means we can we can kind of keep more words. We don't have to truncate at 200 most frequent ones. So, let me check. Next assignment is going to ask you to do also classification.
(07:09) But now you now you going to obtain 90%. It It was not so difficult by the way. Don't worry about 90%. It is achievable. It is kind of straightforwardly achievable. Actually 75 was indeed somewhat difficult. Last time 90 in this case is not so difficult. But I want to tell you it's not much better.
(07:53) I mean it's not better because we used TF because we also used number of most frequent words to be 10,000. Last time it was only 200. Remember I told you let's use only 200 most frequent words. Let's transfer every word into one hot encoding. It will be sequence of vectors. That's why we not maybe not maybe nice accuracy. And if you if you use like one hot encoding in that case of course uh keeping 10,000 is quite complicated quite difficult.
(08:17) Maybe you could use 10,000 but also with embeddings and then you create sequence of those embeddings and then you can use recurrent type of network. That would be kind of nice exercise. But if you use only 200 most frequent ones think about this what is most frequent ones. For example, maybe I is most frequent.
(08:37) I really I didn't like this movie. I is is going to be most frequent maybe. Then also maybe something like movie is very frequent word. Maybe things like excellent, I enjoy it and so on. Maybe they are not even among among top 200 frequent words. That's why last time it was quite quite difficult. Now it is going to be much easier because we easier because we have 10,000 most frequent words.
(09:01) If you try to represent every word Y 100, it will be basically impossible to train it on your regular machine. Or maybe using clusters you could do it. But typically even if you do it this way via sequence of vectors you would have to maybe right away try to switch to embeddings as we discussed you don't really need to use this one hot encoding which means it is also kind of doable actually but problem may become already more complicated if you have 10,000 most frequent words.
(09:30) In this case, we say 10,000 most frequent words. It means my uh vocabulary will be effectively around 10,000, right? Maybe even less. Something may be dropped. Uh it means it is kind of manageable 10,000 dimensional vectors. Every document will be represented by 10,000 dimensional vector. And we do it exactly this way.
(09:53) Basically, you know, it means it will be input to our network. If you're talking about some kind of full network, it means you don't really care about the order. As you understand, we can always flip like neurons, right? We can switch neurons. It doesn't matter which one is first, which one is second. That's why typically we don't really care about the order of our words and vocabulary.
(10:14) It is it is kind of irrelevant because of specific symmetry of network, right? We just need to kind of stick to particular order and we kind of move move forward with training using specific predator and order. Now let's u remove uh some of the stop words. In this case I'm going to use English stop words and I'm going to change one single line.
(10:35) I'm going to say stop words is going to be English uh English right? Uh I think actually I have to change it remove stop words. Now we have to remove stop words this way and we are going to get already much shorter vocabulary because we remove like a lots of different different words. For example, and is going to be gone.
(11:13) For example, though is going to be gone. That's why becomes already uh of length four much shorter now. And we can see what happens here. Cat is still there. That's why we have it. Cats is also in the first document. That's why we have have it. Dog is next one is there in my first document. Mouse is on there. That's why I have zero for the first document.
(11:40) It is simply vector of length four becomes much much shorter clearly. So we can remove horse to reduce dimensionality. See what happens. Dimensionality can be reduced significantly. Maybe if you have like thousands and thousands of unique words maybe it is not so maybe critical but typically the words are not so not so helpful.
(12:00) It depends on application as well because it will remove also history and so on. So you kind of get you kind of lose information about gender for example which means sometimes you maybe want to remove something kind of specific to your particular problem. You want to create sort of custom list of stop words. Now let's see in this case how it looks.
(12:19) I can simply look at my English stop words from secret learn. I can print it and this is basically the full list of my stop words though else someone about sims even sims is removed into it perhaps find except you can see right so that's what it removes. There are different also uh types of stop words.
(12:47) MIT I think has popular one that is published somewhere. You can type like MIT stop words and you can find it. I think R uses for example often stop words produced by MIT researchers. So this is what's going to be removed. Now what if I want to change my list of stop words to something custom? Yes, I can do it. I can say custom stop words.
(13:11) I specify my list. Let's say by some reason I believe that cat and dog is not so useful. I'm not doing any kind of classification. I'm not talking about dogs and cats and dogs. Maybe I'm solving something completely different. It means my cat and dog will be not very not very informative for me and I'm going to remove them.
(13:34) In that case, I simply say stop words will be equal to my custom list of stop words. Now it is maybe useful to take this one and uh add something to this list, remove something from this list and then specify what your final list is going to be. Right? You don't have to kind of do this from scratch. You can take the given one and then you can maybe add something, remove something depending on what you what you are doing and we we can do it that way. Now the rest is exactly same.
(14:03) We fit transformer on these documents, right? We're going to remove on the way because we specify that we remove on the way the stop words and then we get results. So cat is gone but cat stays. So now if cat stays what can we do? Now remember we discussed we have to do some kind of maybe limitization or steming if you don't like it.
(14:26) We're going to talk about this later. Now first of all any questions about this? So now let's talk about two grams first. What if I decided maybe one gram is not informative. Maybe I'm talking about movie reviews. Like in your case you're going to do movie reviews. Maybe you want to use something like two gs. It is not bad, right? Not bad.
(14:51) You want to keep like two words. Maybe um uh maybe you can use one g and 2 g at the same time. You can place them on your vocabulary. For example, maybe 1 g and also 2 g. That means you would have to specify range to be one to two. If you want to keep one grams and also two grams in this case in this example I simply say let me keep only two grams. It means n and g range will be 22.
(15:19) I take only by gs and the rest is again is going to be exactly same as before. I don't specify anything else because I just simply keep default values and remember default values means no stop words means this is what I define as word. No of words. I only change by basically my uh one grams to two grams to my grams and then I get my result which look this way also my comes from somewhere also my then and also comes from here. It is clearly arranged alphabetically.
(15:58) Then I say let me look at my TF representation. My first one corresponds to also my it is there. It is going to occur only once I believe that's why we have relatively high TF. Next one and also it has some kind of positive TF then next one and the cat and K is not part of first document that's why we get zero and so on.
(16:30) So basically my first document will be this type of vector in terms of frequencies in these frequencies is it will be much longer of course but maybe if I have to keep negation like not bad for example not great maybe it is important actually who use grams especially for classifications it could be important any questions about this so now next What about actually uh let's say NLTK? What can we can we do with with an LTK? Remember NTK can can be helpful if I want to do some kind of uh let's say some kind of u uh like in this example some kind of steming right if I want to do stemming it can be helpful. Let me just show what
(17:19) I can do without any even maybe stemming first. I have my documents as before. Then I will say let me preprocess my documents using NLTK. I tokenize it and every time I join it back sort of so I tokenize and join join it back to sort of recover my original document in some sense. What it means? It means I I kind of restore the sentence as it was.
(17:52) Uh it is u going to be a cat, a dog, comma, and also my cats is my first document as it basically was before. But what I did, I first break into my tokens and then I join it back and you can say why do we do it that way? Because on the way I can apply some kind of technique such as steaming and limitization, right? So if I know how to do it this way to sort of break into my tokens and then kind of join it back.
(18:24) It means on the way I can apply stemmingization it means I can reprocess my text. This is not going to use pre-processing. I simply break into tokens join them back and I restore my document essentially. Then I can apply again before TF IDF vectorizer but I use my reprocess documents. It means those ones which I obtained as a result of every time joining my tokens back. It doesn't do much difference from the previous example yet.
(18:56) But if you see that it works, we can move forward and say during this process of tokenization, we can also apply for example staming right and let's see how it how it works. Now let's do one grams based on stems not original tokens but based on stems. So it means in this case I'm going to uh join them back but not original tokens. I'm going to look at my stamped tokens.
(19:24) So I loop over my tokens and every time I apply my stemming as we discussed it means for example cats become cat dogs become dog and so on. As a result my document will look this way a cat a dog and also my cat not my cats anymore but it become my cat.
(19:53) And if I want to obtain my turn frequency inverse frequency, the rest is going to be exactly the same way as before, but I need to fit it on my pre-processed documents and preprocess documents will inside of it sort of have already stamped tokens, which makes it already much much maybe nice in some cases if you want to remove those prefixes, suffixes. Any questions about this? So, I hope everything is transparent.
(20:18) We break on the tokens. Apply stemming every time and draw it back sort of. Then we ready to apply TF factorizer as before. Oh before it was just original tokens. Now it is those pre-process tokens basically pre-process text. So now uh let me talk about embeddings. So in this case we are going to use words to vector embeddings.
(20:52) This one we take some kind of documents. In this case I took some news about some kind of hurricane or something Florida hurricane and so on. Some kind of news. I just took it from the web. It is my example of my documents. Then I say let me apply limitization and then finally final step will be to fit this type of words to a vector.
(21:26) So what do I do here? I take my sentences tokenized ones. I say that vector size is going to be two. In this case it means basically that I'm going to represent every single token y vector in two dimensional space. It may be not sufficient. Of course, in practice, in practice, you want to have like 200, 300. Maybe maybe 100, right? Two, two is not enough.
(21:51) Two is not probably not nice idea. Most likely not. Maybe 200 is okay. Maybe 100 is okay. Not 200. Now a window a window in this case basically means how many words we take before how many words we take after you know it means like when we remember we discard that we try to make prediction of word based on context.
(22:15) So we we sort of take context around this window will specify how many words we are going to take around current position when we do this type of predictions. What's me open my documentation? I got a go quick question whenever I got chance. Yeah. So the uh the the window here are we predicting five words or um like five words before and five words after using one word? What does a window mean here? So we uh we going to make prediction of uh uh basically 11 words right based on single word. Okay. Yeah.
(23:10) So window means that's what I'm trying to check. I think window means before and after like it means together it will be like five five and also one it's a it's an interesting idea but why should it work? I think it's a bit too difficult if I just use one word to to predict the entire that's why there are two techniques.
(23:43) Sometimes we predict context around word based on current word and sometimes on the contrary we try to predict word based on context two different approaches. That's exactly why they use two different approaches. Yeah. But yeah, that's how it is. We try to sort of shrink, right? the model tries to do its best basically and uh that's why we learn how to sort of shrink dimensionality of this vector and still be able to predict something.
(24:09) Uh it it's not about prediction is about sort of most efficient way of representing current word. That's a good point. But we're not trying to actually make predictions. We are trying to find most efficient way of this mapping right based on training data set.
(24:31) And this most this most efficient way basically allows you to represent your vector by a lower dimensional vector. Nobody said that you're going to make nice predictions but what we are saying is we are going to find most efficient representation of current word and and just along these lines how is that compared to learning embedding for for a task specific. So if we have like the embedding layer in in a network, I understand that this is not specifically tied to to neural networks. We're going to learn independent embedding here.
(25:02) Uh but I'm just wondering what about the quality of the embedding space if we just learn the embedding for specific task uh like classifying the the reviews for example. Yeah. So you see the thing is it is completely different ID basically right. So what is word to vector? In this case we use two things sort of chunk of text around current word and current word. One is used as input another is used as output.
(25:27) We don't have any kind of you know problem at hand. We simply say cut you know scissors cut piece of your text like maybe 11 words I believe around if Windows 5 means 11 words around and try to make maybe prediction of your 11 words based on current word. It is what kind of mapping you have in order to make this type of uh representation.
(25:49) It is one approach why it why it is nice because it is going to use basically context around your uh current word it means it's going to effectively uh uh catch some kind of semantics right but when you use when you use embedding in your problem when you do for example classification you say let me just artificially add this kind of layer is going to do embedding so what you are you doing you are doing essentially u minimization of cost which is classification and on the way it will have to find a way of best
(26:23) representation. It is not bad idea at all but it is completely different concept. In that case on the way of solving your particular problem you do also embedding which sort of helps and will not work to reduce dimensionality in the best sort of possible way. But drawback of that approach is you don't use any context.
(26:43) your every word is kind of independently being mapped to this you know embedding layer and will be represented anyhow your network will have to sort of decide how to do it in order to produce best classification results. For example, in case of word to vector we are explicitly saying two things context around word and current word one is being mapped to to another one.
(27:10) It means we are trying to in this case sort of incorporate into representation incorporate our our basically relations right I would say this way it means uh this representation may may be a little bit more meaningful it means different words with similar meanings like synonyms for example will be forced to have similar representation in case of embeddings if you just you know take your embedding layer and try to do classification It's not like it is not not possible that they will have similar representation but there is no really kind of guarantee that it's going to happen because we never kind of consider
(27:44) correlations between neighboring words. We kind of don't care about that. We simply minimize cost function as ultimate goal. If classification of your particular u text it is important if it is important that different words uh which have similar meaning have to be represented similar.
(28:11) it will maybe do it right given you know how to train your your network and so on but it may not happen simply because if network decides that it's not really important it will it will not do it this way or maybe you will not be able to find minimum where it it should be the case because when we try to understand text like humans we probably understand that two synonyms have similar meanings right maybe it is helpful to understand text that's why maybe for network it is also useful to kind of understand that two different similarly completely different words have similar meaning. That's why this two word to vector is quite quite
(28:42) interesting idea because it allows you to sort of catch sort of semantic right catch correlations between the words but yeah this different approach I'm not sure if I answered your question but it did very clearly thank you different different approach a different approach when you just place your embedding layer it's formally speaking doesn't really care about correlations it what it cares about is minimization of cost function That's all. If it is important for minimization of cost function to identify different words as similar
(29:15) ones, it will do it maybe or maybe not because training itself is quite difficult task. It has many local minimums and so on. It means even if it is important, it may still not do it. That's why you can consider this as a sort of you know uh first task which is independent of your problem.
(29:33) you simply try to uh do embedding in order to represent your words as as as as most efficient in our representation. So I couldn't I couldn't find this uh documentation about the window. Excuse me. So maximum distance between current and predicted worth. So it is probably going one way only right probably one way. Okay. Now let me u uh continue.
(30:35) So we build this uh embedding, we train it. Uh see this is is is useful because next time it may be doing different embedding, right? Uh and then we have uh my vocabulary. So we can just plot vocabulary and see what we have. And then we can plot uh representations. Remember we chose explicitly two dimensional vector that means every vector will be two dimensional.
(31:10) And here is example we say let me select some words and let me apply my embeddings which I already obtained to this particular words and see what I get. So I get these four vectors four four vectors in two dimensional space. Now let me kind of briefly mention what happens here in this example.
(31:39) We essentially say I'm going to represent every word as two dimensional vector. I did it this way so we can sketch it nicely. So we can visualize it in reality two dimensions is maybe not sufficient but in my case I chose two dimensional representation on purpose X and Y two dimensional space right I will say power 19 and 18 and 44 or some kind of vector which look this way and this will be power so basically this point of this vector into dimensional space represents water power. Second one guess.
(32:23) 17 and 36 something over there will be my vector which represents guess this way. Next one resident 19 and 36. So some kind of 19.36 maybe somewhere here somewhere over there is my resident this way and storm 13 negative something three and negative something okay storm this way. Okay, this is my basic representation of my words. Now in this case, uh you can ask a question.
(33:15) What is closest to power? For example, right now let's say power is like one vector A and resident is vector B. If I ask a question, what is like similarity between those two words? know when they basically occur often together maybe interchangeably occur essentially and people say if I take a vector and compute that product with b it is simply that product this is simply that product there is going to be a no maybe I should say it this way.
(33:56) A lens time B lens time cosine of alpha which is angle between those two. This is like angle alpha. Then what happens is from here we understand that cosine of alpha is going to be that product a * b over length of a and over over length of b. Now it means if you want to kind of be explicit it means in my case a first b first plus a second b second is my dot product.
(34:43) This one is a 1st squar + a 2 squar I take square root of that and length of b is b squar + b squ I take root of that and this is going to be essentially my uh cosine between those two and I can say that cosine is some kind of metric which is similarity right so I can say this is essentially similarity between two different words can be different words.
(35:09) You can clearly can uh put it yourself if you want. The way to compute a coine is is is obvious. Now if I ask you a question which word is closest to power. Now you have to sort of look over all your possible words in your data set and see which one will produce smallest co smallest angle alpha.
(35:31) It means largest cosine basically because cosine looks this way. So cosine looks this way. It means if alpha is over there and this is cosine of alpha. Alpha corresponds to zero. Cine is one in that case. And it means cosine is most similar ones will be when cosine is equal to one. Essentially alpha is close to zero.
(35:56) And you can even you can even do it yourself and you can say I want to loop over my words and find the one which will have smallest cosine angle. the smallest angle alpha all is cosine. That means cosine is closest to one. But there is also another way to do it. You can simply you can simply actually um use uh uh function that is my model MV WV do most similar and I say distraction for example is my particular word and I say I want to display top and equals to two.
(36:36) because I want to display top two the closest ones and I print those similar to destruction that will be first one which is the closest will be flood waters and next one is frustration this is I believe it is casine right so cosine is almost one sol is almost 0° cosine is almost one or is almost 0° and uh you don't even need to basically loop over your uh vectors in the data set you and simply use this uh method most similar and you can get it.
(37:13) Now what about my vectors? Now in this case I say I want to take my vectors from the vocabulary. I plot my x I I get my x coordinates and y coordinates because first component is x coordinate. Second component is ycoordinate and I plot my vectors. They're going to look this way. And you see what happens. For example, destruction is right here. And we can see which one is closest.
(37:43) Again, by closest we mean that alpha angle is smallest. It means we can see that flood waters is closed. Destruction occurs in my text. Again, this is just text which I randomly picked from the from the news and it gave me this result. Destruction is close to flood waters. Also, you can see frustration that is over there.
(38:07) Turns out that frustration is second closest to going to the to the angle and that's how I can do it. I can say I can say most similar ones two top ones and I get those. So any questions about that? So finally there is one interesting task you can also try to kind of recover what analogy is going to be in this case right if you understand that example flood water and destruction they are closest to each other it is a kind of interesting result it means we understand that this embedding kind of understand that this already kind of meaning understands meaning of the words it means we can solve interesting puzzles. Let me do example which you considered last time in class.
(39:00) Let me say analogia. Basically, we are trying to solve analogy task. Let's see what we have here. Analogy task. Again, analogy means we have to essentially find the word which is closest to current location. Let me satisfy X and Y. First one and second one. For example, king and man. King and man.
(39:38) If I move from king to man, it is no sort of kind of they sort of related in some specific sense. What if I now move from uh queen? Let's say from queen from queen in the same direction as this vector. It means I'm going to ask a question. King from king to man. What happens if I move from queen to to to what? No kind of expected to be a woman maybe. Right now the question is how to to find it.
(40:08) Again you can simply say my uh question mark essentially minus queen. It is like shift is supposed to be same as man minus king. It means question mark is simply queen plus man minus king king. Right? It is queen plus man minus king. If you want to solve this type of analogy problem I have to say queen with plus man with plus and king with minus.
(40:43) If you do it yourself you can absolutely do it. It's not a problem. You can simply say I'm going to take queen vector plus men vector minus king vector. You're going to get some kind of result here. And then you say in the space of my words let's let me find the one which is closest to current location and will be basically a result how to find closest.
(41:04) You can apply most similar function for example but there is already kind of build built-in method for doing for doing so it tells me let me solve this analogy problem. I'm going to find most similar ones with plus I'm going to supply for example Phil and outage it would be like queen and man basically in my case and with minus I would have to supply a word which is king and then I say top first I want to display only the top first one in this case it is like fuel outage and power right so what it means it means essentially We are talking about in that example we are talking about case where
(41:54) by analogy I have fuel and then I move to out H right out H this way. Then I say let me take another vector which is going to be maybe not correct. Let me think about this. So power is negative right? So power is actually going to be this one. Yeah, power is this one from power to outage this way and then I say from fuel from fuel and we'll see where we going to move that is like in that case king with minus that's why power with minus man with plus that's why is plus and fuel is going to come with a fuel is going to come with plus as well so This
(42:53) is my fuel fuel then I will say move by the same vector and what I get is my result. So this is my essentially question mark and when I compute it will display me remained right remains okay power outage fuel remained I wouldn't say it is like completely correct maybe power outage fuel remained maybe because that's some that's how it is used on text maybe maybe it was like I'm mentioning something that didn't stream I don't know so quite interesting very very small text just literally one single news which I which I grabbed from the web and there's already something meaningful here any questions about this
(43:48) I'm wondering if there is I don't know something maybe I should play with uh since we're not normalizing for for length here I was just wondering if we look at at words on the same exact vector but with different scalers and if they have some some relation in terms of size or um uh so when you say we don't normalize when we talk about similarities we effectively do normalize because we look at angle right but but we could have multiple words on the same um I see here so some some of these vectors are short and some of them
(44:23) are long we're not normalizing for size here and I'm looking at the Yes, we don't normalize it. Representation is just vector in this two dimensional space. That's correct. But when we're talking about similarities, we look at Kasan which means effectively when we talk about similarities, we actually normalize it, right? Effective.
(44:42) Yes. Yes. But but but I'm just saying just saying if if we just um not not this formal similarity. I'm just looking if we look at uh if we search for some words uh maybe um you want to like look at the neighborhood basically maybe like Chihuahua like like Chihuahua and and and maybe maybe like Labrador and we see them if they are in the same exact vector because both of them are dogs but one of them is small one of them is large and and I was just wondering if they will if if if this relationship will be will be reflected on the on the vector eyes.
(45:18) So the answer is if it is used this way in your text it will be reflected if if if they used kind of interchangeably on or I would say in same context right they should be close to each other absolutely again because remember how we do it we try to connect surroundings with current word if you sort of within same surrounding have different uh uh dogs right it means Um there should be some sort of symbol to each other clearly but it is only defined by not what you know it is defined by what what your text represents.
(45:58) Yeah. Yes. Yeah. Thank you. You can actually play this kind of game. You can say let me replace maybe half of you know do dogs with cat half of dogs with dog. Then your cats and dogs probably will be very similar. All right. For example, so it depends on in what context they used.
(46:24) If they used in the same context in your text and your corpus, then they should be same. And by same we mean same angle. Not same lens but same langle. This is kind of feature of this you know representation that we by similarity mean actually same similar angle. But we also could use the occludian distance as as also measure of similarity right or uh you know what it is not used uh the question is could we use it I couldn't say it is like obvious here uh head street destruction um this text is quite small it maybe not so representative uh But typically we
(47:10) actually use not uh the angle um only angle right not not the neighborhood in this space but but even if we use Eidian um flood waters and destruction would be very close. They would be quite close, right? But the flood waters probably would be close closer to head to here, right? Still probably I would say destruction flood waters quite nicely aligned.
(47:48) But if you move to out of space, I'm not sure it is. Yeah, it is approximately same, right? Step is point 2. Step is point 2. That means like what we see visually head probably would be closer. It would be probably the closest if you move to L to norm. Yeah. But angle seems to be seems to be more meaningful in this case. Exactly.
(48:14) Because we sort of kind of uh said that length is less important, angle is more important. So you say that even though we we we don't normalize we could and we could represent represent them all as unit vectors and that wouldn't affect any yes we could actually it is often used uh yes exactly they would be basically points on this hypersphere basically right we could normalize them and use this way yeah it is not so not bad idea actually people try to to do it this way as well uh yeah and the only reason that they would like have A similarity of one would be if they were like perfect synonyms and and
(48:50) used interchangeably like gradient and slope would be would have a have a similarity of one and therefore be because they're interchangeable. Yeah. Yeah. Okay. Uh anything else to discuss? Okay then we ready to stop. We can stop now. Okay. Thank you much. Yeah.