109 day6 -ppt only - YouTube
https://www.youtube.com/watch?v=vLhti7gtVj4

Transcript:
(00:01) What would be really funny? I think that's me. You know what I mean? Yeah. But then that means you s me too. So that means I'll pay attention. All right. Good morning. Good morning everyone. Good morning. Can you hear me? Okay. Hello. Good morning. All right. All right. So now 10:30 sound like a great time. Uh everybody seems energized. I like that you're chatting.
(01:04) It's fantastic. That means you're awake and I guess the weekend was fun. You have a lot of sun and everybody seems to be in good mood. Uh just to start in the regular. We're welcome here there over there and online. I'm going to I'm your instructor. My name is Pablo Protoabas. just in case you all of a sudden decide to watch the videos from lecture six forward. Okay.
(01:31) So, uh this is uh today we're going to be talking about interaction terms and polomial regression. Today we're going to extend beyond simple linearity. Are we now we're going to start doing some more complex models? And I did say the first couple A 109A and B will be built is we start with something simple.
(01:57) We fully understand it. We say can we do better and we do more and then we keep going. I want actually that to be part of your the way you think about projects when you do u any project you do for this class or overall start with something simple fully understand it interpreted it and then jumping into I don't know visual transformers right off the bat or confus diffusion models out of the bat because you found a tutorial that works I don't recommend it actually I dislike it with passion habit. Okay, we're going to start with
(02:35) something simple. Actually, we're going to start with EDA. We're going to start with something simple model. We understand linear regression. Yes, it's not going to work, but I want to try it. I want to see how much I can understand and then keep building. So, today we're going to get to the next level. Going to go beyond linearity.
(02:54) We're going to be talking about interaction terms and polomial regression. But uh before I do that, I think last last week I didn't finish those one slide left. Uh late all of you two three minutes. I'm going to three minutes late because I have a lot to talk today. Uh let me get the other one and I'm going to share on Zoom. Don't worry. Okay. and the zoom.
(03:43) You good at this? While you down happily [Music] Yeah. Note for Zoom people, there should be a button. There's a Microsoft PowerPoint. Group them all together. Chris is a different thing. Yeah. Okay. All right. So, we left here and I want to bring it back. No more coins. No, I think already Sal got that.
(04:20) But let's move to the next thing which is uh um so if we have two predictors we talk about dummy variables or one hot encoding. What is that? I hear chatter. Be quiet. All right. So if I have a a predictor a predictor that is categorical, what should we do? Yes, zero and one. Okay, great. Uh so we're going to do that.
(05:06) So we talk about that. And in this case, for example, the data we gave uh one for what was it now? I forgot. Uh yeah it was yes biological sex we gave 0 and one and we even interpreted the coefficients we found. Now what if we have more than two categories? What do we do? Shall we give 01 and two? Okay.
(05:40) Why not zero and two or one two three? certainly you are tiny more to one of the categorical variables. Uh I'm going to rephrase it a little bit or zero the gap between the one zero and two there. If you're concentrating, you have to be very put some order first and second the the difference between zero and one is one between zero and two is two.
(06:12) So you inherited put some importance some weight. I think you both said the same thing. Uh so that's not good. So we go into your name Zoe said uh we go and do 0000 one 0 0. So let's explain that uh detail. Let's we have the ethnicity and I have three categories Asian, Casian, and African-American.
(06:47) So I'm going to have to have three dummy variables and each one of them tells me if that person is Asian or not, Caucasian or not, African-American or not. In this case, we consider exclusive. not usually the case but nevertheless uh so in this case so we have three dummy variables so each one one will tell us if that person is in that category or not do we need three can we do two if I know there's only three categories and I know the person is not Asian or Caucasian that means is African-American right so in principle we only need two okay so we can drop so we don't have many product lying around okay so that's the idea is that you can do uh just one less so you don't
(07:41) have to worry about that okay so in this case we have beta 0 plus beta 1 the person is Asian beta 0 plus beta 2 the person is caucasian and beta 0 the person is African-American okay Good. So what is the interpretation of beta 0, beta 1 and beta 2? I don't have the answer into the slides, but I want you to think the same way we did last time.
(08:04) What is the interpretation of beta 0? If the only predictor we have is ethnicity if everything is zero, beta 0 will be the average balance for African-American. Beta 0 plus beta 1 is the average for Asian and beta 0 plus beta 2 is the average for uh cokes. Okay, is that clear this idea? It's going to be in the quizzes and in the midterms. So I want you to understand it and heavy loves this question. So watch out.
(08:33) Uh so let's now think about so far we have assumed uh linear relation between X and Y. Uh the residuals are correlated and these assumptions need to be verified with the data. So I'm going to stop here, go to the other lecture because I have it all started from the beginning with all the assumptions we need to worry. In the meantime, this is the time to talk to your friend.
(08:57) What happened in the weekend until I get it uh figure out here it will only take a second. Let's start with the zoom. Yeah. And then let's do the other one. Change. All right. Here we are where we started 5 minutes ago, 10 minutes ago. So we're going to go now talk more precisely about nonlinearity. So let me get to the road map, the outline.
(09:47) First, we're going to be talking interaction effects in regression models. This is the first kind of nonlinearity we're going to throw into our models. And this is when two predictors depend on each other. So we're going to talk about that. Then we're going to go to polomial regression extending the linear models.
(10:05) And then we're going to go to the fun parts model selection techniques. And we're going to introduce today one of my favorite things which is cross validation. Okay. So let's start. And now you have to repeat after me. Too many predictors and colineal lead to overfitting. Good. All right. So game time. We start with a fun game. Actually I have two fun game times today. Uh okay.
(10:33) So the first one is just to warm up. It is for a student. any student who wants remember if you get it right. Wait, did you introduce Jacob already one? You really don't like this class together. Um, let's give it and you you remember your name, the school you are. All right, go ahead. I'm Jason. I'm an Adams and I'm actually Jacob Fate. You go what? I'm Jacob Frew.
(11:02) Oh, come on. All right. So I'm wondering what is the discussion in your room at night? All right, let's not go to that class. U all right. So here is the question. Okay, any fun facts? Ah, the fun fact is that your roommate to Jacob, right? All right. So the question, if your mother was a student, what would overfeeding be like? Let me explain.
(11:28) I haven't defined overfeitting yet in some ways, but I think most of us have an idea, but I'm going to define it, of course, right? So let's start by asking what is overfeeding. Imagine of is a student right who will be option number one studying just the night before the test memorizing every lecture love and officer word for word only studying one chapter for all subjects and I think I have the taking extensive notes but forgetting to actually understand the concepts what is overfeitting the way you understand it for students if you're a student I think either like B or C like sound
(12:08) like pretty close to the definition, but I think B or C memorize your like C I think I would probably go for B. All right. I think like C would also be like a decent death. This time let's use the audience, right? So your name again? I'm Jason. Jason is between B and C. Uh, if you think it's Bclap and if you think Ccl.
(12:37) All right, Jason. Okay. So, I think it's B. All right. He believes he's the audience and the audience correct. Thank you, Jason. I need the microphone. Now, we have one more, but not. So, in this case, this goes for one of the TFS. Where are my TFS? Is it only you here, Chris? All right.
(13:04) You're the only TF here, so you're gonna answer that. And if you get it right, you can skip coming to class once. All right. Your name? Um, my name is Teddy. I'm a junior and I took the class last year. You're junior in which house? Uh, in career house. Okay. Okay. All right. So, yes, in quad. All right. Best place on campus. All right, let's cannot get it is in fact the best place. Okay, so here's a question to you.
(13:36) Uh in your model was a TM, what will overfeeding be like? Okay, so number option A grading papers while you wear 3D glasses to see the errors in a new dimension. No, wait, wait, hold on. See all the options here. Number B, using a magic grade ball to decide students grades. D.
(14:01) Give everyone the first letter that comes on their name. Sorry, Frank. Is Frank here? Any Frank? Okay. And subtract points for every answer that does not include the word overfeeding. It could be D. If TF learned to Well, we said this is an overfeed TF, not you, of course. Yes. Um, for overfeing the one that comes to mind, I think is D. Okay, Theo, this is your final answer. Final answer D.
(14:32) If you get it right, you skip one lecture. The correct answer. Thank you, Theo. Excellent. All right. So, this is the idea. We getting into what overfitting is slowly slowly. Of course, eventually we have to define it, right? So, the correct answer is D. So back to too many predictors and colinearity links to overfeitting but we heard that the question is what what is overfeitting in some so I think most of you some idea but let's just talk about over well including his noise and outl unseen data. Okay. So understand so when he gets a new question cannot
(15:25) answer the question because he hasn't learned the concept. He just learned to memorize. Okay. So that's the idea overfeitting. So the model is learning nonsensical patterns learning only the data. All the things I see is in the data but it's not learn the trend. Is that clear to everyone? Yep. Okay.
(15:50) So now let's start with moving forward and keep the overfeed in mind because it's going to come and bite us very soon. Right? So we know there is a problem overfeeding but when does it occur is when I have too many predictors why because the model has the possibility the option it has a lot of parameters to adjust in order to fit the training data exactly right but it doesn't generalize. Okay. So we'll see more of these examples as we go. So okay.
(16:17) So let's now extend the linear regression the linear assumption by doing one thing that's called uh no actually before we do let's actually talk about the assumptions of linear regression. The first one uh instead of define let me show you what we have done and just going to pull it out of there and then the next slide will have them nicely list.
(16:37) So in linear regression what we have done I was bored last night. Okay. Uh so the first thing we said is that the relation between X and this function of X is linear. Okay. That's an assumption of linear regression. There's no doubt about that. Right? And this next thing we did you remember we we define the data.
(17:01) Then next what we define the model. What's the next step in machine learning? Before we train, train on what? You train means minimizing what? The error, the loss function. So we define the model. The next thing we have to do is to set the loss function. So we set the loss function to the MSE, right? Okay. Now let these are the only two things we've done.
(17:31) And then of course we train, we find the betas that minimize the loss function. But the two things that we did and their assumptions is the function and the messy minimizing the me there's no assumption we say exact as a matter of fact right the data we don't have any assumptions we did the data given to us so two things we did define the function to be linear and the second one was the MSE okay now let's look at that so the the first assumption for linear regression is the linear relationship Duh. Okay. Obviously, right? Cool. One. The second one is a little trickier. We
(18:15) every square and we aggregate them. We just sum them or average them. That that that means that every error is independent of each other. Okay. So that assumes independency because we just sum the square of errors. Okay. The third one we put no weights on anyone. Every point error was treated equally. That assume homoskeasticity.
(18:48) I said it right. Okay. So that means the error that we expect from every point is equal for every point right if they were not what would you do I leave that there we'll come back to and finally is square of the errors that's assume normality normal distribution of the now if that last point confuses you and actually the the the First the second one the independence confuses you in lecture nine I believe Kevin will show you all these assumptions how they come to play to come to this gloss. Okay. Um, right
(19:36) now is the fact that we sum without doing any coariance or anything. It means independence. The fact that we just take the square of the errors that assume normality. And the fact that we don't weight the errors at all we just keep them all the same. That assume homoskeasticity. Okay. All right. So that's the thing.
(20:03) And I wrote them down nicely for you so you have a reference slide. Constant variance of residuals, normality of the residual and normality distribution. Okay. Um this is the same thing I said ex but there are two other things to consider that they don't violate the linear uh assumptions the linear regression assumption. Uh one thing actually I think it's fine. I'll ask Kevin in a second.
(20:29) So we assume that measurement of X is constant. there's no error in the measurement of X. So if you do have error in X, you need to go into more basian approach. The second one is no multiolinearity, low correlation between predictors. Multiolinearity will not affect your linear regression assumption, but will mess things up. Okay,
(20:57) so the last two things are just warning signs. Okay. uh but the first four that we have there are important four assumptions are made. Okay. Now how do we know that our data well described by linear relationship? So I give you a data you run your linear regression and you give it back to me. How do I know that my assumption for linearity was good? The first one actually we check all the how do we know these assumptions? It depends on the data of course, right? How will I know that? Well, our main diagnostics tool we have is called residual analysis. Residual is the difference between my prediction
(21:42) and the true value. Okay. So, what are we going to do? So, on the left here, you see here, this is my X and the Y nicely, right? And then on the top here, let's see what I plotted here. X on the X axis and R which is the residual on the Y axis. Okay.
(22:07) Now and on the bottom plot here I'm doing the histogram of those residual values. Okay. Now if assumption number one is true these residuals here will be distri will will be scattered around zero equally. And if assumption number three and four if the normality any dependency that means that this is going to look more like a normal distribution. So I have a question um actually let's yeah let's look at another this is you see the data obviously are not linearly cor related okay this is my data and now you look at the res nicely scatter zero you see that there is kind of a U- shape here
(22:53) now let me put this way if you see any pattern in your residual plot even if you imagine dragons flying over castles any pattern that you can see that means your assumptions are violent. Now what it is is another story. Um so this is a so then usually this histogram may not look like a nice bell shape. Now let's think about one thing.
(23:23) What if I see residuals and I don't have it on the slide so I'm going to draw it uh and tell me if you can see up in the legs up there. So let's say I have residuals everywhere can see over there. Yeah. Let's say the residuals are like that. So they are small here and they become bigger on the what assumption doesn't hold. Okay, good.
(24:03) That means the variance is not the same. Yeah. And which diagnostic tool you think is going to work? Will the histogram show us that it's like that? Yes. I mean once I see that I know that it's not homos ketastic. Okay, are you following me on this? So if my residuals have that shape it means it's not homo.
(24:37) Is it normally distributed? Why not? Actually let me draw another one to confuse you even more. I like to confuse you. I want you to think this. This is my residual plot. X residual nicely. This symmetric is not is normal. Okay, maybe we should do the histogram. The histogram will look like something like that. Two is the normality holds.
(25:32) Does the normality hold? No. Anyone to say yes or no? Who say yes? Yes. You said yes because the histogram look like normal, right? Exact it's symmet. Okay. Can you still hear me back there? Can you hear me up there? Yes. Thank you. Argument is that the histogram there looks like to have a bell shape. All right, I'm going to leave it there because I want you to think about it and it's part of your homework.
(26:28) But start thinking about is that enough to test normality? Do we need something else? Kevin, you smile. So that's a hint. Yes. Okay. So let me actually let me actually make an example. Let's see. Let's say these guys here are nicely but here have big outliers. by doing a histogram it's like I'm doing histograms of all of them together marginalize over so that may not be good enough to test so you need and I'm not going to tell because I'm going to put it as a homeworkish then you have fun to think about all right
(27:20) let's move on so the residual plots is our diagnostics and you can deise other diagnostic tools to do to test the assumptions but in general if you see something like this. Meaning your zero um are scattered around zero randomly. It look like a white noise and your histograms look like a bell-shaped normal. I think we're good.
(27:49) Okay, there are other diagnostics and we're going to talk about actually we're not you going to do it for me. Okay, so any questions about that? So we start by saying we know what is a linear regression assumption. Now we have a diagnostic to say is good or not. What if it's not? Let's say the data like that.
(28:16) What are we going to do beyond linearity? So let's going beyond linearity. The first thing we're going to do is the synergy effect or interaction effect. So what is what we assumed before so far is that if I increase the budget for TV by 1,000 the sales will increase as some fixed value which we call beta one.
(28:45) Right? Is that clear? But we then consider what happens to all of the other budgets. Maybe if I increase my TV budget by 1,000 and at the same time increase the newspaper or the radio or the other media by 1,000 maybe that would not have the same effect. Okay, you follow me on this? So this means interaction terms means that the effect of one predictor on the overall um response variable may be affected by what's happening on the other predictors. So this is what synergy effects we call.
(29:25) So to model that remember the first thing we do is model it. What do we do with linear regression? We said beta 0 plus beta 1x plus beta 2x2 blah blah blah. Now I'm modeling it. And here is my model down here. I think it's going to give me a nice here. That thing. Now notice I multiply X1 with X2. Is that linear? Actually, what is linear? Linear means every term I have for predictors is to the power one. Here I have two terms. I multiply them.
(29:58) That is not linear. Okay. So x * x1 * x2 is a nonlinear term. Yes, we going beyond linearity. You ready for that? Okay, that's our first nonlinear terms we had. So to demonstrate that, I'm going to go back to this data set that we showed last time, which is the uh credit card balance data. It has a bunch of predictors and we're trying to predict the the balance of the credit for every individual 700 or so individuals here. Uh and we're trying to predict there's a bunch of predictors.
(30:36) One is income limit rating etc. But I'm going to focus on only two predictors here because I want to talk about interaction term. The first one is income and the second if is a student or not. So before I even show you something, let's look at the intuition. The intuition is the following.
(30:57) If I want to predict the balance and I ask the question, what if I increase the income of the individual by one unit which is $1,000? What is the expected increase in the credit card balance? Okay, that's the question. Right? Now there is let's say two populations students and non- students right. So without interaction term we say that if I increase the income of an individual by one unit which is $1,000 I'm expecting the credit card balance to increase by so much which is the better one right but now I'm saying well students behave different from non- students right the students get $1,000 more say hey let's
(31:36) go party right that means more balance right I'm making this up of course maybe it's not the case but it depends what population one which is a student may react differently if you increase the income by 1,000 where the population two which is not a student may react different okay is that intuition gives you a good grounds to understand what I'm going to get right all right so let's do that so first I'm going to say I'm going to show you without interaction terms so here is my my model is balance equal to beta 0 plus
(32:13) beta 1 income plus beta to students. Remember student is going to take two values is categorical. What are the values? Zero non- studentent one student. Yep. Good. So, so if it's a student is Yeah. Uh if student means is not a student one time beta 1 income plus beta 2 student but the value of student is one hello is getting awake you like this all right so if I rearrange the terms meaning I take the beta 2 and put it with beta 0 now I have balance equal to beta 0 plus beta 2 plus beta 1 * income. What is
(33:06) that? This is without interaction term. Does the slope change? If I increase the unit, will the balance change equally for students or no student? Yes, because it's beta one. If I change the income by one unit, my balance will change by beta one.
(33:32) That's the interpretation we have for the coefficients, right? So that means if I don't have interaction turn what we're going to have is this. This is the two model for better as poor students and no students right it just shifted up the balance the slowly not changed that means that the way we react to the income is the same that's we don't like. So let's go with interaction term um with interaction term now we have the third term beta 3 comes out students.
(34:02) So if it's zero, we still have beta 0 plus beta 1 income. But if it's a student now, I have this term here. You can see here beta 3 times income. Let's rearrange the terms again. The slope changed to beta 0 plus beta 1. But the the bias the intercept changed to beta 0 plus beta 2. But the slope has changed too. Beta 1 plus beta 3. Therefore, the way we're going to response to the income will be different for student or non- studentent. Boom.
(34:32) See, now they have two different slopes. That means if a student gain uh increase his income by 1,000, he will spend more. I mean, makes sense to me, but maybe the data are not real. Okay. Well, when I was a student start making a little money, I spend them that plus more. Uh, okay.
(34:55) So that's the idea of interaction term. I hope it's clear. Any questions from up there? Oh yeah, go ahead. Didn't quite well like how is it very different to the reason like how the interactions actually making the difference? Is it because of the errors only? Because of this term here. So if you remember before it was just beta 1 times income. Now I have beta 1 plus beta 3 income.
(35:31) So if beta 3 is zero it means we saying the model will is the data was going to say that I fit the data I have beta 3 to be zero. That means it doesn't matter if you're student or not your balance will always be proportional to your income. But if the data tells me otherwise that the beta 3 is not zero could be negative or positive that means the the response that you get from changing the income will be different and you can see clearly here this intercept this slope has changed now where do I find I'm going to ask you the question because that's what I do where do I find beta 1 beta 2 beta 3 beta zero
(36:12) you said it earlier How do I find you? We fit. We use the data to determine the values of these coefficients. The t the data will tell us if beta 3 for example is not zero. If beta 3 is zero, the data said look there's no difference between students and non- studentent. But if beta 3 is negative it tells us hey students are more uh saving more right and if beta 3 is positive it says oh students do like the data will tell us we don't just give the option to the model to do that okay remember these coefficients will be they
(36:56) will be found by fitting means we want to deter determine them from the data yeah this becoming like a Netflix show you get the overall picture is entertaining but you miss the details that's how I feel sometimes all right so that's the thing the comparison digestion time um the next thing we're going to do while you're digesting the previous one it has to go twice one more you digest it Good.
(37:39) You know, I have the theory that student cows or sheep will correct me. You eat, it goes down, it comes up. Process again, it goes down. How many times, Kevin? You're a farmer? Three, four times. Yeah, it's the same. You hear it once for me, you get it to the section, you talk to your TFS, you hear it again, you read thing, you do quizzes, you do homeworks, you just it takes time. Okay.
(38:12) All right. Okay. Again, too many predictors, colinearity and too many interaction terms lead to overfeit. Very good. Right. Now, let's make it even more complicated. All right. So uh I put interaction term but if I feel like my relationship is of the input and the output the predicted response variable is not a straight line as I showed before we may have the curvy curve what should I do? So here is the example.
(38:44) So if I have this data the blue dots are the data. This is my best linear model. Does it look good? Right? The residuals will tell me I'm going to have residuals that have some powder. Now, what to do? We may try interaction terms, but may not work. What's the next thing to do? Well, we want a model that predicts something like the red line. Okay, that would be cool.
(39:10) So, what does this look like? What is the mathematical term that will give me this line on the right, the curvy line? Polomial, right? So what is polomial? What we actually is a function f that will capture that and the that function f is always parameterized by some beta. Okay. Those are the coefficients that we're going to find by training on the data. Okay.
(39:44) Now let's take that and say which is the model that will give me something like that. It's a polomial. Right? So we adding instead of x we add x squar x cub all the way to x of m. m is just some number 10 20 whatever okay that kind of function will be able to create curvy lines of that form. Okay, cool. Now, just as in the case of multilinear regression, I'm going to make the claim that polomial regret is a special case of linear regression.
(40:23) How Jacob Let me finish this slide. So what I'm claiming is that the polomial regression is a special case of linear regression that is very easy I put from gyps. All right. So how remember the design matrix? Think about the design matrix we have for multi-linear regress X. It was a matrix. First column was once you remember right you remember and then x2 x3 x4 here it is right and we have the y a vector 2 and was a vector two.
(41:26) Okay, that why should be lower case but we'll fix. Now let's look the polomial regression. So now the polomial I'm going to put x1 1 to the power one and then I'm going to put the x to the power of two as a column the x to the power of three as a column etc. So I take my data I take X and I'm square it cub it etc.
(41:58) Now I have a matrix. First column is one. Second is x. Second is x square x cub blah blah blah. Okay, here we are. So if you compare that with a multiline multilinear regression, it's the same matrix. Okay, it has the same formula. Now I'm going to do even the trick. So how do we train this? Okay, so I have my x's and I have x1 x² cub all to the xm. Here is the trick.
(42:34) Are you ready? Easy peasy. I'm going to just call x to the power of whatever I'm going to call it predictor number k. You remember in the multilinear regression we have one predictor, two predictors, three predictors, four predictor. Now I have one predictor, but I'm going to call X cube a different name and XQ 4 a different name.
(43:04) Now if you think about that, how do we find the the coefficients? How did we find the coefficients on the multilinear regression that site anyone? How did you find better for multilinear regression? I did some you remember moving things around and then I end up with a formula. You remember that? Does anyone remember the formula? It's imprinted into your regular, right? It is that formula. Okay.
(43:34) So, it was x transpose x inverse * xrpose y. Doesn't matter if you remember or not the formula. I like you to remember, but you don't have to remember it. Okay. Well, take that back. You do have to remember that Chris is looking at me. Don't say things like that because then they're gonna come back in the midterm say no you told them. All right. So this is the formula.
(44:06) But remember now the only thing I have is that X the big design matrix has the terms. Every column represents one polomial term. End of the story. That's it. I take my data. I square them. I put them in a column. I give it to skarn and everything works. So if you want to actually create that design matrix, skarn will give you this function we call polomial features and you give the degree degree three. It's going to create that design matrix for it. So you don't have to do. Okay.
(44:36) Number two, once I have the design matrix, I'm calling linear regression.fit because it's the same model. is the same thing that will find you the term. There's more one warning that some may forget. Yes. Okay. The most important step I call X cube I call it X3 tilted. I call it a new predictor and it's just one call. Yeah, easy peasy.
(45:20) So, and you don't even have to do it. Skarn will do it for you. Okay, you just say polomial features. It's going to create the design matrix. It's going to have the first column is going to be one. Second column is going to be X. Third column will be X². fourth column x cube etc. Zoe said yes and you say whatever you feel actually the next part of this lecture is to decide what that end is right but you just have to say because if you say three it's going to create four columns if you say five it's going to create six column remember the first column is one
(46:01) okay wait there was a warning here important if you do if you ask skarn to do polomial features remember it's going to create all the interaction terms so if I have let's say m is equal to 3 uh it's going to do the interaction actually let me back up because the statement about the same if what I talked so far we have x x² x cub x but what if I have two predictors it's going to be x1 x1 squar x1 cub etc and then x xy x2 2 x2 square x2 cube etc.
(46:42) And if I have three ones you get the picture right now if you have more than one predictor and you ask skarn to create polomial features keep in mind that it's going to create the interaction terms. Jacob you had a question earlier on and I forgot to come back. Yeah. So um so just keep that in mind. If you don't want the interaction terms good luck is not so straightforward. You have to remove the columns by hand.
(47:13) Okay, there's as far as I know doesn't say doesn't have a flag say no interaction. You can say it's interaction but it doesn't say no. So keep that in mind. Uh and Kevin says in R you can do it. Yeah. Uh the second warning which is very super important which usually we make a mistake all of us some I'm sure you do. Uh and pay attention to that.
(47:45) These are little nuggets that you if you pay attention to save you some time. The once you design the the matrix it's going to create a matrix with the ones. That means you don't need an intercept because your first coefficient will be the intercept. Okay. Now if you use polomial you have the one and then you fit it with the intercept it's not catastrophic don't worry it's just your interpretation of the intercept is going to have two intercept.
(48:21) the one that comes from your vector and the other is going to be the intercept together is the true intercept just put a note and I'm going to say simply tlddr two long did not read if you use polomial feature when you feed your model just say intercept false I think I got the right keyword right okay so write that down is important to remember. All right.
(48:52) Now the other final thing polomial regression is a linear in terms of the coefficients not the relation between the independent and dependent variable. This is a question we usually get. How can you call a linear regression? Well, it is not a linear regression but it fits into the linear regression because it's linear in the coefficients and therefore we can find those coefficients right of the bat. Now so fitting a polomial model requires choosing a degree.
(49:16) Zoe asked earlier he said what is m? Well, it's something we choose right now. What is a good choice of M? And how do we go about choosing M? Once we introduce a free parameter, we need to understand what it means, right? How do we choose M? Should it be two, five, 6, 7, 24? Yes. Yeah. How do you scatter the idea is to visualize and it's okay to visualize. I agree with you.
(50:14) If I have one predictor, if you have four, five gets confusing, right? What? Pray. Yeah. Hey, pray. It's a good thing. We're going to pray through cross validation. That's what we're going to do. But let me explain. Let me show. So, this is degree one. Obviously, is underfeeding. It's not good. This is the what's happening there right bleaches there you're in your computer you're making so what's happening on the right part of this anyone what is that what do we call that overfeeding thank you very much overfeeding underfeeding But there is some degree that is the right one. And how do we going to find that? That's the
(51:17) the next thing. So fix scaling I'm just going to skip it because we have seen it. So finally too many predictors colinear too many interactions there's a high degree of polar leads to overfitting. All right. So and the model selection leads to overfitting. These are over the students. Okay, let me go to the next one.
(51:45) Um, the scaling. Don't worry, we have it in the previous slides. Oh, why did I take this out? Model selection. Oh, by the way, I forgot to mention the beautiful photo we saw before. I apologize. I'm going to go back that um zoom. Good. Yeah, this look good. Okay, next topic is model selection.
(52:48) And I'm going to u now just because you found a model that minimize the MSE, the square error, it doesn't mean the model is good. Of course, we've seen this. You can investigate the R square too. And if the R square is high, you say, "Ha, I'm done." Are you? So in this case MSE here is high due to noise in the data.
(53:14) In these cases the MSE is high for other reasons. So just to sum up what I'm saying here MSE and R square is just part of the story. Okay you need to do more. You need to actually look because as you see here okay the MSE is whatever number is because the data have noise. There's nothing we can do about it. But this one for example here has an outlier.
(53:39) The outlier will throw up our model because if you have remember the MSE is the aggregation of the square errors of every individual point. If one point is far far away for some strange reason and it has a huge square error when you average it's going to push up the MSE but everything else looks very good.
(53:57) So if I remove that outlier there my MSE would be good. Okay. uh in this case um the MS is high not because of the noise of the data it's because my model is not good enough and in this one uh whatever there is something strange going on in in the domain that we uh measure okay so the idea here is and I think we have seen this before is to take my And I find Simon is check it out on unseen data. Okay, there's many scenarios here happen.
(54:40) Your model trains well on your gets a good performance on the training data. Then you go into the unseen data validation test and your MSU shoots up. That means overfeit. Okay, there is another scenario that you your training data you get an MSE or some performance you go to your validation and it goes very down. That usually means you have some outlier in your training data.
(55:05) Okay, so that's just a thing. So here in this case my uh training data has an outlier that kind of pushes the model to go up. You see this is my training points and I have one outlier. So the model said okay I'm going to catch it but then when I see that unseen data that outlier is not there.
(55:32) So my MSE on the ANCO validation test data will be off will be different right okay so that's one scenario there's more I want you to think more I we want you to think more so I have this example here for TV budget and and sales and this is the model I get y is equal to minus.5x + 6.2 two. Okay, it kind of fits the data. You do the visualization. Looks good.
(55:59) Any problem with that? And I'm going to go with that group of students. I'm smiling at look at that model. Tell me, are you okay with that? Yes. Go ahead. Yes. Uh where look at them again. the y= Okay. What does 0.5 mean? Decreases, right? As you increase the budget, your sales will decrease. Does that make sense? Exactly.
(57:04) So in this scenario it's to me I mean it's either the data wrong maybe we forgot something something pre-processing some normalization didn't go right it may be such that the the the TV advertising is so bad that annoys people and they say I'm not going to buy that could be a scenario but you see now I'm looking at the model trying to understand it now on the right hand side okay the the the slope is at least positive But we have a negative intercept.
(57:32) What negative intercept means? What does it mean to have negative intercept? You have to go back. [Music] People giving you stuff, right? Okay. I like that part but it doesn't make sense. That's the bottom line. Right? So again either the data are wrong or my model is wrong or we don't understand something. The intuition is not right. Okay.
(58:09) I'm not giving you answers but these are the kind of things you have to think once you do did fit to your model. You just don't say ah my R square is 0.9 here it is. No you have to think a little more. What are the coefficients telling us? Does it make sense? Okay. All right. So generalization error in general is how well the model is doing in unseen data.
(58:30) We call that generalization error. Okay, that's the term we're going to use. Now the goal of model selection is to reduce the generalization error. In other words, the goal of model selection is to reduce overfeit. Okay, good. You I overfeitted you enough. Okay. So, every question I answer now, you said over fit. You're getting there. All right.
(58:59) So, that's the goal. So, our our goal is to find the model that will give us the smallest overfitting value, right? Decrease the generalization. So, we're going to choose the model that will give us the best results in unseen data. It makes perfect sense to me. It's such a common sense, right? I have a bunch of models different degrees of polomial different models neural networks decision trees logistic regression whatever right which model should I use the one that performs the best in unseen data it's as simple as that right so model selection techniques help us to do
(59:43) that so let's go into that so before actually just before we So there one more thing let's remind ourselves what contributes to high generalization error what to over makes our model over fit let's go through them because we've seen few of them the feature space has high high dimensional means I have too many predictors we know that if I put many predictors eventually I'm going to overfeit the polomial degree is too high we saw it before if If I get degree 50, e you all over fit. Too many cross terms are considered. We have too many cross terms. Now I
(1:00:27) summarize this all with the following statement. We giving the model too much power. And what is the famous saying? Too much powers comes with too many. Huh? someone responsibilities, right? So once we give the model too much power, it's going to overfit. Okay, so that's kind of what we getting. The more the complex the model is, the more the tendency to overfeit.
(1:01:02) Okay, so what we're going to do, we're going to try to find the model that doesn't overfeit. Now, if I take away all the powers of the model, what's going to happen? It's going to underfeit. So either underfeit or overfeit. We want something in the middle, right? Okay. There's one more that I haven't talked about it. We're going to talk on Wednesday. So I haven't talked about that, but it's going to come, right? So to extreme.
(1:01:28) All right. So you remember this thing train validation test. What do we use the train for? To train. We use this to train a model to find the coefficients to find the parameters of the model. That means minimizing the loss and with respect to the parameters and the loss is a function of the data.
(1:01:55) So in other words, we find the parameters that minimize the loss given my data. Okay. Boom. Clear. Right. Second validation. We use this to select the model. Okay. We're going to use the validation set to do that. And what is test? test is only to uh is to report the model performance at day and I have a famous saying there's a special place in hell for people they use test data to choose the model do not use the test data for anything leave it there actually the best way is to give it to me and then I give it to you at the end of the semester okay and I do that with my graduate students believe it or not some of my PhD students I don't give
(1:02:37) them the test state they have to give me the model and then I test it because there's a tendency to check on the test date right to do so leave the test state on the side never ever touch it only the last line of your report says and the model did so so on the test date okay all right so here's model selection uh okay there's many ways to do model selection many let's start from the beginning exhaustive search means But I'm going to try all possible models, train them, and once they train, I apply them on the validation set. Okay, let's say you have two models, easy
(1:03:23) peasy. Let's say you have 1,000 models, not so easy peasy. That means I have to train 1,000 models and test it, validate each of them. So exhaustive search the model space is small is pretty cool. Oh, I forgot these people over 3 gy algorithm. Greed algorithm that at every step don't see the picture and we do the best.
(1:03:57) Actually, we kind of run our lives like that, right? So, we we pretend we have five years plan. We're just trying to make it to the next week, right? So, that's called a greedy algorithm. and tuning the hybrid parameter and then is regularization that we're going to see on Wednesday. All right, exhaustive. Okay, so let's talk about exhaustive surive predictors X1, X2, X3.
(1:04:39) how let's think about I can have x1 only x2 only x3 okay that's three models I can have x1 and x2 x1 and x3 or x2 and x3 another I can have all of them so I have actually a model with no predictors just intercept I have three models with one I have two predictors are I have models with three predictors so I have eight moves from M0 to 8. In reality, if I have J predictors, how many? Huh? Two to the J linear is right. I said you said polomial or interaction terms.
(1:05:31) If you start including interaction polomial terms, it's going to be bigger. But 22 to the J is already a big number, right? So, and you can prove this very nicely. Do it in the meantime. I'm kidding. I said you can prove it very easily. The two to the J is a nice little proof not part of this class. Greedy algorithms.
(1:05:58) Greedy algorithms are the ones that say look I'm going to start somewhere and I'm let's say if I want to I'm going to first test one predictor find the predictor that gives you the best result keep it and now only consider two predictors with that one take the pair and then get the third one these are called greedy algorithms uh I think have slides for that so basically it's a little bit the slide here which is fine to be format but I the explan And it's quite straightforward. I have let's say three predictors.
(1:06:28) First I consider one predictor and I find the predictor that gives me the best validation. I keep that and then I consider only that when I do pairs then I get the pair and only consider those go and I keep going. It is greedy because it's no guarantee it's going to give you the best results out because you may have you know the the pair may not include the one that you have for a single one.
(1:06:57) So these are called greedy algorithms and we're going to see quite a lot of greedy algorithms and you're going to see a lot of greedy algorithms in many of your classes. Gradient descent and other ones that you see a lot. They're called greedy algorithms in the sense that they don't guarantee they're going to get you the best result but it's a way to go about this fast. Okay.
(1:07:17) Uh and with the greedy algorithms we have only order of j squar which is much much small to to the j. Okay. Good. Now let's finetune on the hyperparameters now. So I show degree one underfeeding I show degree 50 overfeeding I want there. How do I find that validation set? remember so what I'm going to do I am going to do this plot choose the polomial degree find the validation value validation error change the degree find the validation value change it again okay so I'm going to have this plot here let's take on this plot for a second I don't know why it says protobabas there should have been there uh so on the x-axis we have the degree
(1:08:12) okay one two three four five whatever okay on the y- axis we have MSE for two for two the training and the validation the training uh the training MSE will be going down more more degrees we add more complex the model is better it's going to become okay is that clear why it's guaranteed to go down or stay the same why because if I add an extra term I can put the coefficient that to zero so it gives me the same performance as the previous degree now I can change it up and down but if I want I can guarantee to be at
(1:08:51) the same level so over the training will go down okay so you see it here is that clear to everyone now the validation is the purple line it starts with a small degree K here and What will happen? It goes down and then it shoots up. Now where it start shooting up, that's the overfeitting region. This region here on the left is the underfeitting because I can do better. This region here is overfeitting.
(1:09:25) So where I want to be right there. So if I have where is Zo? You say how I choose my my degree of polomial. I make that plot. I plot it and I find the degree that gives me the smallest validation error and that's model selection with validation. All right. So one more final thing I have five minutes because they were late. I was a yes.
(1:10:08) Can you write H2O? Yeah. So, while we're finishing up, and don't panic because we're going to repeat that on Wednesday, but I want to at least show it. the secret word for the day is H2O. Write down please. All right. So, we're going to just go until um finish, but I'll come back on Wednesday. All right. So far, let's repeat what we've done.
(1:11:00) We had added interaction terms and then we went to polomial and we know how to do polomial and every step of the way we're seeing overfeitting and then we said well we're going to find the model that overfeits the list minimize the the generalization error that's all I said today pretty much okay peasy now we found out that we can select the best model by looking at the validation set That's all good except one thing.
(1:11:32) Peace. I want you to look at this example carefully. Let's look at this example very carefully. Get out of your computer for a second and look at this. So what I have, I have my training data which are the blue dots.
(1:12:05) I have my validation data which are the orange or whatever pinky thingies right and I have three models degree one degree two and degree three okay so I fit each one of them right with my training data which are the blue dots and now which of these model will be selected when I go to validation this is the trickiest question I ask in this class but you can see it degree one very good because it goes close to the validation data.
(1:12:40) Now what happened here? Obviously degree one is not the best model which is the best model by looking at it degree three right most likely. So what happened here? Yes, that's another data that they are the validation data is very representative of that or can I say a little bit differently the validation data happens to be like that remember how do we choose the validation data randomly and you remember lecture four I said someone here was saying hey I'm not 100% sure I think it was Zoyek he was very suspicious of me right uh when I was getting the K was it you no anyway
(1:13:25) uh when I was choosing K someone was suspicious say I'm not sure if K3 is correct why because I'm not sure if my validation data is correct this is what we see here we choose our validation randomly it's just going to be a random subset now let's say I try not three models I try 1,000 models there's going to be one that is just going to be perfect for that validation set and that's a problem okay And that is called uh that is called overfeitting to the validation set. Okay.
(1:14:02) So two val overfeeding overfeed training and the other is overfeeding the validation. Yes. Under overlapping case. Can you is a unique and unlucky case. Very good. The distribution between the validation. So they said make sure your distribution is the same. How do I do that? Or what is the remedy of that? Could make a histogram of the X value.
(1:14:27) Make a histogram. Make sure they match and do S kale divergence or what can I do otherwise? Yes. Do it a monthly. Do it a bunch of times. And that's what we're going to be doing. So I can do it a bunch of times and average it out. Okay. in uh in the last 30 seconds. We're not going to do it exactly bunch of times randomly because that doesn't guarantee every training set to be every training point to be used.
(1:14:59) So we're going to do what is called cross validation which goes and I'll explain that on Wednesday. We're going to talk cross validation is what you said do it a bunch of time on steroids. Okay. So, wait. Don't forget the word H2O. Um, I'm 5 minutes behind. Five minutes behind. That's it. Then, I'm Trisha. I emailed you on Friday about actually turn it.