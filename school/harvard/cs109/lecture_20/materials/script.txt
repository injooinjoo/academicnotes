109 day 20 - YouTube
https://www.youtube.com/watch?v=i5e1uJlr4x0

Transcript:
(00:01) Test, test, test. We good online? Great. Thank you. Good Morning data scientists. Welcome,
(01:08) welcome, welcome to another edition of CS109A dot dot dot dot dot. Hope you're doing well. I'll lead the ship, hopefully not sink it today. Uh, and we're going to get back and catch up on some stuff that we missed last week when I was unable to come. Um but first just a couple announcements you know what is the first announcement what's happening next week quiz and well homework due.
(01:49) So there's a homework due and a quiz. All right. And so just to frame the quiz a little bit. What does the quiz cover stuff since the midterm? Okay. What did the midterm end on? So if we look at our list of topics of lectures, we basically ended here at the PCA lecture 12 and then since then we've just been doing classification modeling mostly.
(02:20) And so we've talked about logistic regression and general classification ideas. We extended that to BA's framework for logistic regression. We talked about hierarchical models and doing a little MCMC through the boot posterior distribution. And then we talked a little bit about missingness but not really that was kind of MCMC continued and then Pablo took over talked a little bit about decision trees and this is the little bit of regression that comes in decision trees both for classification and regression and then today we're talking more thoroughly
(02:57) about missingness and visualization. I don't know if you can see all of these. I think a lot of them are hidden but where are we headed? We got about five more lectures after today and we are talking about all of these topics which are building off of decision trees and so these are ensemble methods that use decision trees. Okay, but today we're taking a bit of an aside.
(03:28) Great questions on that. As far as the quiz goes, we'll get you a practice quiz hopefully released tomorrow at some point. Sorry, tomorrow, Friday at some point. And then uh we'll talk more about the quiz on Monday. How many cheat sheets are you allowed? That I don't know that I think we have to have a conversation about.
(03:54) How many did you have for the midterm? Two. How many did you have for the first quiz? One. So, I'm not sure. I My guess is one because it's not cumulative. That would be also how I feel, Pablo. But yeah, um because it's not cumulative. It's going to be on the new stuff and it's gonna follow more like quiz one where it was like, you know, understanding the basic basics, the fundamentals, I'll say, of those topics, not the more complex stuff.
(04:24) Okay, other questions? Is the quiz material uh through today's class or does it include any of next week? I don't think it includes any of next week. It's not going to include bagging and random forest, right? No, no, no. Through today's class. Yeah. Yeah. Yeah. So, through through missingness.
(05:01) So, you have time to like study and and and understand Students always want more practice problems, more practice problems, more practice problems, more practice problems. Don't expect more, more, more, more practice problems. We encourage you to come to office hours. come to office hours to ask questions about concepts, ask questions about relating it to the practice problems we do give you to the section material.
(05:28) Conceptual questions are great to help in understanding. We can give you all the practice problems in the world, but we're not going to ask those questions. We're going to ask a question that's not in the practice problems. And so understanding conceptually the ideas is more helpful than doing more practice problems.
(05:44) though I do understand more practice problems are helpful. Okay, we will get you some. Don't worry. Office hours. Office hours. Office hours. Yes. Lecture 20. Missing this. Hopefully your PDF is updated. It should be because it says lecture 20. All right. So, we're doing missingness and a little tiny bit of visualization. And so, we're going to do some review.
(06:21) We talked about this very, very briefly a week and a half ago. And then we'll talk about ways of doing imputation. And we'll get a little bit into visualization for interpreting your models. Okay? Interpreting blackbox models. And then the rest of this we're not really going to get into, okay, today we won't have time, but it's there.
(06:39) And it was there back in lecture two or a lot of it was there in lecture two. I think lecture two, whatever that lecture was. Okay. What is missing data? Missing data is when you have a data set that has holes in it. That some of the measurements are unavailable to you. Okay. Why are those measurements unavailable? Unavailable is the million-doll question. The $64,000 question that we're trying to decipher.
(07:02) Okay. Why it's missing and how it's missing and how we handle it is really dependent upon that first question. Okay, how does this show up in pandas? We talked about it before. Nans, how does SK learn? It vomits, right? It throws up at you. It says, "I don't know how to do this." And so we have to find a way to handle it.
(07:26) Okay, how does this affect our modeling? Well, you can't model. If you just simply drop your Nans, then that opens up Pandora's box for the big statistical issue that we talk about all the time, which is starts with the B and ends in an bias. Okay? When you're doing estimation, when you're doing predictions, you get biased or could potentially get biased predictions, systematic errors in your predictions, systematic errors in your estimates.
(07:58) Okay? And that's what we're trying to avoid. Systematic issues. The simplest way to handling missing data is what would you do before you took this class? You would probably drop the observations. You could drop the rows, you could drop the columns, or you can imputee something really simple like a zero or the mean, median, or most common class if it's a categorical variable.
(08:27) But we don't want to just unthinkingly do that. We drop the observations. We can do that very easily. We could imputee the mean median. We can do that really easily. And when we do that, we could get as a consequence some biases in our prediction, some biases in our uh estimation. What does that mean? Biases in predictions. What's a prediction? What do we call a predicted value in this class? Typically, what symbol do we use? Y with a little hat on it, right? Why hat? And if it's systematically wrong, we might in certain parts of your data be systematically in a new test set
(09:12) overpredicting or underpredicting over or underestimating that outcome. Estimation. What does that mean? Where do we do estimation? Not in all of our models. We do prediction in all of our models. We do estimation in these parametric models. We have these parameters we're trying to determine the betas for.
(09:43) And those betas could be systematically off if we handle our missingness in an improper way. Okay. Great. All right. So, we talked a little bit about this last time. This is kind of where we ended off is dealing with missingness by creating a missingness indicator variable. And I went through this probably a little too quickly. What you do is you include two variables, a pair of variables for every variable that has missingness. Okay? You have a variable that has the imputed values in it.
(10:14) Typically you just impute zero or the median or the most common class or the reference class and then you include this indicator for missingness. So here's sort of the illustration. We start off with two observations in our data set. Two observations sorry two variables in our data set. X1 and X2 we have some missing values.
(10:41) X1 numeric, X2 numeric binary. Okay, so really it's representing something categorical. You got it. Okay, and then we impute values. We impute zeros where there's missing values for all those observations. Okay, very simply. And then we create these missingness indicators. Now, of course, you're not going to call them X1 miss, but maybe you could.
(11:03) And you just say, "All right, where was that imputed value of zero put in?" In these two locations for X1. Where was that missing value of zero put in for X2? At these missing locations. And so in essence, the simplest way to think about this is X2, which was originally categorical, a binary indicator in this case, you've now created a set of binary indicators, two of them that are going to represent it.
(11:37) you've kind of created a third group, right? You have the zero group, you have the one group in the original variable, and now you have this missing group. Okay? And it allows you then to do estimation separately for those observations that have missing values. All right? You're going to do inference or modeling always including X2 star and X2 missing every single time you want to consider X2.
(12:03) For X1, you're always going to include X1 star and X1 missing every time you're going to consider X1. All right, which will free up some of the estimation. Does it make it perfect? No. but accounts for some of the issues that you originally had with that missing data and doesn't artificially plug in a zero and treat it equally as an observed zero.
(12:28) Okay, and that's important. Okay, just as a heads up, this is kind of my first go round of handling missing data every single time. I just do this and things usually work out reasonably well. All right, why does this allow you to estimate the betas? uh and helps you quantify any possible bias originally that was in the data.
(12:50) It allows for that missing group to be estimated or predicted separately because those individuals that show up as missing, you don't want to treat like everybody else who didn't have that missing observation. Okay, we'll see some examples of where missingness shows up here in a little bit.
(13:09) Just giving you a quick heads up that we're kind of creating this did not respond. So that's kind of the simple highlevel ways of handling missingness. We'll get into the weeds a little bit here in a bit. But first off, let's frame the types of missingness we might be able to see. There are three major types. If you've taken a stat class that has talked about missingness before, you've probably heard them labeled as such. MAR, MAR, and MAR.
(13:33) The hell does that mean? Okay, we have missing, what's called completely at random. We have missing at random. What the hell's the difference? I don't know. And then missing not at random is the statistical terms we put to them. And that gets used in whenever you're doing data analysis generally dealing with missingness.
(13:55) Those three classes come to us based on why those observations are missing. Okay. So let's talk about a few examples here. We could look at missing data that can come from a survey that when you enter those into your computer, you just randomly missed some of the observations. I've done this before. I've been guilty.
(14:19) I've set sent out surveys to my students, physical copies first day of class and then I'm entering them into my Excel spreadsheet or I have my TFS do it and they just missing a value. All right? And that can cause issues. All right? But those issues are minimal because they're just kind of missing completely at random. Okay, there's the terminology. A responded chooses not to respond to a question like, "Have you ever used recreationally used opioids?" Let let me ask that question right now.
(14:54) How many of you have used opioids recreationally? All right. How many of you have not used opioids recreationally? All right. And for those of you who didn't raise your hand, you are a missing observation. All right, those missing observations, do you think they are all zeros? Maybe. All right, but maybe they're ones.
(15:18) Those of you who raised your hand and said, "I've never recreationally used opioids." Are you lying? Maybe. All right. And so these forms of biases can creep into the data both in what's measured but more importantly what's not measured. We're trying to handle that. Okay. You just started to collect a new variable halfway through your data collection process. You know the pandemic happened at one point. It's a long time ago now.
(15:44) But people started measuring more things because now the pandemic happened. Okay. And so if you have data from before the pandemic and after the pandemic, maybe the data before the pandemic was different. Okay? Or you roll out some new feature on your tech platform and you start to measure things at some point, but you have data from before that and you combine that. That missing data, there's a reason it's missing. It's not completely random.
(16:11) You want to measure the speed of meteors for Pablo, but some observations are just too quick. you want to measure uh certain indicators in blood work, but the values are just too low to show up. And so there's these truncation things as well. All right. Why that missing value shows up leads to very different types of missingness and how reliable you can just impute a mean or a median or a single value into there depends on what generated the missing value.
(16:43) So always ask the questions, why are these observations missing? Okay, think to yourself. It's a thinking question. I'm sorry. We want to make you think about your data. Types of missingness. So, the three major types missing completely at random. So, as in order, it's most forgivable to least forgivable or easiest to handle, least concern to most concern.
(17:10) Okay, missing completely at random just means someone poked holes randomly in your data set. has nothing to do with anything other than randomness. Okay, that's me entering information in my Excel spreadsheet based on the surveys that we're given. Missing at random, the probability of missingness, the chance an observation is missing can be gleaned based on the available information, the other things measured in your data set.
(17:41) So you can recover any biases that treating that missingness improperly by modeling that missingness with what is measured. Okay, so that's the important distinction here. There's still some randomness involved, but missing at random, you can recover that missingness through what was measured. and then missing not at random.
(18:07) Unfortunately, then you can't recover why those observations are missing necessarily. Okay. And so the probability of missingness depends on information that has been recorded. And this information also predicts the missing values that has not been recorded. This is essentially based on an unmeasured variable, something that's lurking out there, some latent variable. You can label it with whatever you want, but it is not in your data set.
(18:30) And so you can't recover why it's missing and you don't really know how to imputee properly. Okay, so based on that last slide, those examples that we gave, which one would be labeled as each of these three types? I already gave you the MCAR one. Kevin doesn't enter things properly in his spreadsheet because he's going too fast.
(18:53) What about MAR and MAR? What was the list of examples? I don't know. [Music] The opioid one might be missing not at random. Okay. Why someone is missing is not measurable. They might decide not to answer the question because of something that wasn't measured because they either used or did not use opioids directly or had a higher probability of using. All right.
(19:34) What about collecting a new variable partway through the data collection? What do we need to measure to model the probability of missingness? Yeah, it's not the data collection. We did a time stamp and if we have a time stamp, we can predict whether an observation is missing or not. And so, yes, we can sort of recover the reason for missingness if we have something else measured in our data set.
(20:12) Okay? And you can think of other reasons why people may have not responded to this. You might have other predictors that predict it in your data set as well. Okay? And things like the speed of meteors just don't show up because they're too quick. That would be hard to recover too based on other measures you have. If you want to predict speed and speed shows up as missing because it's too fast, what is predictive of a speed of a meteor? That's a question for Pavlo. You need to know the domain. Did you ever study meteors? Did you ever study meteors?
(20:45) Yeah. Aster ask, did they go fast? Some of them do. Did you ever miss them because they were going too fast? Sometimes. There you go. There you go. All right. Important questions. Faster is easier. Oh, detecting. Yeah. Yeah. If they're just kind of sitting there and they're not spinning, you don't get the reflections. Yeah. Yeah. I see that. I see that. Okay.
(21:12) With me? Any questions on that? All right. More specifically, what is the M car? Best case scenario, coin is flipped to determine whether an entry is missing. Great effect. If you ignore, there's really no in effect on the inferences. Okay, meaning ignoring means just drop the observations.
(21:30) If you drop the observations that have missing values, you're good to go. Okay, when you have missing completely at random, right? Lots of ways to handle lots of options. You might impute something simple or you could just drop the observations if you know you have missing completely at random. That's a big if. It's a really big if. Okay, missing at random is a case that can be h uh handled. Here's an example.
(21:55) Men and women respond differently at different rates to the question, have you ever felt harassed at work? Some individuals choose not to respond. Some individuals choose to respond. Okay? Knowing that someone did not respond could be recovered particularly if they were harassed or not, but also if they were male, female or other. Okay? So, sex or gender could be predictive.
(22:22) You can in recover any biases invoked by using the other variables that were measured. You have to measure sex or gender in this case. All right, we can imputee a value there using a model based on the variables that were measured. And so now we're going to actually fit models to predict values that are missing rather than just impute a mean, median or zero. Okay.
(22:52) And then missing at not at random is the worst case scenario. People drop out of the study because they experience some really bad side effect. This is where I see it in the medical world all the time. You don't see the outcome of an individual because they had really bad side effects.
(23:10) Most likely their primary outcome is going to be worse than if they showed up and stayed in your study. And so this is something that can happen in a clinical trial or a medical study because it looks like this new drug is great, but the problem is there were all these side effects and those people who actually would have really bad outcomes dropped out and are missing values in your response variable and that causes biases in determining which of your two treatments is best.
(23:34) Okay? You can improve things by doing careful imputation, but you're never going to completely recover any biases that exist because you don't have what's truly measured. You don't have measured what's truly associated with that missing value or that reason for its missingness. Great.
(23:55) What can help the best? Usually that missing missingness indicator variable can really help. Okay, three major types. Here's the biggest question. Can you ever tell based on your data what type of missingness is actually present? Why do you think I asked this question? Because you cannot. It's not possible. Right? So the answer must be no.
(24:23) In general, the problem with lurking data is they're unmeasured and you don't know if maybe you had measured them. You can really get at the heart of the issue. And so you can do the fanciest models, try to recover these missing values as best you can, but you'll never know if maybe I had one other predictor, I could improve that even more and reduce that bias even more. You're never going to know. And that's the issue with fancy statistics is you never really know.
(24:54) Okay, in practice usually we start modeling those missing values and predictors as best we can. So at least we can handle the missing at random situation. And so that's all of these fancy imputation methods at least will be improvements to say all right if my data is missing at random now I've properly recovered any biases.
(25:25) If my data is missing not at random, you're so recovered bias like in practice like what did I mean? What does bias mean? If I wanted to write it out statistically for my statistical friends, what does bias mean? Do I have any statistical friends who want to define bias for me? expectation of your particular value for the predictor. I changed that a little bit.
(25:53) Expectation of your estimate for a coefficient for a predictor is not on average centered at the right place for where it truly is. Okay, where does this show up? Linear regression. Think about linear regression. Simplest case. Do I not have a chalkboard today? You know, it's a math class. Is chalk is way better than whiteboards.
(26:31) I want to feel the chalk as I wrote. Should we write a linear regression or logistic regression model? Which one do you want? What's your favorite linear? Okay, simple. I like it. Linear. What's the linear regression model? We say essentially y = beta 0 plus beta 1x. Simplest case, right? Linear regression single predictor.
(27:03) What are my parameters I care about? Yeah, the beta zeros and the beta ones. These are the true parameters. Probably I should have an epsilon here, but these are the true population theoretical parameters. Okay, this is the model. I don't know what they are, but I believe they follow this relationship that's linear y and x. Okay, what's the next step? Theory, model, setup.
(27:43) What should you do to estimate those betas? Next, you got to collect some data, right? Step two, collect data. Step three, profit. No, not profit. What's step three? After you collect your data, what do you do? Yeah, you estimate it, right? We fit in OS as our simplest case. What does OS stand for? Ordinary le squares. We have this whole loss function. We go through all this whole process.
(28:22) And that gives us our beta hat zero and our beta hat one. estimates from the model. You know, fit a model and we get estimates, right? But beta hat one that we observe from our data, we know isn't perfect, right? There's randomness in collecting data from a population or from a theoretical model. And so what we ask then what is bias or unbiasedness is the expected value of beta hat 1 is it equal to beta 1 as an expression.
(29:11) Okay on average that sampling distribution of beta hat one is it centered at the true beta 1. It's a theoretical question. We will never know for a fact whether that's the case because we can't access the real beta 1. We just know through the principles of fitting a regression model.
(29:34) This should work reasonably if we have what type of model something is truly linear. Our data are normally distributed dot dot dot. Those assumptions in linear regression. Okay. And our data were sampled in a reasonable way. If they were sampled in a biased way, your estimates will be biased themselves. Okay? If you're taking off the high residuals, your beta 1's going to be a biased in your model.
(29:58) So, I think that's kind of the simplest way to think about it. Okay. All right. Does that answer your question? Kind of. All right. So, let's get at this methods of imputation. How can we start doing imputation? We talked about one already, but really you should ask three different kinds of questions.
(30:24) How we perform imputation and handle missingness really depends on a couple questions. The biggest thing is, is the missing observations the missing data? Is it in your response variable Y or in your predictors? If it's in your response variable Y, good luck. All right, it's really hard to predict or imputee values into the response variable because then later on you're going to fit a separate model that predicts that response variable and it's going to screw everything up.
(30:52) All right? And so generally when you have missing values in your response variable, you have to be really really careful and we're not going to answer that here. Okay? When you have missing values in your predictors, that's a different case. This is all methods for imputing values. when you have missing data in your predictors, your features, your X's.
(31:10) All right? Always, always, always you're going to ask what type of variable do you have, categorical or numeric, right? Just like everything else, what model do you use? Depends on if it's categorical or numeric. And then the amount of missingness as well. So if you have just a few missing observations, it's probably okay to drop it. Drop those observations.
(31:28) If you have one variable with a whole lot of missingness, it's probably okay to just drop that variable. Okay? Because I don't trust if it's 60% missing, I don't trust imputing that because I'm imputing making up values for the majority of it. Generally speaking, under other than under extreme settings, it's okay to try to imputee fill in those values. All right? And so today is about how to do that.
(31:55) Several ways. Whoa. Several ways to do that. Simplest, impute a value like the me or median or if it's categorical, the most common class. We talked already a little bit about the imputing an indicator. Okay. Hot duck imputation. There it is. What do you do? Hot duck imputation. I'll show an illustration of this.
(32:20) Just grab a random observation that is observed and just plug it in from that variable. Hot deck imputation. Okay, what else can we do? Model the imputation. That's what this is all about. We're going to try to build a prediction model, which is weird because we're predicting a predictor from the other predictors.
(32:40) This isn't our primary goal of predicting Y. We're predicting one of the X's from the other X's. All right, we're keeping Y separate because we don't want those biases when we do eventually build a model to predict Y to creep in. All right, and then we can model the imputation with uncertainty.
(33:05) What's the difference? Model the the value that's missing just means plug in Y hat. When you model the imputation with uncertainty, you plug in Y hat plus or minus residual. All right. And so it's reflecting in this case the individual randomness, the stochastic randomness, the irreducible randomness around your model, whereas this ignores it with me kind of.
(33:31) Okay, let's get into it a little bit. What are advantages and disadvantages of each approach? It depends on your goal. Here's a toy example. All right, this is X and Y. Really think of this as X1 and this is X2. X2 has missing values. X1 is another variable. How is X1 measured? We have all measurements for X1.
(34:09) How is it measured? That's numerical. You can define as ordinal depending on where you are. Yeah, it's a color. And color can be parameterized either numerically, categorically. What's that? Yeah, some RGB values. You can put it on a rainbow scale. And that's kind of what we're thinking of here. Okay.
(34:31) Is that predictor that's fully measured is on the rainbow scale. Okay. Are we closer to the purple and green or are we closer to the red and orange? There's no purple here, I realize. With me? Okay. And then we're trying to imputee two missing values based on what we observe for color. How can we imputee those values using the information from color? What would be reasonable things to do? First off, the variable that has missing values, is it numeric or categorical? Why here? Numeric or categorical? Numeric. Thank you. What are type of
(35:19) models we could use to predict a numeric variable? What's the latest one we learned? A decision tree, right? a decision tree for a regression. What else could we use? Linear regression. What else could we use? KN&N. What else could we use? All these fancy models that we've used or learned about.
(35:42) We could do some sort of uh I don't know why you do it here. Lasso ridge, something like that. Okay. And so if I were to build a model using X to predict the value of Y, what would I actually do? I'd drop these two rows and I'd fit a model to predict Y from X for the available data.
(36:01) and can and then I can use and predict based on the light red or the medium red and the yellow to predict what that would relate to in the response. Okay, let's use CANN. Let's use a CANN of K equals 2. 10 n k equals 2. What are we going to predict here? Can you do that math in your head? What do you do? You find your two nearest neighbors based on color.
(36:37) What are the two nearest neighbors based on color? Probably dark red and light red. You take their responses and what do you do in k nearest neighbors to predict? You average them. And so what are you going to plug in? 0.75. All right. What about for yellow? your two nearest neighbor neighbors if this is ordered well and distances are all the same is the average of orange and green what's the average of 0.1 and 10 5.05 5. Okay.
(37:04) And so you could plug in for those missing values, fit a model from your training set for your testing set. Plug in the prediction based on your two nearest neighbors. Okay, so we do a CANN model of two. Simple way to do it. All right, we are modeling the missingness based on the available data, right? What have we improved? Rather than just plugging in the mean, we probably have a better guess of what value is likely to have been there.
(37:32) This is relying on a relationship of y with x in this case with me. All right. And then you can plug in a value here 5.05. All right. And so under this ordered list of types of ways to do imputation, what is this approach? It's modeling the missingness. imputing with a model. Okay.
(37:58) Did we impute with a model with uncertainty? The answer is no. Why not? How could we turn this approach into modeling with uncertainty? Say that again. Some sort of randomness. Sure. How can we invoke some randomness here? We can do a prediction interval, draw from that assumed normal distribution or do some bootstrapping of our residuals. There's a whole lot of fancy ways to do it.
(38:34) But for this KN&N of K equals 2, there's a simpler way to do it. How do you predict with uncertainty, with randomness in a KN&N model? Rather than average the two observations that your neighborhood is based off of, what do you do? You randomly sample one of the observations from the observations in your neighborhood.
(38:57) So if I'm predicting medium red, what am I going to predict in there? Flip a coin. 50% chance it's one. 50% chance it's 0.5. If I'm modding with uncertainty and I'm plugging in for yellow instead of putting in 5.05, I flip a coin. 50% chance it's 0.1. 50% chance it's 10. With me? I kind of do a bootstrap type sample of a size of one from your neighborhood.
(39:28) Why is that better? Why is that worse? If I was plugging averages, let's imagine another yellow observation shows up. What am I going to predict if I don't include uncertainty in that prediction model? 5.05. Another yellow observation shows up. What am I going to predict? 5.05. So, all of my yellow observations have the same response when I do this imputation.
(40:01) What's the problem with that? It doesn't actually resemble the true uncertainty and the actual wise that should exist. And so by incorporating this randomness, you're incorporating that irreducible uncertainty. Your models will be better for it. Okay? And so we can imputee with uncertainty as well. All right.
(40:28) How else can we do it? We don't have to do KN&N. We can fit a linear regression model. I apologize for using M and B here. M and B should be B 0 and B1. How do we actually go through this process? We f a linear regression model using what data set? How many observations went into it? What's the training set for my imputation model? I see five. Okay, the complete data set those to use this imputation.
(41:04) There's five training observations and then I use that to predict two in my test set. Missing values go in the test set. Complete observations go in the training set with me. Okay. And then you just fit a linear regression model like you always would using those five predictors.
(41:28) How you handle X here? Color unclear, but we can put it on some scale with me. Okay, the ones I talked about it or we talked about it together about how to impute with uncertainty. If you ignore this fact, what happens? We'll see an illustration here that really shows it. The distribution of the imputed values will be way too narrow and not really represent real data.
(41:53) In KN&N, how can we handle this? In KN&N, that's where we sample randomly from the neighborhood. In linear regression, how do we handle it? You don't have neighbors to sample from. What can you sample from? You predict y hat based on the line. And then how do you incorporate uncertainty around the line? What represents the uncertainty around the line? What was that? data.
(42:33) Someone said it the errors, the residuals. Exactly. Right. So, randomly sample a residual if you want and just add it to the yhat and you're good to go. You bootstrap sample one of those errors. Great. So, here's the general idea. If I do X and Y and essentially imagine our X is fully observed, our Y is income and we're trying to predict X from Y.
(42:57) Note the imputed values when I predict based on whatever these x's are are going to fill in this line perfectly. Right? And so we have we're essentially saying there's a really strong relationship between X and Y when I start predicting those values in really I want to capture the random scatter in the noise in the actual data that was observed so that my imputed values better reflect the true relationship between X1 and X2 that were used in that impute imputation model. Okay. All right. So this is essentially
(43:37) what it's setting up here. The probabilistic model in linear regression. We say y is related to all these x's. How do we take advantage of this to impute with uncertainty? It's a three-step process. You fit a model to predict the predictor variable with missingness. Call it xj which is actually your y from all the other predictors measured.
(43:57) Okay? And then you predict the missing value from the model in the previous part. You get your y hat. And then you can add a measure of uncertainty to that which is basically either randomly sampling an error or you random sample from the assumed normal distribution. Right? You estimate the variance of that normal distribution and you sample from it.
(44:16) So lots of ways to add that uncertainty back into it. Great. Instead of sampling from the normal, you can just bootstrap sample from the observed residuals. And that seems like a reasonable thing if you don't trust that normality assumption. Okay. Great. All right, K and N. That was pretty easy. Two ways. Use a K of one or use any other K but randomly sample an observation from your nearest neighborhoods.
(44:41) That can be done with equal probability or with some waiting but might be somehow related to the inverse distance measure. All right. How would we do this in decision trees? Oo, decision trees. What is a decision tree? It's a tree with decisions, right? What is a decision tree? Depends on if it's a regression or if it's a classification problem. In a classification problem, this is really simple.
(45:15) Okay, what's the simple idea idea? Build your decision tree to predict the variable with missing values based on the full data set. That's your training data set. And then you predict the values in the missing variable your test set. But how do you predict with uncertainty involved with randomness invol involved with the decision tree? Probability maybe in probability select which node to go to.
(45:54) Some that sounds complicated but some inverse probability you decide which node to go to. Like if you're at a decision and you're really far from threshold, that means the high probability that you should go the correct threshold. But if you're on the edge, so you would like either this way with the probability. Okay. The be careful.
(46:14) So I like what you're saying, but I think it's important to think or set this up a little bit more carefully. I think I on my fault. I didn't my fault. I did not explain this situation. We care about a response variable Y. We have a set of predictors X1, X2, XP. This is our goal of inference, our goal of modeling.
(46:44) But uhoh, X2 has missingness. Okay, what's our first step that we need to take? Step one, we need to imputee values for X2 using some model to predict X2. So we have a whole separate model that is predicting X2. And then once we imputee those values then we can build a model to predict original y. This idea of imputation is a preliminary step.
(47:43) It's a feature data wrangling step that we have to take in order to use that variable in the original inference goal. The original goal of modeling. Okay. the housing data set. We wanted to predict the selling price of houses in the Cambridge area. We had some missing values in the predictor set. We had all those responses. We had some missing values in that predictor set.
(48:08) We had HOA homeowners association fees was missing for some val observations. And we need to plug in some value there in order to then use it to predict the selling price of a home. So this is the step of how do we fill in those values step one before we start predicting price as step two.
(48:33) I just need to do something to wrangle that data in order so I can use it in my secondary model to predict price. Okay, that's the framing here. That's why we're doing imputation. And so we have to decide what type of model we want to use for predicting X2 that has the missing values for some of your observations.
(48:55) And so what do we need to do is it depends on the values that X2 takes on. Okay, but remember it's our response variable when we fit this imputation model even though it's a predictor in our true goal model. Question Next to do all creative columns that did talk about one of the ways to handle this is to impute values.
(49:27) We need to handle missingness probably should be the label but imputing values you might create a binary indicator for it. Yeah, I think that's always a good idea. a binary indicator for its missingness. And so you're creating new features as well. Not only are you imputing, but you're keeping track of whether or not it was imputed. Okay, I think that's smart. Great. Ignoring that fact.
(49:56) How are we going to build an imputation model? Okay, back to the decision tree question. This is the response variable. And let's imagine X1 through XP. Everything else other than X2 is complete. Okay. So what can we do? Split our data into train and test. Our train data set is those that for X2 has no missing values.
(50:25) Our test data set for the imputation model has the missing values in X2, but it has all the X1's and XPS and everything else in between. Okay. So we build a model to predict X2 based on the full data set. And then we use that full data set to predict X2 in that test set. And so we are fully able in a decision tree fully able to traverse that decision tree to its base to its leaf node. I don't have missingness in X1 through XP.
(50:57) I can fully traverse that to do predictions. I just don't know what to compare it to with me. Okay. So how do you do predictions in a decision tree? I can fully traverse it. So that kind of clears up that issue. How do you use predictions? Do a prediction in a decision tree. Here's a new observation. You just traverse the tree.
(51:19) You're asking a whole bunch of yes or no questions. Am I bigger than x1 being five or am I less than x1 being five? This observation that has missing in x2 has everything else observed. So I can traverse that tree fully. And then I get to the base of that tree, whatever best tree you decided to use.
(51:38) And how do I use it to do predictions? If it's a regression problem, how do I predict using a decision tree? Do you remember from class on Monday? You just average them. There's your y hat. You average the observations that are in that leaf that you happen to end up in in a decision tree. Okay? You average the responses. That's my yhat. How can I do yhat with uncertainty? What's the simplest way? That terminal leaf probably has multiple observations.
(52:18) It might be five observations that you averaged over to do the prediction. So rather than averaging them, what do you do? You randomly sample one of them. Okay, does that make sense? Just like a KN&N except every leaf doesn't have K every time.
(52:36) Every leaf might have 20 observations you could sample from, but the leaf over here might only have three or four with me. Or you can do other things. You can randomly sample a residual from all of the observations. There's all sorts of ways you can handle it. And it's based on what the assumption is. Okay. How do you do it in a classification decision tree? That's even easier. You get to the base leaf node and what do you do? Instead of majority wins, what do you do? You just randomly sample an observation and it's going to be either zero or one.
(53:06) Okay? Or flip a coin with the probability of then depending upon the number of ones and number of zeros that are in that base decision leaf. A residual number. If you assume your residuals are evenly distributed in all of your leaves, then you can randomly sample a residual from any of the leaves. And the simplest thing is to just sample one observation from whatever leaf you happen to be in that that observation essentially has the residual baked in. Yep, you got it.
(53:46) Great. Okay. Hopefully like in section or something we'll show this. All right. Boob boo classifiers. Same general idea. you fit a logistic regression and you flip a coin with the probability that's predicted rather than just do a p hat prediction probability. Do not just classify blindly using the predict command in sklearn.
(54:14) Why? What's predict going to do in sklearn? ic prediction determin deterministic predictions almost as hard as heteroscadasticity to say gives deterministic predictions and it's a single value. It's going to be the class plurality class in that model and so you're always going to be predicting successes for everybody and that's probably not very reflective of reality.
(54:42) If you did predict praba at least you have a probability measure in there too. Okay. Great. All right, that's all simple and good. That works great if you have missingness in one variable. What happens if you have missingness in multiple variables? At this stage, when you're building your imputation model, what happens if X1 also has missingness? What can we do? It's a simple idea. There's another model.
(55:21) You got to start somewhere though. So what you generally do is you say, "All right, X1 has missing. Let's start there. Let's just plug in the mean, median, or most common value in X2. Build a model to predict X1's missing values from everything else, which already has some imput imputed values in it, and impute X1.
(55:40) Once I now have this new X1 with imputed values, let's use it to predict the missing values in X2. You kept track of which ones were imputed. All right. And then you just iterate down the line. One time through that imputation from X1 to XP is not going to be very trustworthy. You haven't very much converged. And so you just iterate multiple times through that loop.
(55:59) When to stop? When things converge in some way. Okay, similar to the idea of doing MCMC in the posterior. You wait for convergence in some version. What would be convergence here? Your averages aren't really changing. Okay. And so when you have missingness and lots of observations, essentially you just have to iteratively do the process and you have to sometimes do that multiple times.
(56:29) Repeat the runs through all the observations in your data set. Okay. I hate these notifications. right? How do we do this in skarn? There's kind of four ways to do it. More than four ways, but four really simple ways. You can create a simple imper just to imputee a specific value for everybody. You can do like iterative imputer. You can do KN&N imputer.
(57:11) You can do missingness indicator variables. Uh you can create or you can do your own model and do it a little bit more manually. Okay, to make your life a little bit easier or more flexible. Okay, there's automatic ways to do it in a scalar. Okay, we are trying to avoid insufficient data.
(57:36) Where do we have to do this? When there's not enough observations, we want to create more. Did I do something wrong with the cann? Yeah. Okay. I'm aging myself. data we allow students before like it was just all sympathetic and yeah there's some problems so doing imputation you have to make a judgment call as to how much missingness is too much missingness okay it depends on a reasons for that missingness and how reliable that prediction model is okay for the missingness so the example I give is that Cambridge housing data set,
(58:25) we had like the majority of homes did not have a lot size. All right, but I trusted my imputation model. They didn't have a lot size because they were condos. And so I'm pretty certain that those imputed values should be zero because you don't own the lot, you don't own the yard if you're in a condo association, if that makes sense, right? And so how much missingness to allow for depends on the situation. Again, you got to think hard deeply about your problem.
(58:58) Okay? Generally speaking, you know, if it's more than half, you're in problem unless you really trust what you're imputing to represent something real. Does that make sense? Yeah. If it's like 10%, then you're probably good to do an imputation. Great. Okay. All right. real quick. We got 15 minutes here. Let's do a little bit of visualisations.
(59:31) My computer's not loud enough. Game time. Visualizations is review. All right. And so I need a volunteer who wants some extra credit on well, not extra credit, but wants a free day to not come to lecture. A new volunteer. Pavlo. Number of times I have to say that. Pablo. Have you volunteered before? No. Great. All right. What's your name? In what house? My name is Chrissy. I live off campus.
(1:00:09) So great. So Dudley House. Yeah. Great. Thanks, Chrissy. Uh, one fun fact. One fun fact. Um, I can fly single engine airplanes like Ces and Warriors. Man, that's awesome. Great. You can fly airplanes. Amazing. Cool. Are you Do you have your pilot's license then? Yeah, my private pilot's license. That's amazing. Great. Well, you might if this data science career doesn't work out for you.
(1:00:36) We need pilots. We need a lot of people in the aviation industry these days. Okay. This is what is the goal of visualization? We have options. Listen carefully. All right. When do we do visualization of our data? To explore the distribution of variables to explore the data to build hypotheses to communicate results of your model or to trick and manipulate your audience. There could be multiple correct answers.
(1:01:07) And if you need help from the audience or your friend, just let us know. No judgments. I'm going to go with A, B, and C. A, B, and C. You think tricking and manipulating your audience should not be the goal of visualization? Probably not. I'm pretty careful. All right.
(1:01:31) What do you all think? Do you agree, Bob? Yeah, I think that's right. I think that's right. Well done. Well done. Well done. Thank you, Chrissy. And good luck flying. All right. So, what we've talked about visualization. We've talked a lot about step one and step two or answer A and answer B. We have not asked much or done much about communicating your results of your modeling.
(1:01:55) Why? Because we didn't have models when we talked about visualization. But I think it's important to think about how do we visualize our results? When I come up with EDA, that's before modeling. But after modeling, I want to get a sense of what my model is telling me. Okay, what are the relationships? What is that prediction model? But we have a scatter plot of points and it's a linear regression model.
(1:02:18) What do you do? You plot the line, right? You put the line on the scatter plot and that's telling you what your model says about the relationship between X and Y. If you have a high polomial function but a single predictor, I can interpret those different coefficients separately. I plot the model to get a sense of how X and Y relate.
(1:02:41) what my model is telling me about how X and Y relate. Okay, plot your data, plot your predictions on your model. And that's going to apply here for all blackbox models. What matters in regression problems when we have something like parametric models, linear regressions, we automatically can try at least to interpret the coefficients to get a sense of relationships.
(1:03:04) This is an inference question. We want to explore and communicate what our model is telling us about relationships. Right? If we have a blackbox model, a non-parametric one like a KN&N, like a decision tree, it's hard to get a sense of what is the relationship between a predictor and the response just from the output of the model.
(1:03:30) We have to visualize the model to understand the relationships between predictors with the response. Okay. Same thing for classification problems. Okay, what's the simplest thing when you have a highdimensional prediction model and you have a classification problem? What should we use? We can use PCA to represent all of those highdimensional predictors. We could do the same thing in a regression problem. Just gets a little bit tricky.
(1:03:52) Okay, don't forget PCA. All right, so when we have something like linear logistic regression models, we can interpret those appropriate things. If we have lots of interactions involved, things get a little bit complicated. When we have high high order polomial terms, things get complicated. And I wrote these slides before we had decision trees.
(1:04:10) When we have a KN&N model, it's even trickier. All right, we need to plot the predictions. Plot the decision boundary. Plot the predictions uh in this situation to really understand what's going on. All right, what's going to aid us is something that I label as partial dependence plots or partial dependency plots.
(1:04:28) sometimes labeled as PDPs. What we're doing is we're going to predict and depict the relationship between Y and X for a given set of input predictors. So if I want to know how X1 relates to the response Y, I plot how Y and X1 relate as a scatter plot. And then what I do is if this model has lots of other predictors in it, I set X2 at a reasonable value, X3 at a reasonable value, X4 at a reasonable value and change X1 given all of those X's other X's are fixed controlled for their effects and look to see how that relationship changes. Okay, that's a partial
(1:05:11) dependency plot. changing one of your X's to see how Y changes holding the other predictors in the model constant but at some prespecified value. That's a linear regression model, a multiple linear regression model. Okay. And we're going to do sort of the same thing in KN&N. You're going to fit a model. This is for classification.
(1:05:37) How do we do KN&N for classification? It's the exact same thing as it was for KN&N in regression when we're determining our neighbors based on distance. But then when we do the prediction rather than averaging the observations in our neighborhood, we can just essentially look at the majority class just like we would do in a decision tree or we can flip a coin with probability based on who shows up uh in that neighborhood. All right, that's just an aside.
(1:06:04) What we're going to do here is just to show you how to fit a KN&N classification model back to our heart rate data set. What's our heart rate data set? We're trying to predict people in the ER in the ICU whether or not they have uh heart disease. Okay? And so my response variable is whether or not someone has heart disease.
(1:06:24) And we have a whole long list of predictors. Right? The simplest model, the visualizations we've looked at so far is we're trying to predict heart disease based on someone's max heart rate. A high max heart rate means you're pretty physically fit, right? Pablo is probably like 200. Mine's like 150. And so we can predict Y from X.
(1:06:42) We fit a K nearest neighbors of 50 and then fit that model. We can create a synthetic values of X and then we can predict my P hats, my probabilities based on those synthetic values. And we can also predict in this case the classes. And as a result we can create a new data frame my synthetic X's which is ranging from the minimal value of heart rate to the maximum value of heart rate. Some stuff got trimmed off here.
(1:07:22) And then we have a predicted probability and we have a predicted class. Everyone kind of following what's going on? We fit a CAN&N classification model. From that CA K CANN classification model, we have predicted values which can either be proportions, probabilities or turned into a pure classification.
(1:07:44) What determines a pure classification of one when your probability is above.5? What predicts a classification of zero when your probability is below 0.5? Simplest Beijian classification boundary. Okay, with me? All right. How do we visualize the relationship between max heart rate and whether or not someone had heart disease? This model you just plot the predictions.
(1:08:12) It's really simple, a single predictor, right? And so we just plot as X changes. Let's look at the relationship of those predicted probabilities. Okay? And so when your max heart rate is low, your predicted probability is like 75%. When your max heart rate is high, your predicted probability goes down to about 20%.
(1:08:31) And then you would ask the question, what does this plot suggest about the relationship X and Y? I fudged the data. Oh, Pavlo's fudged the data. That's okay. What does it say about the relationship? And it's a true relationship. Is this relationship positive or negative? A negative relationship.
(1:08:58) Higher max heart rate, better chances of not having heart disease, lower chances of having heart disease. You can visualize the relationship through plotting the predictions, plotting the probabilities here with me. Easy to do with a simple regression or a simple logistic regression model. We don't expect things to be linear here. Why not? Because my response variable is 0 to one.
(1:09:16) We'd expect them to taper off when we get out to the zeros and the ones. Okay. What we can do as well is we can create a multiple KN&N model, a multiple predictor KN&N model. And so we can create a new data frame X and fit a new KN&N50 data frame or model based on this data frame. I'm ignoring some issues here. We're going to not worry about standardization just because life is easy. We're trying to keep it simple.
(1:09:47) We're not adding that extra layer on. a KN&N. It's important when you're measuring distance to determine your neighbors. It depends on the units of your X's. And so you might want to consider standardization before doing this. As an aside, I didn't bother. Okay, I don't want to over complicate this.
(1:10:09) And so we fit our KN&N50 model and to interpret it, what do we have to do about the relationship? We bring in lots of other predictors and we have to hold them constant. So what I did was I calculated the median. I like medians more than means to do this. I calculate the median of all of the other predictors. These are only the numeric.
(1:10:32) And then I created max heart rate to iterate through my synthetic X's, my synthetic max heart rates. And then I predict Y based on changing X, holding the other X's at their median values. And I get a new visualization that says, oh, for someone who is the median patient in all values, this is what Y and X look like as a relationship.
(1:10:57) Very similar to the simple KN&N model with a single predictor. Okay, but there's something lacking in only doing this. Why is this prediction partial dependency plot model a little lacking? I've held my other predictors at their medians. That may not be a reasonable thing to do. What could we do instead? Just give me the code, Kevin. I want to go.
(1:11:42) What can we do instead? Well, we held them at the medians. Where else might we want to hold them? At their highest values, at an actual observable person. And so you could perform this prediction for Kevin, for Pavlo. What if Pablo's max heart rate with all of his other indicators changed from 180 down to 80? and plot the relationship for Pablo's. Plot the relationship for Kevin. Plot the relationship for you.
(1:12:14) And those relationships might be very different. Why might they be very different? Because the variable of interest, max heart rate, might have interactions with other predictors in your model. The relationship of X with Y depends on the values of the other predictors. And that would show up in this partial dependency plot.
(1:12:34) And so that's what we did is we created a couple different synthetic data sets. We're looking at specifically age. And so we're going to use the median for all other variables, use the three different plots, median of age, max of age, and min of age. And look to see what that relationship between max heart rate and the response is. And we see possibly a different relationship.
(1:12:59) When your max heart rate is low, it doesn't matter what your age is. When max heart rate is high, it does matter what your age is. When you're younger, it's better to have max heart rate. When you're older, it has less of an effect. All right? And that makes some intuitive domain sense, too. With me? What comes in a can? Again, I'm aging myself.
(1:13:29) Today's magic word is peaches. Peaches. Peaches come in a can. They were put there by a man. Do you know that one? Okay, doesn't. Does anybody get that pop culture reference? All right. Only the older people. All right. Go look up presence of the United States. This is of my generation. All right.
(1:13:57) When we do visualizations, we want to avoid visualizations like this. I had to show it. What is this coming from? the grading document. Thank you. They did not come talk to consult us data scientists when they did it and they said look at the median GPA over time. The median GPA is wrong first off and it goes from 367 367 367 what the to four yeah a minus to a so dumb.
(1:14:33) Anywh who, don't include this in your final report on your data science exam. All right, thanks everyone. Have a good day.