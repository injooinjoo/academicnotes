(3) 109 day 16 section - YouTube
https://www.youtube.com/watch?v=-RiOvEBSJTw

Transcript:
(00:06) Oh, it was not been recorded before. It seems like not. Thank you, Pepe. Okay, let me repeat this in case it was not recorded. So, we will covering classification, logist regression, and multiple logist regression all within the data frame of classification and we'll be using mainly the penguin data set.
(00:31) We have imported our libraries and then we will start with first a binary classification. So we load the data set for from SNS penguins data set and then we're going to be focusing on predicting the sex of the penguins. So as usual it's very important to always do at least some EDA on our data. Let me see there questions. Okay thank you so much.
(00:56) So we will look into the data set. Important and and and then I will show you why we're doing this one. Right. So we have here 344 records, seven columns. Let's check if there are NAS. Hey, is your screen frozen or are you not? Uh or just me. Let me check. It is probably frozen. Okay. Now I see the 344 thing.
(01:23) Okay. Yeah. Good. Good. So we are loading. We have loaded the penguin data set. We're looking at the shape 344 and then we're checking if there are any NA values and they are indeed. There are two for each of these ones and then there is 11 for for sex. Okay. So let me let's see here with this command if these rows are in more than one single line or they are all combined.
(01:57) So we look at the is NA and then we're looking through the sum of in in the axis the axis one is the columns and then check the ones that have more than one and fair enough most of them are or all of them are on these rows. So and this information is really not useful because there we don't have any information here and we don't have also the predicting variables. So it's safe to drop this. So that's what I'm going to do here.
(02:23) So we'll drop them and we have a data frame. Now let me just check the shape that we have 342 and seven. So we confirm that they are not missing values. They are not missing values on this bit but we still have nine missing values on the response variable on sex. So we're going to create an certification layer. Let's remember this nine right now.
(02:49) So the certification variable when we're doing train test split we need to pass a column for which we're going to stratify. That's what I'm going doing here. But if you we want to stratify by in this case sex and species um we need to create a column that will or or a value that will have that information.
(03:13) So in this case we're creating that by concatenating sex and species. And then we have these results. In this case, we have the certification of the species and the sex including the NA values. So, and then we do the specification with the usual train split. We pass the data frame. Remember the data frame has 342. Uh, and then we certify. Let's check the shape of the train in this case. And we have 273.
(03:47) So this is code that I added at this point. But let me show you what code was having in the So we normally will do here the FX value counts normalize to see the percentages. And then we have the percentages of male and female. But where is the NA? What happened with the NA? So that's the code that we're looking here.
(04:10) So let's do the count on sex. So it's 266. And if we do sex value count, we will have 134 + 132, which gives us Yeah, this one's right here. I think it's missing the NAS still, right? Because Yes, because the NAS will add to 273, right? So including NAS will have the 273. So you're missing. So if you add these two it matches the count but it doesn't match the total number of rows.
(04:44) So when you do very careful again when you do value counts this is discarding NA. So be careful when you use this function and um yeah take into account that this these are not being accounted. Okay so this is there. Therefore we're going to drop the values that have subset NA. So the one to have sex nas we drop them. This command will assert if the sum is zero.
(05:15) So when this command I don't know if you're familiar with the command assert but assert is just checks for the statement. So if you say one equals two it will error out when they are similar it will continue. So that's what we're doing here. If this assertion is true, so there's no NAS the code will continue without an error. Any questions until this point? I think it's pretty clear, but just make sure.
(05:49) Okay. So then we're going to take the features that we have uh and the response variable and then we're going to have the train with the features and then we're going to do label encoding. So we create a label encoder object and then we later on apply that le with a fit transform to the response variable and also here on the on the on the test set do the same.
(06:20) So we'll run this code and then the le variable by le object will have let me see we have various objects but one of them is classes and classes will have the name of the classes that were originally encoded and that's what we're using here to print the label and um and the value of that object.
(06:44) So just another way to kind of quickly check what is being encoded right and then if we look into the Y train the one train is just consistence of zeros and ones as we were expecting okay so we have prepared all of the data set we have the train and the split we have cleaned the data and then we have one hot encoder or response variable now we're going to run a canine classifier where in this case we're just going to predict normally the KN&N will use the average of this of the neighbors but in this case it's going to just take the majority class uh uh for predictions. So
(07:18) we are going to do a standard scaler and then we're going to pass uh a K9 name classifier. Any idea why we do a scandal scaler on this case? Anybody on the chat or out of mute. Their variations are very different. Sorry, can can you repeat that? Their variance are very different. Yeah.
(08:00) Yeah. This kind of close. Yeah. the variance are different but also because we're doing kind of distance we want to make sure that the distance are so that the measures for example in one dimension is not uh impacting the measures on the other one. Let's say that we have the height of the penguins in meters and the weight in centimeters. We don't want that difference on units to distort the predictions that you want to make.
(08:23) So that's why you kind of standardize the the variables. Okay. So we pass a pipeline, we create a standard scaler. So this pipeline will have first the data that passes through is going to be standardized and then it's going to be passed to this um KN&N classifier. We pass a parameter grid where we pass KN&N and then KN is this one. So this this name here is kind of like a standard for the parameter grid.
(08:54) So KNN is whatever name you put here and then this underscore and double underscore is just to make sure that there is a parameter that comes for this classifier. And if you remember the CANN classifier has the N neighbors right right there. So that's why we're passing here.
(09:14) So it's very important if you call this one XXX then you need to call this one XXX right just making sure that this is you understand how this is working. So you pass a parameters on the range uh following this pattern and then we create a kfold where we have a kful five. We're setting up with a random estate and then later on we create a grid search that is going to go through the what is it the parameter grid that we just discussed here but it's also going to go in through this K faults and then as estimator we are going to have this pipeline. So why is this an estimator? Because at the end the last value here,
(09:54) the last part of the pipeline is an estimator, right? That something that can be have a fit and a transform and a predict. So that is what you pass here. You pass end jobs minus one so that you use all your available cores. So if you have eight coursees like in my case, you will use eight.
(10:15) Scoring will use accuracy and then we want to return the training scores. So after that it's just a matter of fitting the the grid search and then returning the results. So let's run this one. Okay. And it run right. So what is it returning? Is returning a data frame where it has the CV results. Let's check this one.
(10:43) So it will have all of the different runs that we did. And then this is the parameters that have been run. There are other uh so there's the splits so we have five splits right so you have all the splits here you have the mean test score the standard deviation you have a lot of this information uh as part of the output that the grid search returns and then what this code is here is just returning the best score.
(11:09) So on the grid search you will have the best score and that's what we're using here for evaluating what is the best score in this case was key neighbors six and then we return just the best estimator uh here which is the model that was fed with that best score and then we return also the the accuracy which is here which is calculated based on this one. Any questions until this point? This this is kind of like a a lot of code, but it's important code.
(11:43) Okay, good. I have I have a question. Is grid search used specifically for these KN&N models or is it applicable for other types of models as well? It could be used for any model. Right. So if you use for example here a KN&N, in this case I'm using a linear this classifier, right? If I would plot another one, let me just put it here. Um, I don't know.
(12:08) Let me just put logistic regression, right? Then then you will pass the logistic regression. But then in this case, you need to pass parameters on the grid search that are related to the logistic regression, right? In this case, penalty C or whatever parameters you need here. And then in this case, you will have to do like for example logistic regression.
(12:26) Then that parameter will be passed here as LR. and then the parameters that you are looking here for example penalty and then you will need to modify this one. So it it can be used for any for any type of u classifier but you need to make sure that the parameters are matching or the parameter grid is matching with the expected input for that um model. Okay, great. Thank you. Yeah, sure.
(12:58) Any other questions? Yeah, good. Now, we're going to go to the decision boundary. And then there is this very nice um well-written code here that goes through all of the different ways to plot um decision boundary. And let me just run this code and then let me show you the output. And what this is going to do is plot the uh the depth and the body mass the penguins and it's going to plot the true values of the of them right f female or male and the decision boundary that is the decision boundary that was calculated based on this uh cannon
(13:49) regression right so this this one right here so this is the decision boundary that is created and then you have in one side you have a shadow where it comes is predicting male and then the other side you have a shadow where it's predicting female. Some of this you probably will use for your homework.
(14:10) So there is of course a lot of different code here that is looking at um whether you pass already a pipeline or whether you don't pass a pipeline etc etc. So I I don't think I will go through all of these details but this is what it's using right if it is using PCA it will go to certain code if it is not it will go through this other code here and then inside this one is going to look if the item that you are passing is um an inst of of of PCA or not and it's going to be for Kate etc etc so I mean I will let you go through this uh code
(14:47) right here but what I want to focus is on examples on this part here which is the contour plot and the mesh which is what is actually creating this this part here. Yeah. So for this one I will just go through another notebook and I will create in steps a smaller version of this one. So just for you to understand how this works.
(15:17) So in this case let me start with this one. So let let's in this case let me start with importing numpy the plotting library and then we're going to have a numpy with a linear space. What this is doing is creating 100 values between minus0 minus 3 and three and the same for y. So we run this code we have x right and then we have from minus3 to three we have 100 values here.
(15:42) So if I look into the leng of this is going to be 100 values. Okay, perfect. And then the same is doing for for Y. So 100 values for each of them for each of those. So what are we going to you do with this? So we're going to create a mesh grid. Mesh grid is going to have like a combination of all of these points and then we're going to assign them to X and capital Y.
(16:11) So if if you want this is kind of like a a cartian product of all of the different points in a in um in a x in a 2D map. So this is what met grid is doing. Okay. Then we're going to define this function and this function is going to have xÂ² and y square. And therefore what are we going to have as as c? We're going to have a 2D with all of the points being this function.
(16:51) So if you see here, right, the first point is 18 because it's minus 3 squared plus - 3 square. So that gives you 19. And on the other end you have 18 because it's the other string which is 3 squared and then plus 3 squared. Okay. So this is the function that we have here. So this is the grid that we have here. Any questions at this point. Let me see if I can visualize.
(17:24) See if I this will make it easier to visualize. Yeah, exactly. So you have all this grid right from minus 18 until the last point which is 1818 on this side and all of the grids that have been created. 100 grid points through all this data set. Okay. And then we're going to do is create um a plot, right? And then that is where this bit comes in.
(17:50) So we're going to create if we create a scatter plot, it's going to be like a blob, right? We're not going to see mainly anything, right? So if you do a scatter plot, let me see if we can do this. Probably it's going to be a big blob, right? Why big blob? Because there are so many points in between.
(18:06) So you don't see anything, right? You don't see anything. So that's why we're going to use in this case control uh plot and f is for filling. So it's going to fill some color uh within the certain regions. And so in this case, we're going to pass X. So it's one of the variables, right? X uh Y and um and the height if you will of of this of this of this mesh which is what we just calculated here. So C and then this is the key part.
(18:42) We're going to pass levels levels and a grid of colors. So in this case and let's forget about this one for a minute. So I'm passing one two three four four colors and I'm telling them okay so if you have some so and then we pass these levels that is going to use for plotting. Let me just go with something smaller.
(19:08) Now let's let's do with this one. So what is this doing is if you have something between zero and 0.5 is going to take the first color and between 0.5 and two is going to take the second color orange and between two and four is going through the different elements that you have and this is what it's doing right on the first 0 to 05 is taking those values right here right and Then from 0.5 to two is taking this other area and so on so forth.
(19:45) If I reduce this one, let's say if I remove this one, the plot is going to be looking different, right? It's just going to take from two to eight is going to take the third color. And then basically that part is just not longer. You don't have three chains. You just have um so you have four, we have three chains in this case. And then you can modify to your liking.
(20:09) So if you have binary classification you will only need to have three right and in the case you get two chains of color and then you need to have for example if you return here or if this C is your probabilities you might want to have negative one here zero in the middle and plus one on the other side and that will create your decision boundaries where you have uh plus one in one side and minus one in the other side and then zero in the middle which is going to be really the decision boundary where there's no going to be any color. So I'm not going to run this one because
(20:47) there is no minus one on this example but any questions from you. Okay good. So this is what is being used here to to plot this one. Right. So we are doing let me just stick to the main parts right. So we're doing these colors. So we only have two colors that we want to plot. So for the regions and for the scatter colors. So we use control F.
(21:21) We have the C value which is this one which is coming from the probability of that value being positive or negative. Right? So the predictions. So we pass the X and Y which is based on the grid that we have um generated. Where is the grid? Um yeah. So this is kind of the the this is we have so we take from the PCA we take the minimum and the maximum and then we use also for X1 and X2 and then we use that to create a mesh grid that we discussed right this grid of of items.
(21:59) Then we process them and then we pass them to the classifier to give us the the C value. for model predict on that mesh that we just created and that is what is so here right and that is what is going to give us the input for uh this guy the the contour f right so in this case counter counter f yeah this counter f so this is the binary classification output of the model and then we have three levels as I said minus uh one one on the other side and then 0.5 which is on the middle of the prediction.
(22:37) And then we just choose the region colors. In this case, we're using this one which is light orange and teal. And then we have a scatter colors for the actual values of the of the data that we have which is this bit. And then this one here agent this is just a trick where we are plotting at 0 0. are plotting something.
(23:01) But this is just to mark female or male so that you have this nice little legend here. But this is just kind of a trick to make sure that there's something showing on the legend. And that's where you and then you have the 2D line 2D here also that uh will help you with this kind of like um legend creation.
(23:27) And then you will have this this uh the predictions are on along this line. And then important to know is what side is predicting female and what side is predicting female. And then you get this this plot right this questions. Okay. Okay. And then just before I hand it over to Rashmi then why are we not using logistic sorry linear regression for classification right if we have certain values why don't we put it to a logistic regression any question any any ideas of why we don't do that because we are not predicting a continuous variable. Yes, very good point. Any other reason?
(24:28) Yes, exactly. We're not predicting a continuous variable, right? We want to predict a probability and then based on the probability, we want to have we want to know if based on a certain threshold if the output is positive or negative, true or false. And basically we want that prediction to be within a range between minus1 um to one. So to to be sure it's positive or negative but the probability has to be between zero and one.
(25:00) So that is why we don't use the linear regression because with linear regression we cannot really enforce those limits and that's what is this is showing here. Right? So we're plotting here an x value with different betas and then we're running um linear regression simple linear regression right which is we take these betas here and times the x we we had generated here and then the same with the probabilities and then we're plotting them for all of the different values of beta on the linear part and then on the
(25:34) probability bit. So maybe let's add this one. Let's add a bit more values here. Let's put maybe 15 15. So you see here when the values are really really negative or really positive, you have values that are really above a certain limit, right? They're above one and we don't want that.
(25:59) We want the predictions to be between zero and one because we are really predicting probabilities and that's what you are able to achieve with this logistic regression function here. Okay. And then to finalize here, this is kind of the trade-off between uh logistic regression. So yeah, the logistic function and the lock odds. So this could be maybe useful if if you want to quickly check if you have a lo of zero zero, your probability is going to be at 0.5. And vice versa, if you have a probability of 0.5, your locks are zero.
(26:34) And if your probability is one, your locks are are really high number. and vice versa. So you can use this plus kind of like understand how how is the relationship between uh logistics uh sorry probability and lockouts. Any questions before I hand it on to Rashman? Okay. Right. Thank you, Daniel. I'm going to share my screen. [Music] This works, right? Gives me very little real estate on my monitor.
(27:29) Okay, we are here and I actually want to run before so that way it runs all the way up. I'm not starting in the middle. Uh this is we want to start with maximum likelihood estimations or the how we use maximum likelihood estimation to get to the loss function for logistic regression. I'm giving you an overview of it, but I do recommend you spend some time understanding every single thing that's going on here.
(28:05) Uh to start with, we model the we we assume that the model has a Bernoli distribution because it's 01 for every single row that we have. Uh where pi is a function of our predictors. Uh this one then we have the likelihood. So we assume the independent observations.
(28:29) So the likelihood for all observations is like and this is the PMF for the for a single observation for the Bernoli model. And we assume that for every single row we get the product of every for every uh every sample. So that's what this is doing. uh then we since the mathematically the product of these small probabilities is not going to work out we take the log likelihood to convert the product to sum.
(28:54) So that's what this particular part is doing here. Um this is substituting for pi again and we have the loss function as the negative log log likelihood. Uh this loss function is very important. We use it all the time not just for logistic regression. you're going to see this loss function either binary cross entropy or cross entropy loss uh a throughout your data science career.
(29:20) So yes, it is spend some time trying to understand if you have any questions right now. I'm happy to answer if I can. Uh but do spend some time trying to understand what's going on here. The actual uh logistic regression. Oh, do you all have any questions there? I didn't even pause. By the way, my notebook has it's this is the same notebook that you all see on the screen.
(29:52) I just added there's very little change. I only added the numbers. Uh the logistic regression with scikit learn it's very easy to uh utilize. U here we are going to be um we have max iteration here. This is this is unlike linear regression where you don't have any iteration because linear regression has a closed form solution. This one does not.
(30:23) So it iterates and gives you the best solution. U here we are doing the cross validation score with the uh same kfold object. uh we get the cross validation accuracy to be any questions here I feel like this uh the number of jobs here is negative one that means it's going to utilize everything or all the CPU cores you have um and I think that's about it over here so the next important part is interpreting the coefficients uh I just put the slides here uh let's inspect the coefficients of our first logistic regression model. You
(31:05) should this is from the lecture 13 notes. So you can make a note of it. This was coming from lecture 13 slides. Um this is how the interpretation is working. And if you look here they print out the all the coefficients and the odds ratio. But the logistic regression uh but it's not written exactly in this particular format.
(31:37) So sometimes when you have open-ended questions in your quizzes, midterms or whatever, they expect you to write what each of the coefficient means. And this is the the slides describe the way to write. So I highly recommend taking this coefficient and writing it the way the slides are written in the odds ratio in the log odds ratio.
(32:01) Uh what each of these coefficients mean? What does the sign mean? Uh what does the positive and the negative mean? Uh any thoughts on what does the negative coefficient and the positive coefficient mean here? So my guess is that this in this one the positive coefficients contribute more to the uh what do we have y equal to 1. I believe it's male.
(32:34) Uh we need to check whatever that was uh in the data frame. uh male was male no fe yeah male was one female was zero. So the positive coefficients contribute to probability being male is higher whereas the negative coefficients is the other way around. Uh remember the logistic regression coefficient describes a linear relationship between the features and the log odds not the probabilities.
(33:04) uh this these describe the linear relationship between the features and the log odds. Uh I I highly recommend writing these coefficients in terms of this particular language and how the log arts are working. Uh now next thing I have here is this is a piece of code that you guys don't have it in the notebook.
(33:28) I'd like to understand when they say what happens by increasing coefficient by one or increasing the x value by one what what's the change in the coefficient so I try to put the code here so given these are the coefficients and given these are the x values and we are going to change one of the x value increase by one and see what happens to the coefficients or the log odds okay uh did did you all get what we are trying to do here in this But we want to understand when we say what happens by when we change the value of x by one how the coefficients are changing that's what we want to interpret. Uh that's what this
(34:08) particular piece of code is doing. Uh I put the model the similar to as what we have in our logistic regression model above. So this is what gives us the log odds. You have intercept plus the coefficient and you have the bill length as your x values. build depth, flipper, body mass and so on.
(34:31) So when you do this this what this gives you is the log odds. So this is what our original we assume this is our original log odds. Uh now next thing to convert the log odds and the convert the odds to the probability. So this is how you convert the log odds to the odds original. This is the odds original and this is how you get the probabilities from the odds.
(34:53) uh if you print out all of this you know I'm going to comment out and just print out that particular part first so log odds is 1.14 odds is 3.2 do the probability of being male is 75 and given the build depth is 18.7. So if you remember sometimes the questions ask holding all other things constant what happens if we change one feature by one what's the effect of it in the um in the probability or the odds or the log odds in this case we're going to be starting with log odds.
(35:32) So next thing we are going to do is to change the build depth by one. So now we have a new build depth which is which was 18.7. We're going to make it 19.7 and see how the things are changing. Okay. So it's the same same thing we are doing but we're going to get different uh values back different log on odds. So let's see what do we get.
(35:55) I'm going to uncomment all of this and run the whole thing again. Uh we modified this plus 1 mm increase. Uh the log odds is changed. The odds has also changed. The probability is also changed. So in notice the probability of being male increased significantly by adding a 1 mm increase.
(36:22) So how do you interpret change in log odds is if you take the difference between these two you get this uh these this is the same as beta 1 coefficient. So if you go back up here, if you look at the beta 1 coefficient, this is it. And we use the same coefficients by the way. So this part remains constant. The only part that did change was the probability.
(36:44) Uh the odds ratio also remains the same. So probability change in probability, this is the one that changed. Uh a 20% point increase. This depends on the starting probability, not a fixed amount like a log odds. So, um, this is the only I I might post this on ED if it helps.
(37:08) Uh, but this is the part that's not in your notebooks. I did this because I understand better by changing the code rather than trying to think through what could have happened, would have happened, should have happened. Uh, okay. Next thing um is regularized logistic regression. So to regularize logistic regression, scikitlan has a parameter called C.
(37:42) Uh you do have to note that the parameter is 1 / lambda. So that means larger C would mean smaller penalty and less regularization. Uh this is the key thing to remember. This is kind of it's the roundabout or the opposite way of you would normally think. So just remember this part.
(38:08) We also want to set the penalty to L1 uh because the default solver or the optimization algorithm that cyclone uses to would not work with the whatever they have. So we set it to liinear. Uh we are going to since we are regularizing we are also going to scale. It's the same idea that we had for KN&N yv scale. It's the same idea here. Uh because the features are going to be in a very different range like the grams and the uh the body mass in grams whereas the body or some flipper or the length in millimeters.
(38:42) So very different scale. That's why we do the stand scaler and then the logistic regression CV. Um I think that's about it here. uh pipeline it is binary classification. It gives you the best possible C. Uh we defined the C. We are trying to see 100 values of C within the um logistic regression space.
(39:10) Um and we get the accuracy of any questions here so far. No. Uh the next thing is interpreting the regularized coefficients. uh this gets kind of tricky because you're not actually looking at the original scale of the uh uh coefficients or the since we changed it to standard scaler.
(39:52) So here in this particular piece of code they are converting back to the original scale for uh interpretation. So you have standardized coefficients, you have the original scale coefficients and the original scale odds and the absolute value of the standardized coefficients. That's what this particular code is doing.
(40:16) And once you have this kind of scaled it back, then you can interpret it the way we did earlier. The next cell is uh basically uh giving you the plot of the scaled versus the not scaled coefficients. It's the usual bar plot. Um it's picking the top two features as the best two features. So the body mass and the build depth uh mm but it is actually using it use best two features is set to false.
(41:01) So it is actually going to use all features to create a logistic regression model. Next uh it is using let's see the logistic regression model there's no penalty term uh and it is giving the coefficients back uh x train features y train but this is the same as what we had before there is no scaling or not none of that is going on uh here we are plotting the logistic regression uh boundary uh it's using the same function that we had before.
(41:39) So if we go back to this particular function, I can elaborate a little bit more on this particular function again. Uh I know Daniel did this part as well. Uh we are not using the PCA. So PCA is always set to false. Uh otherwise it would create PCA instead of since we can only plot two features. If we had 20 features to decide then it would do PCA bring it down to two and then do the decision boundary.
(42:08) But right now we are only plotting two features. So uh by default it plots build depth mm and body mass g. Uh it takes the minimum and maximum on the feature space. So this min and max. So whatever is in your data set for the body dep mass g takes the min value and the max value and gives you the lin space 100 values out of that and then creates a and the same thing for the other feature uh min and max for feature one as well as the feature two and creates a mesh grid that means every point so you have total of like 10,000 points in a grid and for the rem well it's going to take a predictions so like
(42:52) If you see here model.predict, it's going to predict the X mesh grid. So this is part of the mesh grid, but it only has two features, but we also had more features. We can't So if you created the model with four features, we cannot predict with two features. Does that make sense? So originally we had like maybe four or six features. But here we only have two features.
(43:15) So we won't be able to create the model predict with two features and not have the other two features missing. So in this particular piece of code they do the average of the remaining two features and then you get the predictions and then it's what Daniel described it creates a this is actually it says contour f this is a fill so it should be a plot fill bound or fill rather than decision boundary whereas this one is the one that does the decision boundary which we see in black. So coming back uh questions here
(44:00) where we yeah the decision boundary. So this is our logistic regression decision boundary. Uh please feel free to stop me if you have any questions. Uh the key point was here if you create a model with like four features, five features, 10 features, you you have to make the predictions with 10 features.
(44:24) You cannot just make predictions with two features. Although it is plotting two, the other features it assumes as constant uh which it takes the average value. um regularized uh polomial. Uh yeah. So the next thing is to make it more complex. We can go more complex. Although this is this has like less than 300 uh samples but it is trying to create polomial features with three degrees uh and trying to determine whether it works any better.
(45:07) create a pipeline with polomial feature scaling and uh scaling and lasso logistic regression. So it starts with polomial features then standard scaler then logistic regression. uh C's are again kept to here we are using 20 C's rather than uh 100 C's like before uh scoring L1 lib linear uh this is the standard stuff minus one max iteration set to 1,00 a random state for reproducibility and this is the pipeline they calling it poly lasso pipeline uh CV accuracy Okay.
(45:45) Then feature names from the polomial transformations coefficients uh features and their coefficients. Let's see what the values we get. Uh let me see what's on the chart. There's something on the chart. Oh yes, uh we might post some code on AD with the contour f notebook as well as the change the x value y1 what happens kind of thing.
(46:18) uh best CV accuracy best C it prints everything non-zero coefficients since this was a uh we put the penalty to L1 lasso model it the everything the non non-zero coefficients is what we are concerned with um nonlinear decision boundaries in the so you can totally see the nonlinear decision boundaries um it would be good to compare like how the KN&N decision boundaries look like very vaguely versus the logistic regression which looked almost linear.
(46:55) Whereas this one is with the uh poly lasso pipeline which has it's a more complex models and you can see the little bit of nonlinearity here. Um the C is the regularization parameter that we use the lambda but in scikitlearn C is 1 / lambda. So that was the thing that they pointed out in the beginning. Uh over here I think yeah the logistic regression model SQL calls the regularization parameter C. This is 1 / lambda.
(47:26) It's not lambda but it is 1 / lambda. So larger C means a smaller penalty and less regular regularization. So keep this in mind uh kind of trips students when you begin. We usually think the opposite, but it's 1 / lambda, not um lambda. Uh did I run this? Yeah. Okay. The next thing we have is the ROC curves and the uh AU.
(48:12) So one of the reasons we do this is it is very easy if the models are uh if the target classes are imbalanced. So let's say I had uh we had in the data set 80% or 90% male and 10% female without even creating the model I'm going to get 90% accuracy like I can say everything in the data set is uh is male so I get 90% accuracy but I missed predicting almost I I basically predicted all the females that were in the data set as male which was incorrect.
(48:48) uh people use ROC AU curves when you have class imbalance. It gives you more balanced picture for uh those imbalanced classes. It it you may not be able to see like it looks exactly the same as your accuracy for this particular case. But if you compare if you take a data set or maybe even change this data set values to let's say 90% male and 10% female and then compare the results and see what happens or if you have working on a project if you have class imbalance problems working on a project then you should definitely try to use ROC curves for your evaluation
(49:26) metric rather than accuracy. Uh, scikitan by the way has a lot more uh metrics. So depending on the use case, you can always pick something more suitable that works for your uh use case. Uh there's a whole list of it. The regressions uh this is a different one. There is a there's another page.
(49:51) Sorry, this is the wrong page. the skarn matrix it lists all the F1 scores and all of that. Yeah, all these. So F1 score is very popular. Uh this NDGC score depending on the case ROC, AU, precision all these are uh regression metrics are different regression metrics.
(50:21) mean absolute error uh mean squared error is very popular again uh multilel ranking matrix depending on the task I think you should go here find the best met metric that works for your data set would I would do uh for this particular problem yes and a is just fine totally works so to be able to of course scikitlearn makes it very easy uh here by default if you look at this is the function that we use to get the false positive rate and actually let's go back go to the actual curve and then we go back to the code this is what the ROC or AU plot looks like you're plotting
(51:02) the false positive rate at the bottom you're plotting the true positive rate uh on the y-axis uh I also recommend looking at this Wikipedia confusion matrix it gives if you scroll down uh it asked what is a false positive rate, what is a false negative rate, what is recall sensitivity.
(51:28) Uh this is the actual by default the actual confusion matrix that we look at is only this particular part like uh I don't know how to highlight this. This this part right here on my screen yeah this part is what confusion matrix typically shows you in scikitlearn or when you print it out or display confusion matrix. But there is all those terms the you hear about recall precision sensitivity true positivity false positive you are going to be able to find all that in this particular u picture. Uh this is one of my favorite I recommend it all the time.
(52:03) Uh if you're confused this is the confusion matrix to look at. So here we have false positive rate. Here we have two positive rate. Um let me see I'll have at one point you can actually so the only thing you need to create this is true labels as well as you need uh predicted probabilities and then you're able to plot this.
(52:37) So you can actually create a given this is the true label given this is the predicted probabilities what are the different thresholds what are the different FPR true positive rate and what kind of uh uh what kind of curve you would get is all that is possible and I think I may have an example like you can actually do this by hand to understand this better what is happening um usually people plot both confusion matrix uh ROC curve uh confusion matrix you can add payoff also it depends on your use case but uh you can do both either one uh in this particular notebook we are only doing the ROC curve but yes you're right uh scikitlearn has
(53:21) a function called confusion matrix so if you go here skarn confusion matrix it it'll plot a very good matrix for you like uh like this one uh you can get a matrix like this predicted label true label uh you you can try this for your projects or something uh this not this this notebook doesn't have it I don't know maybe some other notebook will have it but yes that's the idea for creating confusion matrix now if we go back right now if you look at this plot by default the predicted probabilities are taking
(53:58) the values of 0.5 that means anything below 0.5 is zero or anything above 0.5 is one there's nothing stopping you to change that threshold and you can plot the positive true positive rate again uh uh and ROC curve again so it might happen that you have a class imbalance then you change the threshold for the probabilities u and you you might be able to do a different uh curve uh actually no this actually accounts for all the different thresholds.
(54:34) But yes, if you plot the confusion matrix, that's where you do the thresholding part. You can change the threshold. By default, it would give you 0.5, but this is the place where you can change the threshold to anything works for your use case and get a different confusion matrix.
(54:55) Uh here we we need the uh y probabilities. That's what this is uh y probability and these are the true values y. Uh that's all the things that we need to create a roc curve. Then this is what this particular piece of code is doing. This is all you need to create curve. Um you can have multiple models in ROC curve. You can have multiple classes in the ROC curve.
(55:20) Uh I think in this one they are doing three different models. uh questions so far. Yeah. So, ROSC curve score helps us understand how well a model perform across all possible decision thresholds not just.5. By default, the accuracy that you get is 0.5. Um, I'm going to print this out again. Why is it taking few minutes? It shouldn't be. It's only predicting.
(56:10) Not even It's not even training or anything, are we? Oh, no. It's probably doing this one. cross valid predict cross validated probabilities is what it is getting um is it so yeah CV is equal to KF that's why it is taking longer uh I thought why would it take time when we are not creating model but the cross validation score does take a while [Music] we can inspect the final model performance here but we really should perform from uh model selection based on the cross validation results.
(56:49) Uh so far we have not used the test set at all. It's kind of the hold out set. So that's what this one is doing. It's it's actually predicting on this is the ROC curve for the test set. The final part here is the multinnomial case. But before we move on, any questions so far? um like how would you use it? Would you be able to use this on your projects? Um think about it.
(57:20) Can you use this or if you have any questions that come to mind? Are you all doing regression problems or classification problems for your project? Can put it in chat. I'm curious. Don't tell me you haven't looked at the data yet. I hope you do. Uh you have a milestone uh to coming up soon, I believe. Uh I think it's Tuesday. if I'm not mistaken.
(58:08) So please take a look at your data sets, try to figure out whether you're dealing with a regression problem or classification problem. Um because some of if you're dealing with classification problem, some of the things that we are covering today will be helpful for your projects for sure. Um okay so this one looks very similar to what we did for the binary case says you're doing classification okay good so whatever we did in the section today you can totally use pretty much everything for your classification task
(58:49) um I would also add on this particular uh the confusion matrix since yours is a classification task for Um okay so this one is for the multiclass case. So far what we looked at is two classes binary 01 male female. But what if we are trying to predict different types of penguins like I think they have three or four different types of penguins in the data set. So that becomes a multiclass case.
(59:24) Or if you're trying to predict different types of fruits, maybe you have a data set with 15 types of fruits, you have multiclass keys. Uh you can have any number of classes, but you do need like you can have hundreds of classes, but if you have hundreds of classes, you need thousands of data points as well. So that's the thing to keep in mind. Uh for the multinnomial or the multiclass case, we start with a multinnomial distribution where each for each class k you have this kind of a softmax function.
(1:00:02) Uh the everything else remains the same as what we did for the binary uh classification case in at the end you get the multiclass cross entropy loss. Instead of having a two class binary loss you have a multiclass cross entropy loss. Uh this is also very similar to the other function and it is also extremely popular.
(1:00:36) You will encounter it uh right here in logistic regression in neural networks uh everywhere. Basically these functions are there uh these loss functions are very popular. Uh the next we have is we are going to here in this particular case they're what they're considering is I think predicting species. So they have like three different types of uh penguins or let's see how many different types of penguins they have.
(1:01:10) X uh where is the Y part? That's the species. Yeah, this one. So to do this they have three different types of species is what they are trying to classify. Uh very small data set but it works. Encode species multiple ways uh demonstrate different ways to encode species for multiclass classifications.
(1:01:44) Uh of course you can't leave it as three different uh species like this. So you have to either do 012 uh to be able to label encoding this one way 012. That's what they're trying to demonstrate here. Uh one hot encoding is uh every instead of having 012 you're going to have uh different columns for each species. That's what they're saying.
(1:02:12) uh label binarizer is also very similar I believe to let's see what we get uh in each of the once we print it. So for the label end coding it is 012 uh for one hot encoding it is uh it is species Adele as one column species chin strap species genu is another column. So example row could be this. The label binarizer also looks very similar.
(1:02:38) Uh 0 1 0 it looks very similar to uh one hot u encoding here. Uh I will have to check what's the like what's the difference between these two. They look very similar. Uh I'm going to check that later. Uh choose your preferred label encoding. We are going with 012 which is very common. Uh the next is empty data frame to create to since we are going to do the grid search CV and all of this fancy stuff.
(1:03:13) Uh we are creating a empty data frame where we can store all these uh metrics from our cross validation results. Uh this is also very similar to what we did earlier that uh Daniel showed in the beginning for the KN&N. You have stand scaler K nearest classifier. The difference being here we are trying to uh we are trying to our y- value is different.
(1:03:38) Earlier we were trying to do male female. Now we have three different types of um penguins. So it seems like it is easier to pred three different types of three different types of uh uh three penguins. you have the train score of 100% and CV score of.9963 which is uh too good to be true but seems like it is very easy to predict that kind of a thing. Uh the other thing I wanted to bring up is here.
(1:04:09) So we've been doing a lot of grid search with CV. These things are working very quickly right now in our collab notebook. But imagine you had like 10,000 samples. These things are going to be so slow you wouldn't believe it. Uh you want to be careful if you have like very large data sets for your projects to not run grid search CV because you would be just waiting for it forever or you have to have limited uh parameters to tune.
(1:04:45) The options are you keep limited parameters to tune or you do it manually. So that way instead of going like instead of doing all of this for grid search CV what you would do is you're going to check like you're going to start with five you're going to start with 10 you're going to start with two.
(1:05:05) So, and go like you will get better results maybe with let's say if you're getting better results with five you then you want to test you you you're going to start with let's say by default you assume that you are going to get some results with five you get some score you're going to go lower value which would be two you get some CV score this could be high or low compared to uh the five that you had then you're going to go uh let's say if you got you got better score then you're going to compare between if you got high value here like you this one was higher than
(1:05:38) five you're going to compare three four and five on the other hand if you got like low value that means it did not work very well you're going to try to go for like maybe 10 you might get CV is higher here so the next thing would be to try would be 5 6 7 8 9 uh you you see we are reducing the amount amount of grid search is required.
(1:06:06) It's kind of the manual way to do grid search uh rather than know the whole pipeline. Uh but if you have small data sets by all means get search works. There are other tools that people use. Uh I'll put some in the chat. Uh if you're inclined to go for methods outside the class, that would be one of the tools that is very popular is called Optuna. People use this all the time to do the uh to do the to replace your grid searches or maybe get faster results if you have very large data sets.
(1:06:40) Uh that is just a side note. I think we are almost done here. The same thing we are doing with the ridge logistic regression. Uh notice the penalty here is L2. If we are doing ridge, we're doing lasso, the penalty was L1. Uh the solver is also different for ridge but you can try a different solver. I don't know which ones will work.
(1:07:03) They have psychon gives you uh cyclone gives you multiple options. The thing was oh it's called optuna. Uh so this is uh multiclass ridge uh list regression model. We have the same thing with polomial that we did earlier. Now the polomial is happening with lasso. So this was uh l1 penalty. So this is lasso.
(1:07:35) So uh our only solver option for multiclass cox. So this is the only solver options called saga. Uh if you're using L1 penalty for multiclass. Uh so these are some of the things that you would have to keep trying based on what cyclone provides you. Uh and then we are almost done. Uh we here we are trying to figure out which model is best for the multiclass.
(1:08:03) There are two options ridge pipeline KN&N grid score. I don't think the lasso was giving good results. So the you can't go with wrong with either one of them. Uh yeah, one of the reasons lasso is not giving good results is there is a wide gap between the uh train and the CV score. Uh that's why we did not pick that one. But ridge and KN&N seems like fair choice. Ridge is giving you 100% on X test Y test.
(1:08:32) Uh it could be because we have very small data set. Uh that's why it is 100% accuracy or very easy data set. Uh the other option is to have KN&N. This does not give you 100% but very close. Uh any questions? I think we are done here with the section. Let me stop the sharing. uh any logistics or any other questions related to the section? Is there an estimate for when the uh midterm grades are going to be released? Yes. Uh we are almost done grading.
(1:09:25) You should get it probably Monday I believe. Okay. We we are done grading for midterm the quiz portion. We are working on uh the coding portion which won't be released by Monday. Uh any other homework project logistics section questions that we can answer otherwise I can stop the recording? Um stop the recording. I I do actually have another question uh for the project.
(1:10:18) Um there's something about uh check-in with our uh like TA project advisor which I think is for next week. Um, is that going to be pushed back at all given the extended deadline for the um the milestone for Tuesday or uh I guess when what are we going to find out more about what that involves? That might be meeting with your TF.
(1:10:43) Who's am I your TF? Um, I believe uh let me see. Hold on. Uh, is that milestone 3 or milestone 2? Meeting with the TF. Um, it doesn't say where does it say? It's the project TF checkin um for uh week 10. Oh, in the schedule. Yes. Uh that might be uh that might be in one of the milestones. Uh I I I mean if you want to meet your TFS uh email your TFS they schedule they'll schedule something and that might just account for that uh you met your TF.
(1:11:41) But other than that uh it will be part of one of the milestones before your final milestone. uh so uh it's going to be part of the milestone that uh uh meet with your TF thing and if at any point if you want to meet your TF before then you you are welcome to email them you have questions you have uh concerns with your projects yeah definitely email them all Right.