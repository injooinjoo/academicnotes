(4) 109 day 22 - YouTube
https://www.youtube.com/watch?v=lxAadr9Qxvs

Transcript:
(00:01) All right, it's time to start. Jesus. 212. Hello everyone. All right. Good morning everyone. Welcome, welcome, welcome you on here and online. I'm your lecturer Pablo Protovas. By now I think you know my name very well. Uh today we're going to talk about the random forest.
(00:38) I want to go back to something I said earlier in the semester. thing about anybody who is a a gym goer knows about progressive overloading. So, we're going to start overloading now. Uh after the midterm, I kind of slow it down, make it simpler for you to get your confidence up. Now, I feel like your confidence is there. We're just going to nudge it a little bit today higher, a little more material.
(01:03) Next week, a little bit more and then one lecture you have to give it to me for free. I can go mathy. Okay. So that will be uh next next lecture a little bit more mathy uh because that's what I like but one lecture I can do it for half a lecture right so all right so let's talk about random forest uh to this week is the quiz uh as I said yesterday some of you came to the review don't overstress that's important the concepts are not so difficult it's just matter of getting them sorted out I really don't think from looking at the review interested talking to students. I think you're there. You just some of you
(01:41) have this lack of confidence. You see the question, you said, "Yeah, it looks simple. Maybe something else I'm missing." And then you overthink it. Don't overthink it. If you see a question in our quiz that looks easy, it is easy.
(02:00) Okay? So, there's not many actually there not trick questions, right? Can we don't have any trick questions. If it looks a little bit complicated, it is complicated. But if it looks easy, it is easy. Okay? So that's a point. We don't we're not MIT here. We're not trying to reduce your grades, right? So if you know the material, I'm putting my learning objectives out there. If you know these things, you should be fine. Okay.
(02:19) So take a deep breath with me. It will be fine. Okay. All right. So let's start with the lecture. Uh any other announcement, Kevin? Nothing. Right. Yeah. All right. Let's start. Um so today as I said we're going to be talking about random forest. Let's look at my road map for today motivation why we need random forest.
(02:45) I kind of motivated last Monday and I stopped halfway like a cliffhanger. So that's why you all came ready to find out how we going to do the decorrelation and then we're going to talk about random forest of course and I'm going to cover a few other things. I'm going to cover variable importance.
(03:02) I did mention it on Monday that once you go to these ensemble models, you lose the beautiful easy interpretable models. Right? So, uh today I'm going to go back and say how we interpreted these sample models and then I'm going to talk about missing data parenthesis again because Kevin has talked about missing data in a kind of formal way. There's another little trick we do with trees.
(03:33) So I'm going to revisit missing data again in in particular how we deal with missing data in trees. There's a little trick that we do and I'm going to talk about that and then again something you we have talked class imbalance. I'm bringing it back again for for everybody to see a little bit different approaches and then finally treeb building algorithms but I don't think I'm going to get into that. So we're going to put either section or next Monday.
(03:57) This is what is a standard uh tree building algorithms they are out there. What is skarn using? What r uses it use and all these things. All right. So let's start with the motivation. So imagine we have this data set from geometric shapes that and features they have color shape and size and we want to use any of our well so far we only know bagging ensable methods to uh classify them.
(04:25) So we talk about bagging which is bootstrap plus aggregation. Remember that. So what we're going to do we're going to bootstrap them. Okay. So this should be a review of what we did last Monday. We bootstrap the samples right and remember bootstrapping is with replacement and the way we have talked in this class in particular and I want to stick to that uh idea is that when we do bootstrap we retain the number of the the number of samples of the original even though that my figure here doesn't show that this way do not go confused when I start with
(05:00) the sample when I bootstrap I have exactly the same number of samples right if I have 100 I'm gonna get that slide and I got lazy. I didn't want to cut and paste more of those. Okay. So, in in in these three samples I have here, they should have the same number of objects as the original. Okay. All right. So, we have the bootstrap sample.
(05:22) Is that clear to everyone? We have talked about once we have the bootstrap samples, what I do, I am going to train a tree on each one of those samples. So, here I have three trees. I have three bootstrap samples. three trees sometimes we call them estimators same thing right and now I have three models and I'm going to aggregate them at the end the way we have talked last Monday all right so we let we kind of stop something like that so here I have four estimators for trees that means four bootstrap samples and I have four trees so what what we said is that the trees are correlated
(06:02) what does it mean correl related here is not exactly correlation like we're used to it correlation or like that. It means that if you look at the root note they all have the same picture. And why is that? If you think about it intuitively at each level of at each split, what we're trying to find is to find the picture and where to split, right? And we do that by by maximizing the gain in the purity, right? Or maximize the reduction in impurity, right? Yeah. Yep. Good.
(06:47) So but if you if I bootstrap my data and now I'm free to choose from any of the predictors for all the data it is natural to always pick the same or more often than not to use the same predictor and about the same split. Okay. So the root notes in this case they all look the same. After that we have some repetition.
(07:14) We have CA here and CA here and C here. But the others may change. But you see there is some repetition of the tree. And therefore the trees are not totally uncorrelated. They instead of giving us diverse opinions now they give us opinions that they look like each other.
(07:34) Imagine if you have the the doctors we were talking last time and the doctors instead of being trained at different data they train at bootstrap samples and therefore they start kind of having similar opinions right so you go to the doctor the first thing they tell you all doctors will say we have a doctor here will say take a rest drink water relax and come back in 3 days right I most doctors I have been no matter what I do unless I'm about to die.
(08:01) They said just drink a lot of water, sleep well, relax, and come back in 3 days if it doesn't get better. Right? So, this is what we see here. Every tree tells us kind of the same thing. The first split, the first decision we do is to just say relax, drink water, and sleep. Right? Um, okay. So, that's a problem because our idea was to reduce variance.
(08:25) And the idea of reducing variance, it was the idea that the variance is all due to noise. And if it's all due to if if the the the results we're getting is due to noise, if we average over noise, we start reducing the variance. But if the data correlated, that will not happen. Right? And this is not only for trees.
(08:46) If you have a bunch of data set they are correlated your reduction in the variance which we said is 1 / square roo of n would not be 1 / square of n anymore if the data are correlated it's one over square of n minus row or something like that I don't remember the formula but something like that right but so the same will happen here so the idea here is to have diverse trees in order to reduce the variance but if the trees are correlated the reduction in the variance would not as effect.
(09:15) Okay, that's where we left on Monday. Now we're going to figure out tends to be correlated. Now the idea is how to uncorrelate them. How to do an unsample method with trees. Same thing, but the trees would not be correlated. Okay. And here is where you realize random forest can help you. All right. So in order for me instead of me answering it let's play one of the games so you can think about it and then we'll get the answer.
(09:51) How can we avoid the this issue or how do we decorrelate the trees? Random forest is a modified form of bagging that creates ensembles of independent decision trees. Maybe that's a strong statement at least we should say less correlated. Okay. Uh then the question is how? Okay. Ready for a game? Here we are. So instead of me answering, I'm going to put a bunch of options.
(10:18) So we'll give us an opportunity to think and um then of course I give you the answer. All right. So we have microphone. There was one post this morning say hey I calculate to be 25 times there. Does it count the midterm or not? This is your chance to avoid that being on the border between A and A minus or A minus and B. Who wants to do it? All right. I think this is your Lee Liu Xi Shin.
(10:50) Wow. Thank you. So, introduce yourself. Uh, hi, my name is Lu Shin. Um, I'm a first year data science master. Uh, some fun fact about you. I like traveling and photography. You like traveling. Okay. By the way, I forgot to mention we have a nice picture in the Well, I'll go back to it. Okay. Right.
(11:15) So, the question is, which of the following five I put five so I don't forget. Approaches do you think could be effective in reducing correlation among the trees? I don't expect you to know, but the reason I'm doing it is so we see some options and we all think together. you going to be the leader thinker in this one. Okay.
(11:32) So, don't answer until we see all the options. Right. So, option number one, limit the depth of each tree in a unique way. B. Ensure that each tree split does not include any of the splits used in the previous trees. Randomly select a subset of predictors to consider for splitting at each node. randomly choose a single predictor for every split and then design the split such that the probability of choosing a significant predictor is low leading to an ensemble of weaker models.
(12:05) I work hard to make them all believable. All right, let's start with A. Okay, so A says limit the depth of each tree in a unique way. I think A wouldn't work because like even though we divid the dips, we still start with the same predators. Very good. So A is out. I totally agree.
(12:29) Ensure that each tree split does not include any of the splits used in the previous tree. That would definitely decorate it, right? Yeah. But what do you like this or not? Well, I think in this case we we still split with like the first fleet will still be the same predators, right? No, we we don't use the splits.
(12:55) He says ensure that each tree does not include any of the splits using the previous trees. So if I say due to good morning Kevin um yeah but for the first bleed the first split will be the important but then the next tree would not have that. Yeah. So there's that would definitely correlate but what's the problem with this and you can all raise your hand doesn't have to be I think in this case the tree will not be very accurate the especially the latest trees right would not be so the first one may be strong but as we go down if we start excluding predictors that we know they're strong in splitting are not included and therefore those trees will
(13:33) be weak. So now we make an ensemble of not equal predictors uh estimators right so we don't like that right good so B is out let's go to C randomly select a subset of predictors to consider for splitting at each node how about that as you know I'm a fan of doing things randomly half of my life is randomly select selections because random is powerful will agree with me right doing every if you don't know what to Just randomly select something.
(14:06) Not a bad idea. Don't overthink it. That for them. So, what do you think about that? You like that, right? Yeah, I think that. How about the rest of the class? Do you like this? Yeah. Yeah. Yeah. Okay. Let's look at the other option. Randomly choose a simp predictor for every split. I think that will also work. That should also work.
(14:30) But I'm not minimizing my overall objective, which was the gene index or something, right? Yes. So it is going to work. It's going to decorate, but it's going to give me a bunch of random trees which not optimizing. It's not minimizing the overall loss, right? So sounds good, but it's missing the point of creating a classifier.
(14:53) It's just going to create a bunch of you can use that actually for outlier detection, but not for this. And finally, design the splits such that the probability of choosing a significant predictor is low leading to an ensemble of weaker models. I have no idea what I'm talking about there. So, it's a bunch of war salad that I put just to have one bowl. Okay.
(15:11) So, how do you feel about C plus? Yeah. Not not convinced. All right. So, the correct answer it is C. And that's what we're going to be doing in random forest. So that's the only idea for random forest that we need to know that how do we decorrelated the trees is by at each split we're going to randomly select a subset of predictors and only consider that subset. Okay. All right.
(15:42) So let's actually do it with a very very simple example to see. So I start with this data set that has 1 2 3 4 five predictors. Right. He has age, sex, maximum heart rate, cholesterol and chest pain. Okay, this is our heart data that you've seen before. Someone is calling. Um, okay. So, what we do is we in trees what we do, we going to take these five predictors. I'm going to check all of them and all possible splits and I'm going to choose the best one.
(16:11) Right? That's what is a tree. Yeah. Yeah. Okay. Now instead of doing that I'm going to select first of all a bootstrap just so we don't confuse things. So I'm going to select a subset of the predictors and in this case I randomly select three of them. I randomly select age sex and cholester again.
(16:42) How did I select the three out of five? Randomly right? So I have five predictors. I randomly se select a subset three of them and now only consider these three to find the one that is the best to split in this case is biological sex. So I did that. Now in the next split I still start with the five original predictors and I still select a subset of three randomly.
(17:07) Okay, it's not the same subset I started before. It's not the same subset I did before. It's a totally different new subset randomly selected. Three out of five. And then in this case, maximum heart rate. Now in this side, on the other side again, I select three of the five randomly. And I pick the one that is going to give me the best split. Okay.
(17:32) Uh and I keep doing that. That's for bootstrap one. Bootstrap two the same. Bootstrap three the same. And that is a random force. That's it. Did you get it? Okay, let me repeat. It's very straightforward. We're doing bagging, meaning we do bootstrap samples and for each of the bootstrap samples, we're going to train a tree.
(18:01) But at each step instead of selecting the best predictor from the total number of predictors we select the predictor from a subset randomly selected subset of our predictor. End of the story. Okay. So now I have a tree I have a forest. Each of the trees in the forest will give us a prediction and I'm going to get aggregation as usual. In case of classification, I get the majority. In CA in case of regression, I take them in.
(18:32) All right. In summary, we create B bootstrap data sets with all J predictors. Initialize a random forest with B decision trees. For each tree at each split, we randomly select a subset of J prime predictors from the full set of predictors which is smaller than the original J. J prime is randomly selected and then against the amongst the J prime predictor we select the optimal predictor and the optimal threshold for the corresponding split panos same in the no it's not the question is J prime the same no it's not every single split we subselect another J prime Okay.
(19:23) Is that clear? Okay. Uh all right. So that's it. Just so it would be clear this J prime it will be different and we keep doing it. Now that will decorrelate because let's say we have cholesterol at the top. Now every now and then randomly cholesterol will not be in the subset at the root nodes.
(19:48) So some trees will not consider that strong predictor to start with it and therefore we decorrelate the trees. Okay. All right. So I think that's as simple as it can get. But now let's get into some more details on how we train these trees.
(20:09) First of all last time I mentioned as we increase the complexity or make the models more sophisticated something else comes in. There's no freelancers. We say what is that thing that comes in? More hyperparameters, more things we need to specify. What's the new thing I added here? What is something the size of the J prime? Excellent. All right. So, let's look at them here. So, I have the number of predictors to randomly select at each split.
(20:37) The J prime as I said, right? That's something we have to decide, right? And how do we decide hyperparameters? Because validation of course the total number of trees in the ensemble that's something we need to decide the splitting criteria maximum depth minimum lift node size etc. And the splitting criteria gen and entropy.
(21:01) So there are actually five hyperparameters even though number four is actually two hyperparameters. one is which stopping condition and the other one what value. So if I choose maximum depth I need to know what's the maximum depth value. If I choose minimum lift nodes that's I choose that and then I choose also what is that value.
(21:27) Okay, is that clear? So in the stopping condition, I can choose any of the stopping condition. You can choose all of them, right? But let's say you choose we have between uh maximum depth, we have minimum lift nodes or maximum lifts. Those are three stopping conditions that come to mind.
(21:49) So I have to choose which one to use and for each one of them, what is the value? Right? So in reality here is a fivedimensional hyperparameter space right now if let's think actually for a second I'm going to take a second to think what do we do in the cross validation that will be talking all the time even the dog fall asleep because he heard this story many time let's go step by step what do we mean by cross validating these five different four different hybrid parameters it means the following I choose for each one of them some value. Yeah.
(22:23) So let's say I said number of predictors three total number of trees 25 splitting condition maximum depth value of the maximum four splitting criteria gen this is my set of hyperparameters okay what do I do I'm going to train a tree a random forest in this particular case meaning I'm going to train 25 of those things okay then I'm going and I evaluate it at my validation set once 2 3 4 k times I take the average of that now next step I change my hyperparameters now instead of three number of predictors I do four everything else the same do the same thing again then I
(23:15) change the hyperparameters again now it's five h well shouldn't do five again let's say five I do it again then I change the genie to entropy another set so if I have fivedimensional space and each one of them has I don't know five different options some of them will have two options like the genian entropy some of them will have five options that means before you know it you have a thousand random forest to split okay all right so I don't know if you see the problem here should be facing you what's the
(23:54) takes too much time. Yes, you have deadlines, you have parties to go to. You don't want to be sitting there forever. So, what do we do in this case? Um, instead of checking every possible scenario or every possible value of the hyperparameters, people like experts like myself and Kevin and Chris will tell you, hey, we know kind of what the hyperparameter should work and let's not let's not try all these thousands of different models, right? So for example, I'll tell you um just pick one maximum depth is good enough right don't try everything right
(24:37) or pick genie it's good enough for most cases right you won't see much difference between genie and entropy unless you're trying to squeeze one or two% out of your accuracy right so these are I like this so these are what we call the uh kind of standard things the other thing that you get from the experts or people they have done this long time.
(25:02) For example, the let's say you have 100 predictors, right? Now, the subset I'm going to use is anything between one and 100, right? Well, 99. Yep. Because it could be let's say I have 100 predictors and you remember we select a subset randomly to do that. So that number has to be selected is is a hyper parameter.
(25:29) So if I try everything means we're going to try 100 different options. So here's where the experts the rule of thumbs come in and he said usually for classification the square root around the square root should be okay. So if I have 133 is where you should start. Maybe you do for 20 from 30 to 40 but definitely one or two would not work.
(25:55) Definitely 90 will not give you good result. So these are kind of starting points that you have to consider. Uh if you get regression is the number of predictors divided by three. These are rule of thumbs. The number of trees that's another thing that we don't fully hyperparameter. We just use one of the hyperparameter choice.
(26:20) We we check how many trees we we need before we don't reduce the variance anymore and we fix it. Okay. And the other thing is like between genie and entropy I I don't know if Kevin and Chris agree with me but I will always start with the genie and then at the very end I may do another test on the entropy. The same with the stopping condition. I'm not going to try every single stopping condition.
(26:40) I usually start with a maximum depth. Optimize that. Once the model works and I get the good results I may go and do few more tests on the other one. I don't do the full optimization hyperparameter optimization. Okay. All right. So that's uh that's it. And now you remember we don't do cross validation anymore.
(27:05) What do we do? OB, right? The cross validation was lame. It's old. It's too much October and September. In November we move to OB. What does it mean? It means we don't have to do cross validation. the tree will give us the OB which is out of back error which it means as I train I find the points that haven't been used for that for those trees and I use those points to validate okay how we doing any questions so far yep I heard um the path said that for non um for example. Yeah. Okay. I think I understand. You see being professor one of the skills
(28:02) you have to develop is to understand the question. Let me rephrase again Andre. So I in the beginning of the class I said when I split I get the threshold and I find all possible um splits right so if I have let's say 100 unique points I'm going to do 99 different possible split and I think what you're asking can you do gradient descent on that is that what you're asking no for hyperparameters or for the splitting so there's two different things like example questions.
(28:39) Yeah. Can we use gradient descent for hyperparameters? No. And the reason is because you find your parameters in the training set, you evaluate it in the validation set. So if you take gradient, you have to take gradient in the validation, but then he gives you the thing.
(29:04) Then I have to go and do the thing in the training set. a little bit tricky to see maybe at the end of the class I can show you on the board right but it doesn't work uh there is other ways to do hyperparameter since you ask Andrew um there's software that they do that ray is one of them weight and bias is another one there's few other ones they do that which is basically organize the grid search in a smart way there's one thing that I really like it's called basian optimization another basian cool thing which basically I'll make it very kind of simple is predicting
(29:42) through some basian uh tricks with called Gaussian process is predicting what will be the answer for some other ones and you give posteriors and you say there is a good possibility this hyperparameter will give me good and then you go and try that once you get that point you redo it and it gives you posteriors or out of the scope of this class but if just so you can Google searching to that. It's called basian optimization.
(30:09) Some of these uh libraries do use that. Super cool. Uh Kevin and I would love to talk to you about that, but out of the scope of this class. Okay. All right. So, let's talk about the next one which is variable importance. If they of course if there are any questions, please let me know. In the meantime, I'm going to be giving you the code of the day one letter at a time today.
(30:37) Now you can do bianial posterior predictive here and once you hit it you can leave right this is where if posterior predictives could be useful right as I give you different letters you can predict what's the final word once you're confident enough you put it in and you can leave well yes it is Don't try with transformers please. Uh all right.
(31:06) Ready for the next part? Okay. Variable importance. There is uh three ways of doing it. One is called mean decreasing impurity. The other is called permutation importance. Actually there's four here. The other is called lime and the other one is called shop. I'm going to talk about the two first.
(31:30) The other one the lime we may have it in the section next week or you can go and read. And there is of course sharp which I personally don't like and I can tell you why because I want to have interpability of my model and I need to shop gives me interpretability but then I need to interpret it what my shop tells me. Uh do you use shop at all? Not so lime is super cool. Uh but which some of you may need for your project.
(31:54) We're going to put it somewhere in the section but I'll give you reading material. It will take a little bit long to cover it. So I'll skip it. Let's talk about the first two things. Mean decrease in impurity. So the idea of mean decreasing impurity is the following. Since now I have an ensemble I have a forest.
(32:14) Uh instead of I cannot do the simple interpretation that we have for a single decision tree which gives me rule. If this is bigger than that I went to this and if this basically I can come with a set of rules. If cholesterol is higher than that, his blood pressure is lower than that. You have a high probability to have a heart disease or not.
(32:38) But if I have three, if I have a forest, I have a lot of this. So I cannot make this simple rule. What I can give you is a set of rules, right? Which is not useful. So in mean impurity, what we're going to do, we're going to find the mean decrease in impurity for each feature across all trees. we arrive at the variable importance for bagging a random forest by looking at the mean impurity decrease.
(33:05) Okay, that sounds a lot of words. Let's just go step by step. So I'm going to average this for all the trees. I promise you we go step by step. So we start with this tree. It's just one tree I have. Right? So the first thing I'm going to do, I'm going to calculate the mean decrease in impurity for each node Q in the decision tree.
(33:29) Right? So this is the mean decrease in impurity for each node Q. This of course is a percentage of node in that percentage of samples in that node. This is we sum over all the children of the node n. So I'm in the node n here and actually I can open it up. I have the left and the right. So what I have is the genie before I split and the genie after I split.
(33:55) Okay, is that clear? So I'm looking the genie before the split and then I'm looking at the genie after the split and of course I wait I weight them accordingly right the left and the right if the right is has more point I wait that more than the other also for that node what I'm doing I'm also see this node has 25 out of 100 points so the mean in the decrease in impurity will depend on the number of points in that node Jacob yeah You're using entropy for like the split criteria. Would you use entropy here? Eventually change for genie. I'm going to use entropy. Yes.
(34:36) And if you use MSE that will be MSE. Okay. I mean if it was regression. All right. So this so it means at each node I'm calculating the split at the split how much impurity I'm getting. Right. how much impur I decrease. Okay. All right.
(35:03) So in this example, I'm looking at the first split um the root split and I calculate the impur is of course is 270. That's the total number. So then the impurity at the root node was 0494 and then I calculate the impurity the the gene index here and I'm waiting because it's 87 out of 270 and 183 out of 207. So the impurity decreased. It was 0.44. That's the root node split. Right? Now I'm going to do it for the next one. And I'm going to do the same calculation.
(35:34) But now because this note has 183, I say 183ide by 270. And then I do the same calculation there. I see how much it increase the decrease the impur by doing that split. All right. So this story going to continue. continue right I'm going to do every single one all right so for every split I'm calculating how much the impurity decrease I hope that's clear right all right so now let's say I want to know about cholesterol so I see every node that is split on cholesterol and these are the two ones I have and then I see what is the impurity
(36:17) decrease at each one of those right in this case is 0009 and 0009. Why the same? Accidental. Okay. So it doesn't have to be of course the same, right? Okay. So now what I'm going to do, step two, I'm going to take the average of all the splits in my tree that use this particular feature. Okay.
(36:44) Step three, I'm going to nar normalize I'm going to normalize this value from zero and one. So basically for each tree I'm looking at the percentage of impurity decrease for that particular predictor. Right? So in this case it's going to be point 18 08 divided by the total impurity decrease of the old tree.
(37:12) So step number three now normal this I'm still in one tree right? Step four and calculate the feature importance at the random force of bugging level. We average that over all the trees and that will give us the mean impurity mean decrease in impurity MDI. So this is one of my summary slides when you study. By the way, did you all notice that now I'm putting markdowns in that? If you go to ED, I have my PDFs. I also have markdowns.
(37:40) The markdowns are summary of the lecture in case you want to study and you find the PDFs a little bit not worthy. And I trying to keep my slides with less words, but the markdowns I have more words. Okay, it's a way for you to review and study. If you haven't noticed it, they are there. I started with the decision trees.
(38:00) He found them. Yes, they look good, right? Yeah. Okay. It's a because the slides I'm trying not to have a lot of words. Sometimes you're missing and then you have to go to the video. Who wants to hear me again? So you go to the markdown. Um okay that's the thing. So all right so here's the story mean decrease in impurity.
(38:20) I do that and in this case maximum heart rate is the top chest pain opic etc. Okay. Now what I have I have some interpretation. I know from my data that maximum heart rate is the most important. is equivalent to what we did before with the putting the for linear regression or logistic regression looking at the values of the coefficients right and we just rank them based on that right same thing here cool now we're not done there's one more way to do that which called permutation importance permutation something you all love right
(38:59) all right so this is a little bit simpler and for me a little bit more intuitive so let Let me go step by step. Let's say I have this data set, a little bit different data set looking at the height, weight, and a bunch of other things and fitness level. One is the worst, five is the best. Okay.
(39:17) Okay. So, I want to know how much these predictors are affecting the fitness level, right? So, I start by looking at the validation or OB accuracy of the random pores model. So okay so we train that we find that okay now I'm going to do something slightly different so I am going to take one of these columns and I'm going to shuffle permute them okay so if I permute them that means I'm removing any relationship between that predictor and the outcome and we have seen this remember we did permutation important hypothesis testing right so by shuffling ing them if there
(40:04) was some relation between that predictor and the outcome it goes away. All right. So what we're going to do now we do this and we calculate we random permuted data and we do the OB accuracy of the random forest model on the modified data set and now I get 87. So the accuracy went down by very little by permuting that.
(40:28) That means that predictor is not important, right? Because by kind of just shuffling it, it didn't change my output. All right, so kind of that's the idea, but I'm going to do repeat the previous step K number of times just to take some averaging out of it because you're not sure if that permutation once will work. We assume we do it three times.
(40:53) Permutation accuracy one permutation three and I take the average. So now 085 and the now the permutation importance will be the difference between 88 which was the original one and 85 okay so which is 03 higher that value more important is right because if I permute a column and all of a sudden the accuracy goes from 88 to 0.
(41:27) 1 that means that predictor was really contributing to my model, right? To my outcome. Okay. All right. So, this reference slides again to look is the same thing I said just more algorithmically. All right. Now, let's look at let's compare the two. MDI on the left uh permutation importance on the right.
(41:53) So, let's notice few things here. The chest painalia whatever and all peak were very important but after I do that they are not so important right so the importance has changed by doing either MDI or permutation important I like permutation importance and I think Kevin likes permutation important Chris too we have all agree we prefer it it's actually more intuitive and actually works better why does it work better let's compare the two So here is the thing is that MDI will favor um any predictor that has high cardality because in that case it has many options
(42:39) to choose in each tree where to split. So it kind of bias towards uh predictors with high cardality or numerical features. it will always kind of bias the importance to towards continuous uh features or categorical features with high cardality. What do I mean by high cardality means there are many unique values to that. Okay.
(43:06) So therefore we prefer um we prefer uh permutation importance as a test. You can choose any of the two but just keep in mind those are the two that we usually use. Kevin and I and Chris we prefer permutation important it's a little more expensive because you have to do it multiple times MDI is faster but maybe 30 years ago that was important not anymore because computers are much faster both will give you results quite instead of 0.
(43:33) 1 second maybe two seconds is not you're not waiting there for long okay now all this story we talk about random forest it was to do one thing to decorrelate the trees. Now let's look at the importance here to see if we can see this signal. So I'm comparing on the left the variable importance for random forest and the variable importance for bagging.
(44:01) I don't remember if it's MDI or permutation. Let's say either one. So what I want you to see here is the following. For random forest, the top predictors get all the importance. And the reason because sorry the other way around talking about bagging in bagging the top predictors got all the importance.
(44:31) So you can see chest pain and CA kind of got most of the importance, right? And the reason for that is because you remember the trees are correlated. So the mean decrease in impurity will be high because those predictors are on the top that I have the maximum or the most decrease or in impurity. Okay. So what you see in the bagging is that few predictors get all the importance.
(44:57) Now if I go to random forest on the left ah here if you go to the random forest on the left you see a little bit smoother importance and that was the whole idea to decorrelate and you can hopefully you can see it clearly here Alex you see that right so the these two predictors three predictors or four predictors got most of the importance where here it's a little bit smoother meaning more predictors getting enough importance as opposed to the random force.
(45:30) All right. So final thoughts or random forests. One thing is important to keep in mind. If you have a lot of predictors and most of your predictors are junk, you don't know but you have let's say you have 125 predictors and you run a random forest. Since the random force randomly selects the predictors to split, there is a good chance if most of your predictors are junk that you're going to be picking randomly those junk and you're going to actually let's put it the other way.
(46:04) There's a good chance that you're going to exclude the good predictors because they're only few. Let's say I have thousand predictors. 10 are good predictors. And let's say I go every time I split I get 100 randomly. the chance of selecting this 10 out of the thousand with 10 100 subset is very small or smaller.
(46:34) So therefore a lot of your trees are going to be missing the important predictors at the top of the tree and it's going to be splitting them very randomly. So therefore if you have a lot of predictors and you try random forest and it doesn't work as well as bagging usually this is the reason that means you have too many predictors they are uninformative junk right so what do you do in that case we have so many tricks in you do PCA you do some you look at the confusion the correlation between predictors and remove some of them just to get some of them Another way of doing it you rad you find importance in the
(47:11) futures I remove that fact that I do I remove the bottom predictors and then I go to random force so there's our kind of remember I said that data science is science and art that's where the art comes in okay so is that clear now yeah so keep that in mind that sometimes you run bagging and you run random forest and bagging will be better than random force usually you have too many predictors they are not important okay all right all right this is something I said last time the number of predict the number of estimators the number of trees will not
(47:53) lead to overfeitting it will only reduce the variance we said it last time we said it multiple times just wanted to put it here it is different from other hyperparameters that control complexity the number of trees would not lead to overfeeding. It just reduce the variance and uh final thoughts.
(48:18) What was that? Ah the last one is that let's say you want to do an ROC curve. Let's say you want to do one of those. Those will depend on the probability, right? But bagging random forests do not give us probability. We're talking about classification here, right? I can turn a random forest or bagging into probabilities by simply look at the percentage of the class, the majority class.
(48:50) Right? He's getting upset with me. But because I call it probability, it's not exactly a probability, but it's a proxy, right? Yeah. Uh just to be precise, calling something a probability means more than just the percentage of that. But we can play this game. We can call it probability. Now I can do my ROC curves.
(49:08) I can do all these things by changing the threshold of predicting something to be positive or negative. And before what I was doing I said the majority now instead of majority I turn into probability and I put the threshold right. Yeah majority means half of them right I can change that right and all of a sudden I have an ROC curve.
(49:33) I see you're not getting that. What is an ROC curve? I'm changing the threshold that I call something positive or negative or and now what I do I can do the same game by turning this into I mean you don't have to turn it to probability but it's easier to see. I'm turning my prediction into probability and then I play the same game right.
(49:55) All right. So this is not working. That's good because I'm running out of time. Yes. Ohness. Uh, the slide is not working but I think I look behind so it doesn't matter. Okay. So here I have the summary. I'm going to let you to to read it. Let's move to the next subject which is missing data.
(50:45) Actually start with the imbalance. Zoom. This is easy. I like the picture a lot. You sent it right. Wait, is that real? And the the moose or whatever was sitting there for you to take a photo. That's so cool. This is almost like setup, but it's so cool. Olympic National Folk Park that's in Washington, right? Good. All right. So, next topic for today, data imbalance.
(51:40) We did talk a little bit about that. I want to readress that. Uh, it's only short thing. Don't run away yet. last imbalance. Okay, here's the story. Training grand of force actually any machine learning on imbalanced data it can introduce unique challenges uh to the learning problem.
(52:10) We have talked about that I remember Kevin telling you that and I said it in various things. If you have a training set that you have 999 positive classes and one negative class, your accuracy, your model will be challenged to get the right answer because is if we're right because it's easy to predict always positive. So if I have this case, what do I do? I need to is actually let's go back to this. I like this one.
(52:41) Is accuracy a good metric? No. So what do we do? F1 score. Great. So if if you're accurate if you have imbalanced data set, accuracy is definitely not a good metric to evaluate your model. So let's move to something else. We're talking about F1 score um in case of because it gives the recall and the precision is the geometric mean of the recall and a precision.
(53:10) And in this case, if you have imbalanced data set, it's better to report F1 score. Okay, that's what we said before. This is kind of review. And then of course under then another thing we talk about is the area under the curve as another way of seeing how overall it does between precisionary call or true positive rates and false positive rate.
(53:36) And I don't know what is this t here means um is a threshold. Huh? He supposed that was I don't I long time. Yeah, but that doesn't look like a threshold, but okay. Is point that that's a threshold.3. It's not 4.3. Sorry. And this was a diagram. I couldn't fix it this morning, but I saw it. Okay. So this is AC the ROC curve and the area under the curve we have talked about that and I say we hope you remember these concepts the thing right all right so uh all right now there's three main ways of dealing with imbalanced classes under sampling over sampling and class weight so let me actually put in perspective so we
(54:26) understand this imbalance in our data set don't report accuracy report F1 score. But what do we do for training? We're not training on F1 score. We're still training on binary crosentroy or genie index or the likes. We're still doing the same thing. So that means we have to do something in our data before we train. So there's three ways to do that.
(54:54) So say under sambling, oversampling and and class weight. So in under sampling we have two methods. Random sampling or random down sampling and near miss. This is for over assembling we have two things random sampling or random oversampling and smokeote and then the last is class waiting. Okay, I'm going to go through every single one of them. Okay, ready? This is very straightforward.
(55:21) Now, what is under sampling? Well, I have class A that is smaller than class B is a minority class. What do I do? I throw away some of my points in my majority class. randomly select a subset of the majority class and therefore I have both at the same numbers. Okay, easy. Sure. We throw away training points. I don't like that but we do. All right. So now I have a balanced training set. So that's one way of sum balancing your training set. Okay.
(55:51) All right. So this is the assembly reduces. This can be done in two ways. random sambling or I said near miss. So the near miss has the following idea that I have here the majority class which is blue and the minority class that is red and I'm going to throw away some of the blue points.
(56:17) Right? And I said randomly picks some of them to throw away and therefore the number of blues and the number of reds will be the same. But instead of randomly selecting which ones to throw away, maybe I can think a little more. Which one should I throw away? Now let's look at these blue points here and think about it.
(56:37) How do they contribute to find the correct decision boundary? How they will contribute into my performance of my model? This point here is actually not helping me because it's for sure blue. It's far away from the decision boundary, right? It's like classifying cats and dogs and there's a cat meowing next to me.
(56:58) I have no doubt that's a cat, right? The difficult ones are these dogs that they look like cats or the cats that look like dogs, right? Those are the difficult ones, right? Which are going to be the points near the boundary. So basically what we do, we instead of totally randomly subsampled, we wait them depending how far away they are from the decision boundary. actually with them the opposite.
(57:22) The ones that they are far away will be most likely thrown away and the ones that they're close to the decision boundary I'm going to keep them. Okay. So what you do you run a a a classifier you find the decision boundary and then you uh you could do that or there's other techniques you can find the distance between the two of them and then you do that. Okay. So that's called near um oops near miss.
(57:51) This is a way way to under sample. Now let's talk about oversampling because oversampling is another way of doing it. So now what I do I have the majority class which is class B. I have the minority class which is class A. I am going to increase the number of points in class A. And how do we do that? We have generated fake data from before bootstrap. Great.
(58:26) So we're going to sample with replacement or without uh the same way. So now I can get that right. So if you have u an imbalanced data set, let's say one class is 60%, the other is 40%, you can just increase the 40% to 60% the same number 50/50 by just bootstrapping. Okay, easy peasy. Very good. So this is oversampling and I said there's two ways of doing one is to randomly sample bootstrap as we described and there's another kind of smarter way to do that which is the following is called smokeote uh it's an improved alternative for oversampling so think about this data set I have a bunch of blue and I have some oranges orange
(59:14) points if I do bootstrap on the range. What I'm going to be getting is points on top of each other, right? Because it's sampling with replacement. Yeah. So, if you look here, basically what I do with bootstrap, I'm just going to replace, right? I'm going to sample with replacement.
(59:38) And therefore, what you're going to be getting points on top of each other, right? Maybe not the best idea. So what we do with smoke, we kind of created either lines between the points or triangles depend on the implementation of smoke that you use in some parameters. So if you see here, you notice that now I have points that in an area that didn't have here. Cool. So with smoke, we not over over sample.
(1:00:04) we kind of do data augmentation by going into areas in the uh minority class that we didn't have original things. So that's called smokeote uh which is used quite often uh movement. Okay. Now the final idea of uh balance of data is to do something called uh class reweing or waiting. So the idea now if I have a minority class and a majority class in order to pay more attention to the minority class what I do in my loss function remember the loss is the aggregation of the loss of all individual points right every time I predict on individual example I'm
(1:00:52) calculating the loss right if it's MSE is if it's regression is MSE if is classification that will be a BCE Right? And therefore, you remember the BC is the sum of the individual errors. Right? So what I do, I'm going to weight any example I have for the minority class higher.
(1:01:17) That means I'm paying more attention to the minority class than the majority class because I have less of them. Right? So this is called class imbalance. And I think so this is the way we wait them. So what happened? Ah nice. So cool. Ah, so much fun I'm playing. All right, so here it is.
(1:01:44) So the weight now will be uh the number of total numbers divided by the number in that class times K. And these ones will give you normalized weights and then you go into your sklearn and you say uh weight based on that actually I think it does it automatically. He said class weight balance and it should do it. Okay. Now you may wonder is that good enough? Is that the same as uh oversampling? Not exactly um but very close. Okay.
(1:02:14) So three ways we talk about balancing the data set under sampling over sampling and with uh and class waiting. Okay. So you should do that meaning every time you have a problem classification problem look at the balance of your data set. If it's not balanced, you need to do something. You can't just ignore it.
(1:02:36) Okay? So, one of the three, combination of the three, that's where the art comes in. Maybe you over sample a little bit and under sample the majority and maybe do some waiting. I never use all three of them, but I have used a combination oversampling and under sampling. Oversampling. If you oversample too much, you start getting repetition correlated things.
(1:02:57) uh even small may get out of especially in high dimensional space it can be a little bit slow and maybe get out of the true area that you're supposed to be. So I usually do a little bit of oversampling and a little bit of under sampling mainly more oversampling or you do diffusion models or generative models to do that which you can do you can write your own diffusion model to generate data from the distributions and good luck with that. Okay.
(1:03:28) Anything else here? Uh, all right. So, the last one I wanted to talk is missing data. Um, yes, Alex. What point? Let's see. We have a B. We have 50% of one label and 50% of another. That's balance. But what if it's like 40 60? At what point do you decide to start using? No. Uh the question is asking is give me some idea of when to start balancing.
(1:04:04) Um I think always I mean if it's 40 60 unless you're lazy I will do it right. So if it's 40 45 55 class balancing will work very well either down sample or over sample it will work. There's no doubt about it. So I will do it. I don't see any major disadvantage of not doing it right. Um I think you should be more worried if the B imbalance is huge like 99 versus one.
(1:04:35) It will be very hard to balance it because if you under sample you're just not going to have enough training. If you over sample you start creating repetitions in your data and that that's what I will worry more than the other one. So the answer you always balance your data in one way or another.
(1:04:53) There's no harm to it. Uh Kevin says depends on the goal. You want to elaborate? Yeah, just inference prediction. Yeah. Uh let's talk about prediction. Yes. If it's inference, maybe you stick to that and then everything's taken care in some way. Yeah, correct. All right.
(1:05:28) So, I am going to go to the last piece which is uh missing data in the tree word. What? and it happened. All right, we start with the photo of this thing with by Elonor. I think we have photos from Elon before. This is Juju in China. Where is Jujai? Which part of China? Which province? Yes, I almost died. Almost died once. I was in Ching
(1:06:40) for three days. last day I was my digestive system collapsed. So, uh I've been there. Okay. All right. So, let's talk about missing data. Uh so, the the basic idea of missing data is you have seen it before in the real world data. You're going to have some missingness is almost almost there. Almost always there.
(1:07:13) I very rarely you get data that they're all going to have some missing values and we talk about missing at random missing totally at random not at random we have talked about all these cool things this is slightly different uh so missing values occur due to variety of reason this is maybe a lighted version of the missing at random missing not at random and the likes is either I forgot to put the data um or maybe device malfunction something you have sensors and they not working or even purposely. Okay.
(1:07:45) So there's multiple this right so and we have talked methods for imputing uh so the many ways to deal performing impudation Kevin talked about it uh last Wednesday I'm not going to repeat all these things but there is one way of doing uh missing data in trees which called surrogate splits.
(1:08:09) So I'm going to spend the next six minutes talking about that. So surrogate splits is exactly that. And we have a dance floor here. Maybe we can try to do these dance moves. Um so the idea is to find alternative splits in my data that will perform similarly as in the um as in the the the the predictor I care. All right. So I'm going to put the last letter. So I think you lasted long enough and I think some of you have already did too. This is the last one in just all right let's move on.
(1:08:48) Uh all right so here is the thing it's a little bit tricky to understand actually it's tricky to understand if you read other blogs and paper not in this lecture in this lecture will be so clear is the clearest explanation of surrogates cle ever been presented okay all right so I make a big claim let's see if I can do it so what we do we're going to be counting yes or nos in the response variable after each split.
(1:09:22) So now I have the response variable here and I have one of these predictors. Okay. So what I do is let's say chest congested if it's true if it's a yes I have one heart disease and no no heart disease right straightforward because if it's yes uh it's only a yes right there's only one point here which is yes if it's no I have three yeses and two nos okay so I count this okay so that's my first thing I'm looking at the distribution of positives and negatives in that split Okay.
(1:10:01) So if chest congestion is false, we have three nos and two yeses. If chest congestion is true, we have zero nos and one yes. Now let's look at another one. The good blood uh good blood circulation. If it's true and if it's false again I have if it's true uh no if is if the good blood circulation is false we have two nos and three yeses that you can see here.
(1:10:29) And if is if it is true then we have one no and one yes a zero yes. Okay. Okay. Now I'm looking at arteries block again. I do the same thing true and false. And I look at how many if I go to the false how many are nos and how many are yeses. Okay. So I hope that's clear. Right. Now let's see how we do sate splits.
(1:10:56) So I start let's say on the chest congested we're gonna for the distribution on arteries block I'm going to compare that to the other two. So for arteries block I have if is false three nos and one yes and if it's true I have zero nos and two yeses. And then I'm looking at chest congested and again I have if it's false three nos and two yeses and zero nos and one yes.
(1:11:17) And now the difference between this and this are two basically how many flips I have to do. And for blood circulation the same thing the difference here is six. What do I mean? The difference is the is how many flips I require to do to get from one to the other. Okay. So chest congested is a surrogate split to the arteries block because it will give me very similar outcome if I do that and that's a basic idea. So chess conjures the most similar split distribution to arteries block.
(1:11:54) It will be our second choice for a split during prediction if the value of arteries block is missing. So I know every time I do in the artist block if something is missing for one particular point in prediction I am going to use the surrogate split which in this case is the which one good just congested right okay so that's the basic idea so now you can do that for everyone for weight for example you find that the height is the best um alternative split that will give me very similar results and then therefore we do um we do that. So during
(1:12:38) training for every optimal split we create a rank of surrogate splits. Usually you do about five. The ranking is based on the similarity between the split distribution of the predictor I have and any of the other predictors. So some important points about surrogate splits are can help us understand the primary splitter.
(1:12:59) surrogate splits perform better when there's multiolinearity. Of course, that's something that you should have got from Kevin's lecture last Wednesday. If I have some colinearity, your model imputation will work better and there's no guarantee that useful surrogates can be found. Sometimes you don't find it and then you deal.
(1:13:19) All right, so that's pretty much what I have uh for today and I finish on time. Wow. Um, so hold on, don't run. Remember, we have quizzes, right? Don't forget to go to your section today. It's important. And this is my last. Thank you very much. Next Monday is Thanksgiving week, but we're going to have a lecture on gradient boosting. Gradient boosting coming to you.
(1:13:54) one of the most important models after random forest and then after Thanksgiving if anybody comes back we have two more lecture. Thank you.