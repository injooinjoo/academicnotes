(4) 109 day 22 section - YouTube
https://www.youtube.com/watch?v=m9wIeOoUUso

Transcript:
(00:04) Yeah. Okay. Uh good morning everyone. Uh I know most of you are here for the quiz but we have a section first which is going to be about 40 minutes. Uh this week's section covers decision trees and bagging. Uh I'll be going over decision trees, Daniel will be going over bagging part.
(00:31) Um apart from that if there are any on campus students in this room who's here for the quiz you should leave. We aren't expecting any. Um and next week we don't have section but we will have section on next one on 6th December I believe. Okay. Uh that said, let me start by any questions. The quiz will start we'll start quiz right at uh 9:45.
(01:08) So I'm going to start sharing my uh screen. You can find the notebook on ED. Uh section is going to be 11. I believe I'm sharing the full desktop today. Section 11. Okay. Yeah. All right. Oh, did I miss anything Daniel? Sorry, can you repeat that? Did I miss any announcement? Maybe that we will use the link from ad for the for the quiz and then we will be staying on the same soon link for Yeah, we'll we'll stay right here. We are not changing the links.
(01:43) So, after this finishes, maybe we'll take couple of minutes of break and we start sharp at 9:45. Uh you all can see my screen now, right? It is basic. Okay, great. Okay, the initials is just basic imports. It is downloading a notebook assets.zip from the GitHub uh link. Looks like the link is public. Um, so there are a couple of CSV files that it's going to download.
(02:30) You can actually check what it downloaded. Uh, it's all it's going to un unzip it all in data and /fig. There are a couple of error handling things like if you don't if you don't find the file, if something fails, uh, there is a sis import missing in the actual notebook.
(02:56) I might update it later because it is required in the error handling otherwise the error handling would all would also fail. Um so we downloaded all this. We can check what we have here. The couple of files downloaded Cambridge Homes music CSV spam CSV. Uh the couple of PNG files. I'm not sure if we really need all of that but we'll see. But these are the files. It downloaded everything. Uh we start with decision trees.
(03:24) Uh recap uh we have genie empiric genie index. Genie for region R subr is created by splitting on the predictor P at threshold TP. It is given by 1 minus this term which is the proportion of training points in the class K that fall within the region. Uh the lower the genie the more homogeneous the node.
(03:58) The other metric that decision trees uses is entropy. By default it is genie on scikitlearn u but you can change this to entropy. In entropy you don't have the squared term but you do have the log term here. Lower entropy is again more pure node. The algorithm for decision tree works like this. U how do you choose a split for the continuous features for a given feature? We try different threshold between the unique values. So let's say you have the values like 5 7 10 19 and so on.
(04:38) Sort them. You take the average between 5 and 7. Uh and then you try to find the best feature for each threshold. Compute the weighted impurity of left and right children. Pick the split with largest impurity degrees or equivalently uh lowest weighted impurity. Uh the shallow tree is high bias and low variance. Deep or fully grown tree is low bias but high variance which is overfitting.
(05:16) Uh very deep trees can fit training data almost perfectly but are unstable because it overfits. Couple of hyperparameters for decision tree. uh distant trees have like fewer hyperparameters compared to random forest boosting and if you go for extreme gradient boosting those are like extreme amount of hyperparameters for this one it's a bit simpler max depth that's the depth of the tree uh the others are the minimum sample leaf and minimum samples split a minimum sample leaf is every leaf must have the minimum sample leaf. It can be whatever amount we want to put 4, 5, 10
(06:03) depending on the data set. Minimum sample split. That means the node must have that minimum samples before splitting. Uh these are stopping deeper trees, lower v lower bias, higher variance. Uh shallow trees, higher bias, lower variance. Uh there are a couple of questions here which are kind of nice.
(06:28) So, I'm going to put this on a slideshow mode and we can try to answer it. Let's see if it can allow me to Yeah. view slideshow. Hold on. Okay. At a node with 100 training points, you consider a continuous feature X. How does the decision tree algorithm choose the split on X? Uh, I think I just spoke about this.
(07:10) Uh what would be the answer? Anyone wants to try it? B that's correct. Uh we try the threshold sorted between the unique values. uh compute the weighted impurity of the two child nodes and choose the split with the lowest weighted impurity. Uh the next one is well the next slide is the answer B YA is wrong. U only trying the median is too restrictive. Uh the card algorithm considers many thresholds not just one.
(07:44) Um okay the next one which node is most pure according to genie index uh node A with 50 every every node has 50 points but each one gives a different ratio of the class 25 from class one 25 from class 2 40 from class one 10 from class 2 50 from class 1 zero from class 2 uh what would be the most pure according to the genie index.
(08:17) You don't really need to calculate this but the notebook gives the calculation as well. See yes because all of these are from class one and there is that's the most pure. I think there is one more question. It's hard to see with the zoom thing overlaying on it. Anyway, yeah, u you should know how to calculate genie index by the way. Like even if you you should try it.
(08:54) It you can be given a problem calculate genie index given this this this and you should know what to use and how to calculate. I'm not saying for the quiz but in general you should know uh effects of pruning. If it a very deep tree that overfitits you then prune it. Limit depth or remove weak branches. What is the typical defect uh effect uh bias and variance increase bias decrease variance and what's the possibility uh various possibilities that they have given? Uh what do you think is the answer? Um increase bias decrease variance. Yes. Uh definitely decrease this
(09:38) variance because you're going to prune it. you'll have fewer leaf nodes. Uh when you prune it, you're going to decrease the variance and accordingly the you will notice a slightly increase in bias as well. I think that they talk about it a little bit here. Pruning makes the model simpler, less complex, bias goes up a bit.
(10:02) Uh it's more stable, so variance goes down. It's a classic bias variance trade-off. Okay, I'm going to stop the slideshow mode and go back to the Cambridge housing prices example. Uh, this this example looks very similar to the Boston housing prices.
(10:27) I don't know if they just renamed it to Cambridge homes, but uh there is a similar data set called Boston housing prices. So what they're trying to predict here is the uh let's see the home's price uh this one but the price is given in like thousands so they're going to convert to make it less skewed and divided by the thousand. So notice the pre-processing. I'm going to start by reading the file.
(10:55) We have already downloaded the file from the GitHub link in the earlier uh uh cell. Uh these are all the features that they give. Let's see they they just put the dummies for the type part but everything else is already there. uh it seems they're using uh oh like I said this pre-processing is kind of important uh to get occasionally this kind of target pre-processing also helps while building the model.
(11:34) Uh the usual split is 8020 split random state u by default shuffle is true. U let me ask one quick question. There's a parameter called shuffle which can be set true or false. Do you know why we should set it to true or false or just ignore it? Okay, I'm going to answer this. This method is very important method. The train test split.
(12:05) It's one of the most widely used method in whole of scikitlearn. Reason being even if you're not using the models that are treebased models or uh the regular linear or you're using deep learning so you would be using pytorch or tensorflow or whatever but the t the train test split still remains like even if you use pytorch or tensorflow the parameter shuffle equal to true is shuffles the data set before splitting and if you don't shuffle you might if If your data set is ordered that you might get like all the home prices which are home prices that are sorted it's on the top and all the home prices that are least home prices will be at the bottom.
(12:52) You don't want that. You want equal representation in the uh training as well as test. That's why the shuffle is important. Yes, exactly. In case it is ordered, the shuffle is important. You want to leave it false if you have it ordered by with time series data.
(13:15) You want to keep it you want to keep the time series data in the order of the date. So you don't want to split that one. So that's why that parameter is important and that this is one of the widely used um function in whole of scikitlearn. So worth spending some time looking into it. Um I'm going to talk for five more minutes then I give it to uh Daniel.
(13:41) This just some EDA uh collab is pretty nice with these things just become fancy. Um it's no longer skewed but if you plot it without the without the log and without divi dividing it by thousand you're going to see the skewess in the data set. Excuse me, sorry to interrupt, but uh we're in the online quiz section. The TA hasn't showed up, even though the quiz is supposed to start at 9:00.
(14:06) Do you know what we should do? Uh quiz starts at 9:45. Oh, 9:45. Oh, sorry. We we made a mistake. Yeah. So, we have a section first. Um Oh, okay. That's why I'm going through the section notebook. After me, Daniel will be going through it. We start right at 9:45. Okay. Okay. Got it fixed. Right. Um this is some EDA.
(14:41) Um notice the EDAs are usually focused with the target. So you are trying to see how square feet relates to the price. Uh how a district or relates to the price and number of beds the year and home built and so on relates to the price. U next will fit a decision tree regression model. Uh but I wonder what the default max depth is here.
(15:18) We'll see how the parameter is two. I'm not sure what's the default. Uh cuz usually these are the parameters. Oh, max none. So it's just going to go on. If none the nodes are expanded until all leaves are pure. Uh so by default basic tree fit uh fit predict mean squed error uh and then the root mean squed error. So you can see uh root mean square error is 37.
(15:52) This number can change significantly if you don't do the log base. Okay the effect of max depth we try various different max depths and see which one works best. Here we have a plot of max depth versus the RMSSE. Uh we can see that very high RMSSE for max depth 2. Uh it goes on uh because the model is very simple at this point. This is our the ideal point.
(16:31) Uh where the max depth is uh just right max to highest RMS. the trees too simple under fits high bias. uh increase depth to four and six improves test RMC around six we get the best test RMSSE uh it becomes worse again it starts overfitting when you keep on increasing this is the bias variance trade-off for the decision trees shallow trees high bias low variance very deep trees low bias high variance uh a single tree to an ensemble this was just a single tree we just saw that very shallow tree under high bias and very deep tree over fits which is high variance. Uh but a single tree is unstable regardless of whatever we small changes
(17:15) in the training data can lead to a very different tree. That brings us to uh reduce how do we reduce variance but we average many trees that brings us to the thing called bagging bootstrap aggregating. Uh that was all for decision trees. I'm going to give it to uh Daniel for bugging. Thank you, Raj. Let me check my screen.
(17:49) Which one is it? Is this one? Um, so I'm not sure what screen are you seeing. Um, hold on. Did I stop my Okay, now I see your screen. Uh, did I stop my sharing? Don't know. I I think I did. I don't I'm not sharing anymore. Okay, let me try again. So, it should be visible right now. Yeah. Okay, perfect. So, thank you. So for for bugging is basically another level from the decision tree where we are trying to reduce variance and we do that by doing a bootstrapping bootstrapping Pablo's discussed during the class. It is taken from a training set or yeah from a
(18:40) training set. If you take random samples with replacement to account for the same number of the data you have n data set so n number of rows or then you will take n values with replacement. So you can have some repeated numbers and by doing that you are just passing different trees different parts of the data set and then you will have different splits for each of these data set and then what you do later on is aggregate.
(19:16) So uh the aggregation happens in two ways. If if you're talking about regression, you're taking an average based on those uh predictions or if you're talking about classification, you will take the mean or sorry the mode of those predictions. Right? So this is what we are what we're saying here. So we have a real data set. We take samples of the same size each with replacement.
(19:43) So some some points will be repeated, some will be left out and then we will just u have various decision trees being trained and then we do the regressions or the data degreation right for classification. Now how do we do implement it? So first we have to do the decision tree. So that's what we call base tree.
(20:10) So we have a decision tree Raji just mentioned. So we have trees that will be kind of overfitted in this case. So you put max depth uh to none. So it will it will go until until the previous one. And then you have this backing regressor that will take that tree that you just so that overfeed it here that you just created but it will create many of them.
(20:34) So the n estimator will make sure that you take for example in this case 100 of these trees and those will be aggregated right. So we put bootstrap equal true and then n jobs minus one indicates that you're using as many processors as you have. So with that predictor you use the usual stuff right fit and then predict and in this case we are getting the root mean square error which is 3630 which is should be better than the previous one I don't know I think the previous one was yeah 37 or something right I thought it was the lowest was was 32 so that creates uh that creates an
(21:22) improvement in versus the previous one. Now, one question that sometimes comes up is so how is that that we have a decision tree that is overfeitted then we aggregate the predictions and then we don't overfeit and then that's kind of counterintuitive but if you just test that. So if you have here we're going to have a list of n depths of traits trees and then we're going to plot this these ones right we're going to do n list so we're going to do regressors for different estimator right so this will be just one decision tree but then we're going to aggregate it and we're going to see what is the
(22:04) result on the rmsc so and we're going to plot it and although it seems counter intuitive it's really happening that the at a certain point you have a you reach a number of trees where the overfeitting doesn't happen anymore. So bagging is really having the effect by disagregation bootstrapping and aggregation that at some point you have reduced the variance of your model.
(22:40) So you so that's kind of a win-win right? So you have the improvements from uh decision trees but also reduction of variance by the aggregation part. Any questions until this point? Okay. So now let's talk about hyperparameters. So maybe that's a question for you guys. What what hyperparameter have changed for from bugging to a single decision tree? What do we have changed? Okay. And the answer was pre on the previous on the previous one.
(23:23) So the only thing that we have changed here is basically adding the number of exting number of threes but the other key point that we are adding in terms of hyperparameters is we're adding the number of uh um estimators. So if we compare here right we have all of these free randization three and then we have the random forest sorry the bagging and then we're just adding the number of uh estimators right which is this one here right the number of trees okay now bugging versus random forest what is the difference between random and random forest random forest is one step further
(24:08) than bugging And what we do in bugging is we train many bootstrap trees right and we average the predictions and that reduces the variance as we just discussed. Random forest as one more element and that element is that we add also randomness on the features that we select.
(24:34) So at each split on random forest that the tree is going to look into a random set of features. So you have let's say 50 features and then you and then you choose a few of them right that that those are the only features that going to be used for a random forest. So we by doing that we we make sure that the trees are less correlated with each other because they will be trained on different set of features.
(25:03) So the average so average in all of those um trees should reduce in theory uh even more the variance than just plain bugging because we have added a a little bit more of randomness to the to the equation. So I hope that makes sense. So when we have random forest we have again as we said the number of trees the number that max tree depth and then here is that extra parameter that we're adding. That's what I was trying to show you here actually.
(25:32) So this here this parameter is the maximum features. So in this case you can put a number which is going to if you put a number an integer is going to take a n number of parameters but if you put for example like this one a square root is going to take the square root of the number of parameters that you have.
(25:54) So you have 100 parameters is going to pass for each tree 10 right because the square root of 10 is 100 is 10 and then same thing you use the fit and then the predict and then you get the mean square error which is in this case is very similar to the previous one right it's very similar to the previous one that we got and what we normally see is that um I'm going to speak about this one why they are similar on the next step right so but these are the hyperparameters that we use for for random forest so for in terms of the t complexity we use the same parameters we
(26:32) have been using for trees uh same parameters as bugging right the number of estimators that we have included here the number of estimators so 100 trees and an additional parameter which is the number of features which is the one that we mentioned here right so it's a three so it's more hyperparameters for for the random forest.
(26:58) And the purpose again of this one is to uh reduce barren by adding more trees. But also by the maximum features we are trying to reduce the correlation between the trees because we are having different trees forecasting or predicting on different set of features. Okay. And this is kind of a summary of what we just discussed.
(27:24) So when we have a decision tree we have around uh 32 on RMSC uh the challenge here is that the although this tree is very explainable because we we have one tree that have all of the different features and then we can explain really easily what the model is doing because it's just one tree that model is unstable and a small data changes could really create a new different decision three.
(27:51) Now when we move to bugging, we are adding more um more um splits on the data or the bootstrapping part. So that reduces variance um also reduces bias but then I mean we have many trees it takes a little bit longer to train um I mean various drops normally and when we adding decision tree we are adding an extra layer where we are trying to further decorrelate the trees by adding more features.
(28:24) Now the in practice normally the performance of bugging and random forest is usually similar. So kind of like the rule of thumb is just try to use random forest when you're trying to recor decorrelate three features doing some self pooling right but it's not always going to work but I mean that's kind of the the the the rule of thumb and use bugging when you just want to kind of like uh aggregate uh many strong base that that you have or you think that are already fitting uh your purpose.
(29:01) Okay, that's what I wanted to share. I mean, there's extra material that you can read. I mean, we can cover if you have some questions, but I think that's kind of the main the main points. I have a question. Go ahead. Um, if we use random forest or bargaining for classification, how do we what sort of accuracies are we going to use? Can we apply F1 score and the other accuracies especially where there is high class imbalance in this situation? Yeah, let me see the classifier. So I mean this I mean this is the same part right. So you have these hyperparameters
(29:43) and then you need to tune them based on um you normally use cross validation to tune them and then you can use whatever metrics that that you normally would use. Right? So that's kind of where the hyperparameter tuning comes into place and then cross validation to make sure until what point do you stop and I think on here there is an example let me see uh yeah so so here there is kind of like the the part of the grid search right and then we're doing this random forest and then with grid search you're trying to find different splits different features
(30:23) uh just to kind of like u see what is the best the best split. In this case it was eight and 12 two but yeah you have to do cross validation. There is no one single formula that you can know okay you always use 10 or five no you have to do cross validation and aiming to have your best score that of course is depending on I think you can define here the best score and that that is when it's going to maximize or minimize me anything to add not really okay good Okay.
(31:12) Any other questions? Missing the chat. Okay. The estimations are trees. Yes, estimators are trees like we say here, right? Yeah. estimators are trees but it's not clear for here but yeah but in this estimation in this case are trees and unbugging of or they're also trees so I think unbugging is easier to check yes because you have bug regressor and then you are passing estimator equal this base three um yeah okay what else okay I think that's all what we cover I think we can finish earlier resume there was a question about the
(32:07) link to use so I don't know if we have sort this part out sorry we stay on this link there is no separate link to go to okay u but we still start at 9:45 that's when the ed quiz will pop up okay so we can come back in like 5 minute break and or actually we have like 6 7 minutes here. Uh we start sharp 9:45.
(32:40) Yeah. If you have any questions, please let us know as well. Uh this was always the link for Saturday quiz. I mean it this thing was posted in the when you fill the form. The online quiz link that is there. It is only for Thursday and Friday. This is the usual section time. So, we use this link. Okay, sounds good.
(33:10) So, shall we start the recording? Sure. Uh you can take few minutes