109 day1 section - YouTube
https://www.youtube.com/watch?v=JxhwAreewwg

Transcript:
(00:02) Okay. Hi um everyone welcome to our very first uh section. So this this section is basically designed for the extension students and that's why we we we chose a Saturday because we know that y'all are very busy as much as we are. So um just to make it flexible and we're recording it as well.
(00:30) So if you're not able to make any of the sections then you can um watch the the recording. Uh so we are my name is Victoria Oki and I'm here with my boss Rashmi. So she is the head TF and um Daniel I also see Daniel on the call. So you'll see the three of us um moving forward for all of the Saturday um sections. And we hope to see you all as well. Yes. So you all are stuck with us.
(01:01) Well, um let me see. Am I sharing my Jupyter notebook? Yes. Okay. Do you want to increase the font a bit? Uh I mean the just command plus should do. Okay. Probably on a different screen. Is that better? Yeah. Okay. So, hopefully you have um the notebook downloaded at this point, but if you don't, um where you can get that is you go to add um click on this little icon that looks like a notebook, which is the lessons.
(01:43) And then you would see the section one um web scraping. You click on that. And on the left um hand side you'll see section one notebook. So this this notebook is so then you you click here when you click it takes you basically takes you to collab and you should have access to collab even with your um Harvard email and um Collab is is fine.
(02:14) It has all of the packages that we will be needing for this section. Um, but if you've already set up your Python environment, then you can also feel free to actually download the the notebook and you can run it in your local. And for Collab, I mentioned that you if you're working on collab, you need to make a copy of the the notebook so that um all of the changes that you make can be saved.
(02:43) Um I I'll pause a little bit to allow um people download this and please give me a thumbs up when you're when you have it working. Also this session is going to be very interactive. So please run it along with us. You have questions, pause us. Yeah, feel free to interrupt. If you feel more comfortable putting it in the chat, feel free to do that as well.
(03:09) And we we'll try our best to respond to them. Before you get started, let me ask how many you have done Python in the past. Okay. John, I see. Yeah. Okay. Okay. So, some of you are familiar with Python. Great. So, for for some of you, it could probably be like a refresher or something. And then if if you're brand new, you can I think Pablo shared a a resource on on um on ED where you can take do some crash um course on Python, algebra or um I think statistics. So feel free at your at your free time to explore those.
(04:15) Okay, I saw a couple of thumbs ups and I think we can move forward. So the let's see the purpose of well the title of our of today's section is introduction to web scraping. So web scraping basically collecting data from the internet. There are multiple reasons why you would want to do that to get additional information.
(04:48) If you know how to script the web, you can get basically any information that you need as far as the website um permits you to script it. We you can also you know our pro your projects will come up later on in the semester and you might have data set already to work on but you can you might decide to scrape the web to get additional information that you will combine to to make your project um reach.
(05:14) So for this section we will be going through how to um programmatically pull information from um websites. We will discuss how to use the requests library to get that information and then we will go through beautiful soup using beautiful soup package to make it neatly structured and then we will go over some data wrangling regular expression lists and dictionaries and then we'll briefly touch on um how you can create visualizations using math plot lib I think um rash We'll have additional section on how to dive deeper into creating visuals. Right.
(05:59) The next one. The next one. Right. Okay. So, um for today's section, we will be exploring the Nobel Prize website and you can access it through this link right here. When you click on that, it takes you to this uh Nobel Prize website. And let's see all of the Noble um prizes. So when you look at this, it's um so we have 2024 through 20 and you can get all of the years. So it goes down to I think 1901 or so.
(06:40) So basically it's if you look at it you see that is structured. So starting from 2024 we have Nobel prize in physics and chemistry and then each of the year then you it's also kind of follows the same pattern title the laurates and then the description of why they they got the why they were awarded. So same pattern.
(07:04) So as you're looking at this um a couple of questions might you know pop up in your mind that you might want to know about this website. So you might you might want to know okay who has been awarded more than once or you might want to count the total people who gets awarded per year. Um so a lot of things that you can do looking at this website but we would like to do that programmatically.
(07:39) It's going to be hard for you to manually read through or count how many prizes or how many people. So programmatically we're going to to be doing that. Please if you have a question feel free to to speak up. So for the program we're going to be using Python and that's because um that's the programming language that we're using for this class. There are other alternatives to script the web you can use core.
(08:05) So but we'll be using Python. So to obtain information from the Nobel Prize website or whatever website that you you want to explore or script from, you need you can use the requests um package. It's uh basically um gets the all of the information on that page.
(08:36) But the first thing you need to do is to supply or to provide the the web URL. So here we're adding the the Nobel Prize website. If you want to script wiki or whatever website, you plug that URL here. But um I'll mention that it's very important to take a look at the website um policy. So some websites don't allow people to scrape the website.
(09:03) So just make sure that you read the policy. Make sure you're not breaking um any policies. So policy is it the robot.txt file or I I believe so. Um I think it is it is robots.txt. You can actually look at on many popular websites what they allow and what they don't allow.
(09:37) And then some people, some websites that allow might also block you when you make like too many requests. So sometimes we advise that when you make a request, you space them a little bit and put some time in between. So we collect the web URL and assigning it to all prices URL. And and sorry, you'll see me look up. That's because my monitor is is up here. It's a little weird but um so when we collect that here then with the request we use the get. So what this get does is that it it pulls the actual content.
(10:10) So this is no longer just the URL but it gives us the actual content of that Nobel Prize web page or whatever page you're scraping. So once you hit that get this response we're assign we're assigning to response. Now this response contains yeah everything about that web page.
(10:37) Um contains the status code and this status code is basically tells us how well we if you successfully um got information from that website. So there are a couple of codes for instance when we what we received is 200. So 200 means that we successfully um got the information from that website. um you might get like 4043 um and we've added a link here that um will take you to the page where you have like multiple codes and what they represent.
(11:08) So feel free to to look them up. So here we've um mimicked like a fake URL to see the output. So this is not a valid URL. we're getting the the 404 um code. So if it's it's very useful to to inspect this because I mean you might be scraping a lot of websites and maybe you want to write a program that will only retrieve the successful ones so you don't have errors. So it's very useful to have.
(11:40) So once we obtain um the so still with the response you can visualize the actual content or the text in that um HTML. So with that we use text and here we are limiting this to the first um 200 characters. If we decide to run the entire thing, we would see all of these um display from that web page.
(12:15) And one one trick also is if you have something that you're you know consistently scrolling, you can right click on the left here and say enable scrolling um for output. So that condenses it here and you can nicely um scrolling and don't not have all of that. But to have a you know pick into your data you can you know just let's just use 100 and get just a little bit of information. So now we will assign that to HTML.
(12:46) So we have the object HTML. So speaking of um HTML um what is HTML? So it repres it it stands for hypertext markup language and this is what developers use to to create basically create the website. So it has um this kind of structure where you have the head the body um and then within so these are called tags. So you can have tags within like nested tags.
(13:22) So here we have the head, we have the title and we've we've provided explanation for the tags here. So the head provides the head then the page header title then body is the entire body. Then within the body you have paragraphs, images, hyperlinks or or tables. So um we have the HTML element. So this is when you look at this is an element.
(13:55) How to identify an element is it it starts with a tag which here we're looking at the H1 tag. This is the beginning and then the end is there's a back backslash. So each element will have this. If it's an a tag, it will you see a like this and then back slash a and then you have the the text embedded within that.
(14:21) So here we've provided um a picture of what we would see but then um I think let's go directly to the web page and please feel free to to follow along or do this on your end as well. So every web browser has um something called how to um to inspect a web page and then to inspect what you need to do is to right click on anywhere on the the web page and click inspect down here.
(14:56) So once you do that you'll see this um pop up. It could pop up on the side for you or at the bottom here. Um, but I think wherever it pops up for you, you're going to see this picker tool right here. So, when you click on that, you can um safely choose blocks of code or sorry, not code, text, title.
(15:25) And you'll notice that as I'm moving through this on the right hand side, it's also mapping to corresponding tags. So for instance, this um this section here, this title. So when I click on that, it picks the tag that corresponds to it. So that's a H3 tag. And then you can expand that. When you expand it, you'll see it has another nested tag within it.
(15:51) So this is the text Noble Prize in physics 2024. And then it has an A tag nexted and a tag is um a link a link tag. So you see another link here. So that tells you that this text here is a link and this is how you can you know safely crawl the internet. So you can click on this and it will take you to or you can extract this and it will take you to maybe another page on this same web website or it could take you to a completely different website. So that way you can crawl crawl crawl the web and pull all the information that you
(16:27) need. So um so we have the div tag here, we have H2, we have P tag and then some of them have class. So we have the class here and then the class attribute. So class attribute means excuse me. So here we're looking at the card price. So this is the class attribute of this div um div tag. So you can expand and play around with it and and these all of these tags these are what we need to programmatically pull the content um of this website.
(17:11) Does anyone have any questions so far? Okay, I think we have So this is just an image of what we just saw, right? um on the on that site. So on on your own, try to navigate down. We have a little exercise here and you can navigate down the page and find the block quotes. A block quote is a tag.
(17:45) Find a block quote that has block quote tag that has this text in it. I'm going to put it in the chat. for easy access or you can okay you have the notebook too oh not able to okay there you go then identify the class attributes of of the blog quote if you find it just feel free to to put a thumbs up in the chat so just scroll through the element the side and then find the block quote tag that has um this text embedded in it.
(18:37) And feel free to ask questions or if you find anything challenging, feel free to to put in the chat or speak up. I'm just reading the chat now. So, yes, we have office hours, online office hours. Um, Rush, myself, and and Daniel will be hosting those. It's and it's open to all students actually. Where's the string that we're looking for? It's on the chat. In the chat. In the chat.
(19:41) Also in the notebook. Which browser are you on Charlotte? Uh there shouldn't be a I mean we are using Chrome and it has I think every inspect. Yeah, pretty much every every browser should have it. Yeah, in Safari I think they don't have that brow. So try Chrome. Yeah. Oh, I don't know that. Oh, maybe Safari doesn't have a problem.
(20:16) Yeah, I was looking maybe for an option if they have updated that, but it seems that they don't. They still don't have it. Interesting. We were trying to retrieve the the class attribute of the block quote tag that
(22:29) contains the text that is in the chat and also in exercise one in the notebook. If there if there appear to be um four class attributes nested, which one would you be looking for in this case? Like what I see is div class and then compris and then there's another subclass and another and another.
(23:00) So we're looking for the the block quote tag. So the DVC tag you could see an a tag within it. You can expand keep expanding. can then see the the block quotes. It's it appears like like this right here just like the div. Okay, that's right. Thank you. Also make use of the command F or I don't maybe control F on Windows to actually find the text. The text, right? Yeah.
(23:36) When you find the text, you hover over it and then with the picker tool, it can map the tag that is related or linked to it and your homework will have some of the similar exercise. So, it's good to try this now. That's true. Also, it's it's fun to script web pages.
(24:16) I know once you get a hang of it, you feel like pulling every data you see. But the the main idea is you you get unstructured information from the web and you structure it and when you have that structured information, you can do lot more with it. Very true. Well, no. I don't think it would be cheating. We don't want you to read the whole page for either the text or so.
(25:00) Just use control F. Make it easier. Yeah, what I did uh there's a question that chat if you don't find a sact string just tried for the last part. I
(26:21) was looking I couldn't find it with the same string. I tried cosmic radiation and I I found it that way. Oh, are you on the Let's see the actual page. [Music] Yeah, I'm in the 2000. Wait, is it 2024 20? Yeah, I'm on the exact page and then I did a control F in the elements tab or are we supposed to find it in our collab? Oh, no. You find it on this page there.
(27:39) Here. You can even find it in the on the inspect side on the right hand side. It'll give you it'll hit directly the block code. [Music] So probably let me narrow it down. You can find it in 1948 section. Yeah. Um, I can
(29:22) go on with how I found the string and we can move on to the regular expression. Does that work? Yes, that work. Let me stop sharing. Okay, let me share my screen. Uh I'm using collab. So maybe next time we'll try to use the same thing both. So we were here and I'm going to runtime restart session just so that I'm not running the same thing or missing anything here.
(29:56) Now I'm also going to run before so that way everything before it runs. So we were here we were trying to find uh this particular string on the Nobel Prize page. So this is the page. I'm going to reload the page. Okay. Uh this is what the page looks like on my Chrome and I go and click on inspect and the string we are trying to find is this particular string.
(30:23) So what I'm just going to take a part of the string. I'm going to just click here and do a I'm doing command F. So there is a uh I hope you can see it says find by string selector or X path. I'll just put the string right here and it highlighted the string that we are looking for and the this is what we are looking for.
(30:48) The block code class is card prize laurates motivation blah blah. Um it if there are multiple such strings it'll take you to multiple but it is only one so which we are good. Um next we going to come back to this try to get the actual tag and class and all but before that we are going to also look into some of the regular expressions.
(31:11) Uh regular expressions help you there's a chat question or something. Hold on. Um it's just convenience. Collab is very convenient. Uh I use either cursor or collab. Collab is because I mean you'll provide it with the link and it just takes you right there. So I don't have to download upload. But it's totally up to you. Whatever works.
(31:40) If I have to start a new project or something, I I use cursor locally on my machine. Uh regular expressions are I mean they can be tricky uh but they are super useful to find strings in a large text. Uh some of these are the common examples like the slashes how you match.
(32:09) If you look at the actual regular expressions you might find that it's a bit obtuse bit hard to understand. So some of these common ones are listed what it matches like /w star would be any string uh plus would be one or more repetitions the uh within the curly brackets you can give uh like three characters or the number of matches you want.
(32:33) So basically that's about it. So for example if in the string that we have HTML we are trying to find all instances of the word u Murray you just hit find all. Now this was a very easy one because we are giving exactly but the tricky ones would be we are trying to find all patterns of email addresses or all phone numbers things like that regular expressions are really useful and we use it often.
(33:00) It doesn't have to be necessarily in the scope of uh scraping or anything, but you can use it anywhere as long as you have text. Uh there are other questions coming. Feel free to interrupt me also by the way. Uh because I won't be able to see the hands raised I guess. Uh the next we have it's trying to match any character which is not a unicode wide space.
(33:31) So basically it so slash s is for the tab spaces new lines and we want to avoid matching those. So that's why the capital s. So that's what this is doing. U how would we find the last name that come after Mari? I wonder if this was the question hidden and I just gave you guys the answer. But anyway this is this it it matches the word and star.
(33:56) So it'll match basically everything all worry with any last name. Uh now this one like this is what I meant. This one can get really tricky and complicated to understand. Uh we usually don't do this because one of the reason we don't do this is because HTML tags like you can it it'll work here for this particular case. It may not work for something else.
(34:19) So it's not like you cannot do it. you should do you can do it but it's not going to be portable from one thing to other things. So if you're doing it for one this particular purpose yes it'll work totally fine. Uh there are some links here provided and I think these are pretty good links. I use them often uh to validate the reax to figure out if this particular pattern is matching what I want it to match. So all these uh links are pretty useful.
(34:51) U we are coming back to beautiful soup as to how do we get this particular unstructured or the HTML that you see on the right hand side into the structured format on our collab or create a CSV something that we can open it in in maybe Excel maybe Python pandas and we'll see that later. Uh we use the library called beautiful soup.
(35:18) Um, it's going to get you it it's the if you click on if you print the soup thing, there's a whole lot of things here. Uh, it's hard to understand without getting it into a structured format. And that's what beautiful soup will do it for us. You can ask beautiful soup to give us the page title.
(35:37) Give us the title, give us the H1 tag or from this particular HTML that looks like like basically not readable. Um that's why we use beautiful soup. There are many other libraries like beautiful soup but for the course we will be using beautiful soup questions. No. Okay. Uh there is a full beautiful soup documentation available here. uh some of these common commands are given here and we use them often again uh and you'll be making use of documentation or maybe chat GPD or something to figure out how you want to match exact tags using beautiful soup.
(36:28) Um some of these are pretty like you can read it very you can read and you can understand what it does. Tag pitify it's going to print in a more readable format rather than the unreadable mess that we saw earlier. Uh tag select you can select one you can select all all the matching selections you can select one of them.
(36:52) Uh you can have find all tags you can find you you can put reax inside a tag.find so it'll match with the reax. uh you can you can have a nested tags so you can find children, you can find parents, you can find siblings of the tags and so on. Uh we are going to try to print some it's it says pretty like about 500 characters. So that's what it looks like. This is much more readable. Uh you can maybe like make it 100.
(37:26) It might look even better. uh what we are looking at here uh the next thing we are doing is extract the uh text of first title tag so we we are just saying select one title it's it's giving us the title for the from the HTML uh all Nobel prizes noble prices.org or is the title uh return the first five anchor tags.
(37:54) So within the H3 header, so H3A is what you're going to do and it's going to give you the list of all the the top five the first five it found uh the A tags. Now if you actually we can actually go look at over here uh let's see if we try to look at the H3 tag all the way at the beginning it should be somewhere. Let's see how many H3 tags are there.
(38:20) So if we go here, I wonder if this is what it is extracting. So this is summary. Um there might be more H3 tags in the Oh, there are like 687. So we are only getting uh the first five. So it's this is what it's probably extracting from the medicine, chemistry. Let's see if we got the same thing. We have physics, chemistry.
(38:44) Maybe I missed something. uh so on we have something here on chart yeah there are many tools like this for the reax I use anything and sometimes they have different languages whether you're using python or some other language and how you're using reax oh I have this thing opened. Somebody raised a question about uh robots.txt. So if you go to Facebook/root.
(39:23) txt, you can actually see what it allows and what it doesn't allow. It is it it has listed all the bots and the agents that people could put in there and it is not allowing anybody. But if you scroll down, it says allow if you it allows the careers page on the Facebook safety check and so on. But a lot of the things are not allowed on Facebook page. That's why I actually opened it. Check it out.
(39:47) Uh usually you'll find this on many websites. The Okay, moving on. Moving back to coming back to the extracting award data. Um yeah, we want to we want to select the uh card prize class. So we just say select card prize and let's say tag.prify Prettify what do we get? It is looking really pretty for now.
(40:19) Uh although I must say that if you have actual website it won't be looking this uh pretty. It's a small example working it in the class trying to learn. So this looks great. Uh you can render it in the notebook with HTML. This is I don't think this is part of uh beautiful soap. uh notebook. Uh yeah, LinkedIn might have similar policies too.
(40:50) Uh getting the data out of a beautiful soup node. So if you have a tag that looks like this tag.h3.ext.strip is going to give you uh this particular Nobel Prize in physics 2024. Uh you want this is again string manipulation. It's removing the last five characters. Uh here it is using the uh reax the pattern is uh four digits.
(41:20) So it is it's going to gather everything but the four digits or no actually it is going to gather everything and then it's going to replace this four digits with uh I'm not I I don't recall what that one does but I think it is replacing it with uh spaces or something blah blah sorry oh sorry replace with blah blah 1991 right yeah uh it's replacing this 199 91 U.
(41:54) And so we end up with just the uh characters. We want to get rid of the digits part. Uh and if we want the if we can split it. If we want just the year, you can extract 2024 as well. U block quote. What was the text? And strip B strip is basically getting rid of the spaces on the side. Uh which is useful as well. Uh let's see now what do we have? Uh we are going to work with the another tag which is going to be for each prize we can have multiple uh laurates.
(42:36) For each florate we are going to get the HTML their and their name as well as some other information that they have and we we can create a table uh with all this information. So a tag looks like this. If you want to find the find all a tags and with the attribute class. So it's going to gather everything all of this like all this a class this one and this one and this is what it's going to give us. We we are narrowing down actually to what information we want.
(43:09) Uh so we start at a big the whole HTML and we gradually narrow down what we want and what we want. We have to look at the page on the left not the HTML but here we we have to gather from here what we are trying to look for in the page and gradually uh keep on going. uh questions so far. Um this is gathering the strings. Sorry, gathering the names.
(43:45) We're going to gather the links separate. We are going to create uh this is creating a tpple with the uh this is a tpple inside a list. So we have a tpple for name and link name and link so on. Uh now we are going to create some helper functions.
(44:11) You could do it the traditional way the define functions way or you could do if you have a very small like these helper functions do literally one line thing. So that's where the lambda functions help. Uh it's very common to create bunch of lambda functions if they are single line uh things. So that way this is a function. Get prize as a function. Get title as a function. Get year as a function. Get laureates as a function. Get description as a function.
(44:34) All these are the these are like five functions in five lines. And in this particular block, so like since this is a function get prizes, all we have to do is pass the main soup block that we extracted and it's going to give us the tag uh card price from that particular full soup.
(45:05) So if we run this we going to have like total of extracted information is like 627 uh do I have it twice? No it's just different the length of the par price is 627 and if we want to look at what each one of them looks like we should usually we can do this it might have lot more information as well it's not too much.
(45:31) So this is what a single one looks like and we have 627 of these. U we want to convert all of this information in a uh I think we I'm repeating some things here. Maybe you guys have a different notebook and I was trying something. Uh we want to put all of this and this is the last one. We we are putting all of this in a dictionary here. D and it's already done actually.
(45:57) Uh if you look at the last one, past prices, the negative one is for the last price and how many total? 676. I wonder what this was. Do I have it something different? I No, there is prices list. Let me run this again. 676. I don't know why I had 627 earlier. Uh, okay. Now, breakout sessions. Work together in your groups to answer the following two questions.
(46:35) What are all the unique prize titles? So, you have the past prizes and you have the titles, but you have 676 of these and you're going to be finding how many of these are unique different titles. Uh, we can have like a breakout sessions for like maybe five, seven minutes. and we can come back. Uh let me hold on stop sharing and maybe I can create some breakout sessions.
(47:07) Well, but before that, do you guys have any questions before I do the breakout rooms? No. No, eight breakout troops. Yeah, eight breakout room troops should be fine. And we come back in like 5 minutes. So it records this particular session but not the breakout sessions. Um
(48:19) shall we maybe join the breakout sessions to help students in case they have questions? Sure. I'm not sure if I join it will close the room, but I'm pretty sure you guys can join. Okay. And if they're done early, we can close the rooms early. Okay. There are many groups. Okay. Let's go. Uh I think once there are two rooms where students are alone, so maybe I can move them to room seven. Yeah, let's collapse it.
(48:54) [Music] So, seven rooms and then there's one more room with one alone. Room two. I can bring the student back here maybe. Or you can put the person in a different Okay, doesn't matter. Maybe I I'll join the person. Oh, I moved. Oh, you moved. Okay. Back. Well, you were the only one in the room, so we moved you back. So, we are your partners here.
(51:09) Okay, I will be joining some sessions just in case they have questions. Mhm. And I'll close the rooms in like one or two minutes. If you have any questions, feel free to ask. Okay. Yeah, let me close the rooms because it will take at least one more minute before
(52:16) they actually close the room. All right, we are back. Um, let me start. So, how many
(53:34) unique prizes were you able to find the text? Six sounds about right. Uh let me share my screen. So we were here. What are all the unique prices? Unique price titles. Uh we were looking at the past price. Uh so if you actually click on like if you print bunch of these and you will see that they all the these things look uh some of them are same some of them are different if we can keep going but the idea was to get this extracted uh programmatically uh you have the solution by the way solutions will be provided to you guys for the sections u for all the sections
(54:38) I'm not sure if it is already there but it will be there soon. So you just put it in the set. Set is going to be it's going to automatically discard the common ones. So you're left with only the unique prizes and it displays the unique prizes 1 2 3 4 5 and this apparently this one is a title. So maybe we could be excluding this. Uh this doesn't or maybe it is the price. Oh, it is the actual price.
(55:10) Uh anyway, when was the the next question is let's not do the breakout room, but we can probably try to figure this out right here. When was the economics prize added? Save your result as first econ. We can spend few minutes here. So you're going to gather all the Nobel Prize which includes econ and you're going to get the year for all of that and then you have to find the least year or minimum year.
(56:36) There's a oneline code here, but you could possibly if you are trying to not make it one line, what you would do is for P in past prices, you want to print uh if econ in the title. Oops, sorry. Under this print first, you're going to see how many eco. So all of these are there and then you want to extract the ear.
(57:04) So you want to you can create a list of ears and what you're looking you what you want is basically P of ear. So it can look something like this list ecom years and then you just append to the list do not append uh P no P of year one. Yeah. And then you can do is minimum of this list.
(57:45) It has the year 1969 and this is the one line solution that you will see in the uh in the solutions. So first year for economics prize was 1969. Who has been awarded more than one prize. Uh these are just the questions to help you get familiar with working with so beautiful soap working with dictionaries toppables and all of these things because it's not just for this particular homework these are the things you will be using it in perhaps all the homeworks and I'm pretty sure you'll be using it in all the homeworks maybe not beautiful soap in every homework but the list trles data frames all of that yes uh this is de demonstrating how to
(58:28) append to the list, how to extend the list. Uh this is list of all laates with the name link tpples. So these are tpples. Uh we practice filtering and counting to detect the repeated names. So how many if uh for every if muri is in the name and you checking how many laurates have the name Murray in them.
(59:00) Uh this is the counter. Uh this is also very uh popular. So like you can actually get the in one line you can get uh how many times the names appear in the dictionary. So it's very easy way to handy way to uh convert these things into the dictionary uh into the count format. Uh I'm going to remove this back. There's too many of these.
(59:30) Uh display only those with a count greater than two. Uh again a simple if condition. Uh we are looping through the uh count. This this was a type dictionary I believe and we looping through the collections counter. Sorry it's not dictionary collections counter. We are looping through this particular type gives you what you're looking at whether you're looking at string whether you're looking at list data frame.
(59:54) Uh you use type by the way. Um uh we just sorting the results which also helps. Uh often we do want to sort the results. By the way, collab I'm not sure if it was announced or something. Uh collab is free for students for a year. Although you will not need you can just use without the free version also.
(1:00:29) Uh but if you do need to use collab for GPUs and you are thinking of paying it is actually free for students for a year. U how many laurates were there for each year? Uh this is where we are assembling the information within the dictionary that we have. So this causes an error because if you try to get the key within the dictionary, it it it's a very nice way to do the error handling.
(1:00:54) Otherwise, if you don't do the error handling and if you have a empty dictionary and you try to do this, you you're going to get a type error something like like if I do this without anything without the try catch, it's going to get the key error because you don't it cannot find it's an empty dictionary. It cannot find 2023.
(1:01:16) But if when you put the try except block, it's a nice way to handle the errors if the keys are missing. And sometimes when you try to do the uh scraping because the data is so unstructured, you you do have cases where you you didn't find any data. Maybe there are no Nobel prizes for that particular year for that particular category. Who knows? Uh there's no data.
(1:01:41) the keys will be missing and you will get in you will get errors if you don't handle them properly. Uh now there's a solution for it is the default dictionary which assumes that it is zero for anything uh missing. So you have to decide whether you want to keep it as zero or you want to make it.
(1:02:06) So here it won't give you an error because it assumes that it is uh there is keys existing or it it has zero value. Um this create winners per year dictionary and if you can check how many winners were there in 2023 and you can check in other years also. So for every year it gives you uh how many winners were there in the default dictionary. So let's say if I try to get since this is a default dictionary and I'm going to try to get 2027.
(1:02:31) So it should give me zero without an error. Yeah, zero because it's a default dictionary and there are no no prices in this for 2027 yet. So uh next step is so far we have been just looking at the data reading through the what we got from beautiful soup. We haven't done any visualization yet. But nothing prevents us to visualize this data.
(1:03:00) So what the library we use is Mattplot lil. It's by default there in collab. It's also there and I mean it's it should be in your requirements.txt also given to you. Very popular library in Python data science. Uh we use it often. The other popular libraries are seabborn plotly. Um in the class you'll often students use mattplot lab or seaborn uh some students do use try to use plotly as well but we'll be using mostly you'll see in the homework mpl plot lab or cbun yes the visualization will be covered in the next section in uh more detail.
(1:03:45) uh mattplot lab has a unique way of working with it. So yes, a section by itself devoted to the visualization would be great. If you search for something called as u sorry I if you search for something called as anatomy of uh mattplot liib mattplot anatomy of a figure it gives you a plot that looks something like this.
(1:04:11) What this plot tells you is maybe I should visit this site. It shows you what all what all are the possibilities. It I'm looking at the old one. Yeah, this one. Uh everything is a like the whole canvas it considers as a figure. This whole thing is a figure. Uh you have x-axis, you have y-axis. These tiny things that you see here, these are called minor tech labels. These are called the spines.
(1:04:39) And everything is customizable with Matt Plot lab. So which is the coolest thing. But you have to understand the basics of how Mattplot li works to be able to customize anything and everything in this. Uh you'll be amazed at some of the figures that Matt Mattplot li could do.
(1:04:57) Uh but I like this figure to understand the basics of what is a legend, what is a grid? Uh what is a axis? Uh what does the ax do settle mean? Like there this is the title. So this whole thing this plot this particular plot is called axis and you can change the plot uh title for this particular plot. Uh you have minor takes major text labels and so on. But I I digressed a bit. We come back to here.
(1:05:24) We can plot how many Nobel Prize winners by the year because we have all the information and this is what it looks like. If you notice anything interesting in the plot about there are more winners. You can see as the years are progressing, you can see there's a empty space here. No winners.
(1:05:42) I don't know what happened, but something to try to find out. Um, pandas data frame will also be a part of section. We will be using pandas data frame in everything you do, every homework, every section. Uh, I use pandas data frame. Uh yeah, you're right. Probably of the World War II. Um so we will be doing Panda's D.
(1:06:16) So we can convert all this information that we gathered into a data frame. It the data frame is going to look like this. One of the coolest thing that I like in uh collab is it gives you this data frame and then you can actually click on these things to make it more interactive.
(1:06:37) So you click and it'll you generate with it it creates the distribution on its own the time series the categories and so on. Kind of cool. I want to disconnect it but uh hold on. You can also convert this into table. You can copy this whole thing and it's giving you five entries but you can add more entries because we only did head so by default it gives you five.
(1:07:06) You can copy this all of this thing into Excel if you like into somewhere else but I kind of like it this convenience that collab provides. Um so let me let me first see we we're not printing how many total um print df. It's going to give you how many we have total,2 rows in this and five columns. We can save all our work by writing our data frame to a CSV file uh df.2 CSV uh to read the file again.
(1:07:39) By the way, if you're using collab for the first time, this is not getting saved onto your Google Drive or anything because we are not mounting. So, it'll be saved locally. It's up to you to make a permanent copy of it. Uh we will be showing you I guess later on. But what you have to do is to mount the drive. Uh and you can save this to your mounted uh uh Google drive.
(1:08:03) Uh the dataf frame.board is also Yes, Daniel. Uh I really like this clipboard thing because I it it works very I'm not sure. I haven't tried it in collab uh but I do it locally. It works very well to copy pieces of of CSV files from one place to the other. I use it a lot locally.
(1:08:25) I don't haven't tried on on collab but locally works very well. It locally works really well. Uh I like that uh two clipboard feature. Um so this is saving the file and you can actually check here LSL this file is saved. Uh but this is in the temporary space that we are working on collab right now. It's not anywhere on my drive or on my local machine or nowhere.
(1:08:53) It's going to once the session disconnects I'm going to lose that data. uh reload we are going to reload which is again the same thing pandas read csv uh this is something extra that the notebook has provided it's very useful I am not certain yet I don't think you'll be using it in the homework but these these things are very useful if you're trying to work with lot of data do a lot of API calls do lot of uh downloading or scraping uh you you don't want to be scraping single page at a time. You have like you want to scrape maybe you have
(1:09:32) thousand list of thousand links that you found. You want to scrape all thousand. You don't want to be doing one at a time. You want to be doing all thousand at a time or maybe maybe 50 at a time depending on uh how much the you are allowed to scrape like some websites may not allow you to scrape thousand at a time because it it's a load on the server. uh maybe they allow you only 10 requests at a time.
(1:10:01) So you can do 10 requests at a time rather than doing one at a time. So that's where these async io and httpx and all these are helpful. Uh and this is the case for even these are the basic fundamentals. So it's not something you do it for this exercise or do it one time.
(1:10:26) You will see even the more uh latest like open AI APIs they also they they have rate limits you cannot call open AI API thousands of times within the given time frame. So any model would have rate limit within the rate limit they'll say something like 500 requests per minute you can do. So you can use the async io and the httgpx or and they'll tell you how to call their APIs but these things will help you to stay within the limit so that you're not going if you go 501 and the limit is 500 yes you'll get error but these things will help you to stay within the limit. So instead of going one by one which is too slow and you want to also be in the
(1:11:06) limit and usually people publish when they allow you the rate limits are usually published when they allow you to scrape and they know people will be scraping they have data you want to scrape lot of maybe papers archive papers there are rate limits published everywhere so that's when we use all this asyncio and httpx there there is a whole book in python on async Um it's a very useful concept.
(1:11:38) It takes a while to sink in but you'll usually find lot of materials uh available define URLs to fetch. So here they're defining multiple URLs like 209 2010 so on like four different URLs it's trying it's giving a basic like example with just four URLs. Um you could have 4,000 but we don't want to we don't we're not doing 4,000 right now. Uh if you have to do single this is the function for the single client or single fetch request. Uh there are two two different ways.
(1:12:15) Some of our approach batching approach I have mostly used the batching approach. I haven't used this approach because usually the reason I have used this one is because websites publish this thing. they they tell you how many calls you are allowed in a minute. So when you know this kind of information then you can batch appropriately. So that's one of the reason and here it says great for being polite to the server.
(1:12:41) The semaphore approach at most end request at a time and you can finish u as soon as one finishes another can start. And this one is you have useful if the site has rules like max five calls per second. U they and people do have max x calls per minute or something. Where do websites publish the limits? It's on their website. I can I can tell you for couple of them maybe.
(1:13:12) So let's say if you want OpenAI models, you go to OpenAI API models and let's say you want to find limits for GPT 4.1. You are here you scroll down somewhere. They should be publishing. Yeah. Uh so these are the limits you have to try to understand.
(1:13:40) I mean it's this is not obvious for your first class but uh if you're working with this for a long time you will see this is specific to let's assume that we are tier 2 okay what RPM means is requests per minute minute 5,000 requests per minute tokens per minute is 450,000 u I'm going to simplify what that means is words per minute that means you cannot send open AI more than 450 50,000 words per minute.
(1:14:04) You also cannot call OpenAI more than 5,000 in per per minute. When you try to do it in a automatically, it is very easy to do those kind of things like you can call 5,000 times. You you'll be able to find limits for every website that allows you to scrape data. Um this is the basic code.
(1:14:33) So, some things that I pretty much use even without the async IO code. Sorry, there's a code here. What is this? Uh, okay. I got it. Okay. It's instead of greater than equal to two or greater than two. Okay, we got it. Uh this one by writing star task we unpack the list into separate so you have multiple tasks that you are passing it to work on by doing star tasks you are separating it into different arguments and here is the full list um we can actually run this I think I uh yeah this one run async fetch it's running it from both both both different ways. One is with the semaphore way and
(1:15:31) other one is with the uh batching way. We are almost at the end of the section or and the top of the hour. I can't believe we were perfect. Uh we usually won't be but I'm happy to answer any questions. Uh somebody had question on the office hours. Mine are on Tuesdays 8 to 9 in the morning.
(1:15:55) U Victoria when is yours? Friday mornings. Friday mornings. Yes. The evenings on Mondays at 7 p.m. Okay. So, Monday 7 p.m. is Daniel. Victoria is Friday mornings and I am QJ mornings. Um, yes, you can stay around. I'm going to stop recording. If you have any questions related to not related to the class, I'm happy to answer.
(1:16:21) Uh, any questions related to the sections before I stop recording? Wait, when does the homework usually come out? Homework will come out, I believe, on Tuesday 9th and it'll be due on Yes, Tuesday 9th it'll come out. It'll be due on two two weeks and that's going to be the regular schedule like every two weeks you'll have roughly um they'll limit them and all in the middle. So, that changes things, but that's about the time.
(1:16:58) Uh, any other questions? Okay, I'm going to stop the recording.