%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Harvard Academic Notes - English Master Template
% CS109A: Introduction to Data Science
% Lecture 13: Classification and Logistic Regression
% Version: 1.0
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt,a4paper]{article}

%========================================================================================
% Basic Packages
%========================================================================================

% --- Page Layout ---
\usepackage[top=20mm, bottom=20mm, left=20mm, right=18mm]{geometry}
\usepackage{setspace}
\onehalfspacing
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% --- Tables ---
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{array}
\usepackage{longtable}
\renewcommand{\arraystretch}{1.1}

%========================================================================================
% Header and Footer
%========================================================================================

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small\textit{CS109A: Introduction to Data Science}}
\fancyhead[R]{\small\textit{Lecture 13}}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.3pt}

\fancypagestyle{firstpage}{
    \fancyhf{}
    \fancyfoot[C]{\thepage}
    \renewcommand{\headrulewidth}{0pt}
}

%========================================================================================
% Color Definitions
%========================================================================================

\usepackage[dvipsnames]{xcolor}

\definecolor{lightblue}{RGB}{220, 235, 255}
\definecolor{lightgreen}{RGB}{220, 255, 235}
\definecolor{lightyellow}{RGB}{255, 250, 220}
\definecolor{lightpurple}{RGB}{240, 230, 255}
\definecolor{lightgray}{gray}{0.95}
\definecolor{lightpink}{RGB}{255, 235, 245}
\definecolor{boxgray}{gray}{0.95}
\definecolor{boxblue}{rgb}{0.9, 0.95, 1.0}
\definecolor{boxred}{rgb}{1.0, 0.95, 0.95}

\definecolor{darkblue}{RGB}{50, 80, 150}
\definecolor{darkgreen}{RGB}{40, 120, 70}
\definecolor{darkorange}{RGB}{200, 100, 30}
\definecolor{darkpurple}{RGB}{100, 60, 150}

%========================================================================================
% Box Environments (tcolorbox)
%========================================================================================

\usepackage[most]{tcolorbox}
\tcbuselibrary{skins, breakable}

\newtcolorbox{overviewbox}[1][]{
    enhanced,
    colback=lightpurple,
    colframe=darkpurple,
    fonttitle=\bfseries\large,
    title=Lecture Overview,
    arc=3mm,
    boxrule=1pt,
    left=8pt, right=8pt, top=8pt, bottom=8pt,
    breakable,
    #1
}

\newtcolorbox{summarybox}[1][]{
    enhanced,
    colback=lightblue,
    colframe=darkblue,
    fonttitle=\bfseries,
    title=Key Summary,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
    #1
}

\newtcolorbox{infobox}[1][]{
    enhanced,
    colback=lightgreen,
    colframe=darkgreen,
    fonttitle=\bfseries,
    title=Key Information,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
    #1
}

\newtcolorbox{warningbox}[1][]{
    enhanced,
    colback=lightyellow,
    colframe=darkorange,
    fonttitle=\bfseries,
    title=Important Note,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
    #1
}

\newtcolorbox{examplebox}[1][]{
    enhanced,
    colback=lightgray,
    colframe=black!60,
    fonttitle=\bfseries,
    title=Example: #1,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
}

\newtcolorbox{definitionbox}[1][]{
    enhanced,
    colback=lightpink,
    colframe=purple!70!black,
    fonttitle=\bfseries,
    title=Definition: #1,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
}

\newtcolorbox{importantbox}[1][]{
    enhanced,
    colback=boxred,
    colframe=red!70!black,
    fonttitle=\bfseries,
    title=Critical: #1,
    arc=2mm,
    boxrule=0.7pt,
    left=6pt, right=6pt, top=6pt, bottom=6pt,
    breakable,
}

%========================================================================================
% Code Listings
%========================================================================================

\usepackage{listings}

\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.95}

\lstset{
    basicstyle=\ttfamily\small,
    backgroundcolor=\color{lightgray},
    keywordstyle=\color{darkblue}\bfseries,
    commentstyle=\color{darkgreen}\itshape,
    stringstyle=\color{purple!80!black},
    numberstyle=\tiny\color{black!60},
    numbers=left,
    numbersep=8pt,
    breaklines=true,
    breakatwhitespace=false,
    frame=single,
    frameround=tttt,
    rulecolor=\color{black!30},
    captionpos=b,
    showstringspaces=false,
    tabsize=2,
    xleftmargin=15pt,
    xrightmargin=5pt,
    escapeinside={\%*}{*)}
}

\lstdefinestyle{pythonstyle}{
    language=Python,
    morekeywords={self, True, False, None},
}

%========================================================================================
% Table of Contents
%========================================================================================

\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\setlength{\cftbeforesecskip}{0.4em}
\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsubsecfont}{\normalfont}

%========================================================================================
% Graphics and Tables
%========================================================================================

\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{caption}
\captionsetup[table]{labelfont=bf, textfont=it, skip=5pt}
\captionsetup[figure]{labelfont=bf, textfont=it, skip=5pt}

%========================================================================================
% Mathematics
%========================================================================================

\usepackage{amsmath, amssymb, amsthm}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

%========================================================================================
% Hyperlinks
%========================================================================================

\usepackage[
    colorlinks=true,
    linkcolor=blue!80!black,
    urlcolor=blue!80!black,
    citecolor=green!60!black,
    bookmarks=true,
    bookmarksnumbered=true,
    pdfborder={0 0 0}
]{hyperref}

\hypersetup{
    pdftitle={CS109A: Introduction to Data Science - Lecture 13},
    pdfauthor={Lecture Notes},
    pdfsubject={Classification and Logistic Regression}
}

%========================================================================================
% Other Packages
%========================================================================================

\usepackage{enumitem}
\setlist{nosep, leftmargin=*, itemsep=0.3em}

\usepackage{microtype}
\usepackage{footnote}
\usepackage{url}
\urlstyle{same}

%========================================================================================
% Custom Commands
%========================================================================================

\newcommand{\important}[1]{\textbf{\textcolor{red!70!black}{#1}}}
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\term}[1]{\textit{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\defterm}[2]{\textbf{#1}\footnote{#2}}
\newcommand{\newsection}[1]{\newpage\section{#1}}

%========================================================================================
% Title Styling
%========================================================================================

\usepackage{titling}
\pretitle{\begin{center}\LARGE\bfseries}
\posttitle{\par\end{center}\vskip 0.5em}
\preauthor{\begin{center}\large}
\postauthor{\end{center}}
\predate{\begin{center}\large}
\postdate{\par\end{center}}

%========================================================================================
% Section Spacing
%========================================================================================

\usepackage{titlesec}
\titlespacing*{\section}{0pt}{1.5em}{0.8em}
\titlespacing*{\subsection}{0pt}{1.2em}{0.6em}
\titlespacing*{\subsubsection}{0pt}{1em}{0.5em}

%========================================================================================
% Meta Information Box
%========================================================================================

\newcommand{\metainfo}[4]{
\begin{tcolorbox}[
    colback=lightpurple,
    colframe=darkpurple,
    boxrule=1pt,
    arc=2mm,
    left=10pt, right=10pt, top=8pt, bottom=8pt
]
\begin{tabular}{@{}rl@{}}
\textbf{Course:} & #1 \\[0.3em]
\textbf{Lecture:} & #2 \\[0.3em]
\textbf{Instructors:} & #3 \\[0.3em]
\textbf{Topics:} & \begin{minipage}[t]{0.70\textwidth}#4\end{minipage}
\end{tabular}
\end{tcolorbox}
}

%========================================================================================
% Document
%========================================================================================

\begin{document}

\title{Lecture 13: Classification and Logistic Regression}
\author{CS109A: Introduction to Data Science}
\date{Harvard University}

\maketitle
\thispagestyle{firstpage}

\metainfo{CS109A: Introduction to Data Science}{Lecture 13}{Pavlos Protopapas, Kevin Rader, Chris Gumb}{Classification vs. Regression, Why linear regression fails for classification, Logistic Regression, Sigmoid function, Odds and Log-Odds interpretation, Maximum Likelihood Estimation, Binary Cross-Entropy, Decision Boundaries}

\tableofcontents

%========================================================================================
\section{Introduction: From Regression to Classification}
%========================================================================================

\begin{overviewbox}
This lecture marks a major transition in the course: we move from \textbf{regression} problems (predicting numeric values) to \textbf{classification} problems (predicting categories).

\textbf{Key Learning Objectives:}
\begin{itemize}
    \item Understand the fundamental difference between regression and classification
    \item Learn why linear regression is unsuitable for classification tasks
    \item Master logistic regression: the foundational parametric model for classification
    \item Interpret logistic regression coefficients in terms of odds and log-odds
    \item Understand the probabilistic foundation: Bernoulli likelihood and MLE
    \item Visualize and understand decision boundaries
\end{itemize}

This lecture also includes a review section covering hypothesis testing, permutation tests, and interaction terms for midterm preparation.
\end{overviewbox}

\subsection{Regression vs. Classification}

\begin{definitionbox}[Regression Problems]
In \textbf{regression}, the response variable $Y$ is \textbf{quantitative} (numeric, continuous).

\textbf{Examples:}
\begin{itemize}
    \item Predicting house prices (\$500,000)
    \item Predicting tomorrow's temperature (72.5\textdegree F)
    \item Predicting sales revenue (\$1.2 million)
\end{itemize}

\textbf{Models:} Linear regression, polynomial regression, Ridge, Lasso, k-NN (for regression)
\end{definitionbox}

\begin{definitionbox}[Classification Problems]
In \textbf{classification}, the response variable $Y$ is \textbf{qualitative} (categorical).

\textbf{Examples:}
\begin{itemize}
    \item Predicting whether an email is spam or not spam (binary)
    \item Predicting whether a patient has heart disease (Yes/No)
    \item Predicting which major a student will choose (CS/Stats/Other)
    \item Predicting handwritten digits (0-9)
\end{itemize}

\textbf{Models:} Logistic regression, k-NN (for classification), decision trees, neural networks
\end{definitionbox}

\begin{examplebox}[Regression vs. Classification Question]
Consider a medical dataset with patient information:

\textbf{Regression question:} ``What will this patient's maximum heart rate be?''
\begin{itemize}
    \item Answer: A number (e.g., 152 bpm)
\end{itemize}

\textbf{Classification question:} ``Does this patient have heart disease?''
\begin{itemize}
    \item Answer: A category (Yes or No)
\end{itemize}
\end{examplebox}

%========================================================================================
\newsection{Why Not Linear Regression for Classification?}
%========================================================================================

A natural first thought might be: ``Can't we just use linear regression for classification by encoding categories as numbers?'' Let's see why this fails.

\subsection{Problem 1: Multi-class Encoding Creates False Ordering}

Suppose we want to predict a student's major with three categories: CS, Statistics, and Other. We might encode these as:
\begin{itemize}
    \item $Y = 1$ if CS
    \item $Y = 2$ if Statistics
    \item $Y = 3$ if Other
\end{itemize}

\begin{warningbox}
\textbf{The Problem:} Linear regression assumes numerical relationships between $Y$ values!

When we fit a linear regression, the model interprets:
\begin{itemize}
    \item The ``distance'' from CS (1) to Statistics (2) is the same as from Statistics (2) to Other (3)
    \item A one-unit change in $X$ corresponds to moving one ``step'' on this scale
\end{itemize}

This is \textbf{completely meaningless} for categorical data! The categories have no inherent numerical order or spacing.

If we had encoded differently ($Y = 1$ for Other, $Y = 2$ for CS, $Y = 3$ for Statistics), we'd get a completely different model!
\end{warningbox}

\subsection{Problem 2: Binary Predictions Can Exceed [0, 1]}

Even with binary classification (only two categories), linear regression fails in a different way.

Consider predicting heart disease (Yes = 1, No = 0) based on maximum heart rate. If we fit a linear regression $Y = \beta_0 + \beta_1 X$, we can interpret $\hat{Y}$ as the ``probability'' that $Y = 1$.

\begin{warningbox}
\textbf{The Problem:} Probabilities must be between 0 and 1!

A straight line has no bounds. For extreme values of $X$:
\begin{itemize}
    \item \textbf{Very low heart rate:} Model might predict $P(\text{heart disease}) = 1.1$ (110\%)
    \item \textbf{Very high heart rate:} Model might predict $P(\text{heart disease}) = -0.2$ (-20\%)
\end{itemize}

These predictions are mathematically and logically invalid!
\end{warningbox}

\begin{examplebox}[Linear Regression for Heart Disease]
If we fit a linear regression to predict heart disease from maximum heart rate, we get:
\begin{itemize}
    \item A downward sloping line (higher heart rate $\to$ lower probability)
    \item But for patients with very low heart rates (e.g., 50 bpm), the model predicts probabilities $> 1$
    \item For very fit athletes with high heart rates (e.g., 200 bpm), the model predicts negative probabilities
\end{itemize}

\textbf{We need a function that ``squashes'' predictions to stay between 0 and 1!}
\end{examplebox}

%========================================================================================
\newsection{Logistic Regression: The Solution}
%========================================================================================

\subsection{The Sigmoid Function}

Instead of predicting probabilities directly with a line, we use an \textbf{S-shaped curve} that is naturally bounded between 0 and 1.

\begin{definitionbox}[Sigmoid (Logistic) Function]
The \textbf{sigmoid function} maps any real number to a value between 0 and 1:
\begin{equation}
\sigma(z) = \frac{1}{1 + e^{-z}} = \frac{e^z}{1 + e^z}
\end{equation}

\textbf{Properties:}
\begin{itemize}
    \item As $z \to +\infty$: $\sigma(z) \to 1$
    \item As $z \to -\infty$: $\sigma(z) \to 0$
    \item At $z = 0$: $\sigma(0) = 0.5$
    \item The function is \textbf{monotonically increasing}
    \item Output is always in $(0, 1)$
\end{itemize}
\end{definitionbox}

\subsection{The Logistic Regression Model}

In logistic regression, we model the \textbf{probability} that $Y = 1$:

\begin{equation}
P(Y = 1 | X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
\end{equation}

Or equivalently:
\begin{equation}
P(Y = 1 | X) = \frac{e^{\beta_0 + \beta_1 X}}{1 + e^{\beta_0 + \beta_1 X}}
\end{equation}

\begin{summarybox}
\textbf{Logistic Regression in Two Steps:}
\begin{enumerate}
    \item \textbf{Linear part (same as linear regression):} Compute $h = \beta_0 + \beta_1 X$
    \item \textbf{Sigmoid transformation:} Pass $h$ through the sigmoid to get probability $p = \sigma(h)$
\end{enumerate}

The sigmoid ``squashes'' the linear prediction to always be between 0 and 1.
\end{summarybox}

\subsection{Understanding the Shape}

The parameters $\beta_0$ and $\beta_1$ control the shape of the S-curve:

\begin{itemize}
    \item $\beta_0$ (intercept): Controls the \textbf{horizontal position} of the curve
    \begin{itemize}
        \item Larger $\beta_0$ shifts the curve to the left (higher probabilities for the same $X$)
        \item Smaller $\beta_0$ shifts the curve to the right
    \end{itemize}

    \item $\beta_1$ (slope): Controls the \textbf{steepness} of the curve
    \begin{itemize}
        \item Larger $|\beta_1|$ makes the transition from 0 to 1 sharper
        \item Smaller $|\beta_1|$ makes the transition more gradual
        \item Positive $\beta_1$: Probability increases as $X$ increases
        \item Negative $\beta_1$: Probability decreases as $X$ increases
    \end{itemize}
\end{itemize}

%========================================================================================
\newsection{Interpreting Logistic Regression: Odds and Log-Odds}
%========================================================================================

\subsection{The Problem with Direct Interpretation}

With linear regression, interpretation is simple: ``A one-unit increase in $X$ is associated with a $\beta_1$ change in $Y$.''

With logistic regression, the relationship between $X$ and $P(Y=1)$ is non-linear (S-shaped), so this simple interpretation doesn't work.

\subsection{Odds}

\begin{definitionbox}[Odds]
The \textbf{odds} of an event is the ratio of the probability of success to the probability of failure:
\begin{equation}
\text{Odds} = \frac{p}{1-p}
\end{equation}

where $p = P(\text{success})$.
\end{definitionbox}

\textbf{Examples:}
\begin{itemize}
    \item If $p = 0.5$ (50\% chance): Odds = $\frac{0.5}{0.5} = 1$ (1:1, or ``even odds'')
    \item If $p = 0.8$ (80\% chance): Odds = $\frac{0.8}{0.2} = 4$ (4:1, ``4 to 1'')
    \item If $p = 0.2$ (20\% chance): Odds = $\frac{0.2}{0.8} = 0.25$ (1:4)
\end{itemize}

\textbf{Intuition:} Odds tell you ``how many times more likely is success than failure?''

\subsection{Log-Odds (Logit)}

\begin{definitionbox}[Log-Odds / Logit]
The \textbf{log-odds} (also called \textbf{logit}) is the natural logarithm of the odds:
\begin{equation}
\text{logit}(p) = \ln\left(\frac{p}{1-p}\right)
\end{equation}
\end{definitionbox}

\textbf{Key properties:}
\begin{itemize}
    \item Log-odds can be any real number ($-\infty$ to $+\infty$)
    \item When $p = 0.5$: log-odds = $\ln(1) = 0$
    \item When $p > 0.5$: log-odds $> 0$
    \item When $p < 0.5$: log-odds $< 0$
\end{itemize}

\subsection{The Log-Odds Formulation}

Here's the key insight: if we solve the logistic regression equation for the linear part, we get:

\begin{equation}
\ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X
\end{equation}

\begin{summarybox}
\textbf{Logistic Regression Models Log-Odds Linearly!}

The log-odds of success is a \textbf{linear function} of the predictors. This is why logistic regression is still considered a ``linear'' model.
\end{summarybox}

\subsection{Interpreting Coefficients}

\begin{infobox}
\textbf{$\beta_1$ Interpretation (Log-Odds):}

A one-unit increase in $X$ is associated with a $\beta_1$ \textbf{additive change} in the log-odds of $Y = 1$.

\textbf{$e^{\beta_1}$ Interpretation (Odds Ratio):}

A one-unit increase in $X$ is associated with the odds being \textbf{multiplied by} $e^{\beta_1}$.
\end{infobox}

\begin{examplebox}[Heart Disease Model]
Suppose we fit a logistic regression to predict heart disease from maximum heart rate and get:
\begin{equation}
\ln\left(\frac{P(\text{HeartDisease})}{1 - P(\text{HeartDisease})}\right) = 6.325 - 0.0434 \times \text{MaxHR}
\end{equation}

\textbf{Interpreting $\beta_1 = -0.0434$:}

\textbf{Log-odds interpretation:}
\begin{itemize}
    \item For each 1 bpm increase in maximum heart rate, the log-odds of heart disease \textbf{decrease} by 0.0434.
\end{itemize}

\textbf{Odds ratio interpretation:}
\begin{itemize}
    \item $e^{-0.0434} \approx 0.957$
    \item For each 1 bpm increase in maximum heart rate, the odds of heart disease are \textbf{multiplied by 0.957} (i.e., decrease by about 4.3\%).
\end{itemize}

\textbf{Intuition:} Higher maximum heart rate is associated with lower risk of heart disease.
\end{examplebox}

\begin{warningbox}
\textbf{Special Values of $\beta_1$:}

\begin{itemize}
    \item $\beta_1 = 0$: $e^0 = 1$ (odds multiplied by 1 = no change) $\to$ \textbf{No association}
    \item $\beta_1 > 0$: $e^{\beta_1} > 1$ (odds increase) $\to$ \textbf{Positive association}
    \item $\beta_1 < 0$: $e^{\beta_1} < 1$ (odds decrease) $\to$ \textbf{Negative association}
\end{itemize}
\end{warningbox}

%========================================================================================
\newsection{Estimating Logistic Regression: MLE}
%========================================================================================

\subsection{The Probabilistic Perspective}

In linear regression, we assumed errors were normally distributed and minimized MSE (equivalent to maximizing Gaussian likelihood).

In logistic regression, our outcomes are binary (0 or 1), so we use a different distribution.

\begin{definitionbox}[Bernoulli Distribution]
A \textbf{Bernoulli random variable} $Y$ takes value 1 with probability $p$ and value 0 with probability $1-p$:
\begin{equation}
P(Y = y) = p^y (1-p)^{1-y}
\end{equation}

This is like a single coin flip with probability $p$ of heads.
\end{definitionbox}

\subsection{Likelihood Function}

For logistic regression, we assume each observation $y_i$ comes from a Bernoulli distribution with probability $p_i$ that depends on $X_i$:
\begin{equation}
p_i = P(Y_i = 1 | X_i) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_i)}}
\end{equation}

The \textbf{likelihood} of observing our entire dataset is:
\begin{equation}
L(\beta) = \prod_{i=1}^{n} p_i^{y_i} (1-p_i)^{1-y_i}
\end{equation}

\textbf{Intuition:}
\begin{itemize}
    \item If $y_i = 1$ (actual success), we want $p_i$ to be high
    \item If $y_i = 0$ (actual failure), we want $1 - p_i$ to be high
    \item Good parameters make observed successes have high predicted probabilities
\end{itemize}

\subsection{Maximum Likelihood Estimation}

\begin{definitionbox}[Maximum Likelihood Estimation (MLE)]
MLE finds the parameter values $\hat{\beta}$ that \textbf{maximize} the likelihood of observing the data we actually observed:
\begin{equation}
\hat{\beta}_{MLE} = \arg\max_\beta L(\beta)
\end{equation}
\end{definitionbox}

\subsection{Log-Likelihood and Binary Cross-Entropy}

Products are hard to work with, so we take the logarithm:
\begin{equation}
\ell(\beta) = \log L(\beta) = \sum_{i=1}^{n} \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]
\end{equation}

Since computers typically \textbf{minimize} functions, we minimize the \textbf{negative} log-likelihood:

\begin{definitionbox}[Binary Cross-Entropy Loss]
The \textbf{binary cross-entropy} (BCE) loss is the negative log-likelihood:
\begin{equation}
\text{BCE} = -\sum_{i=1}^{n} \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]
\end{equation}

This is the loss function that logistic regression minimizes.
\end{definitionbox}

\begin{summarybox}
\textbf{Comparison: Linear vs. Logistic Regression}

\begin{center}
\begin{tabular}{lcc}
\toprule
& \textbf{Linear Regression} & \textbf{Logistic Regression} \\
\midrule
Response type & Continuous & Binary (0/1) \\
Distribution assumption & Normal & Bernoulli \\
Loss function & MSE & Binary Cross-Entropy \\
Estimation method & Closed-form OLS & Iterative optimization (MLE) \\
\bottomrule
\end{tabular}
\end{center}
\end{summarybox}

\begin{warningbox}
\textbf{Why No $\sigma^2$ Parameter?}

In linear regression, we estimate $\beta_0$, $\beta_1$, and $\sigma^2$ (error variance).

In logistic regression, we only estimate $\beta_0$ and $\beta_1$. Why?

\textbf{Answer:} The variance is determined by the Bernoulli distribution itself! If $P(Y=1) = p$, then $\text{Var}(Y) = p(1-p)$. There's no separate ``noise'' parameter---the randomness comes from the binary nature of the outcome.
\end{warningbox}

%========================================================================================
\newsection{Multiple Logistic Regression and Decision Boundaries}
%========================================================================================

\subsection{Multiple Logistic Regression}

Just like linear regression extends to multiple predictors, so does logistic regression:
\begin{equation}
\ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p
\end{equation}

\textbf{Interpretation:} $e^{\beta_j}$ is the multiplicative change in odds for a one-unit increase in $X_j$, \textbf{holding all other predictors constant}.

\subsection{Making Predictions: From Probability to Class}

Logistic regression outputs a \textbf{probability}. To make a classification decision, we need a \textbf{threshold}:

\begin{itemize}
    \item Default threshold: $t = 0.5$
    \item If $P(Y = 1 | X) \geq t$: Predict class 1
    \item If $P(Y = 1 | X) < t$: Predict class 0
\end{itemize}

\subsection{Decision Boundaries}

\begin{definitionbox}[Decision Boundary]
The \textbf{decision boundary} is the set of points where the model is exactly undecided---where $P(Y=1) = 0.5$.

For logistic regression, $P(Y=1) = 0.5$ when the log-odds equal 0:
\begin{equation}
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p = 0
\end{equation}
\end{definitionbox}

\subsection{Linear vs. Non-linear Boundaries}

\textbf{Linear boundary (default):}

With basic logistic regression, the decision boundary is a \textbf{linear equation} in the features. In 2D ($X_1, X_2$), this is a \textbf{straight line}.

\textbf{Non-linear boundary:}

To create curved decision boundaries, add \textbf{polynomial} or \textbf{interaction} terms:
\begin{equation}
\ln\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \beta_4 X_1^2 + \beta_5 X_2^2
\end{equation}

Setting this equal to 0 gives a \textbf{quadratic equation} in $X_1$ and $X_2$, which can produce \textbf{circles, ellipses, or hyperbolas}.

\begin{warningbox}
\textbf{The Model is Still ``Linear''!}

Even with polynomial features, we still call this a ``linear model'' because it's linear in the \textbf{parameters} ($\beta$'s).

The trick is that we create non-linear \textbf{features} ($X_1^2$, $X_1 X_2$, etc.) and then do linear regression on those features.
\end{warningbox}

%========================================================================================
\newsection{Review Section: Hypothesis Testing and Permutation Tests}
%========================================================================================

This section reviews key concepts for the midterm.

\subsection{Hypothesis Testing Framework}

\textbf{The Five Steps:}
\begin{enumerate}
    \item \textbf{State hypotheses:}
    \begin{itemize}
        \item $H_0$: No effect ($\beta_1 = 0$)
        \item $H_A$: There is an effect ($\beta_1 \neq 0$)
    \end{itemize}
    \item \textbf{Choose test statistic:} Usually $t = \hat{\beta}_1 / SE(\hat{\beta}_1)$
    \item \textbf{Compute test statistic} from data
    \item \textbf{Calculate p-value:} Probability of seeing a result this extreme if $H_0$ is true
    \item \textbf{Make decision:} Reject $H_0$ if p-value $< \alpha$ (usually 0.05)
\end{enumerate}

\subsection{Permutation Tests}

When t-test assumptions (normality, constant variance) are violated:

\begin{enumerate}
    \item Compute the observed test statistic (e.g., $\hat{\beta}_1 = 0.589$)
    \item \textbf{Shuffle $Y$} randomly while keeping $X$ fixed (simulates $H_0$: no relationship)
    \item Fit model to shuffled data, record $\hat{\beta}_1^{*}$
    \item Repeat 1000+ times to build the \textbf{null distribution}
    \item P-value = proportion of permuted statistics as extreme as observed
\end{enumerate}

\subsection{Bootstrap vs. Permutation}

\begin{center}
\begin{tabular}{lcc}
\toprule
& \textbf{Bootstrap} & \textbf{Permutation Test} \\
\midrule
Purpose & Estimation (confidence intervals) & Hypothesis testing (p-values) \\
Sampling & With replacement (pairs) & Without replacement (shuffle $Y$) \\
Assumption & Data represents population & $H_0$ is true \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Interaction Terms}

In a model like \texttt{price $\sim$ sqft * type}:

\begin{itemize}
    \item \textbf{Main effect for sqft:} Effect of sqft for the \textbf{reference category}
    \item \textbf{Interaction term (sqft:type):} How the sqft effect \textbf{differs} for other categories compared to reference
    \item \textbf{Total effect} for category $k$: Main effect + interaction term for $k$
\end{itemize}

\subsection{Confidence vs. Prediction Intervals}

\begin{itemize}
    \item \textbf{Confidence interval:} Where we think the \textbf{mean} response lies (narrower)
    \item \textbf{Prediction interval:} Where a \textbf{new individual} response would lie (wider, includes $\epsilon$)
\end{itemize}

%========================================================================================
\newsection{Implementation in Python}
%========================================================================================

\subsection{Logistic Regression with sklearn}

\begin{lstlisting}[style=pythonstyle]
from sklearn.linear_model import LogisticRegression

# Prepare data
X = df[['MaxHR']]  # Must be 2D array
y = df['HeartDisease']  # Binary: 0 or 1

# Fit logistic regression (no regularization)
logreg = LogisticRegression(penalty=None)  # Note: use None, not 'none'
logreg.fit(X, y)

# Get coefficients
print("beta_1 (coefficient):", logreg.coef_[0][0])
print("beta_0 (intercept):", logreg.intercept_[0])

# Make predictions
probabilities = logreg.predict_proba(X)  # P(Y=0) and P(Y=1)
predictions = logreg.predict(X)  # Class labels (0 or 1)
\end{lstlisting}

\subsection{Permutation Test Implementation}

\begin{lstlisting}[style=pythonstyle]
import numpy as np
from sklearn.linear_model import LinearRegression

# Observed statistic
model = LinearRegression().fit(X, y)
observed_beta = model.coef_[0]

# Permutation test
n_perms = 1000
perm_betas = []

for _ in range(n_perms):
    # Shuffle Y (breaks any real relationship)
    y_shuffled = np.random.permutation(y)

    # Fit model to shuffled data
    model_perm = LinearRegression().fit(X, y_shuffled)
    perm_betas.append(model_perm.coef_[0])

# P-value: proportion of permuted betas as extreme as observed
p_value = np.mean(np.abs(perm_betas) >= np.abs(observed_beta))
\end{lstlisting}

%========================================================================================
\newsection{Key Takeaways}
%========================================================================================

\begin{summarybox}
\textbf{Why Classification Needs Different Methods:}
\begin{itemize}
    \item Linear regression for multi-class creates meaningless numerical ordering
    \item Linear regression for binary can predict probabilities outside [0, 1]
\end{itemize}

\textbf{Logistic Regression:}
\begin{itemize}
    \item Uses sigmoid function to squash predictions to (0, 1)
    \item Models log-odds as a linear function of features
    \item Coefficient interpretation: $e^{\beta_1}$ = multiplicative change in odds
\end{itemize}

\textbf{Estimation:}
\begin{itemize}
    \item Assumes Bernoulli distribution for binary outcomes
    \item Maximizes likelihood (minimizes binary cross-entropy)
    \item No $\sigma^2$ parameter---variance determined by $p(1-p)$
\end{itemize}

\textbf{Decision Boundaries:}
\begin{itemize}
    \item Linear model $\to$ linear boundary
    \item Add polynomial/interaction features $\to$ curved boundary
    \item Model is still ``linear'' in parameters
\end{itemize}
\end{summarybox}

%========================================================================================
\newsection{Quick Reference}
%========================================================================================

\begin{infobox}
\textbf{Logistic Regression Model:}
\begin{equation}
P(Y = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
\end{equation}

\textbf{Log-Odds Form:}
\begin{equation}
\ln\left(\frac{P(Y=1)}{1-P(Y=1)}\right) = \beta_0 + \beta_1 X
\end{equation}

\textbf{Odds Ratio:}
\begin{equation}
\text{Odds} = \frac{p}{1-p}, \quad \text{OR} = e^{\beta_1}
\end{equation}

\textbf{Binary Cross-Entropy Loss:}
\begin{equation}
\text{BCE} = -\sum_{i=1}^{n} \left[ y_i \log(p_i) + (1-y_i) \log(1-p_i) \right]
\end{equation}

\textbf{Decision Boundary (threshold = 0.5):}
\begin{equation}
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots = 0
\end{equation}
\end{infobox}

\end{document}
