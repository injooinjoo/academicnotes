109 day 13 - YouTube
https://www.youtube.com/watch?v=Bo6LpR7x4uU

Transcript:
(00:29) All right. Hi, good morning. Looks like Canvas is feeling the weather too is down. So, for those of you online, I apologize that you can't watch live because we don't have that Zoom link. Hopefully, you will be able to consume that video otherwise. Uh announcements, we have lots of them.
(00:53) What's happening this week? Midterm insection. Okay. And so the first roughly half of today's lecture will be some review going through some ideas that will uh potentially apply on the midterm and the next homework. And then uh we'll get into the new stuff which is uh logistic regression and classification. Those notes even though Canvas is down, Ed is not.
(01:17) So you should be able to find everything through ED like normal. Okay. So if you don't have the link, ask a friend and hopefully they can send you the link. I just it was in my uh history um on my browser. Uh as far as the midterm goes, what are you allowed to bring? Two pages of reference notes front and back is fine. So the one from the quiz, one additional one.
(01:45) Okay? It covers all the material through last week and then the review at the beginning of today. Uh I would highly encourage you to start the homework if you haven't already. Read through the homework. Think about conceptualizing how to answer those problems on the homework. P set three because homework three because that's all the stuff that is also covered on the midterm.
(02:04) So make sure you at least think about how you would answer those problems. Okay, coding portion not in class coding portion at home two hours and they have from when to when Yeah. Yeah. Yeah. So you get a two-hour window which you can start Wednesday sometime after class will be open through the end of the weekend. Through the end of the weekend.
(02:35) So 2our window to answer some questions similar to like things you've been doing on the homeworks. Simple stuff. No scraping. Don't worry. No scraping. Um and what you've been doing in sections. Okay. We'll have you derive all sorts of things. um on the in-class portion just derive all of bays. I think that's probably what the exam will be. Just derive the results from bay. No, I'm kidding. We'll talk about it a little bit today.
(03:03) There is a practice a few practice quiz problems for you, too. So, step through those to get a feel for like some of the material how what kind of questions will be asked. Did you all get your for those who aren't individuals all get your project assignments? Yep. You will be assigned a TF eventually.
(03:28) If you submit it as a singleton, we got to do some matchmaking. So, it'll take us a little bit time. Oh, there are some matches already. Oh, I missed that. No, you're not following. didn't realize this and some people got assigned to the wrong group. If you did, we apologize. Uh so we just need to do some work on that to make sure that's all corrected.
(04:01) So just email us, helpline, post on ed privately, whatever. We'll find it. Great. Today's lecture just staring at Canvas. Just staring at Canvas. Let's see if I refresh again. Nope. What's the incident, do you think? Did they get hacked? Who knows? Ransomware. Yeah. Okay.
(04:30) Today we are turning over a leaf in the second half of today's class going into the new stuff that's not covered on the midterm that is all related to classification models. And so we'll define the standard parametric nonparametric models within the context of classification. So far, everything we've done, they talk about as regression problems in the machine learning world. Regression problems just means you're predicting a numeric response variable.
(04:56) Okay? From the machine learning perspective, from the statistical perspective, regression means that whole like uh rigomemer role of doing linear regression for now. Okay, we'll do first do a little bit of review and then quickly go through motivating classification and our first parametric model within that world called logistic regression. Um, and that's what we're covering today. Quick review. Hypothesis testing.
(05:21) When we do hypothesis testing, it's this formal framework to do what? In the context of regression. Why do we perform hypothesis testing? It's statistically probable. Well, statistical significance statistically probable are the results the coefficients from our model statistically significantly different from zero.
(05:55) Okay, is sort of the formal formality here. We're testing to see from beyond a reasonable doubt whether a predictor is associated with the response in the context of our model. And we set up this whole framework for competing hypothesis. We defined a null and alternative hypothesis in regression. That's usually beta equals 0 versus beta is not equal to zero as a null and alternative.
(06:19) There is no association. There is an association. We figure out what type of statistic we want to calculate. And so usually this will be the slope coefficient and we need to standardize it or we need to perform uh permutation uh methods to determine its significance.
(06:39) So we collect our data, we compute that test statistic and then we have to figure out how severe extreme is that observed test statistic in our data and we have to build the p value or often we'll calculate the p value either using probability theory or using resampling methods and that's going to be the permutation method.
(06:57) I'll step through it a little bit more carefully today. Okay. And then once you come to that conclusion, figuring out whether your p value is low, you decide whether you conclude in the context of your problem, do we have a significant result or not? Statistically significant means what? Phrase gets thrown around all the time.
(07:26) What does statistically significant mean? unlikely include a chance with the assumptions over conditional and no the chance of seeing results we see or not is really small. Statistically significant means as a conclusion we rejected the normal costs. We have evidence of a real association. Okay, it's usually have a scape when we calculate that big value. We said there's kind of two ways using probability theory.
(07:50) And so we'll base it on distributions, a t or a normal distribution, or we'll calculate that p value based on a permutation distribution, okay? Or your test statistic. When we calculate p values in regression using stats models, it's coming from a t distribution. Kevin's favorite distribution for multiple reasons. It's not the same in Python as it is in R.
(08:13) My favorite command in R is the cutie command. Haha. But the other reason I like the t distribution is it was derived by the head brewer of Guinness. And everybody loves Guinness, right? I have a little Irish culture here. But when we calculate our t distribution p values, we say, all right, let's imagine we get a t statistic that is three, positive3.
(08:36) And we say, oh, what's more extreme than positive three? Anything further out in the tail than positive three. And if we're thinking there's an association or not, what is also as extreme is anything beyond -3 out in the tail there. To calculate those probabilities, you can use the CDF of a t distribution coming from your Python code.
(09:01) Draw it out, visualize it, think about where that statistical significance is. Further out in the tails, the more extreme a result you have conditional on the null. Therefore, your null is likely wrong. Okay. And that's kind of the logic here. How far is far enough? Someone defined it at some point, right? When we build our intervals, what percentage intervals do we usually use? It's a blank blank confidence interval. A blank blank percent confidence interval. 95% confidence.
(09:38) Usually credible intervals 95%. And that 95% for most of these bell-shaped distributions is roughly between -2 and positive2. In a normal distribution, it's negative 1.96 and positive 1.96. And a t, it's a little bit further out because a t distribution has slightly longer, fatter tails.
(09:57) Okay? So two is the ballpark number. So if you're doing it by hand, just use the number two. All right? And so you do plus or minus two standard deviations to build your confidence interval when you're doing hypothesis testes testing. If you're outside of plus or two standard deviations, a t statistic greater than two in magnitude, you're likely to reject the null.
(10:16) Okay, great. Back of the envelope calculations. And so we do a hypothesis testing example. We fit a linear regression line to the scatter plot of data. We know it has issues, but stats models gives us output. Here's our output. And guess what? Stats models automatically does all of our calculations for us.
(10:35) Okay? It calculates the slope. It calculates its standard error and it calculates the t statistic giving us a p value related to the confidence interval. All right. And so if we were to formally write this all out, we would set up our hypothesis. The slope is equal to zero or not. We write out our test statistic. In this case, we're using the t test from our data, from probability theory, from our model. And that t test statistic is just simply the estimate over its standard error.
(11:05) Implicitly we're subtracting off zero because that's what the null hypothesis says it should be. And then we get that t statistic. Python does all the work for us. Okay, we get a p value and that probability. P value is the probability that any old t statistic with 590 degrees of freedom is going to be beyond 25
(11:26) .2. That t statistic and the probability is essentially numerically zero and we just change that last zero to a one and say it's less than that. Right? We have extreme statistical significance here. Choose any alpha error late 0.05. 01 0.001 and you're still going to reject the null. Our p value is low. We reject the null hypothesis.
(11:45) We have evidence to suggest that housing prices are truly associated with square footage. No s Sherlock, right? No problem. We knew that housing prices are related to the size of the house. We prove it beyond a reasonable doubt using statistical uh machinery with me. Okay. Do you trust that p value? Well, I trust that it's lower than 0.05. But when we calculated that p value, we relied on the t distribution.
(12:18) Probability theory. I don't rely on the t distri or I don't trust the t distribution here because the t distribution came out of the fact that we had all of these four assumptions in regression. We don't have those four assumptions here. We have clear heteroscadasticity.
(12:36) And so probably what we want to do is change our model. Rather than relying on the t distribution as the correct probability distribution to calculate the probabilities, we can do the permutation method or do some transformation of our data or use a different model. All right? And so we can change our model to calculate the p value in a more appropriate way.
(12:55) Okay? All right. And that's where the permutation test comes in. What is the permutation test? We talked about it. It's a method of reampling to perform testing rather than sampling. Resampling to perform interval estimation. That's the bootstrap permutation meth uh test is a little bit different. Okay.
(13:16) High level. What do you do? You sample without replacement and you repermute your data under the null hypothesis. And so you're just shuffling your rows. Either you're shuffling your rows of your x variables and leaving y constant or you're shuffling your y and leaving your x constant. Shuffling one variable is a lot easier.
(13:45) Okay, so in practice, you just shuffle Y and see how it relates to all of your X's in a model. Why do we do permutation rather than bootstrapping? Because permutation is a resampling method conditional on the null hypothesis. And when we do hypothesis testing, we do all of our calculations conditional on the null hypothesis. And so we mimic that in our approach. It doesn't have the same distributional assumptions, but it follows the same five steps that we had before.
(14:12) It's just when we calculate the p value, when we collect our data and estimate our p value in order to come to a conclusion, we're going to do that through permutation methods rather than through probability theory. It's the only difference. All right. So, how do we actually do it? Well, generally the null and alternative hypothesis in a permutation test is more broad.
(14:36) Okay. It's that there's an association or not. And really, it's sort of the distribution of outcomes is not related to the value of X. Why is it broader? Because it could lead to a significant result even if it's not just a association.
(14:55) All right? we can have a stronger claim on this null and alternative if we want to make a few assumptions. The assumption we have to make is that our observations are independent. And we're going to do the same essential test statistic except now we're going to just leave it in terms of beta 1 because we're going to allow the permutation method to automatically generate the distribution of our estimated beta ones.
(15:13) Okay? And that's sort of then you compare your uh observed test statistic to the tails of what these beta 1 hats would look like under that permutation idea. All right, a whole lot of context here just to simply say if you make a few assumptions then you can allow for the conclusion to be in terms of an association.
(15:38) And so what it breaks down to boils down to is we're still testing is there an association between X and Y. How do we actually perform this permutation test? So imagine you have a Y and an X. Here I simplified X to just be zeros and ones. And what you do is you have an original data set Y and X. And you shuffle either X or Y. In this case, we're shuffling X.
(16:01) And we're going to look to see if there's a difference between the two groups defined by the zeros and ones and X. By reshuffleling X and leaving Y as is, you're breaking the natural association in the D in the data. And since you're breaking that association, you would expect the distribution of Y to be the same no matter what value of X you have because we're just reshuffling the Y's across the X's or reshuffleling the X's with the Y's.
(16:34) And in that case then when you calculate the test statistic you care about in this case it's a sample mean difference you're going to see those sample mean differences bouncing around zero through this resampling. In linear regression when you reample your y's or shuffle your y's or shuffle your x's you're going to see slopes bouncing around zero because you've broken that association through re through the shuffling. Okay.
(17:01) So then what you do is once you calculate all of those possible theoretical repermuted uh test statistics, you just look to see where your observed statistic falls in that resulting distribution and you're going to look into the extreme tails. I think an example sort of makes sense. Here's the code. We have our x's.
(17:21) We have our y's. And what we do is I just do a simple for loop. Easy to do. And I just repermute my y variable based on the randomly sampled shuffles of my indices. And so every time I shuffle my indices from 0 to 580 or whatever it was, and then I create a new permuted y.
(17:46) And then I fit a model to see how my original x relates to my repermuted y. you would expect that slope to be estimated close to zero. Okay? And then what we do is we kind of uh just create my estimates of beta 1 ones and then we look at the distribution of what those beta 1's look like. What are we looking at here? We're looking at the reference distribution of the estimated beta 1 values, the slope values after reshuffleling, after repermuting.
(18:18) And so not surprisingly where is that distribution centered at zero. What did we observe in the actual data set? 0.58 something like that something close to 6. And then we say to figure out what our p value would be would be like all right through chances alone when there's no association what is the probability the chance of seeing a beta value estimated to beyond.
(18:48) 58 what's the p value estimate through these a thousand empirically estimated estimates under the null that probability is roughly zero this is just illustrating how extreme of a result we got it's not surprising yes there is a relationship ship.
(19:07) Our null distribution is centered at zero with a very tight standard error and we observe something so extreme. It's practically impossible if the null hypothesis is true and so we reject the null hypothesis of being reasonable with me. This is a very extreme result. Usually your null distribution will have tails that will somewhat approach the actual observed value. Question.
(19:34) Yeah, this with the distribution of the possible beta ones like the probabilistic regression model. Okay, so we can compare this to the probabilistic regression model. Okay, so the probabilist probabilistic perspective. What changes? I could recalculate this distribution of the beta ones I would expect under the null hypothesis.
(20:04) Okay, what would it look like? it'd still be centered at zero, but its standard error and the distribution we're using is either normal or t. And its standard error would be like the I think it was 023 was the number we pulled out of the model. And so what that's saying is if we trust that this is normally distributed that we would expect its standard error to be about 0023.
(20:24) And if we zoom in, this is just a zoom in of the picture on the left, we see that that seems fairly reasonable. Okay, though the standard error is a little bit bigger than that 0023 value. Okay, value or like measured null beta one, we it seems like we're comparing a single value of two these tests. Um is that okay if the standard error for beta one in our test hypothesis non hypothesis is kind of right.
(21:06) So the one and maybe I'm not understanding the nuance of the question. So the one estimate here is what we observe in our data. We build a whole reference distribution assuming the null to be true. Okay. And the one value we observe is so unlikely that we would throw out this assumption that the data value is truly zero. All right.
(21:27) And so we're using our data to reject theoretically what it would be under the mole. Okay. Now your question was I think when you did bootstrapping or when you build a confidence interval they are connected and so when you perform bootstrapping you say all right let's do it many many times sampling resampling with replacement from my data and then you would compare zero to that interval and they're going to be in agreement almost every time. Okay it's just the nuance when it's close to that 0.
(21:59) 05 005 level or close to the 95% confidence interval being close to zero. That's where you want to take the more careful permutation method. All right. Is that kind of getting to what you're saying? Now, the probabilistic formulas, the same thing kind of kind of holds too. All right. I trust this distribution more than the tbased p value calculation because now we did not have homoscadastic errors.
(22:28) And so we should trust the permutation method that doesn't rely on that fact. Okay, I'm glossing over a lot of nuance here, but that's okay. We're at least learning how to do some stuff in practice. Well, we did the one on the last zoom. Yeah. Yeah, it's the same thing.
(22:51) I just like here I can show you where our observed value is and here you're like it's not on the graph. Yeah, it's so it looks so narrow here. Does it look reasonably normal? Reasonably bell-shaped, reasonably tea distributed? Not perfect. Not perfect. It looks like maybe there's some sort of right skew and it's wider than what that theoretical t distribution would have expected.
(23:14) Okay, great. Okay, there's a package to do it using sci and Python automatically, but it's just easier to write like six lines of code and do it as a for loop to get it into this setting. It has to be under the right conditions. Your data has to be the right structure. It's just not worth your time. Just do a quick for loop to do it for you. Great.
(23:37) Maybe this is answering the question, what's the difference between permutation and bootstrap? When your the goal is doing something like calculating confidence intervals given your data you're trying to get your best set of estimates then use your data to do that. If what your goal is in this case uh yeah the goal booting is done to estimate while the permutation is done to check whether under a hypothesis a prespecified hypothesis your data is reasonable and so it's done under two different conditions. The implementation
(24:09) is kind of the same idea. The bootstrapping uh is done to perform when a null hypothesis is true or not. Whereas the permutation method is used assuming the null to be true. And so you have to just be a little bit careful. Your type one errors might not match up if you use a bootstrap confidence interval to perform a hypothesis test. That's my only uh warning here. Great.
(24:30) Uh inverting that bootstrap confidence interval uh is a reasonable thing to do, but could lead to that inflated or deflated type one error. What what is type one error? I talk about in stat 104 today. But what is type one error when you do testing? Rejecting the null hypothesis. So when it's actually true, what's called a false positive.
(24:56) And when we get to classification models, we'll talk about false positives, false negatives a bunch. Not today, but later. Okay, great. All right, another little bit of review. Interaction model. What do I do? This is for the regression model to predict how square foot relates to price.
(25:16) But now I think that relationship depends on the type of home I have. There are four different homes. All right. And so I fit a interaction model with those four different types. Condo, townhouse, blah blah blah. All right. And we get a whole regression model with all kinds of interaction terms. Why do we fit this model? What's the purpose of fitting this model when when you want to interpret things? Because it's fun, right? We have fun with our modeling. This is a fun model.
(25:53) At least it's fun for me. But what is the purpose of including interaction terms in a parametric model like linear regression? What's the interpretation of this interaction term 00543? It says that how one variable relates to the response depends on the values of another variable, another predictor. How X1 relates to Y depends on the value of X2 with me? And so what is X2 here? It's the type of home.
(26:33) What is X1? Square footage. How square feet relate to price depends on if the type of home you're dealing with? This value 06659. What does it mean for one unit change in X? That's the predicted change in Y holding everything else constant. And when you're holding everything else constant, you're not a square, you're not a multif family, you're not a single family, you're not a townhouse, you are a condo.
(27:06) And so this estimate 6659 is the relationship of square foot to price for condos specifically a little bit higher than the average of 0.589. Not surprising condos square footage is a lot means a lot. All right. And then all these different interaction terms negative.2863. What does that mean in practical terms? So 6659 combine these two terms. Exactly. Right.
(27:50) So you combine the 6659 and the -2863 to get the estimated effect of how square footage relates to price for multif family homes. It's a difference in those slopes. Square footage relates to price with a lower slope for multif family homes than it does for the reference group condos. All right. About 0.
(28:16) 4 $400 per square foot for a multif family home whereas it was $666. The devil for condos. No. Yes. Kind of with me. Okay. Taking a step back. Does it appear there's a true difference in relationships between square footage and price across these four different home types? Where can we look? That's a question of p values.
(28:46) And if so, if you look at the three different p values associated for the three different interaction terms, those p values, some of them are statistically significant. That tells us when you compare to the condos, watch out. These uh types of homes have different relationships. Which type of home is square footage the most important, the most predictive? Has the highest association? It's those single family homes. They're hot commodities in the Cambridge Somerville area.
(29:18) Okay, those big uh single family homes. Multif family homes, it doesn't matter so much with me kind of. All right. So, when you see one of these models, you need to write out that model statement. There's a lot of different interaction terms. Interpret those coefficient estimates. These are differences in slopes.
(29:37) How square footage relates to the price, how it's different for these other types of homes compared to the reference group of condo. Uh, and does it appear that there's an interaction effect? It's sort of the combination of these p values. But you have to also watch out for who your reference group is.
(29:52) If you change your reference group, you might get different significant results. Not for the context of this class, but if you've taken another stat class, that's an F test more formally. Okay, with me? Be ready for questions about interaction terms on the midterm. Be ready for questions about interpreting permutation distributions on the midterm. Be ready for performing hypothesis tests and confidence intervals on the midterm.
(30:18) These are all interpretations that are so important. Pablo's warned you already. When Kevin teaches, he says, "I interpretations, interpretations, interpretations. It's the truth." This is what I think is important in modeling. I don't care about predicting accurately. I care about the relationships in my data. I'm a statistician.
(30:37) Okay, one last thing to highlight from before. What are we looking at here? Presumably, it's a simple linear regression model. And we put intervals around that line. All right? and we put bands around th that line. There are two different types of bands here. What do they reflect? The wider bands we call the prediction interval.
(31:07) The narrower bands we call the confidence interval. The goal of the interpretation, the goal of the inference is different. The narrower bands are used to say where we think the true population line is for any value of X. The wider bands are estimates or interval estimates for we think the individual observations will lie in the population.
(31:40) Right? And so as you sample more data, as your sample size increases, these narrower bands for where we think the mean of Y is given X or where we think F is given X get narrower and narrower and narrower. If you had an infinite sample size, that narrower interval would be infantestimally narrow. The wider interval won't ever go away. Why not? There's always going to be that epsilon like you and assuming you don't have all squared R square of one perfect predictions, you're going to have the irreducible error around your model's predictions.
(32:23) And so that extra width in around that confidence interval which the prediction interval provides represents the extra irreducible error in your observations around the estimate or the line. Okay? And so that will never completely go away unless you have a perfect model with me sort of. Okay.
(32:49) Three different types of intervals we often calculate in regression. We have a confidence interval for a coefficient estimate. We have a confidence interval for doing predictions shown here, the narrower band. And we have the prediction interval for doing future predictions for a single observation, the wider band shown here. All right. We also have credible intervals.
(33:09) Oo, what's a credible interval? observation perspective on a parameter. You're trying to predict or describe where you think reasonable values of the parameter are. It's just like a confidence interval for that slope coefficient. It's almost no different. Okay? It just incorporates prior belief or combines different things. When you're predicting yhat, watch out. R squ plays a role.
(33:44) Doing predictions sometimes isn't just fitting lines. It's fitting curves or in this case Rex Thor the dog bearer. I love little X XKCD cartoons. Okay, that's kind of the end of the summary of stuff we've done so far. The little bit of review. We're going to get into new models now.
(34:10) So you can think of everything Kevin talked about just now is fair game on the midterm. All right. And so I'm just trying to kick off your understanding, thinking about hopefully you've already done some reading for the midterm. You could follow along as we played. Okay, new stuff. Classification. With classification comes logistic regression. And so we'll try to motivate what logistic regression is.
(34:33) How is classification different than prediction that we've been doing so far? Than regression problems that we've been doing so far. In regression problems, we had linear regression. Along with linear regression came ridge lasso. What was the other type of model we have used so far? K nearest neighbors.
(34:58) Right? And what's the difference between K nearest neighbors and linear regression at a high level? linear regression something that's parametric we have these parameters we're trying to estimate them we put distributions to things whereas when we do KN&N there is no parameterized model underlying it you're just finding your nearest neighbors and using them to aid in prediction we can take those two types of models from linear regression and regression problems into the classification setting and so we'll talk about those both today and on Wednesday, right? Linear regression becomes logistic regression. KN&N becomes KN&N. No change there. It's
(35:38) just the machinery is a little bit different. All right. So, classification. Back to that question. What is classification? Why do we have a whole different set of models when we're dealing with a classification problem? Well, we're trying to predict something that's categorical.
(36:04) That's exactly the $64,000 million question always or answer to it is all the methods we've done so far are dealing with predicting a numeric response and so things are measured numerically and not just zeros and ones. Now we're predicting something that has categories to it and so our machinery has to change. the data we were talking about first when we talked about linear regression models.
(36:23) We talked about predicting sales from three different sources of ads that were used and we had three different predictors. We had a data structure that was tabular rectangular n rows p columns that's coming with us.
(36:43) But now what we're changing is this y variable that we assume to be sort of numeric or even better yet continuous into something now that's categorical. Right? The setting where this gets applied all the time and why I am really comfortable in thinking about these models is because in the world of medicine doctors like everything to be boiled down into categories. Okay? And sometimes it makes sense like you're trying to predict whether someone has disease or not.
(37:07) If that's what you care about, then we need to do a classification model. All right? someone comes into the ER and you do a whole lot of measurements on that someone that are really easy to take and you're trying to predict that outcome. What type of disease does that person have? Well, we could do a simple thing and say, "All right, is this person having a heart attack or not?" And so, we could measure all sorts of things as they enter the ER that are really easy to measure from test from various different bi uh biological medical tests and trying to predict whether or not they're having a
(37:40) heart attack. Okay. Or have uh heart disease in some way. So our x's are still the same. We still have n rows. We still have p predictors. We have an extra response, but it's now indicating whether or not someone has heart disease based on other medical tests with me.
(38:06) Why can't we just fit linear regression? We have a response variable. We could turn it into something numeric. How would you turn that response variable into something numeric? For the predicted variable zero, one and so that response variable y, the thing you're trying to predict, the predicted thing just turned it into zeros and ones. need more and then in regression we have to worry a little bit about learning and things of that nature. Yeah, totally true. Totally true.
(38:47) Okay, so what is classification? When your response variable is no longer quantitative, predicting the temperature outside instead of it's now classification. Is it going to be hot or cold? Is it going to snow or not? Is it going to rain or not? Did you need your umbrella today or not? Okay, based on the weather forecast, hopefully you looked at it. You would have brought your weather your umbrella today. Okay, great.
(39:06) This applies in all sorts of different domains a lot of the times in healthcare because like I said, doctors love things to be simplified to yes no questions or yes no responses. Uh and then it happens in the world of finance, happens in the world of sports, happens in the world of tech.
(39:24) And so these classification models are used just as often as those numeric outcome problems. We want to determine whether a startup is worth investing in. Yes, no. Should we invest? We want to predict whether uh a patient has different types of disease. It might be just more than two categories. It might be multiclass. We want to determine if a user is likely to click on an advertisement.
(39:42) Let's make that advertisement nice and fancy. Uh and we want to determine whether an image is of many different classes or uh a real one or a fake one. Becoming more and more relevant these days, right? You see something on Instagram. Is that a real image? I see it all the time in the world of sports. people post a video and it's just a madeup uh AI video.
(40:05) Okay, classification. We talked a little bit about why not linear regression. Let's just uh reiterate that. Let's say we have a response variable that has three categories. Someone is going to major in either CS, statistics, or something else.
(40:22) Within the world of data science, that's all that matters, right? Either you're CS, your stat, or you're somebody else. No, I'm kidding. I'm kidding. I'm kidding. you're all being treated the same. Okay. How could we try to fit linear regression models? The machine we have so far, we have three categories. We can take that variable Y and turn it into a one two three C variable or a 012 variable or a 0100 200 variable. I don't care.
(40:47) But we can turn it into three numerically measured variables or possibilities. Very discreet, not continuous. But we could use linear regression to model this. All right. And of course, everybody else is better than stat who's better than CS. Kidding. Kidding. All right. But the order that we actually define Y to be will be important when we do linear regression.
(41:12) Those coefficient estimates in linear regression says a one unitit change in x relates to a beta 1 change in y where is associated with a beta 1 change in y and that beta 1 change in y if you're going from two to three or you're going from one to two should be interpreted to be essentially the same. All right.
(41:36) And so here, how we set it up in terms of this ordinal categorical response variable, how we set up the order of those categories would affect that regression model. Okay, it's a bad thing. We don't want to do it h uh half-hazardly like this. We could measure all kinds of variables to predict what uh concentration or major somebody is. You can look at grades. You can look at the courses to taken.
(42:00) You could say how late do they stay up at night? How much time do they spend on their computer? How much gaming do they do? Uh, do they do they take stat 110? Are they Kevin's favorite? I mean, it just kind of depends on what predictors you want to use to predict whether or not someone is in which concentration. And we already talked a little bit about the problem here.
(42:19) The order 1 2 3 is a problem because changing from y= 1 to y 2 siesta stat is not the same as changing from two to three stat to everyone else. And so those changes in the response aren't consistent. You can change who's your one and your two. That categorical response variable is nominal in nature.
(42:44) Sometimes your categorical response variable might have some order to it. And if you do, there might be a reason to do something like linear regression. Okay, order just means your categories truly categorical, but there is an order. Some sort of either hierarchy or just time component here.
(43:03) Freshman, sophomore, junior, senior as the example. Lyker scales. Everybody heard of a Lyker scale before? You know what that is? Lyker scale surveys. Ever fill out a survey and they ask you, do you agree or disagree? And they give you like seven categories.
(43:26) typically is the Lyker scale, right? I think it's seven and then or it could be five categories, severely disagree, neutral in the middle, severely ugree at the high end. And they take those categories and turn them into one to sevens. And those one to sevens then they can fit linear regression models to in a really poor fashion. Try not to do that. Okay? It's truly categorical.
(43:45) Will fit other types of models to those. Great. Why not linear regression? Yeah. Yeah. Yeah. We talked about this already. Why not linear regression when we have a zero and one? Because issues can occur. What we're essentially going to do is talk about modeling the probability now that y equals 1. And so when we draw that linear regression line, its predictions would give us probabilities if we fit it to a response variable that is ones for success and zeros for failures.
(44:17) All right. We could also turn that into predicting classification whether someone truly has heart disease or not just by estimating the probability. Is it a greater than.5 or less than.5? Okay. And so that's taking the probability estimates and converting that into categorizations classifications. Okay. And that can cause issues too. All right.
(44:41) So what can go wrong with this linear regression model? Here's the real data. We're trying to predict whether or not someone has heart disease based on their maximum heart rate. We fit a linear regression model. We see a negative relationship. We're fitting a line to a scatter plot. It's just the y's are zeros and ones. All right.
(45:12) But what's the issue? Johnson has point4 of a heart disease. Yeah, John Johnson has point4 of a heart disease, but it's really yes, the person either has heart disease or not. We're at least predicting the probability that person has heart disease. Okay.
(45:32) But Kevin Rder who has a max heart rate maybe of let's just say down here 50 or Pablo has a max heart rate of 50 or 75. Poor Pablo. We'd predict the probability that Pavlo has heart disease to be 110%. That doesn't sound good. All right, we don't want our model to have that. Yeah, he's comeomaosse over here. All right. Or if someone has a max heart rate that's super high, you know, some really competitive athlete, we'd predict what's that? Yeah.
(45:55) Yeah. Doing too many lectures. Uh have a higher max heart rate, you predict their probability to be negative. And so what we want is a functional form in our model so that the output of that model is giving us probabilities that are bounded between zeros and ones. Okay, instead of a line fit to the scatter plot of points, what do we want to fit? A curve of some sort.
(46:23) What do we want for that curve to have? What properties? To be bounded between zero and one. Great. Okay. What shape of a curve is bounded between zero and one? Technical term fshaped or z-shaped. We want something that asmmptotes to one in one side and asmmptotes to zero on the other side. An Sshaped curve. So we're taking this line instead of fitting a linear regression model, we're fitting an S-shaped regression model.
(46:55) Okay? Called logistic regression. Great. And so that's essentially where we're going is instead of fitting a line to predict Y from X and Y here we're going to interpret as probabilities okay of Y being one instead we're going to fit an S-shaped curve and we just need to define that functional form. Okay I know some of you have seen logistic regression before but what is the functional form here? What type of mathematical functions have that S shape? A sigmoid. Yeah, a sigmoid function. What is a sigmoid function defined as?
(47:37) Everybody hear that before? Sigmoid function. Well, the sigmoid function is this. Sorry, animation got me. We're going to map probabilities in terms of the sigmoid function of 1 over 1 + e to the negative junk as I call it negative h as is uh written here.
(48:07) Okay, what I mean by junk is we're putting a linear model to not y or not to the probabilities of y directly but some functional transformation of that probability. Okay, it's either the functional transformation of the probabilities or the functional transformation of the linear portion of it. Okay, they're just inverse functions.
(48:33) And so what we're saying is that the probability that y = 1 is going to be related to x through this functional form. Okay. 1 / 1 + e to the junk. All right. Now let's take a step back. Why is this a reasonable function? It's sshaped. Okay. What's the most extreme value it can take on? If my betas are really really big or my beta 0 plus beta 1x term combined is really really big. Let's imagine it's infinitely big.
(49:05) What does your probability estimate be? Or what will your probability estimate be? E to the infinity. What's e to the negative infinity? zero. And so you get 1 over 1 plus 0. You're estimating your probability to be one. Okay? So when my betas are big, I'm estimating the probability of success to be one.
(49:34) What about if my functional linear term is really really small? In fact, negative infinity. Well, e to the negative infinity is e to the infinity. And e to the infinity is infinity. And so what's 1 over 1 plus infinity? Zero. So it has those bounds that we want that when beta 0 plus beta 1 the linear term is really big it becomes one. When it's really small the result becomes close to zero.
(50:02) Okay? It's got that functional sshaped. We've now converted our probabilities to be restricted in the range of values between zero and one. And so we're going to write our logistic uh regression model as such. The probability that y equals 1 has the sigmoid form. That's the form that Pavlo likes. I like to write it in the middle form. Mathematically, these are equivalent.
(50:26) E to the junk over 1 plus e to the junk is equivalent to 1 over 1 plus e to the negative junk. Okay. Why do I like to write it this way? Because I love writing betas in latte. It's just a lot of fun, you know. Why do I write it this way? A probability is equal to something over 1 + something. What is the something? It's an interpretable value.
(50:58) How many of you like to bet on the markets? Ever done sports betting before? They report the odds of sports betting. They don't report probabilities of sports betting. I mean, they do sometimes on the side, but they talk about odds all the time. And so odds has a natural feel. And by writing it this way, we're essentially saying we have the odds that yals 1 has this e to the junk form.
(51:27) Okay? And so now we're turning a logistic regression model that has the sigmoid shape into something that's at least reasonably interpret interpretable in terms of odds. Okay. So, we're going to start talking about odds a little bit. What are the parameters in this model? What are the unknown values that we care to estimate? X is measured in your data. Y is measured in your data.
(51:51) The betas are the parameters we're trying to estimate. Okay? Beta 1 trying to estimate the association. Beta 0 the intercept. There's two. And those beta zeros and beta 1 ones determine the shape of that S shape. Beta 0ero basically is telling you where how it shifted. Beta 1 is telling you how steep it is.
(52:18) Okay? And so if you just watch a little animation here, it kind of gives you a rough sense of how to interpret that S shape with specific values of beta 0 and beta 1. If beta 0 is zero, you're at the horizontal line of 0.5 or the vertical line, it's the predicted probability is 0.5. Beta 0 going up, going down, beta 1 going up, going down tells you how steep that curve is. And if beta 1 is negative, it's the opposite direction.
(52:46) Okay, just playing around a little bit with those different values of beta 0 and beta 1 and what they represent with me kind of great. An alternative way to write this logistic regression model. Instead of the probability is equal to e to the junk over 1 plus e to the junk, we can resolve and just solve for the linear version on the right hand side.
(53:06) And on the left hand side, what we're left with is log of probability y= 1 over 1us probability that y equals 1. And now in words, what we're calculating is we're setting the log natural log of the odds of success to be of linear fashion. All right? And so it's still the same mathematical model.
(53:32) It's just interpreting it, writing it out ever so slightly differently. Okay? So essentially what we're doing is we're resetting the log gods to be a linear function of our features, of our predictors, of our x's. And so now when we talk about a one unitit change in x, we talk about a beta 1 change in the log odds of yals 1. All right? And so this is kind of a typo. It's not the log odds of probability y equals 1. It's the log odds of y= 1. Sorry about that.
(54:08) And more importantly, when we're asking you to interpret a logistic regression model, really what you think about doing is you exponentiate that beta 1 term. And so now you're talking about the change in odds is multiplicatively changing when x changes one unit. Okay, x goes up one unit. exponentiate the beta term and now there's this multiplicative increase in the odds.
(54:42) Okay, if beta 1 is zero, what does that mean? Beta 1 is zero. Just like in linear regression, when beta 1 is zero, what does that tell you about your x and y? There's no relationship. Okay, no relationship when beta 1 is zero. And if you take e to the zero, what do you get back? The value one. Okay. And so that multiplicative change of one means there's no association.
(55:04) When beta 1 is positive and you take e to the positive value, what do you get back? Something bigger than one. And so every time x changes a unit, you see a multiplicative increase in the response in the odds of a success. When beta 1's negative, e to the negative is something less than one.
(55:27) Okay? So be careful about what that change means in m multiplicative terms. Beta 1 being zero, no association leads to a odds ratio of one for a one unit change in x. Okay, we'll have practice with that for sure. All right, so let's f fit our first logistic regression model. This is the plot we looked at before.
(55:52) We're looking at predicting whether or not someone has heart disease based on their max heart rate. We fit a quick logistic regression model. Penalty equals none. Be careful when you fit in sklearn logistic regression models. I believe the default is not a penalty of none, right? Is there still a default of a penalty? I think. Oh, did they change it? Okay, they may have changed the documentation. Anyway, I'm saying penalty equals none.
(56:16) What do you think penalty means in a logistic regression model in skarn? magnitude of coefficients. It's related to it. It's not standardizing. When do we penalize linear regression? What do we call that? Either lasso or ridge. And so all within this logistic regression function, you can perform your lasso and ridge models with a different penalty term.
(56:51) Okay? And so I don't want a penalty term because I don't want to use any sort of regularization. And so then I fit a regression model, a logistic regression model based on my x max heart rate based on my y uh heart disease or not. This is a list. This is or sorry this is a single variable. This is a matrix is mathematically what I'm thinking about them as.
(57:10) And so then we can print out the coefficients and the intercepts. And this is what we get. change is still L2 by default. Still by default you want to have it be none. You have to say none object in Python. You can't do a string. You can't do a string of none version apparently. Okay. So you can't use cap you can't use the quote version of none. You have to use capital N.
(57:37) I'll correct that uh when I get a chance and you'll see that in section not this week but next. So then we get our estimates. What are these estimates telling us? If I say write out the regression model, the estimate for the regression model, how would you write out that model? Beta 0 6.32 minus 0043 times x. That's the right hand side.
(58:22) What's the left hand side? Log of probability of y= 1 over 1 minus probability that y equals 1. Okay, it's log of y equals 1. And so we can write that out. It's fitting this sshaped curve. It's decreasing sshaped curve to y. That's a negative coefficient. You start off something high when X is zero and it slowly tapers down in log odds and tapers down in original probability scale after doing the exponentiation as X increases.
(58:53) What's the estimated model? The log odds is that intercept minus 00434 times your X max heart rate. Okay. So what's the interpretation of 6.325? When someone doesn't have a maximum heart rate because they're comeomaosse, what's the predicted probability that they have heart disease? E 6 over 1 plus e to the 6.
(59:30) All right, that's the estimated log odds that someone has heart disease. 6.3 and log odds is large. Exponentiate that. That's then the odds that someone has heart disease. And then turn odds to probability is odds over one plus odds. Okay. And so you can convert that to a probability. The chance that someone has heart disease when they don't have a maximum heart rate.
(59:52) 043. First off, the sign is negative. Which means as max heart rate increases, it's associated with a lowering in the chances of having heart disease. All right. With that said, what does directly the magnitude.0434 mean? A wanting to change the max heart rate is associated with a.
(1:00:21) 0434 change in the log odds of having heart disease. And you probably should exponentiate that. Take E to that and then it's saying a one unitit change in max heart rate is leads to a E to the negative.043 a reduction multiplicative reduction in the chances of having heart disease. Okay, great. All right, so now we get estimates very easy from sklearn and the question is how did sklearn get to those estimates? What is the machinery under the hood that skarn is using? All right.
(1:00:50) And it's all going to boil down back to what we talked about probabilistically in the linear regression setting. Let's talk about the linear regression setting. Remember there was a loss function we try to minimize. And so in linear regression, what was that loss function? Come on.
(1:01:14) This should be one of your knee-jerk reactions in this class. What's the loss function in your regression? MS I say yeah mean square error sums of squares error that's what we're trying to minimize when we fit that uh model to the data what's the probabilistic perspective on linear regression where did that mean square error come from what distribution did we assume in our response area we assumed it was normally distributed where the mean depends on the x's in a linear way.
(1:01:54) Okay, logistic regression has the exact same approach. It's not going to assume a normal distribution because when your response variables zeros and ones, it's not going to look very normal. What distribution results in outcomes that are zeros and ones? A Berni distribution. The simplest form of a binomial. A Bernoli distribution has a probability mass function.
(1:02:20) That probability mass function we can use to build a whole likelihood. That likelihood we can maximize or the negative like likelihood we can minimize and that's going to give us the loss function for this model. Okay, great. So let's go through that a little bit. In logistical regression, essentially what we're doing is we're saying, all right, we're going to fit this S-shaped curve.
(1:02:43) We want this S-shaped curve essentially for every observation to fit the data well. So every time we pick an uh or estimate an observation, every single observation has an x value which for now we're just going to say is continuous. We have this numerically measured x variable. And so given an x, we're fitting an sshaped curve that the point in which that sshaped curve crosses at that x value is interpreted as the probability. And so what we're going to do is we're going to say, all right, let's flip a coin.
(1:03:14) We're going to flip a coin given this x value that has the probability of coming up heads based on where that S-shaped curve says it should be. Okay? And so we flip a coin and that flip of a coin has the probability of heads based on where that line or that curve happens to be. And so we flip a coin with probability 75 in this case that it's going to come up a success going to come up ahead.
(1:03:42) You can perform that process over and over again. You can flip ahead, flip ahead, flip ahead. And what you're going to see, the more times you flip it, the closer and closer it's going to get to that true data generating function. All right? It's not going to be perfect. You're going to have randomness involved. All right? And so if the true probability was 75 and you flip a coin, looks like 1, two, three, four, five, six, seven, eight times, seven out of eight, 87.5% or round it up to 88%.
(1:04:13) with me? Okay. Well, now if you were going to estimate the S-shaped curve, even though the true S-shaped curve says it should be 75%. When we estimate it, it's going to be a little bit different. All right, it's the whole idea of using estimates as our best guess for the true underlying probability distribution.
(1:04:40) Okay, this idea of flipping a coin though matches that story of a Berni distribution. We're imagining we're going to flip a coin with probability P of coming up heads over and over again. And it's just the P of coming up heads depends on the values of X. Okay. And so we build our entire logistic regression model, the probabilistic perspective on it from that framework.
(1:05:03) Now you can do your predictions not just at X star, you could do it at a different Xstar. And so that different Xstar gives us a different probabilities. You flip a coin many many times. Of course, you're going to expect to see more tails than heads under this case.
(1:05:21) And so you get in the long run the probabilities you would have expected on given the true distribution. But you know in reality it things aren't going to match up perfectly. Your data isn't going to perfectly match up with the true underlying curve. So there will be uncertainty involved. Every single probability of heads have that probability of P.
(1:05:42) Every single observation of tails had a probability of 1 minus p. That is that buri distribution. We say the probability that y equals 1 is p. The probability y equals 0 is 1 minus p. And we can write out the formula for the pmf directly. Now if you've taken stat 110, you've taken stat 104, you've seen that probability distribution before, right? It just evaluates when plug in y equals 0, you get 1 minus p.
(1:06:06) Plug in a y of 1, you get p back. matching that intuition of flipping a coin with probability P of being heads. All right. Well, we have to take it one step further because that probability P isn't just simply a single P. It's a function of X, right? What's the function of X? How P relates to Y or P relates to X? What did we just say that functional form looks like? that S-shaped curve, that sigmoid function, E to the junk over 1 plus E to the junk.
(1:06:44) Okay? And so instead of in our PMF, we have P to the Y, we substitute in E to the junk over 1 plus E to the junk, right? And so that's going to give us the likelihood function then for this model. And this is just a little bit of an aside. Thank you, Pablo, for this. I think this was Pablo. No, it may have been a test then. It must have.
(1:07:04) So this is the one slide I stole from Nesh. Uh Natash says essentially let's explain a little bit about what a logistic regression is. Uh Amazon has some nice little demonstrations explanations as to giving you some understanding of what uh logistic regression might uh represent. Okay, back to my away from my aside.
(1:07:28) We now write out that likelihood of my P given my Y when we observe a response Y which is going to be conditional on X. We now want to instead of talking about the chance of Y, we want to talk about the likelihood of P. Which probability model should we pick or we think is the most likely given the set of Y's we observe? We flipped that probability mass function on its head.
(1:07:54) And so now the total probability or likelihood is just going to be the product of all those different likelihood contributions of all our different observations. Okay, we write out this whole product. This is likelihood perspective on estimating logistic regression. What does that mean? How do we use this likelihood to estimate our parameters? What are our unknown parameters? What are unknown parameters in logistic regression? They are in linear regression except we're missing one thing.
(1:08:41) Loud noises. What's missing in logistic regression that we had in linear regression? Yes, our likelihood is different. Our functional form is different. But what parameter is missing? The sigma. Why don't we have a sigma in logistic regression from you guys? Why is there no sigma in logistic regression? What happened to sigma? What did sigma reflect? The error around our curve. What's the error around our curve? In logistic regression, there's only two possibilities.
(1:09:25) Either we're above the curve at one or we're below the curve at zero. We don't have to mathematically estimate it. We're given it for free because of the nature of the data. Okay. What's today's password? Another thing I grow at the farm, hops. I got the hops. I got the hops. All right.
(1:10:00) What are hops used for? There you go. Someone else beer. All right. So, we're back to likelihood. The likelihood function is a product again. The t distribution is my favorite. Sir William Gosset and the Guinness Brewery hops all related. We have a product of probability evaluations. P the Y over 1 - P to the 1 - I Y.
(1:10:23) Where are my betas? I don't see betas in here. These are the unknown parameters. Where are my betas? Implicitly, they are where the P's are. Right? We replace the P's with the E to the junk over 1 minus 1 plus E to the junk. That's where my betas are. Plug those values in. We want to maximize this function with relationship to the betas.
(1:10:48) Plug them all in. Oh man, we don't get there yet. There we go. And so when we plug in all of our betas, eventually this pi gets replaced with that e to the beta over 1 plus e to the beta. And then as a result, when we're maximizing the likelihood, we take the log and we maximize the log likelihood, which is equivalent to minimizing the negative log likelihood.
(1:11:15) And so this minimum of negative log likelihood is what sklearn and people in the machine learning and and uh information theory uh world call binary cross entropy. And so logistic regression is essential essentially uses a loss function of binary cross entropy using the betas in that log gods version in place of p. All right.
(1:11:44) Who came up with logistic regression? Mr. logistic of course. No, it was some statistician that said I know I need this sigmoid function. The sigmoid function we're going to use in terms of something that's at least a little bit interpretable in terms of logs. Okay. Why do we use a S-shaped curve? What other functions follow that S-shaped curve? You don't have to use log odds.
(1:12:13) What else could you use? What's that? Inverse tangent. Yeah, let's use that inverse tangent. The other thing you could use really from a stat one at 10 perspective is any cumulative distribution function. Essentially, you're taking any cumulative distribution function of a continuous random variable and you're mapping it to zeros and ones.
(1:12:37) That's what every CDF does. And that logistic function is one of those examples. Okay, that's it for today. We'll talk about inference, multiple logistic regression, and decision boundaries next time and talk a little bit more about logistic