(4) 109 day 23 - YouTube
https://www.youtube.com/watch?v=yFsqzjhaVXQ

Transcript:
(00:01) Why? Test test test. One, two, one, two. All right. Welcome. Good morning everyone. I am Pablo Protoas and I'll be lecturing you today. We have fantastic lecture today. For the people they are going to be watching this online or in video, you're missing one of the best lectures because we have everything.
(00:33) We have from the very fun things, the medium things and then at the end the very mathy things. So you can choose which part of this lecture you want to focus. Well, everything is sure you should learn but more things are going to be early, very easy at the beginning and then medium at the middle and hard at the end.
(00:54) Okay, are we ready for that? All right, so let's recap very quickly what we have learned so far. Trees done and dusted. We know them, right? Random forest. Easy. No, we went trees, then we went bagging. Bagging is just simple. We just do a bunch of uh bootstrap and a bunch of trees and we aggregate them.
(01:18) And when you aggregate, you remove the variance. Then we found out that this method is good but at one point we have correlated trees mean trees that they similar to each other and we do like diversity. So we want diverse trees. So therefore we did this little twist with random forest. In order to avoid that we just randomly select a bunch of uh predictors from the full pool of predictors at each split.
(01:49) Okay. So, so far so good. All right. So, today we're gonna get into boosting again. Elonor, do you guys so many photos this year? I have to meet this Elenor B. Where is Elenor? Okay, Elenor, if you can hear me, please come to my office hour. Uh huh. Yeah. Well, this is No, she has pictures from everywhere, not only China. She has very nice pictures. All right. So, uh this is the outline.
(02:22) We're going to do the introduction to boosting. Then we're going to talk about gradient boosting and the mathematical formulation of gradient boosting. Uh the week after Thanksgiving, we're going to do gradient boosting for classification which called adaboost. And then the final lecture on Wednesday, December 3rd, I believe, will be the ultimate lecture.
(02:46) We put it all together in this mixture of models. Okay, good. We should put music at the end when we finish, right? All right. All right. So, let's start by the introduction to boosting. This is the simple part of the class and this is just to establish the intuition and as I said the second part I go into the more algorithmic step by step and the last part I'm going to try to justify why this works.
(03:14) Sometimes this happened quite often. I present we present a method you know Kevin talk about MCMC. Sometimes you should wonder does MCMC actually converge to the target distribution or not. There are usually some mathematical proofs that MCMC if you do it long enough will converge to your target distribution.
(03:40) And Kevin will be happy to tell you more about that. I can tell you my office hour happy to prove it. It's called detail balance. Uh okay good but the same will happen with this right? So if we talk about the method you should always ask does it really converge? Does it really work? Right? Okay. So that's the last part today. All right.
(04:00) So this is just to prove to you that there all these there are many models you can choose this 1090 A or 109A then in the spring we have 109B and you taking other machine learning courses is not only this you are hearing and ex you're exposed to many models we start with some models in 109 we talk about regression logistic and linear and we did probabilistic model and now we doing this decision trees there's many many models after this right and in particular neural networks deep learning you you know that's what is being used a lot these
(04:38) days and you wonder why do I learn decision trees if neural network is all that everybody is doing good question but let me tell you that when you have tabular data tabular data means not images text no audio just tabular data I believe random forest and boosting will outperform if not the same as any neuron network.
(05:11) I haven't seen many cases that the neuron network will outperform random forest or boost. Okay, so that's my kind of go-to. If I have tabular data, I'm going to use one of these two mod. Okay, and I think everybody who has done enough data science will agree with me. Say yes, don't say no. Yeah. Okay. He has to say yes. It's scripted. Uh yeah.
(05:35) So it is important to learn this method because you don't always need to get the big hammer to do anything. You don't always need the very deep convolution neural network to do tabular data. You can use these two models. Okay. So in a in Kaggle which is you know very well what it is there's all these competitions and if you follow it you notice that anytime this competition with tabular data either boosting or random random forest was winning all the time and then random when boosting came about boosting start winning too. I prefer both of them. I
(06:12) found boosting a little bit harder to fine-tune and we'll see in a bit why. uh but and I've seen I don't know thousand projects through 109A uh through the years and they try both and always random for boosting depending the data depending on situation will outperform many of the other modes. Okay.
(06:34) Now if you wonder just taking a little bit time until everybody says wonder why we learn linear logistic regressions and I think I said it many times because that is where we understand everything. I and I said it so often even if you give me diffusion model most complicated model I'm always going to think about linear regression or logistic regression with classification because I understand exactly what's happening and I trying to map it to that and if you ever come to my office hours when we did logistic regression I put the linear regression which I understand very well mapping to
(07:04) logistic regression then if you do trees I map it there everything I learn so it is important to understand those simple modes okay So as I said in the couple competition used to be random forest that was the big winner but over the last 6 months a new algorithm called XG boost has crop up and is winning practically every competition in the structure data category structure is what I call tabler right and this is from this I think the founder of carbon okay so I hope you being convinced all right so this is kind of what we have for decision tree shallow trees uh will
(07:41) be high bias, low variance. You see here, high bias, I'm off the target, but low variance. I'm always hitting in the middle. That's shallow trees, underfeeding. We've seen that many times. And looking at the quiz answer, 100% correct on that quiz question. So, you got that. So, I'm not going to spend time. We ask about the tree question number five or six. 100%.
(08:07) By the way, if you're wondering when the quiz results will be out, we have few students taking it today. I don't believe anybody's taking it tomorrow, Chris. Right. No, we have few cases of what? Yeah. Uh we have few cases of COVID flu. And one more thing, if you have a flu, don't take a picture of your thermometer and send it to me. That that doesn't do it for me. Okay? Just just keep that in mind.
(08:32) Um uh so anyways looking at the quiz question I think this idea is very well understood by everybody so I'm going to skip it. So the decision bias various trade-off what we have here I think this is the best slide I ever made. Shall I play it again? Well it's Monday Thanksgiving. We can have a little more but I mean I spent a lot of time but let's look at it again.
(09:00) Look, as we increase the model complexity, right, the tree grows, right? Thank you. Thank you. The tree grows and the model complexity grows. Shoot. Don't tell inference. Don't confuse them. I have them. That's an old picture. Okay. With a lot of less. Yes. All right.
(09:35) So um so model complexity is in the x-axis of course and as we grow it gets the overfeeding and with the variance increase okay the leaves move um so we have two options uh for the random forest the variance although variance reduction is better in the random forces and bagging the generalization is still high okay so but what we have random forces is we have a little bit of heat because we have to do it for multiple trees, right? Okay.
(10:04) So the motivation of boosting is the following. Could we address the shortcomings of a single decision tree model in some other ways? So what we did with random forest, we said okay we're going to make them deeper and we're going to reduce variance by repeating having many many trees. Now I'm going to come from the other angle.
(10:29) Can I do the same in the opposite direction? Meaning instead of go deep for every decision tree and aggregate them to reduce the variance, can I go short shallow and increase decrease the bias by doing something? Okay, let me repeat. So in random forest we have high variance and then we're trying to reduce it by aggregation by including many more. In boosting I'm going to do the opposite.
(11:02) I'm going to say I'm going to have high bias but I'm going to do something to increase the bias with now without increasing the varian. Okay let's try that. So this rather than uh performing variance reduction on complexes can we decrease the bias of simple trees make them more expressed and the big thing is can we learn from our mistakes and I'm asking you can we are we learning from our mistakes yes you do midterm came about people said panic please second quiz you did all much better much better all of you so I I think we're learning from our mistakes and the same will happen here. So a
(11:42) solution to these problems is to make an expressive model from simple trees. Okay. And this kind of an sample is called boost this idea. So what we're going to do going to make simple trees and somehow combine them to make a complex tree by combining. Okay.
(12:09) And the key idea here is that we're learning from our mistakes. Okay. So, so in the random forest we have basically reduced the variance. So we go from that end to the left from a high variance to the middle and in and in uh and in boosting which is our option two is to go from that. Okay. So we're going to start with a high variance and decrease the v the bias go down.
(12:38) We're going to start with a high bias and we decrease the viance going down by moving that direction. Okay, cool. I hope that plan makes sense. All right, so how do we going to do that? So I have an idea. Let's play it out which is the following. So the boosting method came from these two people Rob Shapi and Yav.
(13:06) They wrote these amazing papers about boosting and I challenge you to read them. They're really hard. Uh but I did the work for you. Uh so the first implementation that they presented is called Adaboost which I'm going to present after Thanksgiving. So boosting algorithms to advertise it are fast, easy to compute and very accurate and are the def facto optimization tree algorithms nowadays though I think random forest is competitive then so here is the idea behind boosting so first part of this lecture today is theible easily digestible thing so I'm going to make a story which we all can understand. So
(13:49) you have to take the final exam and my claim is passing grade is eight. You see I have evil ideas right? Um I am kidding. Don't Okay let me make clear this is just for fun. Okay the passing grade is not a of course but let's say I become evil and I just say passing grade is a okay so you panic right? So you need to passing grade A. So what are you going to do? Let's think.
(14:21) So option number one, steal a time machine. Go back to 1996. Meet Rob and Yu. Befriend them giving them stock trading tips from the future. That makes sense. Follow their war for at least a decade to understand everything about boosting so you can ace the exam. Okay? Return to present and nail the test. Good plan. Good plan. But it's not going to work.
(14:45) We know that, right? Um there's a little problem there. this time machine thing. Okay, so repeat for another test. Easy peasy lemon squeezy. Okay, option number two, go to the library and get the previous year's question papers. Of course, libraries now don't have that. So, what we're going to do, we're going to go find a helpful student and ask them to give you a rule of thumb to get at least some answers, right? Okay. So you go to last year's students and say, "Hey, I have the final 109.
(15:17) What shall I do?" And the student said, "Well, don't ever choose option D." Like never. Chris, Pablo, and Kevin never use option D as the correct answer. Okay. Now, if you think about this, this is kind of a silly rule of thumb. It's not a complicated model, right? Is the simplest model you can think of, right? So the idea here is that we're going to be creating simple unsophisticated models to attack the problem of passing with a. Okay. So you say okay fine I'll do that.
(15:52) You test the rule with some previous exams and you find out it works 60% of the time. Not bad. You know if you just never choose uh D and you choose ABC with some thinking you get 60%. That's not bad right? But we said passing grade is A. So you're not there yet, right? But we're getting there. So step number three, find a TA and ask them to also give you a rule of thumb to get at least some answers right.
(16:22) Um, and you make sure you focus on the ones you got wrong. Right. So I'm going to keep the not D option or some of them. And now I'm going to ask the TA to give you another hint. And this is supposed to be Chris. Maybe if you overfeeding, if you see overfeitting in the options, that's the right answer. So the TA said Pavlo always and Kevin always like to ask the question, if the answer is overfitting, most likely that's the answer. All right. So, does the rule work? We test it out.
(16:56) Uh, and we still have some problems, but we're getting there. We did better than these two. Step number three, call your favorite professor and focus on the ones you got wrong before. Uh the right answer is always cross validation. Okay, so now you use that rule too and I became an angel. You see, I I changed for evil. I became good.
(17:21) Uh you test it out the new rule on the difficult problem. And now if you put it all together, you get to accuracy 70%. And now I combine them together with some weights. Right? So now I combine them together. You take the exam and you pass. Good. Final score 90%. A you pass. Okay. Let's think what happened. We have three what do we call weak learners, unsophisticated models. They just give you rule of thumbs.
(17:52) Not D, overfeitting and cross validation. Of course, none of them should work very well and none of them work very well. you get 60 65 70% on the every individual rule of thumb but now you somehow weight them accordingly meaning that um let's say the the not the option I waited less than the balous cross validation a little more etc and now I have my final result and I pass okay so that's good epic win but how does it work that's the big question we want to ask okay this should not Please, thank you. We're not done yet. But all right, so let's sum up what we have learned so far. Boosting is
(18:32) a different way of addressing either high bias or high variance. We want to be in the middle. With random forest, we reduce the variance by aggregating a bunch of tree. With boosting, we're going to try to go uh from the left, meaning decrease the bias as we create more and more trees.
(18:52) Uh so far, we just get the very high level intuition. Part two, we're gonna see how the whole thing works. And part three is can I prove that it works. Okay. Um, let me get the next one up. Maybe I should get everything up. Okay, we're back to Chicago and I'm about to share for our online people. Yeah, I should be sharing and I should be okay.
(19:46) So, now we're in where we Chicago by the way, Greg. Okay, good. Nice picture, too. I like Chicago. Did I see it? Let's go to now gradient boosting how it works. And I'm pretty much going to demonstrate this with some examples again, but more more concrete example. I start with the code from one of our uh colleagues here uh Leslie, which says, can a set of weak learners create a single strong learner? And the one of the good demonstrator demonstrations I have seen is the jelly beans. I don't know how many you see have seen this is
(20:24) basically if I ask every one of you to tell me how many jelly beans you think they are in this jar and I average it comes actually very close to the correct answer. Um does anybody want to take a shot how many there are? Thousand. Who said that? You said 12. Is that you under? Okay, someone needs glasses I think.
(20:56) All right. So, this is the idea that basically weak learners together can actually make a stronger uh tree. So I'm going to as I said is so the key intuition behind boosting is that one can take an assemble of simple models which I call th and additively combine them into a single more complex tree.
(21:26) So the key word here is simple uh models t of h and the second keyword here is additively. So we take simple trees and we're going to add them together. Okay. U so th is a weak learner. Weak learner means in the case of tree, what would you consider a weak learner? A tree that has depth one. Yes. A stump, right? That that would be a weak learner. Maybe depth two, but depth one is good. Yep.
(21:56) So that's what I call a weak learner. Um and H is the collection of all the weak learners that possible all the possible stamps you can make right all right so every one of them might be a poor fit of course I understand it's like what I was saying before the op don't select option D in the quiz that's just not going to be perfect right and I understand that but the linear combination of the ensemble can be very expressive And the way I'm going to do it is these are my weak learners. I have a coefficient in
(22:34) front of it which I call lambda and I sum them all up. Okay, additively. So the question is how we find this weak learners and how we find this lambda. That's all. And then of course we have to prove that this kind of method actually works, right? There's two ways to prove it.
(22:55) You go into your skarn, you use gradient boosting and you show it works. But let's as I said, we need to go a little bit deeper. But let's see how this method works. So so we want to give more weights to the makes to the model that makes the fewer mistakes of course right. So if I have a 10 week learners, one of them does everything wrong and one does 60% correct, which one I should wait more? Of course, the one with the more correct answers, right? So the idea is to actually give them that. All right, so let's go into this is the my final
(23:32) model. Lambda is the weight or the importance and th is one the weak learner. Okay, there's two main ideas. gradient boost method of for interatively. So that's an important thing I said weak learners and additive. The third keyword I want you to get out of here is interatively meaning that I'm going to find the next week learner based on the results of the previous ones. Okay, so that's the key here.
(24:04) Um Uhhuh. and a few simple models added to some compensates or the weakness of the current sample. Okay, so that let's just before we show an example, let's understand the idea here is to reduce bias by introducing this additive collection of trees. And every one of these trees will be weak learners. We're not going to do strong learners.
(24:32) week maybe depth one or and the third idea is that we're going to do it interatively means the next week learner will be informed by what's happening before. Okay, with that in mind, let's do an example. And here the first thing we're going to do it is um by the way, I'm going to give you the the word of the day letter by letter so you can do your predictive models and see if anybody's first letter is can be peach.
(25:12) It could be many trees with peach, pine, palm tree, uh, pomegr. That's a bush. But all right, let's start with this data. We're doing for regression. So what I have X is my predictor and Y is my response. Input output. Okay, and we do regression. I hope that's straightforward. So first thing I'm going to feed a wig learner. A weak learner as we defined it so far it has one node one stump right just one root node we just divide all right so here it is since it's one node it's going to have one cut one split I have only one predictor so I'm going to take that predictor and find the point that minimizes the weighted MSE on the two
(26:04) regions yeah we talk about that a week ago. So I predict of course the mean to the left and the predict the mean to the right. The mean to the left of course will be the mean of the points to the left region and the prediction to the right will be the mean to the points in the right region. Yeah. All right. Step two.
(26:28) I'm going to compute the residuals. What are the residuals? the differences between the prediction, the true and the prediction. Okay, so here you can see the orange actually is on top of the other one. The reason is because my mean, my prediction to the left was almost zero. But if you look to the right, my prediction was 7.
(26:50) 5, which is this blue line. So what I do here, I take the difference between this and this, this and this, etc. And this is my residual. Okay, step three is to actually fit another model. But now here is the key idea. My next week learner and I said it earlier is going to be informed from the mistakes of the previous one or the previous ensemble.
(27:19) Right? The mistakes of the previous up to this point are described as the residuals aren good. Yeah. Okay. So this is what I'm going to feed now. I'm feitting a model on the residuals, right? So I'm learning how to correct the mistakes of the up to this point. So here's my feed to the residual is a stamp again is just a split is trying to find a split point such as the MSE of the residuals is at the weighted MSE of the residuals at the min. All right. So now I'm predicting the residual to the left. So I have a
(28:02) hat on the R because I'm predicting that residual to be two which is here. And I'm predicting the residual to the right which is the mean of the residuals now my prediction to be minus 1.5. Okay. How we doing the balcony? Good. All right. Next step. I'm going to combine the two trees um the step one and three by saying the final tree now at this point is the original tree that I bit on the actual data plus lambda times the tree that I residuals. Okay. And now I'm going to set the lambda 0.5. We'll get back to
(28:50) how to find the best lambda, but for now let's put it 0.5. Okay. Just so we go through the outcome. So here it is. So if X is larger than 6.5, I'm in this region. My prediction was 7.5 for the Y and the prediction to the left was minus.01. Now I have the next tree which is the residual which says if X is larger than 3.
(29:21) 5 the prediction of the mean is minus.5 1.5 and if I'm left from 3.5 my prediction for the residuals is two. Okay. Now I'm combining them with 0.5 and I'm going to get my new tree. Okay. So you can visualize this way and somewhere there I must have yeah all right so this is my final predictions numeric is 7.
(29:52) 5 plus lambda of the prediction there now my prediction at 6.8 or larger is 7.5 plus.5 times that between uh 6.8 8 and 3.5 I'm going to get this and then on the left I'm going to get that. Okay. And I believe that's going to put it all together. Good. Yes. So now this is my tree at this level. I am having the residuals added to the prediction of the other one and I Okay.
(30:24) Shall I? Is that clear? Yep. All right. Now step six repeat step three. Meaning now I'm fitting the next model on the residuals of the tree I have before. And the tree I have before remember is the original tree plus 0.5 because lambda is set to 0.5. And if you're wondering how I set it, we'll come back to that.
(30:50) We just set it for now to some value times the predictions of the residuals. That's my tree at that point. I find the residuals. Now I'm going to fit another tree on those residuals and I hope you can see the rest. I'm just going to keep doing that until we stop. Okay, so I do that. This is repeat and now I have the new tree.
(31:13) I'm adding this all up and I don't know what happened to that. PowerPoint works in mysterious ways. Oh, we'll we'll fix that. But you can imagine, right? So we just keep doing that. All right, let's let's formalize. We fit a simple model T0 on the training data. That's my root, my first T0. Compute the residuals for T for I until stopping condition is met.
(31:39) P the simple TI to the current residuals. So that is train TI for the residuals at that point. set the current model to be the previous tree plus lambda times the current tree that you fit in the simple tree. So compute the residuals again and we keep going right where lambda is a constant called learning rate. Okay.
(32:03) Now lambda we're going to be calling it learning rate and you're going to see a little bit in this lecture why it's called learning rate and we're going to also discuss how to set the learning rate as we go all right final thoughts on this in practice we don't explicitly combine the trees as described in these slides meaning I make these slides which last year I think confused students these trees here I don't build these trees What I need is the individual ones, right? And when I do a prediction, I just follow the recipe for each one of Is that clear to everyone? Because if I
(32:44) have like I'm going to get the original t that split on 6.8 and then I'm going to take the answers and I'm going to add whatever I get out of the next the residuals. And I keep doing that, right? So I don't need to build the tree as such. It's just a bunch of trees and you just add them up to do that.
(33:03) Okay, I just wanted to be clear that you you don't go and build the tree. You just have bunch of trees and when I do a prediction or inference but the machine learning inference, not the statistics inference. By the way, we need to clarify when Kevin and statisticians talking about inference, it means one thing which is inference like we talk about it.
(33:26) Sometimes in machine learning especially, you're going to hear the word inference which meaning prediction. um take a deep breath but that's the nomature I'm not don't shoot the message okay just to make so when I say sometimes inference I should try to avoid to say the word inference because in this class we're doing proper statistical inference so I'm going to try to use the word prediction but if you see it in in literature in blogs and you hear people sometimes in machine learning you hear the word inference it doesn't mean statistical in inference it
(33:57) means prediction just Just so you know. All right. So what I'm saying is that we don't combine the tree. When I'm making a prediction for any particular point, I have my trees. I have the learning rate, the lambda. I just add the answers and I get my answer. Okay. Is that last point not clear? Good. Okay. So that's pretty much it.
(34:23) Uh I think this is the second part. Thank you again. I don't know why we put thank you for each one of them. Let's sit for a second and see if there are any questions about this uh before I go to the last part which is trying to formalize the whole thing. I think as an idea should be clear especially for regression it makes sense. I'm fitting a model to predict the resisions and I keep adding them up.
(34:48) Before I go to the next one, let let me ask a question to see what you think about. Why don't I just add the residual? Why do I just put the lamb in front? That will help us to understand the next part that figure out if there's accuracy like it's not as accurate as the last one. That's our waiting term.
(35:13) Uh the answer is John, right? Johnny says is that is because of the accuracy. You're getting there, but there's one slightly different thing. Uh Zoe looking is kind of like the new part, but then we have a history of the Okay, you're getting very close to it. Getting warmer like retreat.
(35:48) We want to take into account that there's already like information to use that warmer Jacob before you do that new model any models before that you would add that new model to it and you don't want those to be weighted equally because there's inherently more summation in that first model. So then you need both of you have a good answer which I wasn't the answer I was going to do but it doesn't mean it's not a good answer.
(36:15) So one of the idea is that as you go and adding more more trees the previous tree if you think of it as an information gain it has a lot more information and as you go down you're adding a little bit less and less. Okay that's all good. Um I'm going to say the following is going to help us for the next thing.
(36:40) If you have to remember when I'm I have my y's my prediction and I know the mistake this is training the mistake I'm making is the residuals right so the idea is that if I add the residuals I'm getting where I want to be right but remember when I'm fitting the residuals I'm not getting the residuals I'm getting a model that approximate these residuals so there's a good chance that my approximation for the residual visuals is not so good maybe in some areas right so there is a danger of having okay I have a mistake here let me correct it and I hit it hard and now I go on the
(37:18) other side right it's like that game you play with the golfer you hit it and it pops on the other side you hit it there it pops on the other side right so if you if you believe your residuals that you predict with the model is very good you put the lambda very high and and you're done. But if you don't believe that's true, which is most of the case because we're using a very weak learner even to predict the residual, you acknowledge that your prediction to your residual is not so good. So you said, okay, just go a little bit notch them in the right direction. Okay, so keep that in mind
(37:58) when we go to the next step. All right, let's put another letter there. And then you can go home. What? E. I was going with the eye. Too easy, right? It's okay. I mean, the fact that you're here tells me a lot. You're not going to go away. And the best part comes now. You don't want to miss the next one. All right, let's go to the next one.
(38:39) If I find my mouse. All right. And now we are JP close to my house. It's Kevin. What did he just said that I didn't put it there haran put it there but I have seen it I would have removed all right we're thinking snow I think by the way the snow in the north yeah all right so we're thinking skiing it's the time we start thinking that so the the the least part is why does gradient boosting work so I'm going to go a little bit through the mathematical ical formulation. I have plenty of time, so I'm going to go slow and please stop me if there's something confusing. We can
(39:38) go on the whiteboard and explain it. Okay? So sometimes maybe I skip a step or something. So intuitively each simple I'm saying intuitively every one of these that we we add to our ensemble model models the the residual of the t right those what I'm claiming is that we with each addition the residuals are reduced okay yeah and that's where this lambda comes handy because I have to make sure the residuals reduce right so do you believe But you have the answers usually how okay all right so let's investigate that to see if that work
(40:28) so we start with and I'm making the following statement the previous example we start off by modeling t0 that give us uh some predictions so if I put x into the original tree I'm getting my predictions s for the first tree, the the zero tree, right? Okay. So, we then train T1 on the residuals of T0 such as the new one, the T1, it will be Y minus the prediction of the previous tree.
(41:02) Okay, so this is my you see this is a key here. It's an approximation, right? The new tree is trying to approximate the residuals of the previous tree. I mean modeling you can think it as an approximation to the truth right so we we have said that early on so the new tree is basically modeling the residuals approximates what are the residuals are from the previous so this is my residuals are zero so what I'm doing this tree the the the second tree the t1 first one is zero we're pythonic here is going to be a model of the true residuals or we'll say approximate that. Okay.
(41:43) So then we combine the two from my new t now is t0 plus lambda of t1. Right? So now the residual for this model will be t minus t prediction is going to be since t prediction is the prediction of the t I'm substitute with this. So I get y minus the prediction of the zero tree minus lambda of the prediction of the first tree the second tree t1. All right.
(42:17) So now notice these two together are r0 right. So I call them r0. So this let me point out this is we call it this R0 prediction residuals is the residuals between the original thing and my the data and my original prediction the prediction from my original tree. Okay, you see that? All right.
(42:48) So that means now and this is the residual at one is R0 minus lambda and the new thing. So I can write this in this format meaning the new residual is equal to the old residual minus lambda ti of xm. That means as I go through this iteration adding three my residuals are reduced. I just went step by step. I don't know maybe the last step is confusing. So let's stop here for a second.
(43:21) So this is the residual of 1 which is equal to residual of zero minus lambda times the prediction of 31 which is feeded to the residuals and then if I keep going I'm going to end up with this algorithm. Okay. So that means that the residuals are decreasing. convince. Is any doubt on anybody's mind? Yes. You're concerned about opinion. That's a good concern to have. We going to get to that when we address lambda.
(43:59) But good concern. But anything about here that maybe well there is yes. Uh but I'm not sure why this is something that explains why it works. We should I guess that because you could say the exact same thing about polinomial regression as you increase the degree polom and we would have said yeah okay so let me rephrase the question said what why should we be impressed the same thing will happen in the case of polomial as we add degrees of polomial you're going to be decreasing the residuals right yeah like yeah so in the polomial we don't do this
(44:42) inter iteratively right we just put them all in is true. As we increase the degree of the polomial, I am going to reduce the residuals on the training set. Uh I think is this is not a strong statement. It's just to put things in order because with polomial I can make the argument hey if I add a degree the residuals or the loss will go down.
(45:09) Why? because I can always make the coefficient of the last degree of my polomial to zero and therefore is the same and then whatever I add should go down or remain the same. Uh that's a nice easy statement to make here I just say do this in iteration add trees and it's going to work and you say hold on is it first thing I needed to prove to you is that by adding trees my residuals decrease right so I think now you may be a little more Not impressed but at least say why I'm making this statement. Correct. Yeah. There is one little problem here. Yes.
(45:50) Are they just a regular tree or do you do like a random ball? No, just a regular stamp. Is there a model that random forest each time or like each time iterate? There is the question is are there any models that use random forest as the weak learners? um when the first boosting came about in the next five years there's hundreds and hundreds of papers that they try different combination um don't remember all of them of course that's the one that prevail is a survivor of the fetus if you like this is what is implemented in skarn some derivative of that is x boost which is
(46:32) kind of the the library on top of uh gradient boosting um etc etc. Okay, clear. There is one thing here that should bother you a little bit and nobody seems to be well if t is negative. This is my prediction for the residuals, right? The residuals will grow, right? Because I'm I'm subtracting negative number.
(46:57) So don't worry about that because if my residuals are following if my prediction follow the residual that means if this is positive that will be positive is negative is not sometimes it's going to flip but overall we going in the right direction. Okay let's move on though. So if we want to easily reason about convergence and how to choose lambda right so the P key is lambda because some of you start worrying about overfeitting right so let's go through that and understand how to find lambda and investigate the effect of lambda we need a little bit more so in particular
(47:36) how can we effectively descend through this optimization in an iterative way and I want to formulate this in gradient boosting as a type of gradient descent. You heard me saying and I think you heard Kevin say gradient descent and a lot of you know what gradient descent is maybe uh but I think it's time to talk about gradient descent.
(48:00) Why? Because sooner or later you're going to need it. So why not now? Okay. So and I'm going to try to match it together. So in optimization this is something that is already stated before in optimization when we wish to minimize a function we call it the objective function we call it the loss function whatever we call it and we trying to do that over a set of variables what we going to do we compute the derivatives or partial derivatives if is multivaried and we set it to zero. Thank you.
(48:41) So now if these partial derivatives are simple then we can analytically find a root and we have seen this in two cases linear regression and then Kevin showing to you in logistic regression when the predictors are binary we could do it but in general that doesn't work right so we know that um if the function has probably a big convex then the stationary point is precisely the global mean and this is an opportunity for me to make a statement.
(49:20) You can have convex objective function but you cannot solve for doubling. One example of that is logistic regression. It is convex but we cannot find the analytic expression for our betas. Okay. But it is convex right? Okay. All right. So now let's do a brief sketch of gradient descent.
(49:45) So we initialize the variables at any value W. So let's say we have a function that is going to be minimized over a bunch of variables which I call W. Okay. So we're going to initialize to some values and then what we're going to do we're going to adjust them and look at this formula. X now become oh this should be W. Sorry WW.
(50:07) Let me correct it on the fly. I miss this. Okay. Okay. So, the formula is which is a gradient descent formula. You're going to see it about before you graduate about 5,000 times. So the new W's, the updated W's will be equal to the old W's minus the learning rate times the gradient with respect to W of your loss or your function. Okay? So please remember this formula. You use it.
(50:59) We're going to use it not in this semester but next semester pretty much every lecture we'll talk about. Okay. Yeah. Why I'm using the the symbol W because the slides I have for neuron networks that they call weights. Okay. So Kevin is hugging me to talk about when we talk about neuron networks the parameters of the model there will be the weights and biases of the neuron network and that's what we optimize.
(51:35) So I use the same uh symbols here. Um all right so the claim is that if the function is convex we can find the global minima. If it's not we're stuck. So now let's talk I have a nice illustration how this works. W here are my parameters. I haven't defined what the parameters are yet but the some parameters of the model.
(52:01) In case of logistic regression for example if I would have done this W's will be the the betas right beta zero and beta one just to tie things up. So let's say now I'm going to find trying to find the minimum of the function. The parameters here I call them W's because I'm working and it doesn't matter I name them whatever I want. Think about betas or W's or whatever.
(52:29) So I start with some random point. Here's my random point and I'm looking at only one particular parameter here. What I do? I calculate my loss or my function that I want to optimize minimize at that particular point. Okay, so far so good. Yes, of course. Right, I didn't do much.
(52:52) Next thing I'm going to do, I'm going to compute the slope or the derivative or the gradient since I have only one par parameter here is the derivative, right? And the derivative and the slope are the same, right? So, I have my slope and since this an iterative method, I need to make a step. Which direction shall I go? Left or right? Why? Because it's lower. Because the slope tells me, right? All right.
(53:21) So, I'm going to make a step to the right in the opposite direction of the derivative. Why opposite direction? Because if the derivative is positive, it's going upwards. If the derivative is negative, means it's going downwards. Okay? It's like the slope. If the slope is positive, is this. If the slope is negative, is this.
(53:40) All right? So I'm making a step this way and then I go to the new point. I calculate again the derivative at that point. If the derivative is negative I go to the right. If the derivative is positive I go to the left. Easy peasy. And I keep going. Okay. Eventually hopefully we got to the minimum and at the minimum the derivative should be zero.
(54:04) So I don't know if I want to go left or right. I stay there. Okay. Of course, that's not exactly what we do. If the derivative becomes smaller than some value, we stop, right? Because there's always going to be some machine imprecision. Okay. All right. That's gradient descent in few slides. But um I missed something. Okay.
(54:29) Here is supposed to say that if you have more than one variable should be the gradient instead of the derivative. Uh I think it's just the colors are off. I'll fix it, too. All right. So now the next step is how big this step should be. We say we're going to go left and right, right? So if the derivative is high, shall I make a big step or shall I make a small step? Big step. Okay. So it should be proportional.
(54:56) Okay. Why you say big step? I mean there's two options. Yes. because know that your risk of it being zero. So you know at least when you get to the bottom the derivative will be small and you better stop. Right? Think about you're up the hill and you trying to go down.
(55:19) The way you should imagine is like you're up on a mountain and your goal is to go down because there is food and you're hungry, right? Uh so you and it's dark. You don't see. So I'm here. I close my eyes. I don't see anything. I just feel and say, "Oh, that's the steepest thing. I'm going that way." Now, when is the hill is steep, I'm making giant steps. When the steep is shallow, I'm making a smaller step. Okay? So, we need so we say it is proportional, right? Okay.
(55:46) So, that means the new W is the old W plus the step instead. We said is negative is opposite direction and proportional. I am going to turn it into a proportionality constant times the derivative right so that's what you said right there proportional so I have a proportionality go and okay this is gradient descent okay that's it so I'm changing the notation so I don't have all the new so I say the the W at interration I + 1 is equal to the W at I minus this proportionality constant which call learning rate times the
(56:28) derivative. If the learning rate is small enough, this guarantees to go to the minimum. They say global minimum to a minimum. That's only if the the learning rate is small enough. Okay. All right. So this is the basic idea of gradient descent. We know that's the formula I said you should always remember. It's very straightforward.
(56:54) It is an iterative method and L decreases guarantee to go to the minimum if the learning rate is small enough. All right. Now, why did I do all these things? Because about gradient descent, there's not just these five slides. If you go to the library, there is rows of books about gradient descent.
(57:23) This is a machinery that has been going on for many many decades. Right? We know a lot about gradient descent now. I want to cast boosting yesterday. What do you get stuck at like a local minimum net? The question is what you do when you get stuck at local minimum gradient descent. Uh there is multiple ways we can address that.
(57:46) The first one is that we do random restarts. So you stack here, it stacks here, you stack here and find the minimum of the minima. Okay. Second is we use something called stoastic gradient descent. Not the scope of this class. If you take 109b, we talk about that. And the third one that is very important.
(58:08) You just get used to it to the idea that you don't find the global minima. Actually one of the greatest thing about deep neural networks until 2003 people have this concern that this is a not a convex optimization we're not find the global minima there's gazillion papers you you go to a conference a whole section about better optimization techniques until hinton said maybe your local minima is not as is not the global but good enough cross validate it if you get good answers.
(58:43) Who cares? Yes, there may be another minimum there that will give you another 0001% better performance, but if the goal is to have good performance predictive models, don't say anything. Uh then there's no point of going and kill yourself of that. So, three things, get used to it, Jacob, because they are there. You can't do you can't guarantee. The only method I know that guarantees to find the global minimum is MCMC.
(59:09) But that means you may have to wait until the end of the universe. Okay, let me keep moving because uh I'm gonna All right, I think I answer it enough, right? Yeah. All right, so why did I do that? Because I wanted to cast this problem into gradient descent. And here is where all the fun begins. So assume we have three training points. This is where I want you to pay most attention. Assume we have three training points.
(59:36) X1 Y1 all the way to X3 Y3 in this space each point each point is a possible set of predictions. Okay. So for our three data points we're not using the X here. We're simply treating Y's as coordinates in a 3D space and optimizing over them. So let's say now I'm writing my MSE to be basically y - yhat y2us y2.
(1:00:07) Okay, this is my MSE, right? This is the square of the residuals added up. All right, so now this function, this loss function is convex, right? If I'm going to try to find the y hat that would minimize that, what is the y hat that minimizes that? It's y itself. Huh? I'm playing tricks with you.
(1:00:36) You know that, right? So, there is a solution if the goal is to find the y hat that minimize this loss is to set my yhat equal to y. So think about it. We start in the origin and anywhere else we'll we end up straight into the true solution. If I'm optimizing, I consider the yhat to be my parameter and I do gradient descent.
(1:01:04) I don't need more than one step because take the derivative go straight into it. Right? Is that clear? I know you never seen anything like that because I'm cheating here. I'm not optimizing over parameters. I'm just saying what if if my goal is if I treat y hat as my parameter getting to the true solution is very easy. It's just one gradient straight into that.
(1:01:32) Okay, cool. Now let's formalize it a little bit more. If this is my MSE and I take the gradient of the MSE with respect to the yhat then the the the gradient is the residual right itself. So if I do gradient desend y plus twice the residuals I'm going to go straight into it. Put lambda is equal to one one step you're right in there baby. You're done.
(1:02:04) What is the problem with that? Looks very easy. You say hand anybody dare talk at this point or you all confused? You should be confused. Yes. The convex is not an issue. Here it is. So convex. It hurts me. This is as convex as it can get. Yes. just said that's not really like that's over. This is the most overfeeding I could do.
(1:02:48) Basically said if I say y is equal to y I'm not learning anything. I'm just overfeed. I mean you couldn't overfeed more, right? This is the ultimate uber hyper uber overfeeding that you could do. Okay, that's not good. So it's not interesting. We know where the minimum is for every J. Learning sequence does not produce a model. This is because we have not by no means learned the map between the input and the output.
(1:03:20) And the goal of creating a model is to connect the input to the output. Okay. So we fail. Let's do it a little bit different. Uh yes. So when trying to new gradient boosting as a form of gradient des and this is where I'm going to try to connect them now imagine what we have we have a space of possible functions that we're trying to find right so I call it the abstract function space okay so these are all the functions that cloud that nice little fluffy cloud is the space of all possible functions you can do so a point there is just one of the functions right It could be one of your choice. It could be logistic regression.
(1:04:00) Could be anything. Right? Now when we do to parametric space, what we do is we come here, we go to the parametric space, what we do every point in my abstract function space is parameterized by certain number of parameters. So in the case of logistic regression, my function is 1 / 1 + e to the minus beta 0 plus beta 1. Those betas are my parameters for that parricular model.
(1:04:31) Right? There's many more betas I can do. But that's how I parameterize. Okay. So now what I have I have a fixed dimensional parameter space. In case for example in logistic regression I have beta 0 and beta 1. In linear regression you have beta 0 and beta 1. You have trees. What are my parameters of the model? If you don't know the answer, that's good because it's not a straight answer. I'm going to leave it for after Thanksgiving to explain that.
(1:05:08) But think about logistic regression. The parameters beta 0 plus beta 1. Elinear regression is beta 0 plus beta 1. In neural networks, just so we talk in the for future things. So you have heard the parameters of the model are the weights and the biases of the neural network. Those are the parameters of the model.
(1:05:28) Right? So what I do I do a mapping between the functional space of all the function in logistic regression. Now I'm describing them with parameters in this parametric space which has in the case of a simple logistic regression two dimensions beta 0 and beta 1. If it's a multi- logistic regression, we have the number of axis will be the number of parame that I can work right now.
(1:05:51) I have some weights some parameters which I can work if you call them betas or ws that can work and we have done that gazillion times. Now we take the loss we minimize the loss the loss with respect to the parameters. We don't look in the functional space we look in the parameter space. All right now you understand this.
(1:06:16) Now how about the nonparametric models in boosting? We optimize in the prediction space because we don't have a fixed number of parameters. That's the problem with the trees. So we have a bunch of functions we can go to and that's the idea. So I go from the parametric regime that I have parameters to nonparametric regime. Now I'm talking about the y predictions.
(1:06:39) But I said if I do gradient descent with respect to these y hats I'm learning nothing. And this is where the fun begins. Let me skip this because I want to get we get this out. Okay. So we said that the ultimate direction I can take and I prove that it is the residual right. I said that before.
(1:07:07) If you take the gradient with respect to the loss the ultimate the best direction I can do it is the residual it should be a minus sign there actually no it's fine it's good so the optimal direction step is along the residual because the residual is a true minus the prediction so the optimum direction step is along the residual we know that right okay cool so here is where the the keys here.
(1:07:34) If our model class is simple, so remember at each step what I'm doing, I'm doing this very simple stumps, right? I know I'm not going to get into the ultimate direction. I'm going to have some approximation. So here it is. This is the ultimate direction.
(1:07:57) I could go and go straight to the correct answer, but I am not going to do that. I am going to get some approximation to the residual or some approximation to the gradient and this is my T. This is my stamp instead of going that way is not so good. So it goes that way. So if we're not if we're not restricted to a simple family of models like using an unrestricted is you could directly take a step in the direction of the residuals.
(1:08:30) Jacob read that because he was you mentioned that but this means we overfeit it. So if you do put strong trees it's going to overfeit. It's going to immediately learn how to go to the Y and you're not learning the mapping very well. All right. So how does this pan out? Let's see. We start at the origin. Start somewhere. And we know the ultimate direction is this is the T star is the is the residuals.
(1:08:54) We know that right? But I'm going to fit the residuals with the model which is an approximation of that gradient or that residual and I'm going to go somewhere there. Now what I'm going to do I am going to do repeat I'm going to do that I know from here to go to the golden place here I'm just going to take this direction which is the residual which is the derivative and because it's convex I know I'm going to go there but instead of doing that I'm going to go from here to here with my weak learner which is learning to approximate the residuals or
(1:09:31) in other words is learning to approximate the gradients. Okay, so I do that that and eventually I end up on the final thing close enough but not exactly. Okay, so instead of using the gradient the residual we use an approximation of the gradient that depends on the predictors and that's where the predictors coming in and of course I may overfeit but let's get to the end.
(1:10:03) So in gradient boosting we use simple models to approximate the residuals in each iteration which is the n the negative gradient right so technically no gradient boosting is descending in space of models or function mapping between x and y okay so let me recap for a second instead of doing optimization in parametric space because trees do not have a fixed number of parameters there's two reasons why we say trees are not parametric. My explanation and Kevin's explanation.
(1:10:39) So in Kevin's explanation is no parame I actually I'll say tell me if if that's correct is because we don't have an underlying distribution of the data. In my explanation is that we don't have a fixed number of parameters because you can keep splitting left and right. Right? So if I don't have a fixed number of parameters, I don't have a parametric space to do my optimization. Did I say right? Yeah. Okay.
(1:11:04) So because we don't have a parametric space to do that, we have to do these other things. So what we did, we thought about gradient descent and we map it into kind of a inter iterative method in the model space by just approximating the derivatives into these trees and we do it interative. All right.
(1:11:28) All these things are for some reason I didn't do all this to confuse you is to choose the learning rate. One thing we gain is not the only thing. First I know now that this is a gradient descent and everything I know about gradient descent that guarantees me convergence applies here because it is a gradient descent.
(1:11:50) The second thing is that this lambda that we have put there we don't know what to do. We're going to borrow from the ideas of gradient descent. So if you see how we do gradient descent. Now if the lambda is too small, what do you do? You go down that hill but with very very timid steps like slowly slowly slowly right eventually I'm going to go down but it may take five hours if I make my learning rate too large. Okay.
(1:12:22) I can't demonstrate that with me walking on a on a hill. But if the learning rate is large, I may overstep over the hill. And you can see it here. So my learning I'm here. I know I have to go opposite to the direction of the slope. So I'm going to go that way. And I know that is proportional.
(1:12:43) But if I put the learning rate very large, so I'm starting from here and make a step, I end up here. Okay? So I'm ending on the other side of the of the valley. And if you have not a convex optimization, I think that's better if I draw it. Uh let's see if you can see on the top. So if I have a function like this, let me make let's make this and I'm here and I make a step.
(1:13:21) I know it's going to be this way. But I can go from here. I can jump here. Then I can jump here. Then I can jump here. Then I can jump here. So if the learning rate is very large, I may be stepping left and right. Right? So, we want the learning rate small enough so I don't have to see these jumps.
(1:13:46) Um, but I want also the learning rate to be not too small because then it's going to be too slow. So, the next thing is when to terminate. I think that we let me let me actually say when to terminate. There's two actually ways we terminate. One is to limit the number of iterations. So you just say I'm going to do a 100 iteration or 100 little stumps or you say as long I'm going to keep going as long as my uh residuals or the average or the square of the residuals are not improving by some threshold.
(1:14:26) Let's say I am adding a new stamp and my loss has not improved by the square the sample the square of the residuals in this case has not changed by more than.1%. Or something like that. So you say look if if it doesn't improve just stop. Um yeah no but where there is one more thing ah here how did I miss that I went through that so how we choose lambda lambda can you actually check if this p is in the pdf I updated in the morning okay I push it it's already in the GitHub So if lambda is constant then it should be turned through cross validation.
(1:15:17) So okay so how we turn the lambda? Well of course always cross validation and there is many other methods that's where gradient descent comes in and all the things we have learned come in for us to understand how to turn the learning rate. So one trick we have is to make the lambda to be inversely proportional to the magnitude of your gradient.
(1:15:42) Uh and that's kind of methods we see in 109B which is called atom or momentum. For now when we do u boosting we just set the lambda. We look at the loss and we show you next week. You look at the loss. If the loss goes down nicely or the accuracy goes up nicely, that's fine. If it fluctuates left and right, it means your learning rate is too high.
(1:16:07) All right. So that I talk and that's it is actually exactly the time. All right. Let me say thank you. Have a very very nice Thanksgiving. I'll see you all after Thanksgiving. on Monday where we're going to do Adabus and then on Wednesday that we're going to do before you go we are going to announce some review sessions for the last week.
(1:16:33) I am going to do one on Tuesday December 2nd and Kevin is doing one on Friday December 5th and Chris is doing every day. All right. Thank you.