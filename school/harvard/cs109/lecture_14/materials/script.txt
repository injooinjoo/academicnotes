109 day 14 - YouTube
https://www.youtube.com/watch?v=JRQwSBkEl0c

Transcript:
(00:00) covered on the exam in under the guise of linear regression. Okay, everything almost everything. So, it's going to be a good review. Coming here today before your midterm, you got a little bit of a cheat opportunity. Okay, so a little bit of a preview. Today we are talking about logistic regression.
(00:21) We introduced it last time as a reminder, as review, what is logistic regression? When do we use it? when you tell us to use it. Now, when do we use logistic regression? What's important? How is it different than linear regression? The response variable is what? Categorical. You know, and in the simplest context for now, we're just assuming it's binary, zeros and ones.
(00:48) Of course, categories can not always binary as we know. And so the response variable one will be a success and a zero will be a failure for now. That's how we're conceptualizing this idea. Okay. And so that's our response variable. And our predictors can be anything. How do we link that y that zeros and ones to the predictors that are numeric or zeros and ones? How do we link them? not through a linear equation like in we don't just fit out a linear model we say we link through the probability of success right has that log gods kind
(01:34) of relationship okay so as a reminder we'll start real quick with a little bit of review do I have the right thing yeah I have the right thing um so let's the probabilistic perspective on logistic regression let's set it up our response 's variable is y, it can take on zeros and ones. We're going to say that the probability success p depends on those predictors.
(02:00) And since y is zeros and ones from earlier on in this class, what kind of distribution describes that scenario? The normal distribution. What distribution defines that scenario? the binomial distribution sort of the Berni distribution. So if you actually think about underlying this are zeros and ones each individual observation will be bernoli the binomial with an n of one.
(02:38) Okay, when you combine them all, you can think binomially, but it's thinking at the single observation level. Y is only zeros and ones. And we link them through that probability P. So Y given X has a specific Bernoli distribution with parameter P. And that parameter P we say has that exponentiated odds idea. Okay.
(03:04) And so that probability is E to the junk over 1 plus E to the junk. That leads to a likelihood. This is your Berni PMF. P to the Y 1 - P to the 1 - Y. And so if Y is zero, you're left with oh, this drops out. This becomes one. You're left with just 1 minus P. And if Y is one, the second term drops out, and you're left with just P. Matches that intuition. Okay.
(03:31) And then in here, we don't see any betas, but that's because we're just covering it up. And we could just plug in this into where the P's are. And so there is implicitly betas in that formula for the likelihood. Okay, so now we have the probabistic perspective. We're looking at the likelihood of P really the betas based on all these conditions. Great.
(03:52) What do we do to solve for our betas? What did we do? We use maximum likelihood methods. We have a likelihood. So what do we do with it? We maximize it. How do you maximize it? Take the derivative. But before we do that, what should we do? Take the log. Take the derivative. Set it equal to zero and solve.
(04:21) What are we taking it the derivative with regards to the unknown beta 0 and beta 1. Okay. Okay. And so we have two unknowns, two equations and we solve. All right. The problem is there's not a closed form solution. Usually in logistic regression there is like a special case. And then because of that we have to numerically solve.
(04:47) How do we numerically solve? How do we optimize a function? We do some sort of gradient descent. We do some sort of Newton's method, Newton Rafson method, something like that. And so we're just going to talk about gradient descent in this general perspective. Great. Okay. So we can run gradient descent on the minimum which is the negative log likelihood. So that's one way to numerically solve.
(05:12) That's what happens under the hood in skate. Great. Okay. Now that we have that all aligned, we can get estimates. We can interpret our model. That's what we're going to work on for now. And then we'll talk about performing inference. What is inference? Statistically speaking, two major forms of inferences.
(05:35) There's decision making and there's estimation of our parameters. Decision making is what we usually formalize as hypothesis testing. Estimation, we usually estimate something point estimate and then also put bounds of uncertainty around it. And so that could be a confidence interval, that could be a credible interval, that could be a prediction interval.
(05:53) Okay? So it's trying to glean the uncertainty in using data to estimate the unknown parameters. How do we do that in logistic regression? How did we do it in linear regression? You got an estimate, a beta. If you want to put an interval around it, what do you do? Yeah, you use it get its standard error either through bootstrapping or through probabilistic modeling, probabilistic formulas.
(06:23) and you do estimate plus or minus its standard error. That would give you a 68% confidence interval. That's weird. What do we do? We take an estimate plus or minus two standard errors roughly and that gives us our 95% confidence interval. Where is the two coming from? from the t distribution because things with linear regression follow t's.
(06:57) And why is it two? Because t distributions with any reasonable amount of degrees of freedom lead to something close to two. And so this is a back of the envelope on your exam kind of question. Just throw in the number two. Okay, that's for t's. In logistic regression, things are a little bit different.
(07:13) Our response variable, well, okay, our response variable is binary, but we can also handle the situation when our predictor variable is binary. This is a special case in logistic regression to help us interpret the model. Here's the example we're going to look at. So these are still that heart disease data set.
(07:32) And one of the binary predictors we said is what is the sex of the individual? Are they male or female? And given that what was the percentage of heart disease? A very simpl simplified visualization of the results EDA of the data. Percent of males that had heart disease that showed up was a little over 50%. % of females that had heart disease that showed up a little under 30%.
(07:54) Okay, we can define a binary predictor X to define males versus females and then we can fit a model to that. So if we predict heart disease based on biological sex, how could you calculate beta 0 and beta 1 hat? This is the closed form solution in logistic regression. I can solve for it here. When it's a continuous numeric x, I can't.
(08:19) when it's categorical binary 01 I can because I know the interpretation right out the model what is the model telling us gods is equal to beta 0 plus beta 1x close your eyes everyone close your eyes close your eyes close your eyes okay how to check all right you Open them. My X variable is female. Okay, I wanted to know which one was which. I forgot. My X variable is female. And I called it female because a success is a female.
(09:03) And so X= 1 implies it's a female. Xals 0 implies anybody else. We only have two options here. And so it's male versus female. All right. What is the interpretation then of beta 0? It's the law gods for when x is zero. It's the law gods for male. Okay. So this is the law gods of male. What do we call the males in this situation when x is binary? the blank group, the reference group or baseline group.
(09:53) And because the binary indicators are all in reference to that, what is beta 1? The difference in log odds. difference in log gods comparing females to males. We have our data. I mean, I don't know the exact numbers here, but we have our results. And so, the baseline group is males. What's the probability they have heart disease? Roughly, what do you guess that bar is? 52 53 something like that. All right.
(10:50) How do you turn that probability 52 53 into an odds? Make it a fraction. Make it a fraction of that's the other way around. That's when you convert from one to the other. This odds for males, the baseline group is going to be P hat over 1 minus P hat. All right, successes to failures. And so if we're at about 52, that would be something like 0.52 over48. There's my odds.
(11:27) How do I turn odds into log odds? That's obvious, right? Take the log. What log are we taking? Natural log. Okay, so I'm a sloppy statistician right now. When I say log odds, I mean natural log. Sorry. Statisticians are often very very sloppy with their use of logs. Okay, I could go down the rabbit hole there and I just I'm not going to touch it. Okay, we have the law gods for the males.
(11:53) We can get the law gods for the females. Similarly, well, let's just do the odds first. That's P hat over one minus P hat. What's our estimate of P hat? Height of the green bar. What is it? 20. What do you think? Say it out loud. Someone yell it. No. 25. Thank you. Great. Let's do it. 0.25 over 0.75. My odds is 0.33. Okay. I take the log of this.
(12:31) This is like 1.0 1 point. Yeah. 1.08. We'll say 1.08. Take the log of that. What do you get? Something that's just a little bit positive. Okay. Take the difference of these two or I should say take the ratio of those two. Take the log of it and that converts it back.
(12:55) And so if you take the log odd difference, you're going to get a negative turn, right? There's a drop in the log odds when you go from the baseline group to females, right? And so you can work out the actual numbers, the log odds for males, 0.214. So if you take the log of 1.08, hopefully it's pretty close to 0.214. And if you take the log of 0.3, it should be the combination of these two, which is about -1.
(13:20) And so the drop in log odds is 1.272. Okay, that's mathematically how you solve for the coefficients. But if I gave you this output and I said interpret this output, interpret beta 0, interpret beta 1. Don't leave it in terms of log odds, what should you do instead? You should exponentiate it. Okay? And so if you exponentiate it, you're just going to get back to the odds. And so the odds is 1.08.
(13:42) And when you exponentiate negative 1.272, what does it come back as? The change in the odds after exponentiating is going to be a multiplicative change. It's going to be a multiplicative decrease in the odds comparing a female to a male. Okay? It's a multiplicative decrease. All right? And that all agrees with the table.
(14:07) And if you plugged it all in and plugged in the actual numbers here, you would get back what we just calculated with me. How does this link to linear regression? reference group, binary indicator, response variable is just binary as well here instead of continuous. Things are now on the log gods.
(14:27) But the use of that reference group and a binary predictor still holds in linear and logistic regression. It's just now we're on the logot scale. Okay, great. Is that an inference kind? Oh, question. exponentiated probability. Yeah. How do you convert odds back to probability? It was in the formula here. Your P you have to do a full prediction knowing what X is.
(15:06) Then then you take E to the junk over 1 plus E to the junk. This is the odds over 1 plus odds. Okay. So once you get odds to convert back to a probability, it's odds over one plus odds. So we have to be careful. So you when you do a prediction, you can convert it all the way back to probabilities.
(15:36) when you're interpreting a slope in logistic regression, you're unable to convert it back to a probability because it's now a nonlinear transformation and that nonlinear transformation doesn't hold when you convert from odds to probabilities. So we interpret the slope in terms of changes multiplicative changes in odds and where that starting point off off is changes what it means in probability scale.
(16:02) Okay, so you have to be very careful. We're going to interpret those betas in terms of odds and odds ratios. We're going to turn predictions probabilities are a lot more natural to use. Okay, we did our first inference in regression. We got estimates. We interpreted those estimates. What do you want to do? You should still not feel great about inference in logistic regression.
(16:27) What else do we need need to do? Not only estimation but estimation with intervals, bounds of uncertainty. We haven't done that yet. We want to do hypothesis testing. What would be the natural hypothesis test here? The slope. Is the slope zero? Beyond a reasonable doubt, formalize that question just like you would in linear regression.
(16:52) And so we're going to test to see if that slope is significantly different from zero. What? In order to do that, we need that standard error. We need that standard error. Sklearn is great because it will give us estimates in a logistic regression model, but I don't know how to tell it to give me standard errors. I'm sure there's a way.
(17:11) I don't know how to do it. That's where as a statistician, stats models is my friend. And so, we're going to turn then to stats models to build our confidence intervals and our hypothesis tests. Okay, here's an aside. If you follow along because you've taken other stat classes and have seen logistic regression before, great.
(17:28) I talk really fast sometimes when I have too much coffee. Linear regression. When we did our inferences and we built a confidence interval through probability theory, what distribution did we use? T. When we do logistic regression and we do our inferences, we don't use T. We use Z. What's a Z? A standard normal.
(18:07) Okay? And so that T, magical T of two becomes a magical Z of 1.96. Okay? And so our intervals will include 1.96 times standard errors and our hypothesis test will compare to 1.96. Okay, it's a slight distinction. If you care, it's because you get the variance for free in a Bernoli or binomial distribution. In a normal distribution, you have to estimate the variance separately independently. And so that's the technical reasoning. Okay.
(18:36) Anyway, the way you actually do it mathematically, statistically is using something called Fischer's information. We have this whole likelihood function to understand how uncertain where our maximum point is. We have to know the curvature of that likelihood function or that log likelihood function. And that's what Fischer information is telling us.
(18:55) How curved is the log likelihood function where we're doing our estimation. Okay. Anyway, stats models is going to do that for us. So, how do we do this in stats models? I love formulas because I'm thinking like a statistician because I use R. And so I fit a logistic regression model stats models formula.
(19:16) lojit my Y predicted by my max heart rate. Now I've changed it to a quantitative X and this is for the same data set. I fit my model. I look at the summary output and here's what I get. Okay, it looks familiar. It looks just like the linear regression version of stats model.
(19:35) What's different? We still get coefficients, an intercept, a slope. We get standard errors. We get a zstistic. They get lead to p values and we get a confidence interval. Okay, where does that confidence interval come from? Where does that zstistic come from? Just like in linear regression, what did you do? You take your estimate, you divide by the standard error, and that gives you your t statistic. Well, in this case, it's a zstistic.
(20:01) You take your conf your estimate, you do plus or minus roughly two standard errors, 1.96 in this case, and you get your confidence interval. Simple enough. Let me make sure that match math works out. Boo. Looks right. Looks reasonably right. Okay. And so once you get that confidence interval and you get that zstistic, what is the conclusion as based on a hypothesis test? Is there an association? Yes.
(20:39) Why? Zstistic big in magnitude for the slope. For the slope, it's assigned p value tiny tiny tiny, right? And so we have severe evidence to conclude we there's uh a relationship here and in fact there's a reduction higher max heart rate the lower the chances of having heart disease. Okay. Also the confidence interval does not include zero. We're good to go.
(21:05) We used in this case stats models and its formulas based on Fischer's information etc to get our set standard error and to do our calculation. Great. Just like in linear regression stats models your alternative here. What's the alternative form of inferences and the approaches we took? What's the empirical approach? Pavlo wants to answer. No just loves the answer. Oh, he loves the answer so much.
(21:37) It's what's Pavlo's perspective on inference. How did he introduce inference? Bootstrapping. Right. So we can get this confidence interval through bootstrapping. Instead, we can get this hypo who hypothesis test p value through the permutation method instead. Okay, it's less reliant on assumptions and logistic regression because we don't have to worry about heteroscadasticity.
(22:01) We don't have to necessarily really worry about normality. We kind of get those for free because we have a response variable that's just zeros and ones. Do we still need to rely on large sample approximations? Questions? Okay, we know how to do inference. We know how to do interpretations. And now we're just going to extend it to multiple logistic regression.
(22:26) All right, multiple linear regression. What did that mean? What's the multiple mean and multiple linear regression? Come on, you're taking an exam this week. You got to know this. Multiple means. Say it out loud. Multiple predictors. Predictors, features, multiple of them. Hence, multiple.
(22:49) What do you think multiple logistic regression is? Same thing. All right. Multiple predictors, multiple X's, multiple features. Okay. So, now we're just going to generalize this whole framework. When we have multiple predictors, all of the machinery and issues come along with us. When you have multiple predictors, we have to worry about overfitting. We have to worry about multiolinearity.
(23:09) All of that stuff comes into play. Polomial features, interaction terms, we have to worry about that still. Okay. How do we deal with those issues of multiolinearity and overfitting? We're going to learn how to do things through regularization. And how do you determine what your best hyperparameter lambda shrinkage uh value is? You do cross validation.
(23:35) All of that machinery we're just going to bring into logistic regression. You okay with me? Great. All right. Multiple logistic regression. Instead of beta 0, beta 1, we follows the log gods. We just turn that into multiple x's. We're done. Mathematically, there's nothing different, honestly. Okay, we don't have to worry about matrix solving of things.
(23:58) In this case, uh just like in linear models, we're going to do the exact same thing with predicting. We're going to have to minimize the negative log likelihood just like we did before. And we're going to do that through numerical methods. Don't forget when you have a multiple regression or multiple logistic regression, your coefficient estimates are holding the other predictors in the model constant. You're controlling for their other effects and their other associations.
(24:22) So don't forget that tagline. When you have multiple predictors in a model, okay, we're trying to attribute that partial effect control for the other confounders that we've now included in the model. Multiol linearity still comes with us just like in linear regression. All right. So, here's how I think about multiple logistic regression. You know, you fit an S-shaped curve to a scatter plot.
(24:40) Now, we're fitting an S-shaped hype plane, hyper plane. It's not a plane anymore. Sshaped function into space. All right. And so, or threedimensional space. It just gets a little bit uh convoluted to think about with me sort of. Okay. Great. we need to worry about interactions. So let's take a step here and interpret interaction terms.
(25:12) Why do we consider interaction terms when we do modeling? What's the point at a high level? What is an interaction term allowing us to do? Anybody in the peanut gallery want to All right, let's come down to the bottom. So how one predictor relates to the response depends on the value of another predictor in the model. All right.
(25:54) And so if X1 is 10, how X2 relates to the response is one thing. When X1 is 20, how X2 relates to the response might be different. And by incorporating that interaction effect, we allow for that association to vary depending on the other predictor involved. Okay? And so we incorporate an interaction term. This all applies on the law god scale.
(26:24) And then what's nice is we will start interpreting these models in terms of decision boundaries. And so those decision boundaries, the geometry of them will depend on whether or not we're including an interaction term, whether or not we're including polomial terms. Okay, we'll come that's just foreshadowing. We'll come back to it.
(26:43) All right, so here we're going to predict whether or not someone has heart disease from age, whether they're female or not in the interaction between the two. Okay, how are these variables coded? Heart disease, zeros and ones. That's our response. That's why we're doing logistic regression. Sex, female, zeros and ones, categorical, binary, age, numerically measured. What scale do we think age will be on? These are people worried about heart disease.
(27:16) Hopefully, it's not like one and two year olds, though they might be in the in the data set. Hopefully, it's dealing with older individuals. We can fit our model. We set up our data set. We create the interaction term. I'm doing some manual work because if it's just one interaction term, who cares? It's the easiest way to do it. And so then I incorporate all of this.
(27:34) I don't have a penalty. I include the intercept. I think that's the default. I think that might now be the default. You fit your model. You get your coefficient estimates. Great. What's happening? You see this on midterm two. How do you interpret it? Telling you 3.4. What's the knee-jerk reaction for all intercepts in all linear parametric logistic regression models? It's the predicted response for when all predictors are zero. All predictors are zero.
(28:19) But what does it mean for all predictors to be zero here? You're not female and you're not born. Your age is zero. All right. And so it's clearly an extrapolation out of our data set, but that's just the knee-jerk reaction. Okay? And so let's not worry too much about interpreting what it means, but that's where your chance of having heart disease is low.
(28:42) And then age, what's the order of operations here? Age is the first, female is the second. Age, there's a positive effect of age that starts at an early age. Sorry, a positive effect of age that holds directly for the reference group of male. Females at the age of zero have a drop even further in the chances of a heart disease.
(29:06) And then the relationship of how age relates to the response is less positive for females than it is for males. Right? That0007 is the difference in slopes on the logout scale. Okay? And so there's a positive effect of age for males. That's the direct interpretation of this value. And it's a little bit less, not much different for females.
(29:34) Okay. And so here's what our data set looks like. Phew. Our ages start around 30. And what we see is we have two lines that aren't parallel. It's hard to talk about parallelism in with S-shaped curves. But we're allowing for those S-shaped curves to be in on the logod scale, not parallel. After converting back, I can't really interpret that nonp parallel scale.
(29:59) Okay. And we see females always a little bit lower than males. And in fact, that gap kind of widens as individuals age with me kind of. Okay. You can always plug in values of X and use these models to do interpretation. So could say, "All right, predict the probability that a male age 40 is going to have heart disease when they show up at our doorstep.
(30:36) " You plug in an x1 of zero, you plug in an x2 of 40, or other way around, x1 of 40, an x2 of zero, and a interaction of zero. All right, if it's a female who's 50 years old, what do you do? You plug in an x1 of 50. You plug in an x2 of 1. And you plug in an interaction term of 1* 50, 50. Okay? And then you can do the prediction that way. Okay? No different.
(31:03) It's just then when you do that interpretation, make sure you transform back to the odds scale, exponentiate those predictions, and then you can convert predictions to probabilities. Okay. Great. All right, we can write this all out. We can answer some questions. Write down the complete model. Break this down into the model predicting log odds of heart disease on age for males and the same model for females.
(31:29) Let's do that fairly quickly. I think it's worth our while. We say that the natural log that the probability that y equals 1 over 1 minus the probability that y equals 1 is equal to or estimated to be put estimates on it3.40 plus 0.0675 0675. That's age. Minus 1.08. Did that number change? No, it didn't. Minus 1.08 time female minus 0074 time female time H.
(32:26) to help you interpret what this model is saying for males. X2 is zero. How does this model simplify? Log odds. If this is zero and this is zero, you're left with just these terms. And so the log odds of having a heart disease for someone who's male is -3.4 plus 0.0675 times your age. For females x2= 1.
(33:26) What happens? You plug in one here and what happens to this negative 1.08? It combines with the -3.4 and it gives you a changed intercept if you will. So the log odds is now -3.4 minus 1.08* 1. If you plug in a one here, now this 0.074 is times h* 1 combines with the 00675 time h. And so that combines to a new slope and you get 0 675 minus 074 * 1 * h.
(34:16) And so you have a new intercept and a new slope. It allows for the relationship to depend on whether you're male or female, how age relates to the log odds of heart disease. If we didn't have the interaction term, this slope for the males would be same as this slope for the females. And we're allowing that to be different. Okay.
(34:39) How could we tell if those relationships are different? perform a hypothesis test for that interaction term because that's what it's estimating. Okay, these are the interpretations we care about. This is being doing data analysis in the world of data science. Okay, interpret this model we did in interaction term represents difference in slopes on the log odd sale. You can estimate the odds ratio of heart disease comparing men to women using this model.
(35:09) Uh well it will depend on how old they are. That ratio is going to be dependent on age. At lower ages it's closer. At higher ages it's further apart. Okay. Great. So this whole idea of doing logistic regression, getting our estimates. A lot of times one of the goals, we're going to try to visualize what's going on three-dimensionally. We're going to look at a scatter plot.
(35:41) We're going to look at a scatter plot where it's colorcoded or the points code who's the successes and who's the failure. And so on a slide, when you have lots of predictors that are numeric and you have an overfit model, you might want to simplify that visualization by using principal components analysis.
(36:00) This is why we learned it because a lot of times when you have a complex model with lots of predictors, you want to visualize what it's telling you. Okay. How we're going to visualize these relationships a lot of times are through these decision boundaries. So this question is new.
(36:24) How do we use logistic regression to perform pure classifications? Now what do I mean? What is a classification? It's a prediction. We want to predict the actual category for each individual instead of just turning it into log odds and turning it into probabilities. when we perform the prediction, we want to turn those predictions into either a zero or a one.
(36:50) What would be the most reasonable way to do that for a logistic regression model? What should you do? Type type. Study study study study study. It's pretty straightforward. Yeah, predict the probabilities. If it's over.5, say, "Oh, it looks like more likely to be a success. If it's less than 0.5, looks like it's more likely to be a failure.
(37:28) " And so, convert your model, turn it into probabilities on the probability scale, and the magical threshold cut off is if it's above 0.5, they're more likely to be a success. If it's below 0.5, they're more likely to be a failure. And now we can turn that probability calculation estimation into a zero or one. Great. And so that's how we're going to predict the ones and zeros. We estimate our probability through some model.
(37:54) For now, it's logistic regression. And we're going to convert that to a y of one if it's above that magical threshold of 0.5. People ask all the time, well, what if it's 0.5? Yeah, you know, round up if you want. flip a coin. It doesn't matter. Okay, so for now we'll just include the 0.5 in the rounding up group.
(38:16) Okay, it does matter not for logistic regression, but what it does matter for is when you do something like K&N for classification because you can have these ties pretty commonly. Okay, how would you extend this if Y has three or more classes? We've only talked about this classification modeling when you have a success failure situation other than when we motivated it. We had three groups CS stat or everybody else.
(38:42) This 0.5 rule is not going to work. Why not? It's pretty straightforward, right? Why 1.5 rule work? You're not guaranteed to have something over.5, right? So, what do you have to do? What's the rule? Plurality wins. Plural, hard word to say. Plurality wins. So, just look at the max of the predictions. You have to predict for all three categories. The highest might be 04.
(39:20) That's your classification. Okay? your error rates will be higher because you're not going to be at 0.5 very often, especially if you have 10 classes you're trying to predict. Okay, good. So, let's then talk about a decision boundary. We're going to say convert probability that y equals 1 when it's at least 0.5 into successes and failures.
(39:46) And then we can think about plotting that decision boundary on that two-dimensional space or onedimensional space or threedimensional space depending on how many predictors we have where that curve we saw that threedimensional curve right where it magically crosses 0.5 all right and it's going to be a line in logistic regression right because of the structure of the problem.
(40:14) So here's our 2D plot of our heart dis data set. We are looking at responses. Orange are successes. Success here is you have heart disease. I don't know. And then failures are blue. You don't have heart disease. The nose. And then we have two numeric predictors to visualize this. And so we have your maximum heart rate. We have your measured cholesterol level.
(40:38) And creating a decision boundary. Drawing the decision boundary is the goal of in this case in logistic regression. Let's draw a line, a single line on this plot that will separate the oranges from the blues. Do you think that line's going to do a good job? No. There's a lot of overlap here.
(41:01) But we're going to draw the line where above the line or to the right of the line or to the left of the line in one direction of the line the probabilities are above 0.5 and on the other side of the line the probabilities are below 0.5 with me. Okay. But it's got to be a line if we're doing logistic regression sort of. Okay. And if I were to eyeball it maybe it's something like this. I don't know.
(41:26) There'll be some line here that we plot. Or maybe it's like this. I It's really hard to visualize, but I can see there's a lot of blues over here. There's a lot of oranges over here. So, the line's something like here. Believe me? Well, let's see what logistic regression does.
(41:47) We want to fit a logistic regression model where one X is cholesterol, the other X is maximum heart rate. we get our coefficients and that actually defines what the line is because on the log odd scale a probability of 0.5 means the odds is one and the log of one is zero and so you have your beta coefficients and you just have to set that set of beta coefficients to zero and now you have mathematically defined a line how x1 relates to x2 because if you set the log odds of zero to this.
(42:21) So here's the mathematical equation. You turn the left hand side to zero. And so you're basically just solving for what max heart rate will what cholesterol level do you need? So in list regression the decision boundary is defined by that expression of taking your x beta right hand side of the equation and setting it equal to zero. Okay.
(42:48) And so for this problem when you do that get exactly this kind of what we visualize. Everything to the left we depict as an orange. Everything to the right we depict as a blue when we do our predictions. Okay that's just the line that breaks down or comes out of the equation when we set the right hand side to zero.
(43:14) Kind of with me? Why is it linear? Why is the decision boundary linear? Because we used a logistic regression on the log god scale that only had linear terms. We don't have to do that. What kind of features can we incorporate to turn that line into something turn that frown upside down? I don't if you are old enough to know that commercial. That line we can turn into a curve geometrically by incorporating interaction terms by incorporating polomial terms. All right.
(43:53) And so we can model other decision boundaries by incorporating the interaction. Now there's an interaction term and how X and Y relate depends on the value of the other term. And now we have a curve for that decision boundary. And if that decision boundary is a better one, that's suggestive that the interaction term is important.
(44:14) Just visualizing what this model is telling us. So in logistic regression, decision boundaries are linear. But in logistic regression, decision boundaries are not always linear. All right, I've just contradicted myself, but it kind of depends on what features you use. If you use the linear terms for your predictors, yeah, your decision boundaries will be linear. If you use nonlinear terms, your decision boundaries will be nonlinear.
(44:37) Okay, simple enough or may be nonlinear I should say. Cool. And when you have polinomial logistic regression, you can get all kinds of fun decision boundaries. And if you really care about uh decision boundaries, yeah, you can create and so with the right set of features, you can geometrically define a circle.
(45:01) But just be careful that circle might not really be might be a big extrapolation. Okay. And so really it just gives you a curve and we don't have to worry about the circle here so much. Why do we set x? Why do we set x beta to zero? Yeah, I did that really fast. Here's I think kind of where I talked about it, right? Why are we setting X beta to zero mathematically? What's the machinery as to why that defines the decision boundary to tidy up that question a little bit more? I can't say it any better, so I'll just repeat it. When you set the probability to be 0.5, that defines the threshold. A
(45:55) probability of 0.5 is what in odds? 0.5 to 0.5 is one. Okay. So probability 0.5 that's defining our threshold. That is an odds of one. And logistic regression is on the log out scale. And what's the log of one is zero. Okay. Okay. And so mathematically when we're predicting probabilities of 0.
(46:26) 5, we're predicting log odds log oddses of zero. Okay, great. And so that's why we set x beta to zero. Cool. The circles are fun. Great. How do you mathematically define a circle? It's kind of there for you. You know, incorporate all of the quadratic terms and their interactions. And since the betas aren't forced to be the same, it'll be a oval. It doesn't have to be a circle.
(47:00) Great. Okay. All that's kind of where I wanted to talk about last time. We got 15 20 minutes to talk about some new stuff. Regularization in logistic regression. What's regularization? How do we use it? Why do we use it? What do we call those types of models? What are the two classes of models that we defined as regularized models? Lasso. Yeeha.
(47:33) And ridge up on the mountains. Okay, they go together like you know cowboy. Think cowboys or cowg girls. So regularization in logistic regression is basically saying now that we want to think for an over dispersed over dispersed overparameterized model and overfit model we don't trust our beta coefficients we're overfit to the data and in linear regression when we perform lasso and ridge we regularized the loss function in linear regression what was the loss function if you don't know this for the midterm you're going going to have a bad time. What's the loss function in linear
(48:15) regression? Sums of squares error, mean square error. Either way, that's what we're trying to minimize is the error in quadratic terms of our predictions. Okay? And so that's what we did. We took our sums of squares in linear regression and what did we do to it? We added a penalty term.
(48:38) What was that penalty term? Well, it depends on if you're doing ridge or lasso or if you want to do elastic net, a combination of the two. Ridge said, "Let's penalize large values of beta in squared terms." Lasso said, "Let's penalize large values of betas in terms of absolute values." Okay? And so we just added that to the loss function as an actual penalty term.
(49:04) Okay? And then we had to tune what of lambda perform the best in prediction through cross validation. Okay, how do we take that machinery and apply it to logistic regression? What do we got to change? One thing, the loss function, right? It's not MSE anymore. What's the loss function in logistic regression? If you want a word for it or a phrase for it, do you remember what we called it? It was like the last slide I talked about on Monday.
(49:47) O entropy. Plug in binary cross entropy. Okay, is the term for it. But where did binary cross entropy came come from? Do you remember? I went through it fast. Where did the MSSE come from? From a probabilistic perspective, the log likelihood, we penalize the loss function coming from the negative log likelihood.
(50:22) In logistic regression, we're going to penalize the loss function that comes out of by minimizing the negative log likelihood. Okay? And that's what we call binary cross entropy. So instead of using MSE or SSSE, what we're going to penalize instead is that binary cross entropy term. And so now we just add our penalty terms to that. We get our arg min. Now we have a lambda and our beta j squared.
(50:54) So which one is this? Beta j squ means it's ridge or lesso. It's our ridge. It's our ridge. And note, it's going from one to P or one to J. It's not going from zero to P or zero to P to zero to J. Important to note. Does skarn still do this? Yes. Okay. Okay. So, sklearn instead of when you give it the parameter when you do uh logistic regression instead of doing applying the lambda directly you have to give it C where C is one over lambda.
(51:30) And so it's confusing at first but you get used to it. Okay. So just think of in terms of one over lambda. Big lambdas make your betas what? shrink to zero. Really small lambdas make your betas what? The standard OS in linear regression or the standard logistic regression in the logistic regression paradigm. Okay? Because if you're not shrinking them, you're getting back the standard method.
(52:02) Okay, cool. So, how do we tune lambda? So, now we have a formula. Give me a lambda. I can figure out what this penalized loss function is. How do we tune lambda? Through cross validation. Right? One of our knee-jerk reactions here. Knee-jerk reaction in this class. What's always the answer? It depends.
(52:29) But if it's not it depends, what's the answer? Cross validation. So on your exam, if you don't know how to answer it, just think, all right, maybe it's cross validation or maybe it depends or maybe it's MSE. Okay, great. So what are we doing when we regularize logistic regression? So if we have a overfitit polomial regression model, logistic regression model one way to conceptualize that is we have a two-dimensional plot.
(53:00) We just have x1 and x2 as our polomial features and in red we are mapping this is of course handdrawn beautifully mapping a very intricate decision boundary that has all kinds of folds and curves because it's like 10th order polinomial term with all the interactions of all those polomial effects. And what we want to do is we want to sort of smooth out all of those curves.
(53:25) And so we regularize it and we get more of a smoothed more expressive than just the quadratic function with me. Kind of sort of. Okay. Great. And so just one perspective on why we regularize. It's overfit, too clunky. We smooth it out. Great. Simple enough. Simple enough. All right.
(53:56) So now what's actually new for I think a lot of you today is we're talking about now change gears a little bit. We're still in the world of logistic regression, but now we're talking about the multiclass setting. What's that mean? Multiclass setting. Instead of zeros and ones, you got A B's and C's, zeros, ones, and twos, XY's and Z's, red, purple, and black.
(54:26) It doesn't matter. Different categories when there's more than two. Why does that complicate things? What would be reasonable approaches to attacking the situation when there's more than two categories in the response? Any ideas for those of you who are SAT 110 lovers we got a couple TFS I'm sure what is the broadening of the Berni distribution which has two outcomes to a variable that has more than two classes.
(55:18) It's not the binomial, it's the multinnomial distribution. And so we can think about modeling the multinnomial distribution here as our response variable. Okay. Okay. So this is kind of what we're going to do. Uh instead of having two categories, we're going to have more. Our response variable kind of depends on whether it's nominal or ordinal.
(55:39) When there's more than two categories, it matters if there's an order to your categories or not. In this class, we're only going to handle the situation where your response variables are nominal, meaning there's no order to them. If there is order to your response variable, when there's multiclass, there's a whole broad set of models called ordinal logistic regression, okay, that can handle that.
(56:04) Take stat 143 if you really want to learn something about ordinal logistic regression. I hope they teach it in that class. They don't really. All right. Anyway, oral logistic regression is basically when you have ordered regular categories. There's just a slight variation here. You can still use what we're going to learn as tools for nominal multiclass setting in that setting.
(56:26) It just puts a little bit more structure to it. Okay, great. So, nominal is when you have categories with no inherent order. Eye color. You ask me what my eye color is and I say it depends on my shirt. So, I think I'm green technically. I don't know. It's it's like it depends on what I ate that day.
(56:45) Two common approaches to handling those nominal cases. So this is the setup that sklearn uses and there's a default and then a notdefault. It's kind of at odds with the way I think of it about it as a statistician but it's not okay. There's reasons for it and we'll get into it.
(57:03) the two different approaches is now what you're going to do, not surprisingly, you're going to define a reference group and then you're going to have all of your other categories in reference to that reference group. You're going to build separate logistic regression models essentially standard 01 where you have a reference group, the browneyed group, and then you compare the greeneyed group to the browneyed group and look at the change in the probability.
(57:26) And then you look at the hazelideeyed group to the browneyed group dot dot dot with me. Okay. The other approach that you could do. Okay. So we'll just keep going with that. That's called the multinnomial logistic regression. You build a reference group and you build separate logistic regression models comparing each one to it. Okay? They all have the same reference group.
(57:52) They all have a different response group. Okay, for the case where we have three outcomes, we build in this case CS, stat and others. You pick one group to be the reference group here. Let's pick the smallest one in order CS from Y= 3 others. So my reference group is others and we want to predict whether or not someone is CS.
(58:18) We fit a whole logistic regression model to that subset of data. All right. Then separate we fit a model to predict y equals 2 from y equals 3 others. So this is a completely separate model for a separate subset of data though they both include the others. And then once you do that we have to think about how much computational time is that going to take. All right.
(58:45) Well it depends on how many predictors we have. In this case we're going to have predictors plus the intercept here. We're going to have P predictors plus the intercept here. And so we have twice as many in this case predictors as if it was just a standard logistic regression model. Okay. How can we use these models to estimate the probability of an individual falling in each concentration? This is the sort of important trick-ish question. You have two separate models.
(59:13) What comes out of them? One model is probability of CS versus other. The other model is probability of stat versus other. How do I combine them all together? Those probabilities don't add up to one. Right? And so you have to come up with a way to combine them together to do three probability calculations where they all add up to one. Okay? It's a little tricky when you do it this way.
(59:45) The interpretations of the betas are pretty straightforward, but it's a little tricky to combine those predictions into one. All right. Essentially, what this is saying is when we build this model in multinnomial logistic regression, imagine we have stat, CS, and other. Let me make sure I did that right. Uh, no, sorry. Stat, CS, and other.
(1:00:10) And so, we fit one model to predict CS from other. we predict another model to look at stat versus other. And so we're kind of building two different classification models. And so we look at model one to model K. Oh, sorry, group one to group K as a logistic regression model. We look at group two to group K as a logistic regression model. And we have to fit K minus one different models.
(1:00:35) Okay. When we do this, each separate model is fit independently and as standard logistic regression models. And then when we do this in logistic regression in sklearn, we just have to say multiclass equals multinnomial. And then we get our separate beta coefficients. Here we're trying to predict back to Kevin's uh expertise.
(1:00:59) We're looking at predicting the play type of an NFL play based on down and distance to go to the first down. And the three play types are pass, run, and other. And we're trying to predict whether it's a pass versus other and a run versus other. And we get our various different beta coefficients.
(1:01:24) Here, what's happening is really I just told you there should be two separate models. But what uh sklearn does is gives us three separate models. Oh, okay. Skarn change. So by default, logistic regression is the multial. Oh, they're now thinking like statistitians. I love it. You want the one verse rest you there's a separate like object you have to import. Okay. Skarn multiclass one verse rest class. Okay. Okay. We're doing multinomial.
(1:01:52) I should have I should look at this. code we get. Okay, so in section this still would work. I think uh you might get an issue with multiclass equals multi. It's still an older version of SK. Yeah, it's still an older version of SK. All right, so realize all of the math I talked about works.
(1:02:12) Let's not worry about so much here what the output is telling us because it will look a little bit different. But what we're looking at here is we're looking at three different play types. We have whether or not there's a run, that's group two. We have whether it's a pass, that's group one. and we have everybody else. That's group zero.
(1:02:30) And what we're trying to do is predict how each of these plays are going to be, whether it's play type zero, one, or two. And two will be our reference group in these models because that is our largest value. It's our kith group. All right. If you're not following the application, don't worry. I'll come back to it next time. Okay. With the upcoded sky, it will work. It's just it's deprecated and will be soon.
(1:02:50) Yeah, it will yell at you. It will yell at you. So the key idea when you fit multinnomial logistic regression models essentially what it's doing is then in the end when you want to figure out the probabilities it has to do some rescaling. All right. And so here we have k minus one predictions.
(1:03:10) And if we just kind of combine them all together by doing a little bit of substitution and then convert back into probabilities that are going to add up to one. Essentially sklearn can do a prediction of whether or not a single observation is in class K versus in any other class. Just has to rescale all those beta coefficients.
(1:03:34) And so what you get out of the multinnomial logistic regression I think unless they change the output is going to be interpreted slightly differently. Okay, I'll come back to this. I'll come back to this. And then there's a whole machinery of one versus R. And so I'm going to skip all this because apparently it's not an sklearn directly.
(1:03:58) Once I get that sklearn stuff tuned up, I'll come back to it. Okay. All relates to softmax. I'm skipping all this blah blah blah blah blah. Okay, fun stuff. Regularization. Okay, let's get into the heart of doing some classification because we're out of time. We're not. You got a a midterm coming up. We're running out of time. You got some good clues by paying attention today until Kevin screwed up the sklearn implementation and its updates.
(1:04:39) Passcode today. Rain, rain, go away. Rain, rain, go away. Good luck on your midterm. Hopefully today's class will help you out. Don't show up late. A couple little things. Some of the problems on your midterm are denoted 209 only. If you're a 209 student, you have to do it. If you're a 109 student, you don't have to do it, but you can have fun with it if you want. Uh just don't waste time on it.
(1:05:14) They're at the end of the multiple choice part of it. There's a line breaking them in between. And then the last couple of problems are the short answer. One of the problems mixed in the multiple choice is actually an asking for a number. So just be aware one of the problems feels weird because it wants you to give a number not a a b or c. Okay.
(1:05:35) What's the answer to default answer? Cross validation which is always going to be b. I have no idea. Right. B. Is that what SAT says? C. B. I don't know. C. See. Cool. Good luck. Good luck. Good luck. I'm gonna have to see go. Okay. You need to pick up