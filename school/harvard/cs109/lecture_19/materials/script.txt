109 day 19 - YouTube
https://www.youtube.com/watch?v=-T_tMqGQzqk

Transcript:
(00:00) Three, two, one. Test, test, test. Can the balcony, can you hear me? Okay. Yeah. All right. Good morning everyone. Welcome, welcome, welcome. I am Pablo Protovabas and I'll be your lecturer for today. Um, one quick announcement as people come in.
(00:27) Uh, on Wednesday, we're going to have Kevin, uh, the lecture that was supposed to be last week, he's going to do it on Wednesday. He will be talking mainly about missing data and how to deal with missing data. He started a week ago a little bit but he will complete that on Wednesday. That's an important aspect for any project you do. There always some missing data some missingness you have to deal with that.
(00:50) So we're going to cover that on Wednesday and he has a little bit visualization if time permits and then therefore next week we're going to start with what we call unsample models which is going to include bagging random forest gradient boosting adaboost. So the next two weeks we do all the ensemble method and then the last lecture will be about mixture of experts actually which has got a lot of attention the last year due to the fact that all the large language models all of a sudden start using mixture of experts.
(01:21) I have to say when I was teaching it last year a lot of students said do we really need to know that and of course you do because now everybody's using mixture of excurs ore that will be the lecture after Thanksgiving all right so as people coming welcome again I am pablo protovas welcome all we're going to be doing uh regression today decision trees with regression what we're going to do going to take everything we have learned from last week today's Monday right yes what we have learned from last week and we're going to apply it for regression. It is easy peasy today uh as
(01:57) I say it's all progressive overloading but every now and then we have to do a de loing. So we're in the de loing week this week and then next week we start putting more uh complexity. Okay. So we start with uh kind of reminding us that winter is coming.
(02:16) I can't wait for the snow to come so we can do some skiing and things like that. So we go to Italy today. The Dolommites. Uh this is Elon Orwen, which tends to be always the pattern. Whoever put gives me the photo and I put it up. The student is not here. Is Elonor here? No, it's just always the case. It's one of those paradox. All right. Ready to start? Balcony VIP section. Yeah. Good. All right. Let's go.
(02:46) So here's what I'm going to cover today. I'm I'm going to talk about uh decision tree spot for regression and then we're going to go back and revisit numerical vessels categorical attributes. We have seen that with one hot and coding. We'll see it a little bit more in the sense of trees and a little more examples.
(03:06) And then finally we're going to be talking about pruning. Uh every good lecture should only have three bullets. I kind of violate that every time, but this is all we're going to do today. So, let's start with decision trees for regression. Okay. So, by the way, this is an actual photo I took in Littleton, New Hampshire. I think it's very funny tree. Do you need new clothes? Yes.
(03:30) Shade demas. No. Sure you do. Shows demas. I I did talk to Emma, so I got permission to use that. And she has few other ones. So every time I go to uh this a place in the north New Hampshire uh she always have one of these funny ones. So I take pictures of this. All right.
(03:51) So how can we uh how can we use these kind of decision trees uh for regression? Uh this is when we have quantitative outcomes. So we started this uh course with regression. Then we move to classification. Now we going back to regression. These two problems we go back and forth between classification and regression. Okay. So today we're going to be talking decision trees for regression.
(04:17) And the question is how do we use the decision tree ideas we learned last week. You all I heard from all of you said yeah that was clear. I got it. Let's apply now to regression. Okay. So these are the questions we want to consider when we do that. One is how would you determine any splitting criteria? Remember when we did for classification we have some splitting criteria anymore.
(04:42) Anyone is awake enough to tell me what were their splitting criteria we use for classification or decision trees? I know everybody knows but who is awake enough to raise their hand and answer this question? We look at three different splitting criteria. Anyone? Yes. going gen index classification error and the last one it starts with an E ent good. All right we warming up.
(05:16) So and what would be a reasonable objective function to do that goes hand inhand with the other and finally how we do prediction. Uh these are the three main things I want to consider. If I have these three I'm going to follow exactly the same pattern as before. All right. So let's start with a very simple example. Okay.
(05:36) So you'll see the idea in this very simple example for regression. R Y will be continuous variables. In this case it gets values between minus 1.5 and 1.5. And I only have one predictor one feature one coariant. All these mean the same. I don't have three. Okay. One feature X. Yep. So what do I have here? I have the Y versus X. Easy peasy regression problem.
(06:07) So we have seen how to model this with two models so far. one linear regression polomial regression in particular will be appropriate for that and the other KN&N we have seen that right now we're going to do a different approach and as I said last Wednesday uh linear regression even polomial are good when the surface the response variable has linear behavior or at the worst uh polomial so here is not linear obviously and let's trying to do it with decision tree.
(06:40) So as I said the first thing we do when we do decision trees E2 is to split right? Yeah. So we're going to split this uh domain the input range into two parts. So let's say we split there. Okay. So now my first note it says if x is larger than 6.5 you go to the right. If it's less than 6.5, you go to the left. Yeah.
(07:16) So now on the right, my prediction here, that's an important difference. In classification, the prediction was what? If I'm in a lift node, how did I predict? What will be the prediction of the lift node? Do I take a random variable? Do I just decide yes or no? What how did I decide in the lift node? What will be the prediction? I give you 10 seconds to think.
(07:52) We took the majority we took right so because it was classification. So in each region we have either blue or red or apples and oranges, lemons and oranges. We have all these things. So what was the prediction in that region was the majority of the uh examples we have or the training example. Okay. Now in this case we don't have majority. I mean there's no majority because it's a continuous variable.
(08:19) So what do we do? We take the mean clear. Okay. So we predict in this region here the mean of the data in that region which is 697 and on the left of course will be again the mean of the points that they are in region one the left region which is so happened to be negative 0.008 008. Okay, cool. All right.
(08:49) Now, how did we choose this splitting criteria for this regression tree? I can tell you or I can give you 30 seconds to think and anybody who get an idea, raise your hand or just shout. We're not so many. 30 seconds is the time I go to crease and back. Pacing thinking how will I decide where to split? Is it 6.5? Is 6.1 or 3.2? What would be a good splitting criteria? Minimize MSC. And on the back row there, somewhere there's like a visible gap in the data.
(09:25) A gap in the data. Well, you mean in the X or in the Y? Think about it. Our goal here is to create as pure region as possible. That's what we did last week. Pure in this sense, they should be similar, right? I want each region to have similar outcomes, right? Because at the end of the day, I'm going to predict with the mean.
(09:56) If the mean is not very good representative of the region, uh it doesn't matter if they're gaps, then I won't. Does that make sense? Yes. Okay. So, minimize MSE. So I'm going to say it in plain English is that what I want in each region is I want to have as similar points as possible because what I'm going to predict is with the mean and if the mean is not good representation of the points there I'm making a mistake right a big not if it if the mean is not good representation it means what we have a lot of scatter there you remember the R square we had on like lecture
(10:30) seven what we wanted is the R square to be small. Meaning that I want to explain all the variance with my prediction. Right? So what I want to do here is I have to have a region where all the points are very close to my mean and therefore the variance is small but the variance in a region is the my favorite loss function MSC. Right? Good.
(10:57) All right. So what I want to do is to create regions that in each region the MSE is as small as possible. Let's see. I have on the left my MSE here will be the prediction which is in blue and I apologize if you cannot see it very well. Uh is the true value minus the prediction which in this case will be the mean of the points in that region uh square and then average it over. Okay.
(11:26) on the right exactly the same. I have the mean of the region two which is this blue here. I get the true value minus that square and then I average them over the points on that ridge. I told you it's easy peasy today. Right? So what I do I'm going to move that threshold such as the MSE in each region is minimized. Right? Now think about it. We have two regions.
(11:52) By moving it, I can make one region very low and the other not so low. Or I can move it this way. So that region is very low and the other maybe not so low. What is the next thing I have to consider here? Because I have two MSE's to consider. I want both MSE to be low.
(12:14) What is the next thing we did on Wednesday in order to make sure that Yes. Waited the MSE. You're competing with Jacob. Jacob. No one. Yeah, you two compete now. Who's going to answer the most question? Okay, everybody, let's just break the monotony of Jacob and Ricky. Ricky, Ricky and Jacob are bouncing all the questions. So, I want today I'm going to be focusing on this Richard. Okay, Nea, you have the answers.
(12:39) I know you do. So, don't be shy. Leing and chem and all of you. Let's have a competition. We have team A and team B. Let's see who's going to win today. Okay. And everybody else is welcome, of course, right? All right. So, we're going to take the average of the two. So, we have that. Um, so this I said is the variance of that thing. So, I'm going to get the weighted average.
(13:06) So, if a region has more point, I'm going to weight that region more than a region that doesn't have many points. That should make perfect sense. A common sense thing we do, right? Okay. So, now what do we do? I minimize over the threshold but also since here I only gave you an example that it has one feature one predictor it's kind of too simplistic in reality what do we have we have multiple of them right so I'm going to try to find for it I'm going to kind of find the minimum for every predictor and every threshold right now let's think about it for a second let's say I have five predictors so for each predictor I'm going to
(13:48) and then start splitting at different values and then find out which splitting criteria gives me the lowest weighted MSE and I go to the next predictor and I play the same game and then the next predictor I play the same game etc. Now for each predictor I have multiple splits I can do and then let's say I five predictors I have five predictors each one different threshold and I compare and I find the predictor and the threshold that will give me the lowest weighted MSE okay now one question which is not on the slides but I want you to think uh let's say I have thousand data points
(14:33) How many splits will I do for one predictor? I have thousand data points. How many splits I'm going to try? Let's put this team back. Uh you do it until it didn't give you like a margin. Yeah, I would split it until the change to MSSE wasn't helpful anymore. Sean, let's think about this. This is not we're not splitting down.
(15:05) We just have one predictor and we're going to try different splits, right? So I split I split and then at one point you say, well, okay, it's not getting better. Stop. Well, you don't know if the other side would give me a better split, right? Tit maybe there's some self condition. No, no, guys, you're confusing. We're not going down the tree.
(15:33) I have one predictor I'm going to try all I'm going to try different thresholds which how many threshold shall I try says choose a note until your your computer starts smoking right that's pretty much what you say yes the answer is in principle I am going to try every possible unique value in that predictor. We're not going to do any fancy gradient descent or anything. We're just going to try all of them.
(16:14) Okay, all of them digested all of them. Skarn does some tricks. I'll explain at the end. It does some tricks to avoid doing all of them because in the region you may have some splits that you already know that they're pure. So you're not going to go back to it. But if I ask you to implement which I think I'm going to ask you during the section this week when you do it from scratch, you have to go all possible values that you have in that predictor. Okay, cool. All right, here we are. Now we split.
(16:45) There we have the mean. Now let's now move into going down the tree. Another split, right? How do I split? Well, I'm going to take the left and I'm just going to do the same game. Find me a split that will minimize the weighted MSE. And I did that. Okay. And I predict the mean for each one of them. Okay. And then I go to the left and I do the same one and I predict the mean again.
(17:12) Good morning, Kens. Okay. So, we split, we split, we split until the usual stuff, right? Is that clear to everyone or is that too simple? Shall I go a little faster or a good? No. Okay. All right. So now I can do this five times. This is maximum depth of five.
(17:35) And if I do maximum 10 depth of 10, there is some red lines there that you can imagine, but it doesn't matter. It's just the splits. Right now, what will happen if I keep splitting? And you can kind of see it. If you turn off the lights in this room, you'll see it better. But let's not do that yet. So what if I keep splitting hot? What will happen at then? Overfeeding, right? Yeah, we'll be overfeitting, right? So what do we do when we overfeit? What did we do for overfeitting for classification trees? Cross validation on what? On our stopping condition. Excellent.
(18:19) Okay, so we're going to put some stopping condition. And what are the stopping conditions for classification? Let's name them first. Maximum depth. Then we did oh I don't remember the minimum lift nodes, minimum numbers in the leaf. Uh and then there was the other one number of lifts into the thing. And you remember we talked about um level first or performance best.
(18:45) Um yeah so the same will apply exactly here. Okay, the same ideas. Maximum depth will be one of my stopping conditions. Why not? Maximum depth three, maximum depth four. U minimum lift minimum lift nodes is how many points I have in each lift node. The same here. Now, how did we decide what is the best let's say maximum depth for classification? Cross validation. How will you decide here? cross validation always.
(19:21) The only difference is that in classification the cross validation we're looking at the validation accuracy. Here we're going to be looking at the validation MSE. Okay. So we check between different maximum depths and see which one gives me the best MSE or R square.
(19:46) I prefer we always use MSE when we comparing and not use R square. And there's a reason for that. If you use R square is not a mistake. A lot of people will do that. I and I believe Kevin and Chris we believe using the MSE when we've choosing models because with the R square your mean can change if you change your validation set.
(20:07) If you do different trials your mean can change. So you introduce a little bit of stoasticity on your R square. So I don't like that. So I prefer to always use MSE. Okay. Cool. All right. So let's move on. So here's the stopping conditions. The number of points in the region all the same. The only the only one that we have talked is that we keep splitting until the genie index was not improving.
(20:33) Here we keep splitting until the difference between the MSE without split and the MSE after we split is smaller than a threshold. Okay? So you say I don't want to split if my improvement of my MSE is smaller than 0.1. Okay, so everything else are the same. So without splick and after split. Okay, how do we predict? You already went through that. I finished training. I have a tree. I give you a data point.
(21:06) How I'm going to predict the value of Y? Remember Y is a continuous variable. What would be the idea here? And you already seen it. You just have to narrate it. Now I take a point and I have the tree already built, right? What do I do? I get to the first note. I decide left and right. I go to the next note. Left and right.
(21:32) Eventually I end up in a lift node which is going to be the average or the mean of your points in that region. Good. Easy peasy. Okay. I give you a time for questions. If there any doubts as I said today's a little bit del loying less concepts easier concepts the number of people in the balcony is increasing versus down by end of the semester they all going to be there watching videos or whatever they're doing on their computers I don't mind it's fine Sean John John get it closer I'm doing gradient descent. Oh, I was I was curious.
(22:17) How do you go about setting a stop? What's a good stop? Like you said 0.1 with that gate. Um there's not a magic formula. It it also depends on the scale, right? So let's say you're you're measuring as we seen before an example. we measuring in let's say in units of sale or thousand of units of sale your MSE scale would change right uh you may want to look at the percentage of the change versus the original one and you can put 1% that's what I would do I look at the percentage of the change of MSE not just the change
(22:57) of MSE I look at the percentage and if it drops below 1% I'm happy but also depends on the problem if 1% of your MS may see means a million dollars for your company, you may want to go a little bit lower, right? If 1% means one cent, you said 10% is good enough. So, it it's very dependent on your data and your problem.
(23:29) Okay? But I would recommend to look percentage, not the absolute mclassification for this one. Go for class for classification what will be the val validation criteria is usually accury or F1 score or uh ROC area under the curve that's what I'm saying use one of those so let me repeat the question for the video is when we do cross validation we cross validate with some metric some criterion metric right so the question is what shall I use we definitely don't use usually cross entropy binary cross entroproy because at the end of the day what I want is not a model that has good
(24:11) crossent binary cross entroproy I want a model that gives me good performance so if your data are balanced I will use accuracy if your data are imbalanced I use f1 score if you care if you have some utility and that's important I'm glad you asked a lot of people uh code the area under the curve which is gives me the heily gyps when I see it because the area under the curve tell me how good the model is in general right but if I have a utility and I think we have it in the section do you see this thinking they do have it in the section yeah in the section this
(24:51) week you'll see if I have a utility what do I mean utility I care about false positives I care about true negatives it depends on the problem if you're let's say in a medical field and you want people to save lives, you may want to minimize the the true help me uh the work I'm missing and the D false negative. Thank you.
(25:19) It's hard to remember to speak something. So you want to minimize the false negatives. You don't want to miss anyone. But that's a particular case for for example medical field. If there is another problem maybe what I want is to minimize the false positive. So you have a utility. That's what I mean by utility.
(25:39) You have something in mind, right? And in that scenario, I maybe use that criteria for my cross validation. Does that make sense at all what I'm saying? Yeah. So at the end the cross validation I want the best model for my particular uh problem I have. And for you for your projects, you may have different things. Accuracy is just one metric. And especially when you have imbalanced data set, you have to be careful. The accuracy is not a good metric.
(26:05) We talk about F1 score. I believe we did. That's a a better metric to have. But also consider the fact that you have a particular problem in hand. You may care about the false negatives more than the false positives or the true positive. It depends on the problem.
(26:25) And in the section, I believe we have that we have three different scenarios with three different utilities. And they will show you our amazing TFS how to deal with that. I mean, it's just obvious. I have different utility depending on the problem. Sometimes I care about this, sometimes I care about that. The area under the curve tells me which model performs for most scenarios. Okay.
(26:47) Uh, all right. Good. Did I cover this good enough? The dog agrees with me. So let's move on. All right. Uh so let's talk about numerical versus categorical attributes. We have talked about that. It's a little bit of review plus I want to go a little more step by step. So here's my example and there is a reason I'm bringing it up because skarn has a little limitations there or at least it had until last year when I did this lecture. Uh here is the the example we have. We have flowers.
(27:20) I have sunflowers, rose, tulips and orchid. That's my target to predict which of these flowers uh we have and I have two predictors the seal width and the color. Okay, now I want to make a decision tree out of that. Okay, so let's try to do that. So I start with the seal width bigger larger than four.
(27:46) What is the prediction is of course orchid because it's the only one I have in my example that has simple length more than four. Okay. All right. Now the next thing I want to do is to split on the color on the other side. How do I split on the color there? Remember uh it the whole idea about decision trees is this idea of compare and branch right split and go compare and branch right now the color here cannot be done that because is categorical there's no larger than red it means nothing when we said larger than red clear okay so what shall we dohing
(28:34) recording. Team B is coming from behind or team A. Sorry, your team A. They're team B. All right. So, team A says, or you God, I'm good. First round, I'm going to get your names here. And if you guys start talking up there, I get your names up there. Even though I know some of you uh from office hours and others. All right. So, one hot encoding.
(29:00) But before we do that, why don't we just encode the simplest way zero for yellow, one for red, purple for two. We said that oh the ordering is bad or whatever. Let's just actually look what the problem is. Okay, because I don't see any problem by just putting zero is yellow, one is red, two is purple, right? All right, so I do that. If it's larger than four, the seal width, then it's or kit.
(29:27) And if it's smaller than that, let's look at the color larger or equal to two. Right? Given the encoding I did on the top, what would be? Let's see who can tell me what if. Yes. What will be the answer? It has symbol length less than four and the color is bigger than or equal to two.
(29:55) Which is the only answer possible here? is the tulip right okay full on the other side I'm going to again split by color and I have color bigger equal to one so is rose and the other is sunflower brilliant I have a decision tree who said that's a bad idea Kevin I didn't say let's see if Kevin is right he tends to be right all the time so let's see the example if we do this encoding let's see and the possible not trivial splits on color right just splitting on color that's what I'm saying was it a question no so I if I split at bigger than zero I have yellow and then red and purple if I split at
(30:40) one I have yellow and red and purple okay so those are the only splits I could do agree okay so this is bigger than zero bigger than one but if I change my encoding now yellow is two red is zero and purple is one. If I do the same story, now I have red and yellow and purple or red and purple and yellow.
(31:02) So I get different answers. Okay, that's not good. That means it depends on my encoding. I will be I will be getting different answers, right? All right, that's Kevin was right. So let's do what Kevin says. He says no, I'm not never right. But it's okay. Uh so depending on the coding, the split you optimize over can be different.
(31:27) So why don't we uh use ordinary encoding? Actually I think was me giving that lecture. So I was right. Damn it. All right. So we're going to do one hot encoding as you said. So now what I'm going to do it means I'm creating new columns. Each column now is zero and one. Column one tells us if is yellow.
(31:53) Column two tell us if is red and column no column one is purple column one is red and column three is yellow zero and one right easy peasy and by now you should think about it and tell me so let's do it so now if column is bigger equal to one that should be the tulip right yes if now the column is equal or bigger than one should be the rows and then the other one is self. Okay.
(32:28) Now notice one thing we need to point out here is that we don't need that right one we can drop one because is the exclusive of the other one. Okay. Yeah. Okay. So we don't need this one. So here is the statement. This is where I did this as it stands. Scalar and decision three do not handle categorical data. So you'll have to do it before you pass it to a scaler.
(32:58) And I believe if you do it, what happens Chris if I don't do it? Does it give me an error or its Okay, let me let me let me actually sum up few things here. It's important. We use sklearn as a decision tree for our model decision trees regression or not. Uh actually I think our handles categorical correct Kevin. Yeah. Now uh decision trees in general and we're going to see the main the more serious decision trees next week which is random forest and boosting.
(33:33) uh there are libraries that they use they do that which is not skarn is xg boost and adab boost and the likes and cat boost and couple of others those have categorical I mean those will deal with categorical var but if you use your plain sklearn for your project keep in mind you need to do the one hot encoding by yourself okay cool all right so let's summarize um this is my learning objective. I've been doing that for all the lectures.
(34:04) So, you know what I expect since now we are been told to give you fair grades. Of course, the university puts it back to us. When we get the score evalu core evaluation of three, they will not come and save us. But nevertheless, uh let's talk about the learning objective. What we want you to know. So that's nicely summarize it.
(34:28) The first one, how does the prediction process prediction process differ between classification trees and regression trees? I know the colors are not friendly for this slide, but can you see? H yeah. Uh the answer is classification trees predict a class labor for each leaf node while regression trees predict the average value of the response variable in the leaf. My favorite emoji. Yes, of course, we just talk about it.
(34:53) uh explain how mean square error is used as a splitting criteria in regression trees. MSE's calculate for each potential split representing the average square difference between predicted and actual values in each region. The split with the lowest weighted MSE is chosen and describe the concept of accuracy gain in the contest or regression tree.
(35:15) I put this question on purpose because the word accuracy usually is associated with classification but accuracy could mean anything as long as your model is accurate. I can use the term accuracy. Yeah. Yeah. So accuracy gain is the reduction of MSE from the split. If the gain is below a threshold the split skips effectively pruning the tree.
(35:37) And finally why is the simple numerical encoding for categorical feature often problematic? Numerical encoding can impose an artificial order on categories leading to illogical split and then compare the contrast ordinal encoding with one whole encoding. Okay, but we just saw that I explain why comparison like feature is not applicable to categorical features. We saw that because it doesn't make sense.
(36:02) Okay, this seems to me very easy and I hope you agree with me. All right. So, the next thing we're going to do is a little since you guys ask me ask us all the time. Do you have practice practice questions? Practice. This is our first practice question. Get your phones out. Take anything uh join the quiz.
(36:27) So, these are actually questions that I would have put in the practice one. Uh we're going to do it now live. And because also Kevin and I and Chris are thinking for the B, whoever is thinking about taking B, we're not going to have attendance, but we're going to have pop quizzes. So get ready, right? You're going to drop 30%. Don't panic. All right.
(36:54) Ready? All right. You have 30 seconds. Why is a greedy algorithm approach justifies justify for decision tree? You can read it. I don't have to read it. Are you taking the quiz, K? You better do well. Three, two, one. All right. the computer finding. Oh my god. Okay, it's pretty good. Next one. Ready? And then I'll give you the leaderboard.
(37:43) Okay, so B is correct. Okay, next one. Ready? 30 seconds. I think so. I think I I I don't have full control of the slide though yet. I'm experiment. All right. Five, four, three, two, one. All right. The correct answer is true. Actually, I put this question on purpose because I wanted to make this point.
(38:28) You're allowed to split on the same one. Okay. And I have some example. You see color color. Right. So you are allowed actually not allowed. It is in the in the algorithm you can split width with with w with there's no reason not to. So the threshold will be different. So, it's like Yeah.
(38:57) So, uh if you haven't got this right, I wouldn't feel bad because I didn't mention I put it on purpose because I was meant to mention it to make it clear, but now you see it. Uh the next one, 30 seconds. We making a prediction. That should be easy. 2 1 zero. Boom. All right. So 75% seems to be our thing, right? When making a prediction for new data point using train regression, what is the final step in the prediction process? That's the one. Uh we have 154 people participating.
(39:54) Some of them are not able to get to it, but that's fine. Uh, okay. 75% success. That's a point to to start. Yes. Doesn't the tree already have the value we should have at? Yeah. You have the average value from the training. Is the average necessarily the it should not be the average response. in that area. In that area, maybe I'm not understanding your why you need to search every single possible.
(40:35) Oh, you're just saying that's that that's a last last one. This is a little bit trickier to read. At least a regression tree produces four terminal regions. Uh R1 90 points. So the second number is the number of points in that region. Uh what is the question? What is the MSE of this? Sorry. No, no. Yeah, it is.
(41:11) What is the MSE you expect from this tree? Yeah, I apologize. Random number generator. Well, I don't blame you. I now now you understand the question. You may want to think there is a region with 90 points. the other regions are very small, right? So the final MSE is a weighted average. So it's going to be dominated by the biggest region which is 0 2 and the other numbers don't make sense to me.
(41:38) So three is a correct answer. I didn't even calculate it. They just put it there. Okay. All right. So let's see now who is the winner. Kevin Raider alcohol. You get one attendance free. Kevin, you can no just don't come when I teach. Uh Aush, where is Aush? Aush. You get an attendance two attendance free. And Danny.
(42:10) Well, Danny, what is TF? Is that you Danny? Where is Danny? You get an attendance free. Okay, we need to write this down at one point. Yeah. And Kelly, where is Kelly, by the way? Let's give Kelly something, too. Kelly. H Kelly, you get an attention. Congratulations. All right. This is fun. I hope testing what we learn.
(42:41) And um this is the kind of questions will be in the next quiz. Okay. Um, all right. Let's move on into our next uh part. No, don't keep it active. You have your chance. Slider. All right, we continue with Elonor from Switzerland. I said I'm thinking snow now.
(43:26) I want the snow to come so we can go skiing and play in the snow and have fun. Okay. Um, so this this part is just about pruning. Um, this is another way of reducing the complexity of the model. I'm gonna go I have very detailed slides about this um just so we go through it and understand who put it there. I didn't put it. Okay, Kevin, that was for you. You put it in there. Okay. All right. Let's move on.
(44:00) So, what is the major issue of pre-specifying a stopping condition? You may stop too early or stop too late. How can we fix this issue? Uh, choose several stopping criteria and cross validate to decide which one is the best. Okay, let me explain that. Um, we talk about four or five stopping conditions, right? And for each one of them there is a hyper parameter.
(44:23) Let's say maximum depth is is the maximum depth. Uh, minimum lift nodes lift points is the number. So for each one of these stopping conditions, I have a hyperparameter. Okay. Now, if you do cross validate, I kind of I don't know if I said it, but usually what we do, we choose one of these topic conditions and we find the hyper parameter and we're hopeully good.
(44:50) But in reality, what you have to do is to actually use all of them. So now I have to do cross validation for each one of them. And for each one of them, I have to cross validate on the hybrid parameters. I'm going to stop here and make sure everybody understand this because that's key for many things to come in your journey as a AI data scientist.
(45:15) Meaning that if you have some hyperparameters either is the uh choice of stopping condition or some value that is determined that you need to cross validate for everything. And what would that be the problem? Why am I concerned about that? Why don't we do that? Yes, you can. But why am I skeptical about that? What makes me feel uncomfortable about that? It will take forever.
(45:50) Right? I think every time we have a new cyber parameter if we do cross validate remember is the curse of dimensionality here because what I have one axis will be the stopping condition and the other is the choices I have for that hyperparameter. Now you add another axis which we're gonna do next week with bagging we have the number of trees and then on top of that in random forest we added another one and in boosting we added another one we're going to have six seven choices to make that will make it extremely slow to run to do the cross
(46:26) validation. Okay so sometimes we want to avoid that. So instead of doing that, we're going to use an alternative approach to this method which is prune. So you let the tree grow all the way and then you go backwards.
(46:46) Now think about stopping condition is like you have a tree and you put something on the top and every time it goes above that you chop. Here I'm going to do the other way around. I'm going to let my tree grow and then I do the ddd dry dead and What's theility in pruning? You're a farmer. Dry, dead, or something else. When I prune a tree, not the decision tree.
(47:15) All right, let me explain what I'm talking about. I'm referring to actually pruning physical trees. When you prune a physical tree and I have trained I have pruned actually trees. I climb trees and I prune them. There is a rule dry dead and something else disease. Yes, thank you. Yeah, the farmer is here. I'm I was a side remark that I confused you all and apologize. Let's get to the point.
(47:39) Okay, so I'm going to prune the tree, right? So, I'm going to let the tree grow and I'm going to start chopping branches. Okay, so how do we do that? Here is an example. This is evaluating applications to graduate program. We look at the GRE. If it's bigger larger than 320 then we look at the GPA and then we work experience how many years and to now if you look at this and you start getting anxieties because you're thinking to apply for graduate program don't look at the above average only for pedagogical purposes nobody uses a decision tree like that trust me I've been in the admission committees okay so we don't use a decision tree okay we
(48:19) just randomly select people no I'm kidding All right. So let's focus on that one. Now I'm getting a little bit. So if I take that branch out. Okay. So meaning that I'm going to I'm not going to do the splitting there. I'm going to look what is the majority of my training set and I subsidy with reject. Okay. So that's the idea of pruning.
(48:45) So if you look at this plot, we have model complexity on the x-axis. Then the prediction error. We have of course the the train and the validation train or test the train goes down as the model complexity increases of course as we make the model better it's going to go it's going to do better and better on the trainings but that means over fits right yes and we have the test that it goes down and then it start coming up okay now we're going to let it grow all the way here and then I'm going to start pruning it right so I'm going to reduce
(49:17) the variance of the model, right? We have seen this before. Same ideas. We want to reduce the variance of the model, right? Okay. All right. So before I show you this, let's talk about think about when we did regression for linear regression or logistic regression. What did we do? Go back to your memory.
(49:46) What was the step we did? We did both for linear regression and we did it for logistic regression. We started with our loss function and then we did what? Huh? We added an extra term, right? We added an extra term to regularize. In the case of reach we said the square the sum of the square of the coefficient should be small too.
(50:18) In the case of lasso or L1 we said the sum of the absolute values of my coefficient should be small. Now can I apply this here? I don't have coefficients. Okay. I don't have coefficients better. Maybe the thresholds. Yeah, that doesn't make sense, right? Why would the threshold should be small? What makes the tree overfitted? If you see a tree, before I tell you if over fits or not, I show you a tree with two branches and I show you a tree with 20 depth.
(50:58) Which one you think is going to overfeit? Most likely the large one, right? But we want to be somewhere where not underfeitting, but we don't want to overfeit. All right. So with that idea of how I regularize, I don't want my tree to be very deep. So let's add a term into my objective function that will encourage not very large trees. Okay, so that's what we're going to do.
(51:25) Here it is. This is called the cost complexity pruning. Let me let me go step by step. That's the decision tree. T here represent the decision tree. This is the classification error of my tree, right? And this will be the regularization term. It has two things.
(51:50) The number of leaves in the tree is what I want to keep low. And of course, we have alpha which is the complexity parameter or regularization parameter. We have encountered things like that. Let me see how I'm going to fix this alpha. I'm going to go step by step and let's see if you how do we fix this hyperparameter alpha? How did we fix the hyperparameter for reach regression? What? I'm going to ask the doggy. What do you think? Yep.
(52:26) Here cross validation baby. Okay. Yeah, here he is guys. All right. So, cross validation of course, but let's go now step by step to explain this. Okay. All right. So, let's say we have this tree. The full tree has I call it T. The error I have is 32. The number of leaf nodes are eight. Correct? Agree. here one 1 2 3 4 5 6 7 8 trees lift nodes right okay so the cost complexity will be the error and I put alpha 002 so I calculate to be 1.
(53:17) 92 okay now let's prune and I I'm going to throw away that thing right uh now my the smaller tree will have error of 33 higher. This tree has a higher error in my training data. Okay, the number of lips now seven because I eliminated one, right? And now the total cost complexity is 1.73. Now you see the point.
(53:46) I'm going to select this because it has lower cost complexity. It's the same thing we did with the rich regression. we added an extra term my MSE or in the case of logistic regression the accuracy of the regularized model may have worse accuracy in the training data. Okay. All right. So that's good. So that that's good. So let's go now step by step.
(54:16) Um yes, we're going to try this. I'm allowed to show the steps. Okay, good question. So, let's start. Here's your answer. You remember what I said, the good professor, amazing professor. I anticipate your question. Ah, okay. Let's look at the whole tree. So, here's my whole tree.
(54:36) So, the question, how many possible ways are there to prune this tree? Uh, anyone? One, two, three, four, five, six, seven. Can you see them? I can cut here. I can cut here, here, here, here, here, or just kill the tree from the beginning. Right? So, there's seven possible ways. I'm not sure if I want to cut the tree from the beginning, but let's just say seven.
(55:04) Okay? So, these are seven possible ways I can go just chop, right? All right. So, u let's do the first. Let's say I do that, right? That's my first one that I found. If we prune here, this now becomes a lift node. So how do we choose the best prune tree? Well, here it is. We will choose the one that maximize the difference of the cost complexity between the full tree and the prune tree. So let's do that.
(55:32) So here's my cost complexity. You remember it? So we have the decision tree, the classification error, etc. So what I'm going to do, what I want is to maximize the difference between the original tree and the prune tree. Here it is, right? And then what I do, I arc max over the t.
(55:58) I put now I expand CT because CT is the error of the tree plus its complexity factor and then minus this minus that then I divide by alpha ttt I get that then I the one doesn't matter and then I take the minimum by switching the order so at the end of the day this is I'm not worry if you don't follow step by step you can do that at home. But the at the end of the day, what I'm trying to do is minimize this thing here.
(56:28) It's equivalent as maximizing this. Okay? A little bit algebraic manipulation to make sure that's so the reason I put it here is because when you read the books or other books or other sometimes they just or skarn or anything they were just going to have this thing. So it's a little bit confusing.
(56:48) So I said why don't I just at least show the students where that coming from. It is basically maximizing the difference between your cost complexity between the original tree and your prune tree, right? You want it to be as you want to reduce it as much as possible. Okay. So now we again consider all possible pruning from T1. So let me explain what happen.
(57:11) I have the seven possible pruning things and I try all of them and I choose the one that has that maximum. Now I have this tree and I'm going to prune again again. Now I have three possible pruning points. Right? This part is already gone. So now it's going to be doing that.
(57:35) What is that algorithm tells you? What is it? Is greedy, right? Because I'm going step by step, right? All right. So now I have three possible ones. I'm going to do the same maximizing the difference of the cost complexity of the of this tree minus the cost complexity of any prune thing I'm going to do and I'm going to choose the one that gives me the maximum. So in this case I prune them. Okay.
(57:57) Now I'm going to do it again and again and again until I end up with the root note. Okay. So now I have all of them and now I have to decide which one of all these should I choose. So let me repeat what happened. I start with the full tree and I said I can prune it seven different points. I choose which one of these points will give me the best model.
(58:22) Then I cut that and now I have a tree with bunch of branches. I ask again where should I prune next? I did this and I keep doing until I end up with the stump. Okay, now I'm asking the question, which one of all these should I be using? Team B, what do you think? I'm waiting for Adam. No wrong answers. You can just guess.
(59:08) Well, how would you go about I have now a bunch of two substrings and I need to choose which one should I use? Cross validation. Cross validation. Yes, we're going to cross validate to that. Of course, we're going to just check each one of them how well is doing in the validation set. Okay. And if it's a classification, I'm going to use accuracy F1 score or whatever. If it's regression again in my validation, I'm looking at the MSE, right? Yep.
(59:35) All right. All good except there's one more thing here. Are you ready for that? Take a deep breath because we're not done. There's one thing that we haven't specified that alpha this prunes that we do and we choose where to prune does depend on the choice of alpha. What shall I do then? Cross validation. So there's two levels.
(1:00:07) One is cross validate to find which true tree to prune given alpha and then I'm changing alpha and I'm doing it again. Then I change alpha and I'm doing it again. I'm change alpha doing it again. All right. So that's pretty much the pruning technique we have. There is variations to the theme. I'm not saying this is in general. Let me actually make the following statement.
(1:00:27) Going forward since we started trees. Uh what we're going to be presenting is kind of the canonical models. Their variations to the theme you can find them especially when we go to XG boost and all these things. But what I'm going to be presenting in the lectures is what I call the canonical tree. So this is the canonical pruning. There is variations.
(1:00:52) You can do all kind of smart things here. um if you feel free to do it. What I'm saying this is the one you're gonna find in most libraries, right? Okay. So, it's not the end of the story how to prune, but this is a way to prune and this is the standard way, right? Any questions? I'll give you 30 seconds. That means I walk here and I walk back otherwise I start too fast.
(1:01:19) Go ahead. Yeah. cross validate like twice. Yeah. Yeah. For each pruning level. Okay. Which pruning level. You can do them together, but that's a little or you choose alpha then cross validate that find which prune tree. Choose the other alpha. Do the again pro them that. And now I have a list of for each alpha my best prune tree. And then this will have a score. And then I choose the one that doesn't. Yeah. Cool.
(1:02:05) All right. So let's talk about my summary again. I this is my summary for that and then I have a little bit of time. Not so much time. I mean I I can finish a little bit earlier. You don't mind, right? Do you? No. Okay. Uh the summary and then we have again another quick quiz to see how we doing.
(1:02:31) So what is the main drawback of predefined stopping criteria for decision trees growth? And I said predefined stopping criteria may lead to trees that either too simple or too overfeitted since it's challenging to determine the optimal stopping beforehand. Okay. Even though we do cross validate that may be not enough to punch you. The second one explain the alternative approach of using stopping conditions for decision tree.
(1:02:51) Instead of stopping condition and alternative approach is grow a tree large and then prune it back. This is going to be find and balance structure of reflexibility for optimal complexity. What is the core concept behind pruning? Pruning simplifies a decision by removing branches with minimal impact on overall accuracy and hasing the treere's ability to generate.
(1:03:18) And then describe the formula using the cross complexity while CT is equal to the error of the tree plus alpha which is a hybrid parameter time the number of live nodes. Right? That the number of live nodes is a proxy to the complexity of your model. Right? The cost complexity. How does the complexity parameter alpha influence? Let's think about it.
(1:03:44) If the complexity parameter adjusts the penalty for tree size now, the higher values alpha, we're going to be favoring smaller trees, right? If I make alpha very large, that means the penalty is higher. So, I'm going to be favoring shorter trees. If the alpha is very small, I'm going back to unpun trees. Right? Uh how do you determine the best prune tree from a set of candidate trees generated by pruning? The best prune tree selected by maximizing the difference in cost complexity score. Okay. All right.
(1:04:13) So these are the learning objectives that I have in mind. These are the kind of things that I want you to know. This is how they motivate me when I write quiz questions. It doesn't mean that this is it. that if you know these things very well, you're a good student and you deserve a good grade. Yes. And finally, we have five minutes, 10 minutes to do another game.
(1:04:45) Who wants to participate today? If you don't, I can get the TF. uh you get extra attendance for free in case you're on the threshold between 66% and for people that they have declared attendance they were not here please remove them from your calculation because I know who you are okay and I'm going to remove it at the end of the semester so if you're on the threshold and you count sometimes you're put the attendance in but you're not here I'm going remove it at then.
(1:05:22) So our grade at then will depend on actual attendance. Sorry but I know who is here who is not. Um yes let's play the game. All right. So your name is Sidi. Siddi. Yes. And you're a sophomore in dunster. Oh d are you the only dunster student? Maybe anyone from dunster here? No. Okay. I go houses not by the quality of the students, but the quality of the food. So, my favorite one is the one on the quad.
(1:06:01) What is it called? It has really good food. Huh? It's not curry. Is the other one with what are the other houses? Huh? No, the other ones. Yes. That has the best food. D. Huh? Yeah, I think he has the best. I think so. Okay. I haven't tried this year to answer the foot. I go by the foot. Sorry, I'm a shallow person. Okay. Ready? Um, so the first question is consider a decision tree initializing the following using skarn.
(1:06:44) So this question a little bit harder so to get you ready for quizzes and midterm, right? Uh so this is the the command which they haven't seen yet but which of the following is true? The model will have four leaf nodes. The model will have a linear decision boundary. Skarn computes the maximum likelihood estimates to decide the best feature to split on and the model will have a nonlinear decision boundary. back to in depth one.
(1:07:15) I'm not entirely sure. I feel like it's C, but I'm not sure. Let's think about it. By the way, I put sometimes in my quizzes, I put these fancy words to lure you in. Did we talk about maximum likelihood estimate in any of our so it can be don't don't get fooled by me. I put these fancy words to see if you're keyword matching person or not.
(1:07:46) Uh so maybe B we talked about linear decision bound. Yeah. But why not A? I feel like the command doesn't tell us much about oh well they said maximum depth one. So there's only one split. So how many live nodes you gonna have? Two. Yeah. Split, right? So A is out, C is out.
(1:08:13) Uh well, the D could be if it was maximum depth higher than one, right? Because max depth one, I'm just doing one cut, right? So it's going to be linear. So the answer is final answer B. B. Of course. Good job. I have a couple of more. Anybody else wants to participate? All right. And your name is Kenza. And your which program? MDE. MDE. Y. We have a lot of MDs here. All right.
(1:08:52) Say I'm Quincy House. Okay. So, Kenza remember the rules. You can ask a friend or you can ask the audience. Okay? Now, friend does not include Kevin or Chris or Theodoro. Just to be clear, you can ask any of your friend. Nea is back there by the way. She I feel I I heard her. All right. Are we ready? All right.
(1:09:24) So the question is when splitting on a continuous predictor such as height of a threshold of 178 cm the possible splitting value will be height not equal to 178. Is that true or false? Do I have a third one? Sometimes I do the third one. Yes. can determine. So let's think decision tree is to what? Split. Would that split? Yeah. Who said yes? Confusing your friend is not equal to 178.
(1:10:09) Let's say I have in my training data four values. 160, 172, 178, and 190. And will that split it into two or not? Unless we have like 17. If we have 178, we split, but otherwise does split into that, right? So, is that true or false? You think it's false? Let's ask the audience here. I'm What are you asking your friend? Oh yeah, she's calling you up. What is it? Huh? She said false. Nea said false.
(1:10:57) Are you Do you trust her? Okay, false is the final answer. And very good job. MD rocks. All right. I believe I have one more. Do I have time for one more? Yeah, I have time for one more. Um, actually I don't have time for one more. Uh, the word of the day is oak. Oak tree. Oak. O a k. Thank you, Chris. I don't pronounce it right again.
(1:11:43) That's why nobody left. I was wondering. Can you all see it? All right. Thank you guys. Thank you very much. I'll see to you all. Kevin will be here for people that came late. Kevin will be teaching missingness on Wednesday. Next week we is going to go a little bit faster and we're going to be