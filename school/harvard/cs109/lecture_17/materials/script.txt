(3) 109 day 17 - YouTube
https://www.youtube.com/watch?v=pYQRUPHyUHg

Transcript:
(00:02) Check, check, check. Good online. Great. Good morning, data scientists. Welcome. Welcome. Welcome to our first class in November. Yikes. craziness. Hope you had a good Halloween weekend. Hopefully you added some time to, you know, have some fun. You know, we need to do that at times, too. I went trick-or-treating with my kids. Two pillow cases.
(00:43) The little one's a little bit of a packrat, full, 20 lb. The other one about half full. Doesn't really care. So, about 10 lbs versus 20 lb. So, come by my office anytime this week to get some candy from my kids cuz they don't eat the candy then. it just goes bad. Anyway, a couple announcements. What's due this week? Two things, right? Milestone for your project.
(01:14) Just update the the project topic essentially. So, play around with the data, get a sense for it, and redefine your project question. Okay? And then the big thing is there is a problem set a homework due uh tomorrow as well. So come to office hours uh and get some help on that. It's doing some modeling, interpreting results, and doing a little bit of bay.
(01:41) Okay? And we're going to talk a little bit about bays on this homework as well on the next homework. And that's where we're kind of starting with today. Questions out there? Yeah, everyone's a little slow when it's cold. Just like, you know, my car just takes a while to get it going. What are we talking about today? We're talking about two very disconnected main big topics.
(02:11) One is performing this uh sampling of a posterior distribution which we didn't get to finish last time and then the other completely unrelated is dealing with missingness. Okay. and so missing data. So we'll try to merge them in some way, but that's our task today. Okay, we talked a little bit about posterior distributions and posterior predictive values last time.
(02:42) And so when we perform a bay analysis, we bring our prior belief. We have our likelihood. As a result, they combine into a posterior distribution after collecting our data. And we can use that posterior distribution in a lot of different ways. It gives us a posterior distribution for the unknown parameters, treating them like random variables.
(03:08) We can use that posterior distribution after collecting the real data to predict where future observations will be and that kind of has two pieces. There's the natural variability of the observations conditional on the parameters and now the parameters have a whole distribution and so it just makes that process a little trickier. That's why we had to do this posterior predictive distribution. Okay, it's sort of a two-stage process.
(03:32) And when we do that, oh boy, my notifications are on. What did mom say? Let me turn that off. She's on a cruise, so who knows what she said. She may have also said happy birthday. Who knows? Stop, Mom. Happy face. Wow. Hey, thank you. Thank you. Yes. Thank uh I get to teach, which is my favorite birthday present.
(04:18) Uh so couple of things when dealing with a distribution, we had to worry about making things sum to one. Mathematically, that's important. uh when we combine this is what I talked about with a predictive distribution for future data. It's that tool for forecasting uh our choices of likelihood and prior the posterior distribution a lot of times have the same format mathematically that's convenient because we can write things down and solve things nicely both with a computer or by hand when we want to do calculations. That was the property of conjugacy.
(04:51) And we have lots of conjugate examples. If you want to see them all, they're on wiki. But we saw two big classes of conjugate types of distributions. We said when we had the regression problem and we had normally distributed response variable, okay, we would bring a normally distributed prior for the mean parameter mu or the betas involved with mu and the posterior distribution would also then be normal.
(05:22) Okay, in that same system of models, a lot of times the unknown is the variance and one over the variance we assumed would be G as a prior and then the posterior would also be gamma. Okay, we saw that. How important is that? I'm not sure, but it's nice to use mathematically. When we then got into classification models, we talked about logistic regression.
(05:47) But first we talked about the binomial distribution directly and its unknown parameter was P and we put a beta prior on P directly. The binomial likelihood led to a beta posterior as well. Okay, we turned that beta binomial model into a logistic regression model and everything fell apart because we had our unknown betas linked to that unknown proportion through a nonlinear function and that nonlinear function the logistic function destroyed that beta binomial beta prior.
(06:23) Okay. And so we fit prior distributions for the unknown betas in a logistic regression as a normal and we out of that we get an unknown named distribution in the posterior. And so the question is how do we handle a posterior distribution that we can't say has a specific family name? And that's what today is about. Okay.
(06:50) How do we deal with a posterior distribution that is a function and we don't know how to sample it from it because it's not just simply CDF of a normal or something like that. Okay, that's what today's all about. Uh dealing with this complexity. We're going to handle that through simulation through these Monte Carlo methods. Okay, Monte Carlo because we like to game. It's not a one-sizefititall solution.
(07:15) Uh sometimes we might actually get something out of using pen and paper and the conjugacy property because it makes our life a lot simpler if computation is slow. Okay, hopefully we're sort of motivated. Oh, here's a slide. Did you get your midterms grades back? Want to know how you did? Here are the distributions.
(07:46) I used R. It's just easiest for me just to get a quick plot. But here's the 109 distribution of grades as a percentage. Here's the 209 distribution of grades as a percentage. Denominator here I think was 44. Denominator here was 36. On canvas, it's given to you as out of 80.
(08:11) Why all these different numbers? Well, the points on this is the the in-class portion. Doesn't take into account the coding. Okay. Okay, the in-class portion was out of either 36 or 44. This is just turning it into a percentage. That's easy to do. Divide by either 36 or 44. And we put it on Canvas out of 80 because the coding portion's coming. Okay.
(08:30) And so the midterm in total is 80% of this and 20% of the coding portion. And so that'll add up to a nice 100 points. Okay. So all of these grades realize are almost all of them will get pulled up because the coding portion the grades are much higher. Okay, just realize that. All right, let's look at these distributions. How would you describe them? We're good data scientists.
(08:51) What do you think? Left skewed. Almost all exam grades are left skewed. If the mean's above eight above 50, they're going to be left skewed almost always. If the mean's below 50, they're likely to be right skewed. Why? Because if it's below 50, we're closer to the zero than the 100.
(09:08) If we're above 50, we're closer than the 100, then the 50, then the zero. And so that kind of defines the tail. Looking at these, you know, the centers, you know, between 80 and 90 is a bulk of data. Between 80 and 90 is the bulk of data. Maybe between like 75 or 70 and 90 is the bulk of the data.
(09:26) Okay? So, if you had to guess a mean and a median to these two distributions, how would you compare them? Two different groups. 209's a little higher than 109 on average. On the average though, the highest grade was in 109. Someone got 100. Kudos to you. How else would you compare them? 209's a little higher. 109 had a little more spread. 109 had the highest grades.
(09:56) What else do you notice in the distribution? There might be some biodality maybe two modes. Okay, that's suggested of two groups of people potentially in each of the groups. Specific numbers mean here 75, median here 77. If you care about the standard deviation is 13 doesn't really tell me much in a very left skewed distribution.
(10:16) And then the mean for 209 two points higher roughly two and a half points higher. Okay, all in all pretty good. Okay, it was a hard exam. A lot of you did pretty well. And if you did worse than you were hoping, come talk to us. We'd love to talk. Okay, solutions are posted. I believe should be on ed.
(10:39) People can see them percentages, right? Great. All right, we've reviewed, we've caught our uh caught our breath. Maybe we'll come back to reviewing some of the answers on the midterm as well at some point. Uh probably what I should actually say before I move on is how are you doing? All right, after we see this graph, how are you doing? How am I thinking the class is doing? To me, you know, there's the big thing that came all out uh from the deans about grades are too high here at Harvard. Okay? And so we're going to try to make grades lower. No, we're not actively trying to make grades lower in
(11:19) this class. We want your grade to reflect your understanding, your mastery of the material. So if you show mastery, a 90 or higher, you're on the way to an A or a minus. This is just one piece of the exam of your final course grade. Problem sets get play a role. The quizzes play a role.
(11:39) They were higher grades. The final exam plays a role. They all contribute. Project plays a role. Contribute to your final grade. If in the end you get 90 or higher, you're probably going to get an A minus or an A. If you get in the 80s, you're going to get in the B range somewhere.
(11:59) And those boundaries typically get pushed down ever so slightly because we look for a gap in the grades. And so if one person gets a 90, another person gets an 89.99999999, we're not going to cut the grade off at 90. We're going to push it down until there's a natural break in the grades. Okay? because someone who got an 89.999999 is the same student who got a 90.0 with me. And so that's our perspective on grading.
(12:21) And that's what you can expect in the end pass history. Majority of people got A's and A minuses. A little less than that got B's and just a few people got below that. Okay. So just realize that's kind of the distribution we're working with when we come to the end. Anything to add? Happy with that? Okay. I don't want to offend my uh fellow fellow instructors. Great.
(12:48) Back to real stuff. Well, of course, the exam's real stuff. We're talking about sampling from a distribution in order to get a sense of where that distribution is. Okay? We're going to create random observations from a distribution. The simplest way to do it is just to draw. If you know it's a normal distribution, just draw random observations from a normal distribution.
(13:13) Plot the histogram and pull out from those histogram results the mean, the median, the middle 95%. We've kind of taken this approach before. Where did we do this empirical estimation of a distribution? In bootstrapping, in permutation testing, we don't know. We don't trust the probability theory. We don't know what formula to use.
(13:42) So we just empirically create this sampling distribution. Okay, we're going to take the same approach now, but now we're doing it under the Beijian context under the posterior distribution. Okay, the rationale for it is a little bit different. We're still applying probability theory. We just don't know how to sample from the resulting formula because it's not a nice name distribution. Okay, we're going to take two major approaches.
(14:08) The two major approaches is something called rejection sampling. It's like throwing darts at a dart board. And then the other is through this Metropolis Hastings. One is called Monte Carlo. The other is called Marov chain Monte Carlo. Okay? And we'll explain the difference here. That's the MCMC. All right. So, we're going to throw some darts.
(14:32) Imagine you wanted to uh sample from the unit circle, but all you knew how to collect was the unit square of observations. And so what you do is to determine which pairs of observations X and Y to keep is you throw a dart to the dart board using another random variable or set of random variables. And if it hits the target, you keep the aligned piece of data.
(14:56) If it misses the target, you throw away the aligned piece of data. Okay? And that's sort of the idea here when we do rejection sampling. Let's think about this in context of a real problem. All right. So, we have a target distribution. We want to sample from. What is our target distribution here? Put a name to it. The posterior distribution from some bay method.
(15:20) Okay, that's what we want to sample from. And it's not a normal. It's not a binomial. It's something complex, a combination of things because the prior is not a conjugate or a nice mathematical combination with the likelihood. Okay? So the posterior its functional form is unclear.
(15:39) So what do you do? Well, you guess you use some other distribution and then you say, "All right, let's sample from that distribution and do our best based on that other distribution." We want to sample from the unit circle, but all we got is the unit square. Okay? And we decide whether or not we're keeping it based on a separate random variable which is coming from a uniform essentially.
(16:07) All right? And so what are we doing? We're looking at the height of our target distribution. We're looking at the height of our proposal distribution. And we get to keep that observation if our target distribution, the posterior, is bigger than what our proposal distribution says.
(16:34) Okay? And so we flip a coin, a biased coin, to determine whether or not we're going to keep it. And it depends on how close our actual posterior distribution is to the proposal distribution. Okay. And that fraction is important, right? I think a visual is worth a hundred words here or a thousand words or a million words. And the example we're going to use is Cookie Monster.
(16:56) I'm stealing this from my colleague uh Alex who loves to talk about Cookie and Gonger because he's got two little kids. Okay. So, here's Cookie Monster. All right. What's the issue? Cookie Monster is looking at and is trying to determine some characteristics of a bakery that makes a lot of cookies.
(17:20) Sells four varieties of cookies: chocolate chip, oatmeal, raisin, peanut butter, and sugar. Which of these are the worst? Yeah. Nobody wants raisins in their cookies, right? Yeah. I'd throw those out. All right. And what we're trying to figure out was the diameter of all these different cookies.
(17:39) And we want to look at essentially what those distribution would look like if we combine all four types of cookies together into one distribution. All right. When we have four different cookies that all have different distributions in the sizes of their cookies, what do we expect the resulting marginal distribution to look like? One cookie centered at 10, another cookie is centered at 20. They're varying around that 10 and 20.
(18:08) If you combine them together to one distribution, what do you think it's going to look like? What did we say about the grade distribution? We saw sort of biodality. That biodality suggests there's subgroups in your data. Okay, there's four subgroups here, four different cookies.
(18:27) So if we look at that marginal distribution combining all four types of cookies together and we're kind of looking at the diameter of those cookies because size matters for cookies for cookie monster here we can just look at that overall PDF. Okay, how did it this is synthetic data. We're just sampling from four different norms. Okay, and we see there's kind of three peaks here.
(18:51) Where's the fourth peak? It's kind of hidden in the right tail of this distribution. Okay, depending on how far away the means of those normal distributions are determines how many peaks you get. Okay, and we want to sample from this distribution. I don't know they're coming from normal, but I give you this distribution. I say sample from it.
(19:11) How would you do it? You take the rejection sampling approach is one approach to do it. Okay. So, what would be a good guess for sampling from this distribution? A single distribution. What does it look like to you? Looks like a collection of normals to me just viewing it. But let's imagine we try to use a proposal distribution.
(19:41) This is our posterior use a post post uh proposal distribution that is just a single normal distribution. Okay. And so we could start off with a single normal distribution and use some rejection sampling to get us there. Okay, so we have our target population and what we're going to do is just say, all right, the area under this is one. We're going to set up a proposal distribution that has area underneath it that's one.
(20:11) And the problem is if we sample from this proposal distribution that's a single normal centered at the middle this proposal distribution's PDF is sometimes below and sometimes above our target population our target distribution. So all we do is we essentially just rescale that distribution so that our target population distribution posterior distribution is contained within that proposal distribution. Okay.
(20:47) So that we can determine once we actually get the target distribution has to be sort of a subset of all the possible values we could get. Okay. So, how are we going to actually play this game? All right, we want to use the red distribution in order to create samples from the black distribution. What we do is we sample from the red distribution.
(21:11) Where is a single observation from the red distribution most likely to be? Around 10, right in the middle. Okay? And so you're most likely going to get a observation a cookie diameter when we sample right near 10. Maybe it's 8, maybe it's 12, maybe it's nine, maybe it's 9.2, whatever. Okay.
(21:36) But the problem is in our target distribution, we don't want tens as often as what our proposal distribution says. So what do you do? Every time you get a 10, you keep it only with the fraction that the black distribution takes within the red distribution. Okay, roughly at 10, the height of the black distribution is about 0.
(22:00) 05 and the height of the red distribution is about 0.55. So if you sample the value 10, you're going to keep it with about a 1 in1 chance. Otherwise, you throw it away. Okay? If you sample from the red distribution and you get a value of about 14, which is unlikely to happen, but you get a value of 14 from the red, the black distribution is about 90% as high as the red distribution.
(22:25) And so you're going to keep that value with about 90% probability. That's what that uniform separate uniform observation is used for to determine whether or not you should keep it based on the relative height of the proposal distribution to the target distribution with me. What are we doing? We're throwing darts to see if the dart falls in the black distribution with that uniform probability with every vertical slice in this picture. Okay.
(23:00) As a result, we're going to get a sampling distribution that reflects that black distribution pretty well. So, that was kind of just giving you the logic between drawing an observation from the red, drawing a uniform distribution, and then compare the heights of the red to the black to determine whether you're keeping it. So, we com repeat this process many many times.
(23:18) We draw a candidate observation, create a randomly sampled observation from the proposal, the red distribution, generate a random uniform 01, compare their heights, and keep it or throw it away depending on whether or not that separate random uniform distribution random variable uh says we should keep it or not.
(23:44) Okay? And we repeat that process many many times until we have enough observations. Okay? through this process, what do we do? Well, in the end, we can sample our data through this rejection uh sampling approach. And here after 100 accepted samples, we see this distribution starts to reflect the target population. And the more observations you get, the more correctly we're sampling from that target population.
(24:10) What we did is we sampled uniform distributions. Sorry, we sampled normally distributed random variables. that was really high peak. We separately sampled uniform observations and we kept the normal distributed random variable only if it met that based on the uniform distribution whether it met that criterion or not to accept or reject. Okay, so this was based off 10,000 accepted samples.
(24:38) How many actual randomly observed observations do we get from the normal distribution? It was a lot more than 10,000. We scale the height of that curve by 11. And so on average, we'd expect it to be around 11:1 ratio. So we probably to get 10,000 accepted samplings, we actually started off with a 100,000 or more observations from the normal.
(25:10) Okay, only 10% of them were kept because we scaled that probability distribution up 11 times with me. Okay, this makes Cookie Monster happy. Now we have a histogram to reflect a theoretical curve. And so how can we summarize this histogram? What's the whole point of doing this? Why are we doing this procedure of sampling from a distribution in the posterior? This is determining cookie diameter but in a baze p out a parameter and where we think that parameter is. It might have some weird biodal distribution. It might have some very
(26:03) non-normal distribution in the posterior and we want to reflect that in our sample data. Data being sampled parameters. Okay. And so how would we summarize this histogram? You see a distribution. How do you summarize it as a histogram? Talk about its center. Talk about its spread. Talk about its modality. Talk about its shape.
(26:35) Where's the center of this distribution roughly of the histogram? 10 maybe. What do we call that? That's the posterior mean estimator. Where's the middle 95% of this distribution? Look at the histogram. Maybe it's going from about six to about 14. There's your 95% credible interval. Okay, these are just like when we build a bootstrap. We're doing the same thing here for the posterior distribution.
(27:13) Trying to figure out where that parameter lies given our data and our prior. Okay. With me? Okay. Rejection sampling one approach we can do. And so we might talk about all right well theoretically I knew this here. We can talk about the estimated expected value of that posterior distribution through the empirical histogram. We can talk about the variance too.
(27:39) And we see as the sample size increases of sampling from our posterior. What will we expect to happen to the mean estimate from the histogram? It should get closer and closer to the mean estimate. Sorry, the true mean in that posterior distribution. And we see that. And the same thing for the standard deviation. There's going to be wiggliness there, though. Okay.
(28:02) So, what are we doing? We're trying to get a good guess, a good handle on the posterior distribution. How do we do it? We sample enough observations from that posterior. Each of these observations were sampled independently and so we can use those independent independence properties pretty well. Okay, sort of with me? All right, this takes a little bit of work to do.
(28:26) One problem with it is we said on average we'd expect to keep about 1 in and 10 or 1 in and 11 of our randomly sampled observations to begin with. This is for a onedimensional posterior distribution. There was one unknown parameter that cookie diameter we want we needed to estimate.
(28:48) Gets really complicated when you have like a fourdimensional parameter space or a 20dimensional parameter space. You're fitting a logistic regression model with 20 different predictors. You're going to have 21 parameters in your model. And now we have to fit a distribution to sample from that has to encapsulate that entire joint distribution of 21 parameters. And to make sure we cover it with height, we're going to have to make that curve really high that we sample from that multivariable normal. And so the how often we're going to keep it is going to be really really small when our
(29:17) parameter space gets high. That idea of distance in high parameter space and high dimensionalities is that cursive dimensionality. Everything seems to be far apart when we have high parameter space. Here's an aside. We're not really going to put this into practice, but this idea of rejection sampling can be really computationally inefficient because a lot of times it depends on how often we sample or keep the sample that we collected from the proposal distribution.
(29:49) An alternative to that is something called important sampling. We can rather than reject or not, we can sample from a target population and use a little bit of math to estimate a particular expected value that we care about. All right, this doesn't give us the entire posterior distribution. This just gives us a single value of that posterior distribution.
(30:15) And so if all you care about is the posterior mean, then this is a good approach to take. If you care about describing the entire posterior distribution, you're going to be so with this approach. Okay, this is why I'm just not highlighting it here. Okay, so rejection sampling is a tool we can use when we don't know the mathematical form. It doesn't get used in practice all that much.
(30:34) There's only very special cases in which it gets used. What does get used is something called MCMC. How many of you have heard of MCMC before? Fantastic. What does it stand for? Markoff chain Monte Carlo. Okay, so let's break that down. The first MC Markoff chain. Oh, sorry on the slide too early.
(30:59) The issue with this sampling is the computational efficiency. It can take a long long time to perform either of these samplings because your sample is just not often going to be accepted. It is really reliant on your proposal distribution. how well it's and how similar it is to the target distribution. Okay? And that's not easy to set up.
(31:23) All right? It might take many many samples to get a real good estimate as well. So it's just going to take you forever. The choice of the proposal distribution is paramount. So instead, we're going to take this MCMC approach. The Markoff chain Monte Carlo Markoff chain.
(31:42) For those of you who have taken a probability course before, what is the Marovian property? Memoryless. Memorylessness. So what does that mean? We're going to take this simulation of sampling and we're going to step one iteration at a time and every iteration is going to depend on the previous one but it's not going to depend on anything back in history beyond that. Okay. So that's the marovian process.
(32:12) It's this markov chain that is a random process that is memoryless that doesn't remember anything then beyond the previous step. So we're taking one step at a time. Okay, Monte Carlo just basically means we're randomly generating observations, randomly generating samples to empirically estimate properties of some distribution. Okay, it's a simulation.
(32:40) Okay, so we're randomly simulating through a posterior distribution where we're going to do this through a memoryless uh random process. We've already been doing the Monte Carlo portion of it through rejection sampling and bootstrapping. The Markoff chain just adds a little bit more uh details complexity to that. We're going to construct this random process and as it evolves, it's going to eventually uh resemble that posterior distribution.
(33:05) Okay, markup chain. We already kind of talked about this. Here's some mathematical framework. We're going to talk about steps in a process. And the place we are at time t is going to be dependent upon everything in the past. And so when we're doing this evolution random process, random walk through the distribution, if you will, you start off with initial initialization and we're now up to time t.
(33:31) That's essentially the whole past evolution of the algorithm. Okay, initialization. Let's see where we are now at time t. All right, we're going to talk about the density of being at time t depends on your past history. This says nothing about that marovian process where the probability that we are at time t depends on what was in the past.
(33:56) The marovian property basically says we can simplify that entire history to figure out where we're going to be now into simply just what happened in the previous step. Okay. So, we're going to step one little bit at a time and ignore everything that happened in the past. Okay. All right. It doesn't matter where we've been in the past.
(34:15) All that matters is where we currently are to predict where we're going to be in the next step. This idea of this random process randomly stepping one step at a time. What can essentially have or what property or where does this property happen in real life? Let's give us an example.
(34:34) In real life, where do we have this marovian process? The weather maybe not exactly marovian, but it could be simplified in that way. What are you thinking about the weather and to be marovian? I mean, if you know it rained yesterday, there's a higher chance it rains today.
(34:57) So, let's imagine every step along our process, every step time t is going to be did it rain today? Did it rain yesterday? Okay. And so t represents the day, theta represents a measure of whether it rained or not. And to predict the weather tomorrow, all we really need is the weather today. Okay? If you believe that, then we have this marovian property.
(35:21) And so to predict tomorrow's weather, we don't have to look in way back in history. We just have to look to see what happened today. With me? Okay. What else evolves in this way that pretty much the majority of stat concentrators want to major in or work in when they graduate? That's financial markets. Yeah, financial markets.
(35:47) You can think of these a lot of times as Marovian processes. The price of a stock today depends on the price of the stock yesterday. And it doesn't matter how it got there yesterday. It just depends on where you were yesterday. Okay? We have daybyday, time point by time point evolutions and you don't have to look too far into the past with me. Okay. So, how do we use this to sample from a posterior distribution? What's going to be the goal here? What's going to be the algorithm? I illustrated this a few lectures ago.
(36:29) There's something called the Metropolis Hastings algorithm. I mentioned it very briefly. It's going to have to do with this proposals and acceptance or rejections depending on where we are in the distribution. Okay. So, we're going to sample from the distribution, not from independent observations or independent samples.
(36:56) We're going to sample from the distribution where we're going to look to see where the next observation will be depending on where the previous observation was. Okay? And so, this chain no longer will have independent observations. We're not sampling independent observations. They're very dependent upon one another.
(37:23) Okay, so that is essentially the hopefully some motivation for this Metropolis Hastings approach. It's like doing a random walk through the distribution and this is sort of the pictorial pictorial idea of what's going to happen. Okay, this is how I think about it. We have a current state of a location in which we have an observation. Okay.
(37:48) And what we're going to try to do is regenerate this PDF, this distribution by stepping from one location in the distribution to another. Okay. And so we need to find two different things. We need to evaluate two different things. We need to propose where to step. Okay. And then once we propose a new location in the distribution, we have to decide whether to step there or not.
(38:14) Okay, that decision of whether to step to a new place in that distribution is similar to what we did in the past where it's doing a random draw from a uniform distribution. Okay, just like we did with rejection sampling and that target proposal distribution has a little bit different feel.
(38:38) Okay, it kind of depends on do we want proposals that are far away because we think that distribution might have some weird biodality in it or do we want to stay pretty close to where we are and it kind of depends on where we are in the evolution as well. Okay, so here's what we're going to do. We're going to let some proposal distribution determine what new observation we want to propose to sample from given where we currently are in the state.
(39:04) Okay, so this is the underlying proposal distribution. It doesn't have to be uh static as we do this uh sampling. And essentially this is just giving us the density to change Y from X. At time point t we're trying to figure out where our guess to move to in time point t + one. That's what x is where we are at time t.
(39:29) Y is where we propose to go to at time t + one. Okay, there's a proposal distribution that we get to determine. We get to choose it. It has nothing to do necessarily with the underlying posterior distribution, the target distribution we want to sample from. We have some posterior distribution of where we think the parameter is given our data.
(39:56) And really, it's some normalizing constant times some function. And so what's important here is we don't need to solve the normalizing constant. All we need to know is some proportion related to that true PDF. Okay? And it just simplifies our math a little bit. You know, we always write the posterior as proportional to here.
(40:19) We don't have to worry about how to solve things to sum to one. Okay? Nice property to have. Why? Because all that matters is relative heights of the distribution at every single time point t. All right. So imagine we're at current time point t. We're going to generate a proposal new parameter from that proposal distribution. Okay. So this is a proposal distribution has nothing to do with our posterior.
(40:43) And we're saying all right let's consider moving to further out in the tail theta prime where we're now currently at a nice warm cozy spot theta t. Okay. How far should that robot step into the distribution when we do this random walk? We ex compute an acceptance probability which has a pretty rough structure here, pretty complex, but essentially underlying it is there's a minimum of one and some other fraction.
(41:14) And essentially what it's saying is let's look at the height of where we might step compared to where we started off and the heights of the posterior distribution. Okay. And so here f(ub theta star is the proposed place. How high is the curve? How high is the posterior? F sub theta t is where we're stepping from. How high is that posterior distribution?" And so if we're stepping to a place that has lower probability, we're going to step with some less than one probability.
(41:47) If we're stepping to a place that has higher probability, then we're going to step to a place with probability one. And so that's suggesting every time we propose a new step, we always step. If it's a higher probability uh location, a higher density location, and if it's a lower density location, stepping downhill, we're going to step with only some fraction of that probability.
(42:12) Okay? And it depends on the relative heights of that proposal distribution. sorry, the relative heights of that posterior distribution as well as the likelihood of observing each of those two locations from the proposal distribution. So there's a little bit extra tagline there. Okay, we have to take into account if we're always proposing in the middle, we don't want to always step in the middle.
(42:39) Okay. And then we set with probability a let's switch to t + one uh change to the new location otherwise let's stay in the old location. Okay. So we're going to step through this posterior distribution. Change the location. If it says with high probability you should do that. We're going to stay in the same place if that proposal step has low probability.
(43:06) All right. More often than not, uphill proposals, ones that take the markup chain to a local maximum, will always be accepted. Downhill proposals, meaning you're going down lower in the PDF, you're going to accept that only with a low probability relative to the heights of the posterior. Okay, boiling down that process.
(43:26) All right, so let's look at an example here. It's Halloween, just after Halloween. Candies on my mind. Okay. And what we're going to try to do is think about trying to create a prediction for how good a new flavor mango will behave for Skittles if they propose a new Skittles flavor. Okay, new Skittles flavor mango.
(43:53) And they're trying to tune exactly how much of this secret mango essence ingredient they need to put into each one of those little batches to create those little Skittles candies. Okay, so what do they do? They propose a c certain amount. This is like a dose response outcome. How much dose do they need to put into the recipe? And then they ask a whole bunch of different number of taste testers, did you like that flavor? Okay, we're trying to hone in on what the best secret flavor would be.
(44:23) Okay, what is the model here? What are we measuring? What is the unknown parameter? How can we create a data generating process for this setting? What are we measuring? What's random? What's our predictor? The amount of flavor. The ingredient. How much ingredient should we put in? We should label that as X.
(44:59) What's our y? Yeah, the number of people that love the flavor or proportion of people that love the flavor. And so what model should we create here? That response variable, what values can it take on for every person? It's either a zero or a one. that response variable sorry that predictor variable is numeric and so what model should we create essentially a logistic regression however so are we more concerned with proportion or we more concerned with each individual person so it kind of depends on the way the data were collected and the way the data is organized. Okay.
(45:56) With the way it's written out here, my predictor X numeric, my response Y, how many people like the flavor. Okay, conditional on the number of people who tasted at that flavor. So, you can think of this as a data generating process two different ways.
(46:24) One is if it wasn't just a simple table like this, we could expand this out and basically say, "All right, really there's 60 repeated measures here where X is this?" And everybody said, "Yes." So you could have 60 rows of ones, all with an X of 1.8839. And then that would lead to a logistic regression. You could have 62 rows, 61 of which are yeses, one of which is a no, when X is 1.861.
(46:51) Okay, so you could conceive of this as a logistic regression where the number of rows is just add all these numbers up. Can you do that in your head? 1 2 3 4 5 6 7 Time 60 480, one over, three under. We're one over. So 481 observations. You could have 481 rows and this many of them would be ones. 481 minus that sum would be zeros.
(47:20) And these would be the list of all the different x's aligned with that. Okay? Or in just this table, we can create the distribution a little bit differently. Right? Let's see if for this table directly, what would be the setting for every row in this distribution? Sorry, in this data set, not distribution. What's my X? What's my Y? And how do we link them together? Making a lot of noise here. Just moving around.
(48:10) What's my X dosage? Milligrams of ingredient. What's my y in the table? What's my y? Number of people who love it. What distribution do you expect Y to follow? Underlying it are a bunch of individual zeros and ones. When we aggregate it to the Y, you're adding up a bunch of independent zeros and ones.
(49:06) What distribution defines a bunch of independent zeros and ones added together? There's some binomial distribution. What are the parameters of a binomial? We learn this. There's some n. N here is not the same for every level. Sometimes it's 60. Sometimes it's 56. But it's fixed. We know it. We can measure it. And what's the other parameter? The pi, the probability that someone will say yes given that x.
(49:41) And we need to link the x and the y together. How are we going to link the x and the y together? Well, p is the parameter that's going to link y with x. x and n gives us no information really other than the sample size. What did we do before? We link the probability of someone saying yes.
(50:23) You can do it in a binomial or each individual Berni through that law gods ratio. Okay. Log gods. Log gods. I'm going to change this ever so slightly. Instead of talking about beta 0 and beta 1, a lot of times we instead write it as alpha for the intercept and beta for the slope. Okay, if we were going into this, we're taking a beige approach. We have the data, the likelihood is now well defined.
(50:53) How y is being generated through the binomial. We could write out the likelihood. What are our unknown parameters? We have two of them. And to take a basian approach, what do we have to do to alpha and beta? We put a prior distribution where we believe those alpha and betas would be. Okay, kind of with me. All right.
(51:20) What would be reasonable values for alpha and beta as priors? I'm going to put normal distributions on them. Why? Because on this scale, alpha and beta could be unbounded. on the law god scale. And so we put a normal continuous distribution that's unbounded as the prior. It's not a beta because it's not on the pcale directly. It's after this log odds transformation. All right.
(51:49) For the beta, it's saying is there relationship? The mean hyperparameter for the prior probably is zero. Saying no relationship. And we don't want to have a whole lot of information here in our prior because I have no idea before I collect our data what the relationship will be.
(52:09) So we just put a prior with a whole lot of variability so that we have essentially a non-informative prior going into the data collection process. All right. We can do the same thing for the alpha by setting it centered at zero. You're saying going in, I think it's pretty much a 50-50 guess that each person's going to say they love the flavor or not. Kind of with me.
(52:34) All right. So, that's the setup. And what we want to do is we put these things together, combine them together, and write up a whole posterior distribution. And as a result, that posterior distribution is ugly. Okay? It's not Berni. It's not binomial. It's not normal. It's some combination of a normal with a binomial likelihood and it's not pretty.
(52:58) Okay, it's doesn't have a wellformed uh situation or well-formed PDF distribution. So, we're basically writing this all out. You know, our response variable has a PMF, how it relates to P. This is just saying things are binomial. There's the likelihood that we would use. We use a logistic model.
(53:21) So plugging in alpha plus beta into all of those log odds equivalently we can say the probability is e to the junk over 1 plus e to the junk and then we can perform inference for those two parameters and we can take a beige approach and put priors on those. Okay, this essentially is acting like a uniform distribution because those parameters those variances those standard deviations are so large. I'm not bringing any prior belief, any bias before I collect my data or very little.
(53:47) Okay. And we can write out that posterior distribution is proportional to the priors times the likelihood. And underneath this, these pi have that whole e to the alpha plus beta over 1 plus e to the alpha plus beta. But note, when I combine them in such a way, there's not one pretty family of distributions that defines what that functional form looks like.
(54:15) However, I can write it down and I can evaluate the height of this for any alpha and beta you give me. Okay, the y was measured. The p underlying it has an alpha and beta. I have my alpha and betas and I know what n is. All right. And so you tell me alpha beta, I can tell you what the posterior height of the posterior distribution would be with just a fractional constant missing.
(54:41) Okay? And so you give me a set of alpha and betas, I guess a set of alpha and betas, and I can determine which one's more likely, which one's more reasonable, which one I have higher belief in through this posterior distribution. Okay, great. So what we're going to do is actually use a pre-anned package called PMC to do this ma Monte Carlo chain marov chain Monte Carlo for us.
(55:12) Essentially what it's going to do is perform this metropolis Hastings algorithm for us. It's going to run lots of these MCMC's in parallel. And what it's going to do when we perform one of these Markov chain Monte Carlo approaches we going to have to allow for our initialization to play out. Okay, you can imagine when we do this markoff chain, it depends on where we started.
(55:37) If we start off in a bad location, we're going to want to let this uh algorithm run so that eventually we're going to start sampling from a place with high probability in the posterior. If we start off with a good place, we're good to go.
(55:55) Okay, this isn't a too complex model, but you can imagine a model with hundreds of predictors and we'd have to be careful as to where our initialization point is. Okay, if we start off in the tail, it's a problem. All right, so here's kind of how we do it. So this uh PMC is a whole package where we have to define all the different distributions for both our data and our prior uh parameters. So here we're defining our alphas and betas as coming from our normal distributions.
(56:22) We're saying that our data is related to those alphas and betas through this logistic model and its likelihood. And then in the end we're just going to perform sampling through their sample command. Okay, a few little things. We need to define the priors. We have to know the likelihood of the data and we have to give it a pre-anned function for the distribution we expect those observations are coming from. Okay. So here we're sampling.
(56:54) We expect our observations were created through the binomial just like we wrote on the board where we had our observed y's are coming from n and p and the observed y's that we defined here where p is coming from just a mathematical representation of this logistic regression function where my x is this flavoring value.
(57:22) Okay, we have sample size, we have Y, and we have flavoring as our predictors and we're linking them together through the logistic for a binomial distribution. Okay, we have to explicitly write all this out and not just do simply sklearn logistic regression. Okay, because this is very flexible to use a lot of different distributions.
(57:48) And then simply we sample based on this model that was set up 2,000 iterations tuning for 2,000 burn-in period and then we're going to return that posterior distribution that results. Okay, so what's happening in essence? It's running the Metropolis Hastings starting off with an initialization that it basically just g starts off with the prior values and then just steps one step at a time based on a uniform proposal distribution and determining whether or not we should step in that direction based on where we are.
(58:24) Okay, performing Metropolis Hastings. And as a result, after you get through this process, it starts to spit out after the burn-in period the 2,000 observations that we actually are keeping to perform inference on for our posterior distribution. What is this telling us? What are we looking at here? What does it look like to you? What medical evaluation does it sort of look like? What medical record? Where do you see these wiggles go through in a medical record? It's like an EK2, right? And what we're hoping for is that any signal, any change in that
(59:16) distribution to have been stabilized. Okay, we're looking for essentially no patterns in that posterior signal because what we're hoping for is that we have converged to a distribution that is now just randomly changing from one observation to the next. If we see this distribution going up or going down in this mean signal, that means we haven't converged properly when we started collecting our data.
(59:53) Okay, every single time point we're hoping from time point t=0 to time point equals t = 2000 after the burn-in period, we're hoping that it's stable and selecting from every every observation from the same distribution, right? And we're doing that for both alpha and beta. This is essentially every observation as we move along time from time point 0 to 2000.
(1:00:21) And then we can just look at the histogram or in this case the smooth histogram for those different values. Okay, this is our rough guess at to the posterior distributions. That's the histogram of the posterior distribution. How would you describe these distributions? Which one is measuring the relationship between X and Y? the beta. This is the alpha. This is the beta. Where's beta in our posterior? Anywhere from about 25 to 42 is always positive.
(1:01:06) What does that tell us? There's a positive link in the association between flavoring and whether or not somebody likes it, right? Because our beta's values are always positive. We can pull off the middle 95% credible interval. We can pull off the posterior mean.
(1:01:25) Where do you think the mean of that distribution is? 33, 34, something like that. What would be a 95% credible interval? Just like bootstrapping, pull off the two quantiles. I don't know somewhere from about 28 to 40 with me. We're treating this as an empirical estimate of the posterior distribution through sampling where the sampling happened in a stochastic way.
(1:01:54) Great. All right. So you can look at the signal charts. You can also look at the summaries. And so here's sort of the summary output. We said that beta term was somewhere around 34. It had a 95% confidence or sorry credible interval from about 28 to about 40. And if you ask for this algorithm to give us the summary of the posterior distribution, it gives us various different values.
(1:02:24) The posterior mean, the posterior standard deviation, and by default, for some reason, it gives us the 94% posterior credible interval. I don't know. You can ask it to change that. It gives us something called the MCSE mean. What is that? It's the essentially the Markov chain standard error for the mean. And that's using something called the effective sample size of this posterior distribution draws.
(1:02:57) And then it's also looking at just dividing by square root of 20,000. This should be square root square root of 2000 to get you from the standard deviation of 5.143 in the posterior. We can turn it into a standard error dividing it by square root of n. And that n is the number of draws. Here it's just dividing by square root of 2,00.
(1:03:17) Okay, the effective sample size is something less than 2,00. Why 2,00 would work if we had independent draws? And since all of these draws are intimately linked to each other through this process, every observation provides less than an independent piece of information. Sometimes you stay at the same place, sometimes you change, but they're not independent from one another.
(1:03:50) And so over time, you can't treat them independently. Each observation actually is less than or effective sample size used not treated as independent observations but positively correlated ones. Therefore, the effective sample size is much smaller. Okay, how certain are you of this posterior distribution depends on how correlated those draws are.
(1:04:14) One other little thing on this output, it also provides something called R hat, which is just a measure of whether or not this distribution converged. We want that value near one which will indicate that convergence. Large values means that distribution hasn't converged. Okay. Why did we go through this whole process? It's always the $64,000 question.
(1:04:40) What are we doing? What's the goal? Trying to get a sense of where that posterior distribution is. How did we do it? Through sampling. We can sample through rejection sampling. We can sample independent or separately through this MCMC process. Okay, great. Skittles taste the rainbow. It's a lot. Okay, a different approach. When we have the Beijian approach to performing inferences, life is difficult because we can't always just write out a closed form solution. We can't always just optimize a single function.
(1:05:19) We want to get a general sense of where those random variables as a parameter reasonably fall. Okay, that whole posterior distribution. We got 10 minutes. We can go through the first part of this. I'm not too worried about it. But we'll have to come back to this next time. And we'll talk a little bit about visualization next time as well. So dealing with missingness. All right.
(1:05:46) In the world of data and data science and statistics, machine learning, what does it mean for things to be missing? What happens when data is missing? How does Python handle? How does SK learning handle? How does stats model handle when things are missing? When you see an Excel spreadsheet, what does it look like when something's missing? There's a hole in the cell. There's a blank cell in your CSV file.
(1:06:19) In the most simplest case, when you bring it into Python through pandas, how does pandas render that missing value? It doesn't leave it blank. What does it do? Anybody know? But the man that treats it as not a number, as a nan, as a special case, as an NA. You got it. I think today's secret word should be rainbow. Secret word, rainbow. All right.
(1:07:07) So, when you're dealing with missingness, we need to know a a couple issues to handle those missing cases. What does sklearn do? What does pandas do? How should we handle those cases? Okay, at a high level, when you have missing data, how can an analyst, a data scientist handle that situation? There's holes in your data set. What should you what can you do? Let's take something numerical. Okay, you can use a proxy for missing data.
(1:07:44) You can imputee some values into that missing value. Okay, there's lots of ways to do that. We'll talk about methods of imputation. What else could you do? Yeah, remove that missingness. Remove the column that has missingness. Lots of different choices you can make. Impute something, drop something. Okay, drop the rows, drop the columns, or impute something. Okay, we talked about this showing up as Nans.
(1:08:10) How does sklearn handle those Nans? It spits at you, yells at you. You're missing data. Okay, so just watch out. It will warn you. It will yell at you. But really, what we want to worry about is handling it in a sophisticated way. We don't want to just automatically drop observations because that can lead to bias and incorrect predictions, right? Great.
(1:08:37) So, lots of ways to handle it. You can drop those or you can imputee the mean or median. We can use drop na axis equals zero just basically means missing drop the rows. You can fill in an observation. You can fill in the observations based on the mean. Okay. But from a statistical perspective, there are some issues with doing that.
(1:09:00) What happens if you drop the missing values? What happens if you impute the mean for everybody for those missing values? What's a positive of making that decision? By imputing things, you at least have an higher sample size, right? That's nice. More data. You didn't have to drop observations. But what can happen by imputing that mean? Why do I have a job? you computed a lot of if you computed a mean for a lot of data, your distribution is not going to refle reflect reality. It's going to have a biased result in your data.
(1:09:42) And so this can potentially lead to statistical biases when we're handling uh this and our goal is inference. When we're doing our goal of prediction, it's not as big a deal. Okay, just realize that. But it can still cause issues. All right. So what I would say from today the simplest way to handle missingness rather than drop observations or drop the mean we can create a new variable.
(1:10:08) Okay this is called creating an indicator of missingness. And what we're going to do is we're going to imputee for any observation that has missing data. So we started off with two variables x1 and x2 with missing observations denoted as just a dot here. we imputed zeros every single time.
(1:10:31) So x1 star is the resulting variable with the zeros imputed plugged in and then along with that we create a variable of ind that's indicating whether that was an imputed value or not an indicator. All right. And so whenever then we do analysis, we're going to include both X1 star along with X1 miss together. What does that allow us to do? It allows us to estimate the effect of that missingness on the response variable.
(1:11:10) Because when you impute a value zero, it doesn't mean it's a true zero. It's an imputed zero. And for this categorical variable X2, which only had values zero and one, what you're essentially doing is you're creating three groups using two indicator variables. X1 miss, is it a actual one in the response variable? I think this is a typo. This probably should be down here.
(1:11:45) And then x2 sorry x sorry x2 not x1 miss x2 is indicating whether the original response variable was a success and then x2 miss is an indicator as to whether or not it was missing. So really in the end for a categorical variable you're just creating a new category through this one hot encoded variable. For a numeric variable, it's a little trickier, but it allows us to indicate that these zeros aren't reflective of a real zero value in the data.
(1:12:16) Okay? And so, whenever you do analysis, always bring in pairs X1 star with X1 miss. Okay? It allows us to handle those biases in a reasonable way. Why is this important? Because it depends on the types of missingness. There are three big things before I let you go. Three big versions of missing data. Anybody know what those are? Missing. Missing at random, missing completely at random, and missing not at random.
(1:12:47) Okay, we're trying to hone in on why an observation is missing. Why it's missing might be just someone put holes into your data set. It is unrelated to anything. That's called missing completely at random. Missing at random means that any reason that it's missing, we can capture for what was measured in the other variables in the data set.
(1:13:18) And then missing not at random depends on things that weren't measured or the value that is there to begin with or should be there to begin with. Okay, three big levels of missingness. The approach we take will depend on which level we have. But the problem is we'll never know. All right. All right. We'll start there next time. Thanks much.