(3) 109 day 15 - YouTube
https://www.youtube.com/watch?v=aOhkQ0tYMGk

Transcript:
(00:01) Check, check, check. Good. Now, great. I'll try not to scream too much today. Sounds like the speakers are up roll out. Let me Maybe if I switch these, it'll help. Mhm. Morning. Everyone's nice and quiet this morning. How y'all doing? Did you just take a midterm or something? Yeah. How'd it go? Tough.
(00:34) Yeah, we can't unfortunately talk too much about it. There's a few people making it up today. Um, we'll go through some of the details of the midterm in class on Wednesday. Hopefully, we'll get you to your grades before that. If not, it'll be released at some point. I assume on Wednesday.
(00:54) Um but uh generally speaking, you know, it was longer than the quiz and tougher than the quiz as expected as as announced would be. So um hopefully you're still not or hopefully you're not discouraged. Hopefully you'll continue to be motivated to learn a lot. Um and we'll do that today. Uh hopefully continue along that path.
(01:17) Questions? Do we have the slides? They might be hidden. You can do it. They should be up there. Can you see lesson 15 at least at all? The whole lesson's hidden. That's why. That's why. Thank you, Papus. Just give me the thumbs up when it's ready. All right. So, let me frame today's class. If you will recall, what did we talk about last week? We transitioned from talking about outcome data, response data that was numeric to outcome data that was what's the other option? You just wave it. Are you volunteering? Wave at your friend across the hall.
(02:05) What's the other option? If it's not numeric, it is categorical. Right? And so we have these classification models that we've talked about last week and the analog to all of our numerical methods, methods for numerical outcome data, they'll be an analog for classification models. And so the linear regression becomes logistic regression.
(02:34) KN&N becomes KN&N doesn't change. It's the same thing. It's just we have to change some of the algorithm a little bit. Okay, so linear regression becomes logistic regression. And how did we actually do that? How do we actually fit a logistic regression model at a high level? Response variable takes on what values? Logistic regression. Response variable takes on we said its classes.
(03:05) We said its categories. and we turn that into numbers. We turn them into zeros and ones. And so we're now trying to predict the probability that the response variable is a one, that the response variable is a success. And we put this whole framework on it likelihood method to get our estimates. And we say that the probability that Y equals 1 is that E to the junk over 1 plus E to the junk. Right? E to the linear form over 1 plus E to the linear form.
(03:38) All right? And it's all based on interpretations, the log odds model. Okay? And we just call that logistic regression. Sshaped curve to zeros and ones as a response. Okay? And we're going to reiterate some of that today. Last time we finished off talking about what happens if your categorical response variable isn't just two categories.
(04:02) What if it's more than two categories? And we're going to take that and run with it a little bit today and talk a little bit more about the details with uh trying to quantify the accuracy of a classification model and then we'll get into connecting that to bay and bay approach to logistic regression.
(04:26) We good on the lesson in the slides? Okay, everyone can can see the slides. Great. All right. So, this is what we're going through today. multiclass logistic regression. I screwed up the sklearn, but none of the math was screwed up. Okay, so I'll show you the updated code. Not much has changed. All right, but uh I messed it up last time. When we have more than two categories, we have to be a little clever about attacking that problem.
(04:50) We can't just look at the probability that yals 1 versus y equals 0. We said last time if we think of the response variable as a multinnomial response variable, we can set this up. One of our groups was will always be the zero group. And we'll fit separate logistic regression models to predict y = 1 versus y= 0, y = 2 versus y = 0, y= 3 versus yals 0.
(05:22) Okay, there's one reference group and we just fit separate logistic regression models to that. All right, and we have to reiterate that uh for several different groups. Our example was we had three different groups CS, stat and other. We said, all right, let's set aside the other group. I think we that's the reference group here.
(05:41) And then we can fit a model to predict for just the observations that are stat and otherwise fit a model to predict stat versus everybody else not the CS throwing them away. Stat versus others. And then we'll fit a separate second model CS versus others. And then we have these two different models to distinguish between these three different classes.
(06:04) Okay? And then we'll do a little bit of mathematical manipulation. We can pick whatever predictors we want. How much time they spend playing video games per week. How many pet sets do they have every week? Because that might discern the differences between these concentrators with me sort of great. We've had two separate models.
(06:25) CS from others, step from others. We talk about how many parameters there would be. We've just doubled our work because now with three groups, we have to fit two separate models. Okay? So all of that logistic regression we can use all of the same methods for polomial features for interaction terms. We just have to now fit two separate models.
(06:46) Okay, they both have the same reference group but different responses different successes. Once we get these two probabilities sorry these two logistic regression models now we want to estimate turn that into for any one specific observation we'll have this log gods of stat versus other this log gods of CS versus other but really I want to convert that into three separate probabilities I want to say all right somebody shows up they spend eight hours a week doing video games playing video games doing video games that's weird they spend uh 12 hours a week working on problem sets.
(07:27) Based on those predictors, which of these three classes do we want to predict them to be? Okay, what is the probability that that person is in the other category? Is in the CS category, is in the stat category. But we have two separate models and we want to turn that into three distinct probabilities. How can we do that? Do you see the issue here? What are the outcomes for these two logistic regression models? I can get probability of other versus stat and I can get probability of other versus CS. How can I convert those three
(08:09) probabilities into just one set of three probabilities? What's the one missing piece that we have with probabilities? What do probabilities need to add up to? They have to add up to one. And so essentially, we have two estimated probability ratio. We have another estimated probability ratio. And then we have the probabilities must add up to one.
(08:40) And so now we have three unknowns, three equations, and we can go solve. Okay? Okay. And so all we're going to do is just essentially renormalize our probabilities with this third equation so that our probabilities add up to one. And so now we can convert our two logistic regression models into predicting which class with probabilities everybody's in with me sort of. Okay, great.
(09:05) The base case for logistic regression models in sklearn and in the statistical world when you're fitting these multinnomial outcomes, it is that what we mentioned that reference group versus another group and we just shift around who that other group is in reference to the reference group.
(09:25) Our example is we have three different classes, red, green, and blue. And we're going to fit a model that basically says, "All right, let's fit a model. Throw out the greens when fitting our model." And let's just imagine it's a simple logistic regression model with two predictors, X1, X2.
(09:45) Our classes are denoted by red, blue, and green. With me? All right. And what we're going to do is we're going to draw a classification boundary defining the red from the green to predict the red from the green. Sorry, the blue from the red. I don't know why I said red from green. I'm thinking Christmas. The blue versus the red. So the red is our reference group.
(10:14) We want to predict the probability someone's blue versus the probability someone's red. Success versus failure. and we fit a logistic regression model to that. Okay, we get a logistic regression classification boundary that's linear, right? Logistic regression guarantees that, but doesn't really guarantee that. Okay, so one model fits blue versus red. Red is our reference group. We also fit a model that fits green versus red.
(10:39) And so that green versus red has a separate set of beta coefficients with me. All right. Again, we throw out the third class, the blue class here, and we get a second different classification boundary. And then in the end, we have k separate cases classes when k is greater than two.
(11:03) And essentially we then get k minus one capital k minus one different logistic regression models all with their own different betas. Okay model can be fit independently just like every standard logistic regression model. And then when we put it into sklearn here's the change in the code but the output is all the same. Now when we fit a logistic regression model, when we have a multinnomial class and we say our penalty equals none, I've updated everything including the data set to be this year's data set. We can then as an outcome, in this case, we're trying to
(11:39) predict whether or not our next play in the NFL is going to be a pass or a run or a special teams play. If you know football, that's hopefully makes sense to you, but there's three big classes of plays on offense. either they're throwing the ball, passing, they're handing the ball off, running, or they're going to kick the ball in a special teams play.
(12:06) Okay? In football, generally special teams plays happen on fourth downs. Punts happen when it's fourth and long. If it's fourth and short, they might go for it. All right? If that doesn't make sense to you, just know there's context here. We're trying to predict the play call of the offense because that might help inform us as a defense as to what type of play we should run.
(12:25) Okay. Okay. And so we're trying to predict whether the play is special teams, a pass play, or a running play. The zero group is special teams, a special play, the other play. A one group is the pass play, and a two group is the run group. Okay, three classes that we're trying to predict. And the simple thing we're trying to predict with right now is just down and distance.
(12:56) As we say it in the sports analytics world, football is decomposed into four downs. First, second, third, and fourth down. And basically, they're trying to achieve 10 yards before they get to the end of the fourth down. If they don't, they give up the ball. They lose the possession. If they do, they get to keep running more plays.
(13:16) Okay. To go is how many yards? Oh, you're learning a little bit about football. To go is essentially how far they have to go in order to get more plays in order to have a success. And so a big to- go means they're in trouble. A small to- go means they're doing a pretty good job.
(13:34) Okay? And they just go play byplay as it goes along. We'll visualize this in a second, but those are our two outcomes or sorry, two predictors to predict that multinnomial play type. And now we get our beta ones and we get our beta zeros. We have three groups.
(13:50) So, we're going to fit two logistic regression models, all with the other group as the reference, I think, and then we're going to get in the end, uh-oh, three separate estimates. This was the big issue. I expected to see two separate logistic regression models, but sklearn reports three separate logistic regression models and their own betas. Okay, what is sklearn doing? sklearn under the hood is fitting two logistic regression models, but then says all of my probabilities have to add up to one.
(14:20) And so it adds another system of equations into the model and basically just solves and renormalizes so that all of my probabilities add up to one. Okay? And we'll interpret this output here in a second with me kind of so far. Don't worry about the football context. Just think about the math underlying it.
(14:39) So what is sklearn doing? It essentially is fitting in this case lots of different models and then sets a last equation that the probability that you're in your kith class can be resolved in terms of the reference group. And that gives us k minus one equations. We know probabilities that have to add up to one. And so we can just basically say all of the probabilities and the unknown probability better add up to one.
(15:03) And then we can just renormalize and solve for a different system of equations. And so all it does is just uses the fact that probabilities have to add up to one to give us new beta coefficients, which is weird.
(15:21) And what it's basically turning what was originally fit as a model for one group versus a reference group into one group versus everybody else. And so it just does a renormalization. And we need to know because when we interpret skarn models, it's important to know what I'm interpreting with me. Okay. So, keep this in mind. Let's go back a slide. What are the interpretations of this model of this model output? I'm going to write one of these on the board. Oh no, I lost my blackboard. Oh, it's underneath. Okay.
(16:03) Should have gotten here two minutes earlier. This gives you a time to think. It's Jenga. How can I pull one piece out without knocking it all over? He did it. The plus range lens. This looks like a thing for Pavlo. Pavlo, is this a thing you know? Uh, you do. Great. I don't know. It looks like physics. All right.
(16:53) What do we got? I'm going to pull out one expression. All right. I'll let you do the work for the others. We have a beta 0 of -6.2. There's our first beta 0. We have a beta 1. Our first list of beta 1's 1.67 and 0.095. That shouldn't be beta 1. That's really beta 1 and beta 2. Okay. So, sorry for the output.
(17:14) And so, what are we trying to predict? We have the natural log of something is -6.22 plus 1.673. 673 x1 plus 0956 x2. Hopefully those of you in the balcony can see this. What's the something? First off, the easy ones. What's x1 and x2? down distance. So X1 is down. X2 is to go. And what is this response variable for? What do you think would be a reasonable thing to do? We're going to guess would pick to place first in these three different balls.
(18:26) The y equals 0 of those. Right? So this is the probability that y equals 0 over 1 minus the probability that y equals 0. The probability that y equals 0 over the probability that y is not equal to 0. Okay. And these are all of our estimates.
(18:48) And so basically this is saying the chance that y equaling 0 is the other group. We predict a play to be an other group. There's some baseline that's really low, right? I can convert that into a probability. If down was zero and to go was zero, I can write raise e to this over 1 plus e to this to turn it into a probability. But it says as down increases and we hold to go constant, the chance that you get a special teams play or some other play is an increasing function, right? You're more likely to have a special teams play as down goes up. You're holding to go dis uh constant.
(19:27) The more to go goes up, the distance is further. Holding down distance again, higher chance of a special teams play. And if you know football at all, that kind of makes sense. Okay, at the face of it. Now, if you don't know football at all, you can still interpret it and get full credit with me. All right? And then you can write out two other equations as well.
(19:52) You can write out the probability that y equals 1 over the probability that y is not equal to 1 or 1 minus the probability that y equals 1. I'll start writing it out. 1.86 minus 0.0389 x1 plus dot dot dot. Okay. And so now you have three different equations.
(20:22) They can all give you a probability converted into different probabilities and those probabilities should add up to one with me. Okay. Interpreting a multinnomial uh formula. One thing to keep in mind under the hood, it's fitting two logistic regression models but reports three sets of beta coefficients, right? And that's okay because it allows us to do predictions a lot easier. Great.
(20:47) Well, rather than doing this backwards operation of turning these multinnomial logistic regression models into predicted probabilities of classes, why don't we just predict those those estimate those probabilities directly? Estimate those coefficients directly. And so there's a separate multiclass model called the OVR which stands for one versus rest logistic regression model which does exactly that.
(21:13) It just frames this problem estimation in a different way. Rather than looking at group one versus group zero and group two versus the same reference group, it's just framing each of those separate logistic regression models using all of the data.
(21:32) And it's looking at any one particular group versus everybody else combined rather than throwing away a third of your data each time. I put third in quotes because not every class is guaranteed to be the same size. Okay, so if there's three separate classes, then now three separate logistic regression models. A first models to predict CS from everybody else combined. Another one is stat from everybody else combined. And a third is others from everybody else combined.
(21:56) Okay? And all of these three probabilities aren't guaranteed to all add up to one once we do the calculation for every individual because these models are estimated independently but there is very strong dependence among them because the data is not always changing. Okay. All right. A picture is worth a thousand words.
(22:22) And so what we're doing instead is we're trying to fit now a logistic regression model to blue versus others. What's that line going to look like for classification boundary? Where would you draw the line? Somewhere like that. Yeah, somewhere that I like your hands uh motion. So, it's somewhere like this. Whereas the line for the multinnomial was something like this. And so, now we have a more vertical line to separate the blues from everybody else combined.
(22:55) our classification boundary is a little bit different and as a result we'll get different beta coefficients. All right. And so we get an actual slightly different model. We can do that for all three of our classes. Green versus everybody else, red versus everybody else.
(23:15) And then we combine those three classification boundaries into one. Essentially you get these three separate probability models in terms of different betas. And then to do the prediction essentially what we do is we just renormalize what each of these three probabilities become. Okay, for every single observation you can estimate each probability and then just renormalizing those probabilities to ensure that they add up to one with me.
(23:44) Why is this all important? Because you don't want to predict probabilities that add up to two and you don't want to fit models where probabilities might be less than one. So we're just renormalizing things so that we can interpret those outputs directly. Okay. This is the multinnomial. Uh we were good to go.
(24:11) One nice thing to know that in the end how you convert those three separate models into predicted probabilities is there's a separate function called the soft max function which can automatically do that transformation for you. All right, there's a mathematical connection underlying it. But realize what it's doing is it's just renormalizing the probabilities to enforce the out add up to one.
(24:36) But this function of a soft max will be very useful in future models. Okay, it's just related to these renormalization of probabilities to ensure that they add up to one. Where are we going to see softmax again, Pablo? In every month. Okay. And so when we get to that in B, if you're lucky enough to take the second semester here, you'll see that over and again.
(25:01) Okay, we got different outputs. We fit this model to the same uh set of data. Here I do it in a backwards way because I didn't want to change all of my code. So now I'm fitting a logistic regression model. It still allows me to dis uh give it a multiclass equals OVR as an option. This will be deprecated in future uh sklearn uh releases. But for now, we're good.
(25:26) We're fitting an OVR logistic regression model and we get different betas, but we get the same number of betas that come out. Okay? And we can interpret these just similarly to the way we did it here. Okay, it's the exact same estimation or sorry, exact same interpretations, but the estimates were different because the way we drew those lines had a different geometric representation.
(25:57) Okay, what's different? Well, let's just look at one estimate that changed using multinnomial was -6. Using OVR, beta 0 for that first class is -9. Does it matter? Those numbers are different. Not surprising. They were estimated using different optimizations. Doesn't matter which one's better.
(26:33) Which one's correct? Which one's better? That one we can answer. How can we determine which of these two models is better? Yeah, we can use some sort of loss function to determine which one's better. What loss function do you want to use? Don't say hi. Why don't you want to say hsn? Where does MSE work? Well, linear regression.
(27:13) Why? We assume numeric outcomes. We assume normal distributions, things like that. Now, what's our loss function? Binary cross entropy or cross entropy. Someone may have said it. That logistic regression loss function. You know what naturally boils out of it. We can try to decipher. All right. If we fit this model and we fit the multinnomial model separately, which one has a better loss? What do we have to worry about? I see some eyes glazed over. If we're trying to p determine which of these models is better for future prediction,
(28:02) what should we do? We should mimic that outof sample future prediction uh approach. How do we mimic future outof sample prediction? What's the knee-jerk reaction? Cross validation. So we should determine we have now two different models under the very same umbrella of multiclass logistic regression, but they're two different formats.
(28:28) Which model's better? Well, let's see how they perform in out of sample prediction. Let's set up a cross validation. Let's see which model performs better. Okay, we have to redo all of our estimations because now our training set will be smaller than the full training set we started off with. But that's okay. All right. And then you can pick between the two models that way with me. Okay, great.
(28:51) I like to interpret my models. I can draw out these models. Keep in mind what I'm looking at now. Our predictors were down in distance. I looked at a specific down. There's only four options. First, second, third, and fourth. I'm looking at third down and how many yards are to go.
(29:08) Zero means you're about to get a first down or a touchdown. 30 means, man, you're in trouble. You got sacked or had a bunch of penalties. And as that yards to go increases on third down, what happens when third and short? It's about pretty equal pass and run play were called in past history. And as that distance increases, that running chance goes way down.
(29:33) That passing uh chance goes up a little bit. And as yards to go goes up, we see the chance of something else happening starts to increase as well. Most plays are around somewhere between third and one and third and about 15. Everything out here is not extrapolation, but that's based on a lot of outliers in the data with me sort of. Okay.
(29:56) And if you know sports, if you know football, this kind of makes common sense. All right, our model's agreeing with that. What's the difference between our multinnomial logistic regression and our OVR logistic regression? One versus rest. Not much. They look quite similar. They're both picking up the same signal.
(30:19) Okay, they might have slight differences in their predicted probabilities, slight differences in our pure classifications, but the models are essentially the same with me sort of. Okay, great. Don't forget when you have more than two categories, when you have two categories, it's easy to do a pure classification as a prediction. You just say which probability is greater than 0.5? That's going to be my success. That's going to be my predicted outcome.
(30:46) Now, if you have more than two categories, it's not always going to be a guarantee a probabilities greater than 0.5. You're just going to pick your largest estimated probability. And you can just rely on the predict and the predict probab command to get pull out those probabilities.
(31:04) So, what's the difference? When I'm predicting a future observation in a classification problem, if I want to classify that person, I just use the predict command. If I want to predict with probability, I use the predict prova command. Okay, or argument I should say. And so for these different models, I can set up different data frames. And so I'm trying to predict now says 22, but I think it's 24. I'm trying to predict based on down and distance.
(31:30) What's the probability each plays something other than a pass or run? The chance that they are a pass, the chance they are a run with probabilities, and I can convert that into a pure classification. Not surprisingly, first on a first and 10 play, it's pretty even pass and run. a little bit rated towards the pass. And so my y hat when it's a pure classification is group one. All right.
(31:50) And then you can continue down on fourth and five. The predicted outcome is they're probably going to kick. Okay? But they might go for it. And then if they're going to go for it, it's almost certainly going to be a pass in comparison to a run. But purely that's going to be a classification of zero.
(32:10) Right? Similar output can happen for uh this one's the OVR logistic. This is the multinnomial logistic. Apparently, this is slightly different data. Um, and so our outcomes are going to be again just based on which of these probabilities is highest. Okay, with me kind of. All right, simple enough. And then we can visualize things. We can say, all right, let's look at our data. There's a certain distance and down to go.
(32:37) So when you get close to the goal line, there's only certain distances you can have. And then what we can look at as well is the pure classification plot. And so for a specific distance and now this is yard line. So this is distance and yard line. This is the predicted probability on a third down play of what play would be called. And we have passes, we have runs, and there's really nothing else.
(33:04) That is the majority class or the plurality class here. And so a lot of times what can happen is you get a prediction in a classification model that looks like all we're going to do is always predict the same thing. Okay? And this is an issue with classification models. If all you're going to do is predict what class an individual is, it a lot of times in the end it's just going to be let's predict the same thing every single time.
(33:34) However, the model still could be really strong. What can explain that? Let's predict everybody to be a success. But the model actually has ability to pick up relationships. In what situation would you just predict everybody to be a success? No matter what the predictors are. Let's build a model. Okay, let's build a model to predict people in this class.
(34:13) Are you currently on cocaine? Okay, what factors could predict whether someone is currently using cocaine? You could measure lots of different observations, lots of different predictors that might be associated with whether or not someone is on cocaine, using cocaine currently.
(34:36) All right? Right? And there might be really strong factors related to that strong associations. But if I was going to predict any one of you to be actually using cocaine currently, high from cocaine, that probability, I'm going to predict everybody to not be using cocaine. All right? No matter how strong my predictors are, it doesn't matter because the imbalance in my data, it's almost certainly everybody's not going to be on cocaine. But every once in a while, someone will be.
(35:02) Okay? Whether they show up to class or not, it's a whole another story. But that imbalance of classes can lead to a prediction model for classification that predicts the exact same thing every single time. But underlying it, I could have predictors that are strongly associated with that response. It's just doesn't magically take us over the 0.5 threshold.
(35:29) Okay, with me? And so watch out with that imbalance design. That's essentially what's going on over here. Roughly twothirds of plays in the NFL are all passes. And we can have lots of different predictors that are associated with the chances, but under those certain conditions, they're almost always going to predict a pass.
(35:52) Okay, this difference between multinnomial and OVR, yes, underlying it, you're going to get very similar results. Which model is better? We can evaluate that through cross validation. when we care about predictions. Okay, one thing to keep in mind uh and there's not a huge difference in the approach here when we combine the log likelihood of all K classes.
(36:16) This is sometimes called the cross entropy or the multinnomial logistic loss. And so what we're doing is we're just taking that logistic regression model and applying now with a response variable that isn't just two classes. It's no longer called binary cross entropy. It's just now labeled cross entropy or multinnomial cross entropy. All right. And so we change our loss function ever so slightly.
(36:39) Instead of y equals 0 and 1, it's now y= k versus anything else. Okay. And why do we care? Because a lot of times we're going to want to regularize things to prevent overfitting. And so now this is the loss function in the multiclass setting that we will be regularizing. What do I mean by regularizing? What are the two types of regularized models we've used in this class? I hear some murmurss, lasso and ridge.
(37:15) And what we're doing is we're just adding a penalty term on the end of this loss function in terms of squared betas or in terms of absolute values of betas. Okay, the same. Nothing really has changed except that now our loss functions different. And sklearn will allow us to do that. Good enough. Great. Let's stay classy. All right. Once we fit these logistic regression models, we're going to trying to determine their accuracy. We're going to want to figure out which model's best.
(37:45) We have lots of methods to do that. But at the base of it, a lot of times what gets reported is something called mclassification rate as a loss function or as an interpretive loss function. And then we'll combine those all into something called ROC curves. As a review, what's B rule? This will come in handy later today as well.
(38:05) What's B rule? When you want to flip the probability, when you want to flip a conditional probability, you know, probability of B given A and sorry, no A given B and what you want is the probability of B given A. At the base of it, a lot of times we're dealing with law of total probability. We've seen these slides before.
(38:24) If you've taken SAT 1104 or hopefully SAT 100, you've seen this before in a class before coming here. If you haven't and it's been a few years, go ahead and come back and look at it. Where does this apply in real life? When I teach it in my introstat class, I talk about it in a context of diagnostic testing. Okay, what is diagnostic testing? That's where there's a really cheap, easy test to do to determine whether or not someone might have disease.
(38:49) All right, you do, we talked about this, I think, before where you might just do a real quick test to see if P test to see if you're pregnant or you might take some blood work to see if you have cancer. Okay? or PSA or mimographies to check to see if you have some sort of test or some sort of uh disease.
(39:14) In this case, breast cancer screening through self-examination and mimographies, prostate scramming through PSA tests, colurectal cancer through colonoscopies, all different types of models. Uh these diagnostic tests and underlying these diagnostic tests are the properties of the test itself and the prevalence underlying them. So we can break down B rule into key pieces. In the numerator, we're talking about the probability if a test sorry, if a person has the disease, the probability that the test shows it.
(39:47) If the person doesn't have the disease, what's the probability that the test accidentally shows that person has disease? And then we need the overall probability that anybody has disease. These ideas are the sensitivity, the specificity, and the prevalence of the test. given a problem. Those three important factors help determine what you really care about, which is called the positive predictive value of a test. What do I mean by this? You take a pregnancy test.
(40:17) Your pregnancy test comes back positive. What's the chances that you're actually pregnant? Okay, that is called positive predictive value. As a user, that's all you care about. you don't know the truth and you're trying to diagnose the truth, the chance of you actually being pregnant. The test itself will report the sensitivity.
(40:43) It will report the specificity, the chance given you're pregnant, the chance that the test says it's positive. Given you're not pregnant, what's the chance that the test shows you're not pregnant? But there's going to be error, user error, or test error in those situations. And so we want to incorporate that error to get what you care about the probability that now you are pregnant given you tested positive or now you are have cancer.
(41:12) I just had my colctal exam recently. The chance that I have colorctal cancer given my test came up negative. Great. You can boil this down into essentially what is a 2x two table. There's lots of different ways to uh talk about what the results of all this means, but really what matters is the guts of the table. We're gonna have a model that's going to predict whether or not someone is a success or a failure in a logistic regression model. And we're going to in the end have whether they're truly a success or truly a favor uh truly a
(41:50) success or truly a failure. All right, we're turning this diagnostic testing into our classification model paradigm. Okay, they map perfectly. So diagnostic testing, we're trying to predict these outcomes. We care about positive predictive value. You can plug all these numbers in.
(42:13) Why do we care? Because we care about positive predictive values when we fit our model. We care about predicting whether or not someone is truly a success given we predicted them to be a success. We care about if we predict somebody the next play to be a pass, what's the probability it actually is a pass. Okay, we want that to change.
(42:34) We want to update that prediction based on our data that we were given. Okay, couple different things. Now, let's turn this into the classification paradigm. We have a model. It predicts whether some an outcome is or an observation is going to be a success or a failure. That observation is truly a success or a failure. And so we're trying to predict whether y= 1 and whether y equals 0.
(43:00) Sometimes they are correct, sometimes they are errors. And we're going to diagnose turn those errors into two different types. If our response variable is truly a failure and we say we predicted them to be a success, we call that a false positive. Pretty natural.
(43:23) If we predict them to be a failure when they were truly a success, we call that a false negative. And these two different types of errors may have difference importance depending on the context. a false positive might be a problem where a false negative isn't a big deal or vice versa depending on the context. Okay, a lot of times what we'll do is that table we saw a second ago, we'll turn that into what's called a contingency contingency table in the world of statistics and often called a confusion matrix in the world of machine learning and prediction models. Okay, so let's dig into this a little bit. It's all based on what we predict and what is
(44:02) true. We can step through all this. I think it's important to just look at an example. All right. So, here's our classification model for our original classification model where we had individuals come into the ER. We measured some diagnostic tests on them and when we built a logistic regression model to predict whether or not they have heart disease.
(44:27) Okay, our model predicted them to have heart disease or predicted them to not have heart disease. Truly, some of these people did not have heart disease and some of them had heart disease. And we can create that contingency table or confusion matrix as such. It's a 2x two table.
(44:50) We have two different variables that are yeses and nos, successes and failures. Okay? what our model predicts and what is actually true. How did we get these yhats equals 1 and y hats equals 0. We use sklearn and what did we do in skarn? We fit a logistic regression model. We predicted the response predict gives us the classifications.predict predict paraba would give us proportions and we can just compare it to 0.
(45:22) 5 with me. Okay, who are the false positives based on this table? How many false positives are there from this model? Predict them to be positive. They are truly negative. We have 54 false positives. Who are our false negatives? We predict them to be negative, but they are positive. We have 53 of those. Is this a useful model? How can we make that determination? Is this model useful? What would be the worst model here? If our model was crap, what would this table look like? It would predict everyone incorrectly. Okay, that would be a really bad model. Although it's a useful model. If it
(46:36) predicts everyone incorrectly, what do you do with it? They just flip them. You have 100% accuracy when that's the case. Okay. That's like betting and always losing. Well, I'm just going to bet against the guy who always loses. Okay. What would be actually in practice kind of the worst model? Yeah, it's no better than random chance.
(47:06) If your predictors are not useful, you use a model of flipping a coin. You would expect the probabilities, the ratios of each of these true classes 110 to 54 should be equal to 53 to 86. It's not. There is some discriminatory power here. This model has some use. Is it perfect? No. But among the failures, we predict them to be failures with about 67% accuracy rate.
(47:40) among the successes, we predict them to be sex successes with roughly 60% accuracy rate. And so we're doing a decent job of discriminating between these two classes. Not perfect, but better than just chance alone. This was a model based on age, sex, and the interaction between the two. Wasn't all that useful. We can talk about false positives and false negatives rates of this classifier. We just did. Okay.
(48:06) How can we change this table? Well, this table was based off of the intuition that we said, all right, let's classify everybody based on their predicted probability from this model to be a success if that probabilities above 0.5. It's essentially treating false positives and false negatives equally.
(48:28) But there's nothing to say that I have to compare to 0.5. I could compare to some other threshold. Why would that be useful? Well, if I compare to 04, I'm going to predict more people to be successes, more people to have heart disease in all cases. And if it's important to not miss people that truly have heart disease, it might be important to have a lower threshold.
(48:56) If it's important to not mischaracterize people to have heart disease that don't, maybe you change that threshold to 6 or 7. Each one of those choices of thresholds gives us a new confusion matrix. And based on that new confusion matrix, we can calculate a new probability of success, sensitivity, and specificity.
(49:19) Based on those, we can calculate the rate of true positives, false positives, true negatives, and false negatives. Right? Great. 0.5 is not the only choice. We could use 04 or 6. If we used a pi of 04, what are we going to do? We're going to predict everybody to be more successes. And so we're going to have a higher rate of predicted successes within those that are truly uh not successes.
(49:42) And so the false positive rate goes up, but the false negative rate goes down. If we increase that probability of 6, the opposite occurs. The false positive rate goes down, the false negative rate goes up. And that choice of which is a more important correct prediction or a worse mclassification might inform your choice of pi of that threshold with me.
(50:11) Okay, great. Whoa, that's not the right outline. That's from before. Okay, all of that can be boiled down into what's called the ROC curve. Anybody hear of the ROC curve before? Great. What does ROC stand for? Nobody knows. That's okay. It's called the ROC curve. It's like saying curve twice. It's the radial operator curve.
(50:38) There's some whole history of it. Let's not worry about it. Radio operator characteristics curve. And essentially what it's going to do is it's going to illustrate the relationship of how your classification model is performing in comparison to any choice of that threshold for building those confusion matrices. Okay? And so it's going to be a curve.
(51:00) The vertical axis is going to be the true positive predictive value and the horizontal axis is going to be the true negative predictive value. Okay? We want both of those things to be high. As soon as you take on a little bit of uh negative predictive value, you hope to get 100% positive predictive value.
(51:20) The shape of an ideal curve will be shown on the next slide. So this is essentially what we have. We have the true positive rate and we have one minus the true negative rate which is the false positive rate. Okay, every time you allow for a little bit more false positive to take on in your model given any particular model, hopefully you're taking on more true positive rate.
(51:43) Okay, think about these confusion matrices. If I set the threshold pi to be zero, I'm going to predict everybody to be positive. I'm going to automatically get 100% true positive rate, but my false positive rate will also be 100%. Okay, if I pick my pi to be 100% I'm going to predict everybody to be a failure.
(52:11) And so my true positive rate is a zero and my false positive rate is also zero. And the ideal curve, the ideal model, as soon as I allow for some false positives, I'm going to get 100% accuracy in my true positives. But in reality, there's a curve that comes along with it. Every time we take on a little bit of false positives, we're going to get a little bit of true positive and hopefully a little bit more true positive than false positive.
(52:36) The red dash line is called the random classifier. What's that red dash line synonymous with? How can I recreate that red dash line? replace y= x. That's the y= x line. But what's my model? What's my prediction model? Random. Random. How is it random? How can I recreate it? Like you take the recording rate of positive population.
(53:13) Okay. So you take a recorded rate in the population. If you randomize randomly select people based on that rate, that's a particular point along this line. This line is saying one of my models could be let's flip a coin that has probability 0.01 of showing up heads. That's down here. Every head is determined to be a positive.
(53:37) Another line basically is let's say my probability of success is 0.9. I'm way out on that curve. And so this is essentially the model that's random classifier. But the probability of showing up as heads is classifying someone to be a success depends on whether I want it to be 01.5 or.999. Okay? And every value in between.
(54:03) Okay? It's the random flip of a coin model, but this represents different models for different thresholds, for different probabilities of success. And hopefully you're beating that model. Hopefully if my model uses a coin of 04 in comparison, hopefully I will beat it.
(54:21) And so we're comparing to that random classifier with those different weighted probability heads model. Okay. The ROC curve then is created by plotting those true positives versus the false positive rate. I think I had a typo on the previous slide. at various different threshold settings for those confusion matrices. What threshold value was used for this point along the green curve? I have no idea.
(54:46) Okay. But given a threshold value for this model, it shows up somewhere along the green curve. I don't know where it is. I just know there's a threshold value that created it. Okay? Okay. And so I can plot each of these curves for competing models. Maybe a logistic regression model with two predictors. Maybe a logistic regression model with 10 predictors.
(55:10) Maybe a KN&N model with K equals 5. And then I can compare their ROC curves and decide which one's best. Should probably do this in cross validation. Which one of these models is best? Ignoring the random classifier and the perfect classifier. The blue curve. Why is the blue curve best? because no matter how much false positive rate you take on, it beats the orange line and it beats the the green line curve. Sorry, not lines.
(55:41) Okay? And so you want to be as high up in the left hand corner as possible. And so how can we turn that into a metric as to which curve is best? Well, you can do these pair-wise comparisons if you want. You can find what's called the area under the curve.
(56:07) So, you just take each of these models, find the area under the curve, and whichever closer to one you are, the better the model you have, no matter what threshold you choose. Okay? And so, that's a metric that's often reported, the area under the curve. Here's our actual example. We have a logistic regression model. We have a theoretical green KN&N model. And in comparison, we can talk about which model is better.
(56:26) Well, sometimes the logistic regression model is better than the KN&N model for specific types of thresholds. But overall, they had the exact same area under the curve. They had the same overall mclassification rate and classification accuracy rate for all across all possible thresholds.
(56:46) But in the end, you would pick a specific threshold and based on that threshold would decide which of those models is best for classification. with me? Okay, great. Here's the actual ROC curve we have. It's not perfect, but it's better than just random chance. Ideally, we could fit more models, more predictors to this model and get this curve to get closer to this point up here. Great.
(57:16) And so we calculate the area under the curve to compare across different models as a different metric as a different measure of accuracy rather than just looking at pure classification which depends on the bounds of accuracy on the uh how equal those classes are how imbalanced those classes are. We can just change our threshold and turn that into a full area under the curve calculation. Okay, great. We're through it all.
(57:41) We're through logistic regression. How to talk about and and uh measure the accuracy of different classification models. We have multinnomial. We have logistic regression. We have ways of uh measuring their accuracy. Let's take a step away and come back to this idea of logistic regression but now using a different probabilistic modeling approach which is the bay problem.
(58:14) The bay approach to inference the bay approach to modeling. Let's think about a second for a second. Where did we see how did we set up Baze inference before? We've seen Baze. It was on your midterm. What was the Baze model that we have talked about before? What were all the different properties with Baze? At the base of it, what does Baze essentially do? What are we doing with all this parametric modeling to begin with? We think there's a relationship between X and Y.
(58:59) We have our betas that connect how X and Y relate. And we let the data speak. Logistic regression says, let's estimate the log odds. Let's estimate the betas to associate that log odds of response to the predictors. the data estimates or leads to what values beta fit the data best.
(59:26) What does baze add a twist to that approach? Rather than thinking there's a single beta value that's best, it says maybe there's a whole distribution of beta values that are reasonable. Before we collect our data, we put prior belief onto what those beta's values are. We collect our data and we update that distribution of reasonable beta values. We're going to take that whole machinery and put it into logistic regression. Okay.
(59:53) But to do that, we have to define a few things, review a few things from before and learn a whole new distribution called the beta distribution. Okay. Baze rule. We saw this already today. Baze rule is when we update our probabilities. How do we turn that into the statistical inference in the bay paradigm? Don't forget this formula. What is this formula telling us? What are each of these pieces? There are four different probabilities on this in this formula.
(1:00:24) What do those four probabilities represent? Probability of theta. What did we call that? The prior distribution because it's a distribution of theta independent of the data. What is probability of X given theta? That's the likelihood. Given a specific set of theta, what is the chance of seeing our data? And then what is probability of theta given X? That's the posterior.
(1:00:57) We're trying to change our belief in reasonable values for our parameters based on the data through the likelihood. What's the denominator here? Probability of X. It's just trash. We just throw it away. Okay, we don't even worry about it. It's called the normalizing constant. It's called lots of other names, but we're not going to worry too much about that for now. We might come back to it later.
(1:01:14) Okay, so in the end, what we say blah blah blah blah blah. That's all redefining what we just did. What's important is that that posterior distribution when we deal with continuous parameters is going to be proportional to the likelihood times the prior. Okay. And now in this classification paradigm, we have to worry about a likelihood that's based on a Bernoli or a likelihood that's based on a binomial.
(1:01:46) And in order to use that mathematically, well, we need to define a new distribution called the beta distribution. How many of you have heard of the beta distribution outside of this class? Everyone who's taken stat 110 or 111 or has done some of this modeling before probably has heard it. All right. What's the story of the beta distribution? Well, the big story of the beta distribution is it's bounded between zero and one.
(1:02:10) Okay, it has a functional form that looks like this. For any x between 0 and one, we take x to some power minus one and 1 - x to some other power minus one. Those alpha and betas are the parameters of that beta distribution. Okay, that normalizing constant to make sure that this function which is bounded between 0 and one add up to one has some weird form using the gamma function, not the gamma distribution, but the gamma function.
(1:02:44) And anybody know what the gamma function is mathematically? Nobody does. I don't know what it is. It's hard to interpret. Chris knows what it is, maybe. What's the gamma function? What's it related to factorials? It's essentially think of it as factorial function. If alpha or beta or the sum of those two things is an integer, then the gamma function is just the factorial of that integer minus one.
(1:03:13) Right? If it's not an integer, this is just a generalization to alphas and betas that aren't uh integers themselves. All right? So what does a beta distribution look like? Here's what the PDF looks like. What are we looking at for various different choices of alpha and beta? Here's what the distribution looks like.
(1:03:32) Okay, it's bounded between zero and one. Usually it has a peak somewhere. And if alpha and beta are the same thing, it's symmetric and centered at 1/2. When alpha is bigger and beta is smaller, the distribution tends to be to the right. When beta is bigger and alpha is smaller, the bulk of the distribution tends to be to the left.
(1:03:57) Okay, anybody know any magical values of alpha and beta that are important? One, one, a beta distribution with an alpha of one and a beta of one is the same as the uniform distribution. Okay? And so you can see that mathematically here. If we have a function, a PDF where you plug in an alpha of one, it's x to the 0, it's 1 - x to the 0, and you're left with just a constant. And a distribution with just a constant bounded between 0 and one is the uniform between zero and one.
(1:04:31) What's a uniform distribution? A flat distribution. Okay, great. These are the PDFs. Gives you a sense of where the data or where the distribution mostly lies. And what we see here are the CDFs. And so this is directly pulled off of Wikipedia. Thank you, Wikipedia. But you could gener generate this pretty easily in Python.
(1:04:52) Note weird cases where alpha is and beta are both less than one. We have a distribution that's U-shaped that puts most of the weight near the tails. Otherwise, you have weight centered somewhere between zero and one. Okay. Why is this useful? Why is Kevin reporting the beta distribution? We'll get back to that in a second.
(1:05:18) But here is the full function of what that distribution looks like. We already talked about what that gamma function reflects. Whenever I see a distribution, I will like to think about what is the mean of that distribution. Okay, you can come back to this PDF. What is the mean of this purple curve? That's a PDF.
(1:05:47) What's the mean of that distribution? Where's the center of that distribution? Right. Come on, just say.5. Fair. That's easy. Where's the center of the red distribution? It's a weird one. Five. What's magical about the red and the purple distributions in terms of their parameters? Alpha and beta are the same or equal to each other. All right.
(1:06:08) And then when alpha is small in comparison to beta, you get a mean that is moving over to the left. Okay? And so the mean of a beta distribution is probably closer to alpha when beta is bigger and it's probably closer to beta when alpha is bigger. The mean of a beta distribution is just alpha over alpha plus beta. The variance of a distribution is just weird. Okay. And so it's just a mathematical result. It's talking about the spread.
(1:06:40) If you care, you can do the derivation. We just have to find the expected value in continuous form. It's x * f ofx dx. Do a whole lot of solving. I'm not going to step through it. That's for your homework. It's not your homework, but if you care, you can step through it. Beta distribution. Why do we care? Maybe it's a 209 homework. I don't know.
(1:07:01) Why do we care? So this is the whole idea. If we take a closer look at the PDF of that beta distribution, what does it look like? We have something to x to the a minus one 1 - x to the b beta minus one. Think x replace x the random variable with p the parameter. If you plug in p, what does it look like the distributional form of? It's the berni or the binomial distribution. Okay.
(1:07:36) So if you take change the perspective of X being the random variable and replace it with P and a likelihood, it has the same form as a binomial. And so why do we care about a beta distribution? Because it's useful when you're dealing with binomial distribution in the beta sorry in the B uh paradigm. Okay, we can use this for a Berni distribution when our response variable is Y. It is our conjugate prior.
(1:08:03) What does it mean for a distribution to be a conjugate prior? What is conjugacy in the Beijian paradigm? You start off with a beta. You apply a likelihood to that beta prior and you end up with a beta at the end. It's just mathematically convenient. Okay. And so now we can tweak our choices of alpha and beta in order to control the mean and variance of the prior.
(1:08:32) And that leads us directly to the beta binomial distribution. And the beta binomial model, we'll talk about hierarchal models next time. So the beta binomial model is basically saying, all right, let's fit data that come from a Berni distribution. Let's put a prior on P. What are our parameters? What are these hyperparameters? Our data are what? Which of these variables represent data? X. Great.
(1:09:05) What values can X take on? Zeros and ones. We have many zeros and ones. Zeros are failures. Ones are successes. We think they're coming from a distribution by coin flips. Independent coin flips. What the probability of a head is on that coin flip, we don't know. So we put prior belief into it. It has this whole distribution. We'll probably put prior belief.
(1:09:31) A lot of it set a 0 and b 0 to be the same and have a lot of weight at 0.5 because most coins are fair. We can write down that prior distribution. We can write down that whole likelihood function. It's just the product of beta pdfs. And when you multiply together lots of those beta coe uh Bernoli PDFs, we get p to the sum of x 1 - p to the n minus sum of x.
(1:09:55) That is a binomial distribution just without the normalizing constant. And so what you'll notice is that the prior on p has p to some junk. The likelihood has p to some junk. And to turn that into a posterior, you just take the prior times the likelihood. And so we put that all together, the prior times the likelihood, and solve and say that my functional form of my posterior distribution is proportional to P to some junk, 1 minus P to some other junk.
(1:10:31) All right? And what you recognize is that P to the some junk and one minus P to some other junk. Thinking of P as the random variable is just a new parameter for the posterior distribution that involves the prior hyperparameter and the number of successes and number of failures you observe in your data.
(1:10:57) Okay, you just take those data observed successes, data observed failures and weight them, pull them towards what the prior would say. Okay, we're essentially out of time. So there's the beta binomial results. We'll come back to this on Wednesday. Today's magic word is squash. The generalization of pumpkin. All right.
(1:11:43) Basically, what we'll start off next time is baze and we'll connect that to a hierarchical model. And I did this for essentially one student in the class. The example we're going to use might be for more than one student in the class. Let's talk about MBA.