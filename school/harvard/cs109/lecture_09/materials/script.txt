109 day9 - YouTube
https://www.youtube.com/watch?v=w4td49xiN2w

Transcript:
(00:01) Test, test, test. Good. Great. Look good online. Great. All right. Good morning everybody. Happy data science. Happy October 1st. Right. So, now you can officially put out your Halloween decorations, uh, if you feel like it, right? Anything before that's bad. I think I got a announcement from Starbucks that today is officially pumpkin spice latte day.
(00:47) So, if that's your thing, PSL, then go ahead. Um, that's neither here nor there. Uh, today's image is my own. So, brag, humble brag or brag. I got a nice little lightning strike there. you know, you can cheat now with your iPhone. So, um, this is on the farm up in New Hampshire over the summer. Uh, you know, I needed to hay the fields there.
(01:09) So, uh, you see the tractor there and and some nice lightning. Uh, couple things. Uh, two announcements. One, what's happening this week in section quiz? Okay, it is 14 multiple choice questions and a couple free response parts of one question. So 30 minutes at the start of section and then there is material a short notebook afterwards. Okay.
(01:38) So that uh material that notebook is just going over bootstrapping and a little bit of inference which I'll refer to today a little bit. All right. What else is happening in the class right now? What's due next week? Yeah, homework. Boomwork number two. Office hours have been a little light early this week from students uh stopping by.
(02:03) So, make sure you're working on that as well as that's due next week. Okay, questions. All right, let's get into it. Lecture nine, we are talking about probability and the MLE. That's why I'm here. Everything Pablo said the last couple of weeks, I'm going to correct I mean add to uh every once in a while I'll bring in my statistical point of view on all these data science ideas and we're going to connect it to probability theory and stuff.
(02:38) But I love to motivate especially in a grounded class like this with real life examples. So let's start there. We're going to start with some data, connect it to probability and likelihood, and do a little bit of formulabased statistical inference to connect it to what Publish did last time with bootstrapping and whatnot. Okay, don't forget your data science process. There's usually five steps. It's never always in the same order.
(03:02) Uh sometimes you start with data which generates hypotheses and questions. Sometimes you start with a question, things like that. So, what we're going to do is start with an interesting question hopefully. And what I want to ask is or what I want to answer is how much is Pablo's house worth? How much is my house worth? And I'm trying to model selling prices of homes based on other characteristics of those homes. All right. So, we're going to see what is related to selling prices of homes in the Camberville, Cambridge,
(03:34) Somerville area. And that's because we both live in Somerville. Go Somerville. And we're both big Somerville youth soccer fans. All right. And so the data is coming from Red Fin, which is an online resource. Easy to pull off, extract data from it. You don't have to be a fancy scraper, but it's kind of creepy.
(03:53) You can look up somebody's address. You can find that pretty easily, like Pablo's, especially. Don't bother looking at mine. Uh, and then you can find what it's sold for or how much it's estimated to be valued. And so Redfin has their own model underlying it, some sort of machine learning model. A first model would be a linear regression model.
(04:11) So that's kind of where we're headed is use some characteristics of a house to try to predict what the selling price of the house will be. I did a little bit of pre-processing for you too, just to clean up the data a little bit. So first thing, when you have a data set, we should explore the data at least a little bit to get a feel for what's going on.
(04:34) Here's what our data set looks like as far as the variables that are measured. What are we trying to predict? What's our response variable? What did we say? What's our question? Selling price of homes. So, we have lots of different observations, rows that represent different homes that were sold in about a year within the last year. We have 592 homes and we're trying to predict price which is numeric quantitative and therefore our methods of linear regression things like that KN&N will work pretty well.
(05:05) We have lots of predictors. So if I asked you what do you think is related to the selling price of a home? What other characteristics would you want to measure? How big is the home? Yeah. Square footage. What's the size of the floor? Floor space. What else might be related? Number of bathrooms. Yeah, that's usually pretty good related uh variable related to the selling price.
(05:31) Number of bathrooms, number of bedrooms, stuff on there. Yeah. Yeah. That's why some of this is measured because it might be informative and predictive of a selling price of a home. What we're going to look at is we're going to look at type, which we know is categorical. Four categories.
(05:49) Is it a condo, single family home, multif family home, or townhouse? They might have different characteristics. Bedrooms, bathrooms, floor space, and square footage, lot size, and square footage. So, that's basically the yard size. Uh, and then the year the home was built, and then also distance to the Harvard Square T-top.
(06:07) Okay, so distance, location, location, location, a real estate agent would basically talk uh tell you. So, we're just going to do some exploring. The first thing I do is I turn the zip, which is numeric naturally, into something that's categorical because it doesn't make sense to have it as numeric, the zip code on your address. And then I just do a quick describe call on those numeric variables.
(06:33) What do you notice? Why do we do this? Two things. One, we can get some summaries. So of our response variable, look what our average selling price of the home is. How many mill how many dollars? A little over a million dollars. $1.25 million roughly one and a quarter. And we have 592 observations. But note many of these observations, many of these variables don't have all 592 measurements.
(07:10) Okay, what's the problem? There's some missingness. And so eventually we'll talk about missingness in A and B uh throughout the course of the year. But what we see 592 for a lot of these measurements, but lot size, not every home has yards because there's a lot of condos here. Uh and not every what's an HOA, homeowners association, how much you got to pay in as essentially an extra fee.
(07:35) And so if it is a condo or a townhouse or even some single family house houses and neighborhoods have homeowners uh association fees and so we might be able to impute these HOA those that aren't a part of a homeowners association we can impute zero plug in the zeros for the NAS and same with lot size if you don't own the property the lot itself we can probably reasonably impute zero so we're going to do a little bit of data cleaning really quickly and so we're going to plug in some zero rows and then
(08:05) do one other thing and that is transform price into thousands of dollars just to make our scales a little bit smaller. Okay. Um just to give a general sense of the type of homes and whether or not they had an HOA just to confirm. Oh, condos are the ones uh that had an HOA that didn't have missing values generally speaking.
(08:31) And then if we're looking at lot size, which of those had missing values? Yeah, it was the condos and the town houses, not surprisingly. Okay. And then we explore with first visuals. Don't forget a data set. I like to explore with visuals. When we're looking at relationships among our variables, the response variable is what's most important.
(08:51) And so we're looking at the first row or the first column here is looking at how all the variables relate to that. But we also get a sense of how the variables themselves, the predictors relate to each other. What do we call that when the predictors are related to each other? Collinearity. And that when that's too high, that can cause some issues. Great.
(09:10) Anything stick out? Anything interesting? You see? If we had to predict, use one variable to predict price, what would you choose? What seems to be the strongest? Size of the house seems pretty strongly correlated. All of them seem to have some relationship. And you might see different prices as well across the different home types.
(09:39) But yeah, there seems to be a positive strong correlation potentially linear relationship with the size of the home. And that's kind of where we're going to start, okay, is just looking at how the selling price relates to the size of the house. Okay, great. So, let's zoom in on there. Here's the relationship. We're going to fit linear regression models to start.
(10:08) Does that seem reasonable? Well, the big question is, are the relationship reasonably linear? And the answer is yes. But what about the other assumptions to linear regression? What can we say about that based on this plot? Are we good to go? Looks like there's looks like there's some heteroscadasticity and that's not surprising in this setting. Smaller homes have less variability.
(10:34) Larger homes have more variability in their selling prices. Okay, how can we correct that? How can we handle that? Well, one, when we do inferences, we can bootstrap. When we do predictions, it doesn't matter. And when we do other things, we might decide to do some transformations.
(10:55) Okay? So like doing log transformations we haven't talked about yet might fix some of those issues if what you care about are statistical inferences. Okay rough ballpark rough uh eyeball. What do you think that linear regression model is going to have estimates an intercept and a slope if we fit a simple linear regression? What would be your guesses? We'll do it in a second.
(11:26) What do you think the intercept would be when x is zero? If we fit a line, where do you think the line would cross when x is zero? That's the intercept, right? Maybe around zero, right? Which is pretty intuitive. Makes sense. Hopefully, it's a little positive to represent, oh, there might be value to the property themselves, the the yard itself.
(11:51) And then if you looked at, all right, for a one unitit change in X, how much of a change in Y do you expect to see? This is square footage, this is thousands of dollars. What do you think the slope of the line would be? 500 bucks. Yeah. Five. Something like that. For a slope in thousands of dollars, $500 for every square footage. That would be a reasonable guess. Okay. And again that heteroscadasty shows up. We can look at other variables too. Bedrooms.
(12:24) Any concerns there? Why do we have vertical line lines? They are sleep. You can only have whole numbers in the number of bedrooms. Is that a problem? Not at all. It's a predictor. Not a problem at all. If it was a response, I'd be more concerned. Okay, as long as that relationship is reasonably linear.
(12:50) If it's not, then we might want to categorize that variable. And then we can look at violin plots to look at how it relates to the different types of homes with me. How would we bring this into a regression model? It's categorical. confusion. So one code I like that make dummy variables binary variables. Leave one out to be the reference group.
(13:26) Everything in comparison to that reference group which should be the reference group. It doesn't matter but looks like townhouse. It's on the left. Maybe that would be the reference group. I don't know. Okay. Greet your dummies however you want. Whatever you want it in reference to. Okay. The interaction term I'll get to. Okay, but that's not really supported or indicated here.
(13:46) The interaction term would be indicated if we went back a slide and looked at this scatter plot because the home type is depicting the color. And so if we fit a line to the green points and a line to the red points, if we think those slopes are different, that would suggest an interaction effect.
(14:15) Okay? And yes, there is an interaction effect here. All right? Between square footage and type of home and the selling price of the home. There could be other interactions too among your predictors and how they relate to the response. Okay, just getting squared away with our data set, thinking about what models we might want to fit.
(14:32) And so let's actually do that modeling. Linear models is where we're going to start with that class of models. Everything looks pretty linear. We care about associations here. Uh we already thought about what variables might be strong, what might be nonlinear. There might be some interaction effects.
(14:51) There might be some polomial relationships we might find eventually as well. Okay. What other data considerations should we make that missing data could be a problem? We have to worry about that. We might want other measures as well. What's left out here? How fancy the inside of the house is? How recently it was remodeled. How fancy the kitchen is.
(15:07) If you really want to sell your house and you want uh to make a little bit of money, redo your kitchen, redo your bathrooms. And usually that's a good investment. And so we can look at things of those nature. Okay. But we don't have that data. We can make some dummies on type. we can fit some models. So, we just fit a linear regression model.
(15:27) Lots of different predictors in here. What we're zooming in in on here is the effect of square footage 0.64 in this model. What does it tell us? What's its interpretation? A one change in square footage is associated with a $641 change in increase in the selling price of a home holding the other variables constant.
(16:01) All right, that means you're looking at increasing within those categories of type, holding the other factors constant. And it's kind of tough to do because bedrooms might be tough to hold constant when you increase the size of a house. And note the effect of bedrooms is negative. Is that reasonable? I've done this model thousands of thousands of times. I haven't lived that long for probably a dozen different times for a different dozen different data sets similar to this and I get the same result every single time. So I think there is a message here. But go ahead.
(16:40) Holding everything else constant bedroom the size of the bedroom soon. Yeah. So if you're holding everything else constant, if you're cramming another bedroom into the house, but you're not also increasing the square footage, that might lead to a lower price. Really, the better way to think about it is homes that have lots of small bedrooms sell for less.
(17:04) There might be other things that are also confounded with that relationship. Okay? Be careful not to say it's a causal relationship at this point. This is just associative. All right. Why is it negative? Because it was collinear with square footage.
(17:25) And once you adjust for the size of the house, adding bedrooms now adjusting holding square footage constant has a negative relationship with me. Okay, great. We can fit the overall model for square footage by itself, for beds by itself, and yes, they both have positive effects generally and they were correlated positively with each other too. Great. Okay, simple models.
(17:52) We use stats models as an alternative to me. This is my R showing. You'll hear me say this all the time. I think it's even easier to fit models in R and using stats model because we don't have to create all the variables ourselves with the right data structure. We can just use the formula to define what that model is and that makes it a little bit easier just to implement.
(18:13) And so if you want to learn a little bit about this go to section today or this week obviously you have to because you have to take notes. Okay. So that was just generally introducing the data set that we'll be using today throughout to illustrate some of these ideas.
(18:32) I want to do a little bit of review first and I might not have any volume here but that's okay. It's game time. Any volunteers who hasn't volunteered before? All right, we got one. I hope you didn't pay a lot for that ticket. Obstructed view. Yeah, I didn't. Name, house, right? My name is Cameron. I'm a junior in Quincy, just like everyone else in here, I guess. Awesome. Um, yeah. Excited to be here. One fun fact.
(19:11) Um, I love airplanes. Airplanes are very interesting. Love airplanes. There we go. All right. I like it. All right. When do we use cross validation? A little bit of review. Our options to choose the best K and a K andN model. To choose the best lambda and original SO model, to choose the best predictors in a regression model, to choose between families of models.
(19:35) Those are your four options. What are you thinking? All right. Well, I am I'm I'm thinking to myself now about all this this preparation that I've done for our our wonderful quiz coming up this week. Great. You're welcome. Uh and and I'm also realizing that maybe I did it wrong. Um I'm thinking our cross validation is when we are going to be probably choosing between our family of models. Choosing between our family of models. Okay. I'm I'm going to lean towards option D.
(20:04) I don't think we're going to set the parameters. Option D. Anybody agree with option D? Anybody have another idea? Oh, could it be option A? Choose the best K and a K&N model. Could we do some cross validation? To me, that sounds like a choice between models. I like that. Cross validation is good for choosing between models.
(20:31) When else can we use cross validation? What about B? Choosing the lambda and a ridge lasso model. That sounds like choosing between models. What about C? Choosing between which variables to use in your model. It sounds like a choice between models. That's what cross validation is for. Cameron, great job. But unfortunately, it's everything.
(20:56) Trick question. Cross validation in this class is not always going to be used, but it's going to be be used often. Whenever you have the decision of choosing between models, cross validation could be a really useful tool. And all of these to me are choosing between models. A ridge with a lambda of two and a ridge with a lambda of 10 are two different models to choose from.
(21:24) With me? All right. See if you can redeem yourself. When should we standardize predictors, Cameron? Options always. Whenever we use CANN, whenever we use a ridge lasso model. Whenever we want to treat the transform predictors equally. Tricky question. Tricky question. What are you thinking? Well, talk out loud. You know, it's got to really get my TV personality there.
(21:53) Okay. So, if we're going to standardize predictors, we want to make sure that they all can can really tailor because we want to avoid having like massive numbers on one like the home price compared to I'm thinking that it's probably going to be something we're I think I'm going to stick with option D when we want to be able to treat these these predictors equally. Option D, treating predictors equally.
(22:17) What does everybody think? You know, let's hear clap. Option D. Oh, thank goodness. Option D. Any other options? Is anyone thinking? Oh, somebody's thinking something. All right. What are you thinking? Cat problem. What about C? Because they are affected. The models are affected by the choice of the standardization.
(22:48) So maybe it's either B or C or a little bit of both. So I went with option D. Cameron. Great job. Great job. And but this is a tricky question and this is a discussion. So when should you transform predictors? Well, I think the take-home message is whenever you want the predictors to be treated equally, which might be the case when you're doing a ridge lasso model or might be the case when you're doing KN&N, but there might be other situations in which you don't want to do.
(23:18) Okay? Especially if you have a mix of categorical variables and numeric variables. You know, you might want to have your nearest neighbor only be the same type of house in this instance with the case we have. So, you might not want to compare a multif family house to a condo. You might want to force the nearest neighbor to be within those classes.
(23:42) And if that's the case, you can play around with your scales to have KN&N force that make sense. So it's not that you always want to do it, but it is often recommended. And the same idea with ridge and lasso. It's often recommended, but it's not always the case. It shouldn't be automatic. You might not want to treat all of your variables equally, all of your predictors equally. A tricky question. Tricky question. All right.
(24:06) Thanks, Cameron. All right. A little bit more review without the game. What are we looking at here? What's the y- axis? mean square. Why do we use mean square error on the y axis? It's our loss function. It's our error function. It's the natural error function. Where are we measuring mean square error in this plot for a ridge model? Right. Ridge maybe.
(24:36) Yeah, ridge model. Okay. How many different lines do we have? curves do we have? Looks like roughly six, right? A black curve and then the other five. So what did we perform as far as cross validation goes? Five-fold cross validation kfold cross validation with K equals 5.
(25:02) And so the black curve is really where you should zoom in on. It's the average vertically at every little vertical slice. The average of the other five curves. All right. And so what are we looking for here? How do we use this plot? You want to look to see where that curve bottoms out. And that's going to help you decide what is the best lambda to use. All right? And so we're uh minimizing out here somewhere around I don't know -4.
(25:33) Okay? But that's on the log scale. Okay? So lambda to 10 to the4th if it's on base 10. I'm not sure what base it is. Okay. with me kind of. But anything here where it's bottom out are all kind of reasonable. Mean square is a loss function. You're looking to see where it's bottom out. We could also use R squared here as a metric. And then we'd look to see where the R squar curve would top out.
(26:00) Saw these plots. What are we looking at in these plots? We could think about using these for lasso specifically for feature selection. But what are these plots? I call them trajectory plots where every choice of lambda both on the x-axis.
(26:27) What are the resulting estimated coefficients? The betas associated with the predictors as that lambda increases. What you see on the right is that lasso or ridge. Lasso on the left. Ridge ridge. Y. How do you know that? Lasso goes to zero and kind of stays there. The estimates once you get a big enough lambda, enough penalty term. And for ridge, they asmtote towards zero. All right. These plots look fishy to me. These look made up.
(26:58) Why do they look made up? They're not fully made up. It looks like a real model was fit under the case where the predictors were independent from one another because those plots kind of all follow the same general pattern. To me, it's telling me at least that the predictors really were created independently. There's no co-variance, no correlation among the predictors.
(27:23) In real life, that's rarely going to be the case. All right? So, you'll have a messier looking trae trajectory plot in real life. Here are some real life trajectory plots. On top are those MSE's really R squar plots to try to determine what's your best lambda for future prediction. On the bottom are the trajectory plots. What's different? Same idea. Lassos go to zero and stay there.
(27:50) Ridge asmmptotes towards zero but when it's big enough it gets really close to zero. I have depicted as that vertical dash line the best choice of lambda. And what do you notice for the lasso best lasso model through cross validation? One of the predictors got shrunk to zero essentially dropped out of the model. Not worthwhile in the context of the other predictors.
(28:17) The others all have some sort of nonzero beta coefficient. And so that tells you ah for variable selection whatever rel variable is related to green probably can be dropped out. How many variables are there? Count sure to six. Yeah, for my peeps. And then over here, similar idea with the ridge model. They're none of them shrunk to zero or even close to zero by the time we had to best lambda.
(28:47) Little weird action. We see this even though the betas get shrunk towards zero. This brown line, this brown curve actually sees a little bit of an increase in magnitude before it shrinks towards zero. What does that indicate? You can see them cross over zero. In fact, we probably have one here.
(29:09) Maybe not doesn't look like it. What does it indicate when one of them kind of changes actually becomes larger in magnitude before shrinking towards zero or change of sign goes crosses the zero line? It's indic indicative or indicative of correlation among your predictors. Exactly. Right.
(29:38) So this brown curve once the green one green variable gets shrunk towards zero, the brown one takes on some of its predictive power. It was probably correlated with it as an example. Okay, they're not always so clean in real life. Okay, ridge versus lasso. This is lasso. This is ridge.
(30:02) Which is the better predictive model where you could just look at the highest R squar between these? They're very roughly very similar. Not many of the predictors get shrunk toward zero. Which variable is least important? Whatever this green one is. And then second least important is probably the orange or the blue. Can't tell. Uh what is the MSSE and the OS model? Well, you could say remember both of these models as the alpha gets really really long sorry really small those models have uh are equivalent to the OS model.
(30:33) And so you could if we had MSE and not R squar but the R squar of the OS model is going to be estimated roughly similar to what we see when our alpha is really really small. The intercept only model as that alpha gets really really big. The MSSE of the model when you have all shrinkage that's the default model the intercept only model when alpha is really really big.
(30:57) Is there evidence of overfitting? Any multiolinearity overfitting? Not really. There's not a ton of overfitting in the OS model, but multioline colinearity. Yeah, maybe because we see this uh brown curve kind of have that behavior. Okay. And then this is just trajectory plots. Same uh data set just kind of extending those alphas a little bit further. And you can see those R squares should go to zero or close to zero.
(31:23) But these R squares are through cross validation. So you can actually get negative values because you're predicting out of sequence or out of sample I should say. Okay, cross validation baby. Believe it's going to help. Oh, it didn't show up. It just says believe, huh? The slide should say cross validation. Anyway, this is for Pablo.
(31:54) You know, we need a little soccer. the interplay between soccer and football. Okay, that's all just review. Hopefully that's getting you set for today and also set for section and for your quiz this week. Let's talk about some new stuff. Probability and random variables. Everybody close your laptop. Don't actually close your laptop.
(32:12) It's totally fine. But eyes up. How many of you have heard of the normal distribution before? Show of hands. All right. Most of you. How many of you have heard of the binomial distribution before? Great. All right. So, I'm going to go through this pretty quick. So, if you haven't heard of them before, you might have to come back to it. All right.
(32:38) How many of you have heard of the gamma distribution before? Okay, those are for stat 110 people. Great. Let's quickly go through this. Probability. What is probability? What's a synonym for the word probability? Likelihood, I don't like to use that, but that's okay. Likelihood is useful. We're going to use that uh for a different context in a second. Chance is likelihood, frequency, relative frequency.
(33:06) What scale can probability be on? 0ero to one. 0 to All right, great. So, that's kind of what probability is. That's the values you can take on. Why do we care? So, why do we have to worry a little bit about probability theory? Pablo talked a little bit about it because we're thinking of our data as being random realizations from some data generating process.
(33:22) That data generating process could be sampling from a population or it could be some theoretical construct. We're thinking about some alternative universe could have recreated this data and it would have been slightly different. Okay. So that data generating process is what we're thinking of as creating this random realization from some theoretical model.
(33:48) Okay, this ignores the basian definition of probability, but we'll come back to it. All right, so what's a random variable? Random variable is essentially just some random result that has a numeric outcome, right? So we're just taking random values that have result and numeric values. So if we're going to ask Harvard students, what type of OS do you use on your laptop? The three most common might be Mac, PC, and Linux.
(34:15) Okay. And then we could ask questions of all right. Let's measure X1 to be the observed value for a random variable whether or not someone is using MAC. So if we say X1 takes on the value one if someone's using MAC, X1 equals 0. If it's anything else, we have now defined a random variable.
(34:36) X can take on two possible values, 0 and one. And when I'm randomly sampling it, I don't know what value it will be. All right? And so we can describe that random variable with a distribution. And that distribution specifically is going to quantify all possible values that X has and the chances of seeing those.
(34:56) And so for this MAC example, you could get a zero with some probability. You could get a one with some probability. And we're going to define a distribution to say, all right, in this case, it's a discrete version, only two values. Zero has probability 1 minus P. one has probability P.
(35:18) Those of you who have taken stat 100, stat 104, stat 110, what do we call that distribution, the Berni distribution, the Bernoli, the simplest binomial distribution, binomial with an n of one. It's a Berni trial. Okay, it has a probability mass function. So we can turn that table into a formula to just say, "All right, give me an x, return back a p as a formula version.
(35:43) " Okay? And so if you plug in an x of one, this drops out and you're left with just p. If you plug a little x of zero, this term drops out and you're left with a one minus p. All right? Okay. And so when we measure x in our data set, we can start to think about the probabilities that generated it. And a lot of times in our studies, in our data sets, we're going to want to estimate that probability.
(36:07) What proportion of Harvard students use max? What's your guess? 89%. Love it. We could do a quick poll in here. I don't know, 75%, 80%, 90% somewhere in there. All right. And that's the goal. We could have a lot of guesses and try to estimate it based on data or based on our guesses. We talked already or I briefly uh suggested that we have a discrete version here.
(36:32) Our possible outcomes can only take on certain values could be infinite but only specific numbers 0 1 2 is the typical uh integers or they could go on some continuous scale. We're going to denote our types of random variables into these two big buckets because it will help inform the methods the models we're going to use.
(36:53) If we're dealing with a discrete random variable, it's called a probability mass function. That formula we saw on the previous slide. If we're dealing with a continuous random variable, it's called a probability density function because the function gives us the height of a curve because we're dealing on a continuous scale.
(37:11) Now, and to actually get probabilities, you have to find areas under the curve. You need width. Hopefully, a lot of this is review. This is essentially just uh based on some prerexs for the course. So probabilities under that continuous curve are going to be represented with a probability X being a specific value is going to be zero unless we give it some weight.
(37:30) Okay, this might be new for some of you. What's a joint distribution? What does jointness mean in the world of probability? the distribution of how two or more random variables are distributed jointly together. So it's sort of the intersection of two different or more random variables. And we could approach a probability statement to that.
(38:01) What's the probability that X1, my first person is a Mac user? X1 equals 1. And at the same time, my second person is a Mac user, X2 equals 1. and I can put a probability statement to that intersection of two events, the intersection of two possible outcomes for this random variable. And so that's what we call a joint distribution.
(38:25) And if in our joint distribution we assume our observations are independent, which won't always be the case, but it's a common thing. What does independence mean? No information from one observation to the next. the probabilities are unaffected from one observation to the next. Those rows in your data set can be treated as independent measures, independent observations, not affecting one another.
(38:55) And so mathematically, if we want the joint distribution of many observations, we can under independence assume they are just their separate marginal distributions applied together, multiplied together. And so it's just the product of all of those different realizations. Okay, that assumes independence and not all data are independent.
(39:18) If you have time series that might fall apart with me kind of sort of. Okay, this is just sort of just giving you a sense of where we are. So whenever you're thinking about random variables, don't forget two big buckets, discrete versus continuous. The two distributions I mentioned a normal distribution is a type of discrete or continuous normal let's find is discreet or continuous discrete. Great. So let's jump into these normal distributions.
(39:50) What's another name for a normal distribution? The Gaussian if you've heard of that or what do a lot of people call it? The bellshaped curve. It is one of many types of bell-shaped curves and the normal distribution is the common one. Great. So, here's the normal distribution. It is continuous. It is parameterized with its mean and variance.
(40:13) That means there's a lot of different normal distributions. They all have slightly different centers, slightly different widths. They all have the same bell shape. And I need to define where its mean, the center, and where it's how spread out it is based on its variance, its width. Okay? And so we have a function to define that bell-shaped curve. There's the function. I'm a good statistician.
(40:37) I should be able to write that out on the board, but I don't have the board. Okay. So, it's sometimes called the Gaussian. It's the bell-shaped curve. A lot of times we deal with this curve. We often times will standardize things. Hence the word standardization that has been mentioned before in this class.
(40:55) We will standardize normal distributions so that they have mean zero and standard deviation one. That's the standard normal random variable that we reference a lot of times. We call it the letter Z after doing standardization. How do you standardize any old normal distribution? So you take subtract off the mean, divide by the standard deviation, and now you've turned any old r any old random normal distribution into the standard normal distribution. Put it on the same scale and the same center.
(41:28) Okay, Z is interpretable. If I have a Z of positive2, what does that tell you? Or two standard deviations above the mean. If you have a Z of -3, what does that tell you? There are three standard deviations way out on the tail below the below the mean. Okay, lots of examples.
(41:53) Here's a normal distribution, bell-shaped curve, center at zero, standard deviation, variance of one, that's the standard normal distribution. Note, by the time you get to about three, and when you get to four, there's no more area under the curve. So, roughly three or four standard deviations is everything. And then we can shift it over three and make it a little wider.
(42:12) When I write out normal distributions, it's really confusing if you just write normal 32 or normal 3 4 because in some places they refer to the second parameter in terms of standard deviations. Sometimes it's variances. I always write it if I mean variances with a square. So I'll write it as 2 squ or 3 squ or square of 7 squar. Okay, just to indicate that it's variance.
(42:32) Okay. Anyway, that's just a little pet peeve of mine. Normal distributions have a mean and a standard deviation. And why do they show up? This thing, the central limit theorem. Why do we talk about normal distributions? Because they show up in real life a lot. We use them for modeling.
(42:50) Why do they show up so much? The central limit theorem. What's the central limit theorem? My stats friends out there, what's the central limit theorem? the average of the sample between normal. So when you take an average of many measurements or an average of many observations and look at the distribution of the potential averages across many samples, averages will follow normal distributions and many measurements are based off of averages.
(43:29) Okay? And averages are just a linear transformation of a sum. So sums follow the same property. Okay, there's some conditions you have to worry about. We're not going to worry about that theoretical stuff here. This is not statuto. So the central limit theorem says that averages or sums of many other random variables will tend to be approximately normally distributed as long as you've summed up or averaged over enough of them. Okay.
(43:56) What's an example measurement that's often normally distributed that people sometimes quote height height height is normally distributed. Why? Because adults height is the contribution of many little pieces. Genetic factors growth. What you ate, how much protein you got. Did you stunt your growth by lifting as a young kid? you know, things, little things like that.
(44:21) And every one of those little contributions results in your adult height. And so, generally speaking, we see these normal distributions. When we look at height, it's not exactly normal, but it's roughly bell-shaped. There it is. And so, that random variable Xbar is going to be approximately normal.
(44:38) That approximate normal distribution has the same mean as the individual observations. However, the variance is a little bit smaller. Smaller by a factor of Okay, I see normal distributions everywhere. What's this? Have you seen this before? Children's museum. It's called a Quinn Kungs. It's called Plinko board.
(45:02) Ever you see a Plinko board? Just drop a little marble down. It goes left or right with 50% probability every time it hits that little peg. And it ends up looking at where all the marbles end up. Look like a normal distribution. Okay, it's essentially the normal approximation to the binomial, which we'll get to in a second.
(45:20) Daily changes in the S&P 500, if you care about things like that, are approximately normal except for some black swan events. And then, oh, that's not expected to be normal, but my steps one day. No reason to think that would be approximately normal. Okay, let's talk about the binomial distribution. Normal distributions show up because of the central limit theorem, because lots of things are averages or sums of contributions.
(45:44) Binomial distribution, what's its story? What's the simple toy example of the bomial distribution? Number of successes in some amount of number of successes and some amount of trials. And the toy example is coin flips. Coin flips. Counting heads. When you flip a coin n times, how many heads do you expect to see if you have a fair coin and you flip it 10 times? you'd expect five on average, might be six, might be four, might be seven, might be three. And it applies a lot in data because a lot of categorical data
(46:24) can be thought of as separate coin flips when you look across all of your rows. Okay, this is a little bit of look in the future because we will come to binomial distributions again when we get to categorization or categorical variables as our response variable. uh when we get to logistic regression, the binomial distribution since it's discreet counting the number of successes in n trials, the probability that you get a specific value has that p to the x 1 - p to the n minus x because you have x successes each with probability p. You have n minus x
(46:59) failures, tails, each with probability 1 minus p. And you got to count up all the different ordering sequences of heads and tails. this n /x in parenthesis. What do we call that? Bionic coefficient or common forex or the word choose is the way you kind of read it. So it's n choose x in a sample of n. You got to choose which of those n values get successes.
(47:34) And so it's just counting how many ways you can get five heads out of 10 flips as an example. It could be five heads up front and five tails at the end. Or it could be one head, one tail, four heads in a row, four tails in a row, dot dot dot. There's a lot of different ways to do that. Okay? And it's just counting up all the orderings that result in five successes or x successes.
(47:58) Why does it show up? Because in polling data and in survey data and in really any data set that has categorical observations, you can think about modeling that through the binomial distribution or MAC exam. Okay, here's what a binomial distribution looks like. It is discrete, so you can only get certain values. This is a binomial with n of 10, p of five.
(48:23) That toy example of flipping a fair coin 10 times. Yes, most common response is five successes, five heads. You might get six or four. You might get seven or three. And then by the time you get to eight or two or nine or one really, really slow. This is a symmetric distribution because the proportion was 0.5. We can change that. We can flip it a few more times and have a p of 0.2.
(48:43) Since p is now at 0.5, you now have a non-ymmetric asymmetric right skewed distribution. And the most common response would be an x of four, but a three or five or a two or a six. are pretty likely as well. Okay, flip a coin 20 times and it's biased. Probability of heads 02 each time. It would not be surprising to see a five or a six.
(49:09) Okay, great. Our binomial distribution has mean N and P. That makes sense. Has standard deviation square root of N * P * 1 - P. Just some formulas to keep in mind and sometimes it's good to reference. Okay, now we have some probability and some random variables.
(49:28) We're going to connect this to data through the likelihood function and we'll connect it to linear regression this way. Okay, probability of data. A lot of times we're going to have a toy example of saying, "All right, let's assume some probability distribution." A common example would be in a stat 104 stat 110 type class.
(49:48) We know the distribution has certain parameters. What's the chances of seeing some outcome? Okay, so here's the example. 20% of Harvard College students are collegiate athletes, NCAA varsity athletes. What's the probability that there are 50 athletes in a random sample of 200 students? We can go straight to that binomial distribution, apply that PMF, and calculate that probability. All right. And so we could just plug in an X of 50, an N of 200, and a P of 0.
(50:18) 2 two and get the probability of getting exactly 50 heads. Sorry, exactly 50 athletes out of a sample of 200. All right. A lot of times 50, how many would you expect to see? X was 50 here. How many would you expect to see in a sample of 200? 40. And so we instead of finding exactly 50, we might calculate the probability of 50 or more because anything beyond that is more extreme. and we're heading towards what a p value, a tail probability is.
(50:53) So you might instead actually calculate the probability of getting 50 or more athletes in this extreme random sample result. And now on my probability is just a bunch of sums and it's roughly 5%. A bunch of calculations, PMF evaluations. Okay, tail probability. All right, alternative question.
(51:19) Is it more likely to occur 50 athletes or 40 athletes in a sample of 200 students? The mean is most common, so we'd expect 40 athletes. All right. Uh inference now is where we're headed. That's the whole point of this probability theory is to get to inferences. What's a statistical inference? This isn't the inference of like interpretations of your models. This is the formal statistical inference.
(51:42) Probability theory says give me the parameters, give me the distribution. What are the chances of seeing the data we see? Inference is the reverse inverse direction. Here's the data we see. If this is the type of distribution that generated the data, what are the parameters that are reasonable that could have created that data? Okay, we're trying to infer what might the real P have been given the one set of data we saw.
(52:23) What may have the real mew in the population have been given the sample mean that we saw. Right? And this is going to play a role in our regression in a second. So here's sort of the question. There are 50 athletes in a random sample of 200 students. Which binomial distribution is more reasonable? A P of 0.2 or a P of 0.25? We're changing the direction in that calculation.
(52:53) Instead of given P, what's the chances of X? Now we're saying given X, what's the most likely P, most reasonable P of the theoretical distribution. Okay, we're trying to make an inference about the population model about the parameter that generated the data. And that is exactly this idea of likelihood theory.
(53:21) Likelihood theory is saying that all right, we're going to apply inferences to try to determine a model's parameters. And which model should we use? Which parameter should we choose? It's going to be the one that is the most likely given the data we observed. Right? that same question heristic of, oh, P of 0.25 makes the most sense if we see 50 athletes out of 200. All right? And so instead, we're going to flip a PDF or a PMF on its head.
(53:45) We're going to look at the likelihood function as a function of the parameters conditional on the data. Okay, technical note, it's not a probability anymore because we're not thinking of these parameters yet as random variables. So for a set of independent and normally distributed random variables, all of my separate observations X are coming from a normal. The likelihood function, the unknown parameters are now mu and sigma squared.
(54:13) And I'm conditioning on my data. Now it's a product because we're treating our observations independent, a product of PDF evaluations. But now the perspective is oh I've already measured X and I'm just trying to figure out the mu and the sigma which shows up twice that is the most likely set of mu and sigma that had created the data.
(54:36) Okay, so it's just thinking about the same PDF function but from a different perspective from an inferential perspective. And so here's the example. Three observations are collected 3, five and 10. all thought to come from a normal distribution with unknown mean meu but known variance of four. Yep, it's a contrived situation.
(54:54) And if I asked you, okay, what mean what mu from that normal distribution is the most reasonable to have generated the data 3 5 and 10? What would be your guess? What does mu represent in a normal distribution? The mean. Okay, we have a set of data 3, 5, and 10.
(55:20) What should we calculate from our data as our best guess for the unknown mean meu? The sample mean. What's the sample mean here of 3, five, and 10? Six. 18 / 3 6. And so my best guess, my best estimator, my maximum likelihood estimator for that unknown mean meu should be just six. Okay, but it's not the only possibility. There's a lot of muses that could have generated this data. What about 5.9? Is that reasonable? Yeah, probably. 6.2. Yeah, that's probably reasonable.
(55:52) And we could draw the whole function of those likelihood model to say, all right, for any possible value of mu, what's the likelihood given our data? So, this is thinking about it. Oh, these mues that have similar likelihoods are all kind of in agreement with the data we saw. And as you get further away, then that likelihood function starts to taper off.
(56:21) Okay, we don't do this often on the likelihood scale. We often do things on the log likelihood scale because now we have nice concave functions and we turn a product after taking log into a sum. All right? And so if you've seen this stuff before, this is why we deal with log likelihoods.
(56:44) We'll talk about it in a second, go a little bit further into it. Okay? And we're just going to say, oh, like mu, it's maximized at a mu of six, maximum likelihood estimate, but there's a range of values that are all reasonable and plausible. Okay, so that's what we're going to do. We're going to maximize that likelihood function we just saw.
(57:04) That's going to give us our maximum likelihood estimator. And we have to figure out a way to do that. Two options. We can use some math, get an analytical solution, or we can do some computation. How would you do it mathematically? We have this function. We're trying to maximize mu and sigma squar given the data. How would you maximize that function? With two unknown parameters.
(57:25) Take the derivative with respect to your unknown parameters. There's two of them. Partial derivatives. And what do you have to do after you do that? Set them equal z. Two systems of linear equations. Now two unknowns. Solve. All right.
(57:49) what muses make the derivative zero because the slope is zero at the maximum point of that log likelihood function. So that's what we could do. Take partial derivatives with unknown parameters. That's called the score equations. Side note, and then you set that equal to zero and solve and xbar falls out in a normal distribution. The sample mean falls out.
(58:07) How would you do with computers? What if you don't have a closed form solution with partial derivatives setting equal to zero? some kind of solver. What's the solver we're going to use in this class eventually? I think Pablo has talked about it. Gradient descent. Gradient descent. So, we're going to be using gradient descent alternatively.
(58:30) And that's done on the negative log likelihood. So, linear regression, the maximum likelihood equation can has a closed form solution. You can do it analytically. Logistic regression in most cases. There are special cases where it has a closed form solution. in most cases has to be solved with gradient descent. Okay, simple linear regression model. Let's connect these things. Here's linear regression.
(58:53) What are we looking at here? Y response variable X single predictor, simple linear regression, single predictor. Beta 0 intercept, beta 1 slope. Those are the unknown population parameters. All right, we're thinking there's some data generating process that's going to create my Y and X. Given an X, it's going to create a Y.
(59:18) We also have a residual that inherent error stochastic noise variability around that model. All right, its mean is going to be beta 0 plus beta 1 * X. The error term is basically that distance every observation lies around the line. And so we can put this all together into a regression model if we assume that our residuals follow a normal distribution. And the way you write that out is in terms of a conditional distribution.
(59:47) Y as a response variable can be treated as a random variable conditional on X is going to be distributed normal that has a mean based on the line and a variance that's constant. with me. Okay, this normal distribution, we just wrote out the likelihood function for a normal distribution when not when there's a linear term for the mean, but when there's only a single mu for the mean.
(1:00:17) All you have to do is go back a few slides and replace the mu with beta 0 plus beta 1 x. And now you've got your likelihood function for the linear regression model. Okay, so the likelihood of any measurement is just for that normal distribution because conditionally we think y is normal given my y and my x. Now the three unknowns in this equation are the intercept, the slope and the variance variance of the residuals.
(1:00:44) And so now I want to maximize this likelihood function with respect to three unknown parameters. And so what do you do? You take the derivative with respect to each of those three parameters and set equal to zero. Except don't do it on the likelihood scale. Do it on the log likelihood scale. Right? This is our probability model. We can write out that log likelihood function as such.
(1:01:08) That log likelihood function is a product. That's ugly. So what do we do instead? We take the log of it. The log of the likelihood. That product becomes a couple different sums. And so we're going to minimize the negative log likelihood or maximize the likelihood in terms of what beta 0 and beta 1 you want in terms of beta 0 and beta 1.
(1:01:33) There are two terms in this log likelihood expression. I can cross this one out. Has no beta 0 and beta 1. This has a beta 0 and a beta 1 in it. What does that expression on the right look like? that rightmost summation. We've seen that before. What is it? It's almost the NYSE. It's almost Pablo's friend. It's the sums of squares error.
(1:02:06) It's the mean square error times n. And so what we are doing when we solve for beta 0 and beta 1 and we do ordinary least squares minimizing mean square error we're at the same time maximizing the likelihood function if we assume our data come from a normal. That's where we get those assumptions to linear regression from. They are coming from this distributional assumption.
(1:02:33) this data generating process. We put a probabilistic model to the linear regression line of where we think those residuals are. We build a likelihood function and out of it automatically is why we're minimizing mean square error with me kind of. Okay, great. So instead of maximizing the likelihood, we can minimize what's called the negative log likelihood. You might see that loss function used in other contexts.
(1:03:03) And so just put a negative there. We're going to minimize it instead of maximize it. And notice since it's now in terms of respect to beta 0 and beta 1, really what we're just going to do is minim minimize that expression. And so yes, we've now minimized mean square error. A look ahead. Eventually we're going to get to using response variables that are categorical, binary specifically.
(1:03:26) We could write out the log likelihood function and solve. And essentially this is logistic regression. So this is just a foreshadow. I'm skipping over it. I'm going to come back to it. All right. So, we'll see this in about two weeks, three weeks, I don't know, in a little bit of time. Okay. We have that probabilistic framework set up.
(1:03:44) Probabil probability theory is great because sometimes we get really nice formulas. And so, what we're going to do is turn that inferial likelihood function into statistical inferences of two types. Pablo's talked about it before. the two major classes of statistical inferences. He did bootstrapping to calculate a confidence interval. Alternatively, you might care about doing hypothesis testing.
(1:04:10) Go ahead and do uh resampling bootstrapping to do it or use some formulas which we're going to see here in a second. All right. And I'm going to push for formulas under the right condition. All right. So, let's go back to our price from square foot model.
(1:04:28) So this was the model, the simple linear regression model from before where we have an intercept and our estimated slope 0.5898. We saw that in the scatter plot. I went over it very quickly. What's the underlying theoretical model? Our underlying theoretical model says there's a population intercept and a population slope and we have a normal distribution of residuals around it and we get a sample of data 592 observations and we get a single estimate of intercept and a single estimate of slope.
(1:05:00) This is beta 0 hat our best guess for beta 0. This is beta 1 hat our best estimate for beta 1. Right? You can collect data again a new 500 observations roughly you're going to get a different beta 0 and beta 1 right because or estimate of beta 0 and beta 1 because they're coming from a population there's uncertainty involved in collecting data.
(1:05:24) So these estimates are the hats they are the ones from my sample of data. These are realizations from the data generating process that we're going to assume is correct for now with me? All right. So now we have our estimates and we say all right our beta 1 is roughly 0.589 that's the one we care about. We don't really care about the intercept so much.
(1:05:47) Well would it be reasonable to expect beta 1 being 6 to be true? Well yeah it's really close right? You can see that 589 versus 6. They're pretty much the same number. Well what about a slope of 7? Does that seem reasonable? You're starting to get some distance away. I don't know if that's reasonable. What about a true beta one of zero? Yeah, you're pretty far away from a slope of zero.
(1:06:12) So now you're trying to build the idea of what is a range of values for my slope that I can put on from my my data. Not just the point estimate beta 1 hat, but let's put a range of values around it. We call that a confidence interval. Do we think zero is reasonable? Do we think 70 is reasonable? We can formally test that through a hypothesis test. Okay.
(1:06:36) So, if there's a specific value you care about, a slope of zero, no association, you can test it. If you care about estimating, you can put a confidence interval around your best guess from the data. Those are our two big uh inferences. So, let's talk about the confidence interval here. There's two options. Pablo stepped you through doing bootstrapping.
(1:06:57) How do you do bootstrap? Resample your rows with replacement n times the same sample size that you started with. Some observations get repeated, some observations get left out. It's the golden ratio or one over the golden ratio. Anyway, it has nothing to do with this or that. In simple regression, the estimates have closed form solutions.
(1:07:22) And so we saw this uh as the closed form solution for the intercept, sorry, intercept and slope. The slope you might see in another class like stat 104 r* sy over sx. What is r? correlation coefficient between negative 1 and positive one. What's s suby? An estimated standard deviation of your response variable. Sx estimated standard deviation of your x variable.
(1:07:48) Right? Ignoring regression. It's just the distributional standard deviations. And in multiple regression, there's your hat linear uh algebra matrix form. What other parameters have we ignored in this expression? There's one sad little parameter we've ignored. These are our estimates for doing predictions. We're doing estimates of my response.
(1:08:15) But what are we forgetting to talk about? The error. We want the variance of the error. How much error is there? And so we can estimate the variability around that line as such. What's our best guess for the variance? Well, let's look at how spread out all the observations are. And instead of dividing by n like we would with MSE, we divide by a corrected version of n. We divide by n minus p + one.
(1:08:44) Because the more parameters you have, the less error automatically you're going to get around your model. Okay, so it's sort of a corrected version of MSSE is what our estimate for uh the variance of the residuals. Okay, from that variance of the residuals, I'm not going to show you the work.
(1:09:04) You can get the standard error of those estimates. Meaning, how variable would you expect the intercept to be if we collected a new set of data of 592? How variable would the slope be if we collected a new set of data of 592? from one data set to the next.
(1:09:24) What's the standard deviation? And in linear regression under those right assumptions, we have a nice closed form solution. We can bootstrap it or we can look at the closed form solution. And once you get that closed form solution here, it is again it's a based on that corrected version of MSE.
(1:09:43) As I said, that closed form solution for the standard error of the slope is the one we care about. we don't care about the intercept so much is related to hopefully that idea that if you have more data the less uncertainty you have as n goes up that denominator gets bigger the sums of squares of your x's get bigger if you have more coverage in your x variable your confidence interval the standard error gets smaller and if If you have more precise data around your line, if your model is a better fit, your error will go down. The uncertainty in your estimates of your coefficients also goes down.
(1:10:26) So looking for a better model also improves the standard error of your estimates. There's some other stuff going on here, too. In the multiple regression model, there's also colinearity's effect. We're not going to talk about it in this class. All right. So standard error and multiple regression, there's our estimates.
(1:10:43) It's just use a little bit of matrix algebra. There's a closed form solution. Again, don't worry about the closed form solution. You don't need to know this. Just realize that Python stats models in Python has this formula underneath it. Okay? I give you an estimate. I give you a standard error.
(1:11:07) How do you calculate a confidence interval? The t distribution using a t distribution. a t distribution value data by accident plus or minus roughly two standard errors and there's your interval for where you think the true beta slope would be doing things at once. All right. So there is our confidence interval formula. We have a formula for the standard error.
(1:11:39) We have a formula for the slope and then we basically do plus and minus roughly two standard errors. We think things are symmetric and that's going to cover roughly 95% of the time the true parameter we care about. Okay, today's word of the day, Kevin's animal. Kevin's favorite animal. Pablo's favorite animal. You're going to write sheep.
(1:12:09) Yeah, you guessed right. Great. You didn't have to come today, Pablo. You could have just put in the word. Okay, that idea of a standard error is a point estimate plus or minus two standard errors. The un quantifying that uncertainty of that estimate and the alternative version is doing hypothesis testing. Hypothesis testing also has a formula.
(1:12:29) You take that standard error, we saw it before, take the estimate, divide by the standard error, and turn your estimate into how many standard errors away from the hypothesized value are you. All right? The further you are, the larger the magnitude, the more extreme result you have. There's a whole formal process. Hypothesis testing. You set up two competing hypotheses, the null and the alternative.
(1:12:54) In linear regression, what would be the null hypothesis? the data. Your model does not accurately describe your data. So that would be a global hypothesis. That's a good set of hypotheses to be. Let's hone in on simple regression. There's no association in your data. What does it mean for simple regression? Your coefficient value slope coefficient value is zero.
(1:13:27) applies the multiple regression too. It's just the hypothesis. You can do one at a time. Okay. And so we'll set up a null hypothesis that the coefficient associated with your predictor's relationship with the response is zero. The alternative is is non zero. And now we choose our test statistic to be this test t test intercept sorry slope estimate over its standard error.
(1:13:51) And now you're turning it into a standardized version of compared to zero. And so hypothesis testing collect your data turn that into a p value based on the right distribution and dot dot dot dot dot it's a t distribution dot dot dot dot dot here it is is how you can do it in R. We can do inference via bootstrapping or stats model.
(1:14:13) When we do the bootstrap just remember we pull off the percentiles the 2 and a half and the 97.5 percentile for this square footage and price. It's from 0487 to 705. And in stats models, you fit the model and you ask for the summary output and you see the output like this. What do you do is you pull off the coefficients match what they were in sklearn. It gives you the standard errors. It gives you a t test statistic. It gives you p values and it gives you the confidence interval from 0.
(1:14:43) 544 to 636 in this data set in the bootstrapping was much wider. What was the difference? Stats models does it based on the formulas from a few slides earlier. Bootstrapping is wider because there's heteroscadasticity in this data set. This formula assumes no constant spread in your residuals. It's going to be a little off if that's not present.
(1:15:10) So in this example, use bootstrapping because your data have all the other assumptions. Great. Pavos wins again. And so inference, you can do it for multiple regression. A couple take-home messages. Forget about game time. We don't have time. Great. Sorry I went so fast. Come back on Monday. Slow down again.