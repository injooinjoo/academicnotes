(2) 109 day 25 section - YouTube
https://www.youtube.com/watch?v=Z0urYkxTPIU

Transcript:
(00:01) Can you hear me online? Great. We got a few people here. That's great. Good to see some people willing to learn. Um today's review session, my plan is to kind of just sketch out the material we've covered in the class. sketch out the material that's been new since the midterm.
(00:35) Uh sketch out all the different models we've learned and how I think about all those different models. Uh and then kind of just open it up to questions. Um I have slides prepared, but those slides are really just copying and pasting of slides from all the other lectures that we've already had since the midterm. Um, so we might come back to them to reference them.
(00:59) Uh, as questions hopefully will come up. I don't know. We'll see how this goes. Um, but I do have some ideas generally speaking. All right. So, data science one, what is this class all about? Where did we start the semester? What is data science to you? Using models to make predictions. Using models to make predictions. That's half of it.
(01:38) Using models in the realm of using models. We use them to make predictions. And what's the other half? Just as important and from my perspective more to make inferences. to make to make interpretations to understand what's going on along among your variables, association, things of that nature.
(02:02) So yes, make predictions and understand your data, associations, things like that. Okay, that's not the only piece. This is the whole idea of doing data science. And so we started the semester with data forward. We had to wrangle the data. We talked about the data science process coming up with hypotheses visualizing the data and then finally modeling and we spent twothirds threearters of the semester actually talking about models. Okay.
(02:29) And that's where a lot of the focus is in the class and a lot of the focus for today's as well. So that's my kind of just pitch to thinking about what this class is, how it fits into it all. Um, and we'll talk a little bit more specifically about details from before the midterm, since the midterm, and kind of sketch out all the different models.
(02:48) And I'll probably start with that actually um before I get into any deeper ideas. So, in this class, we had the midterm. We've had material since the midterm. It was a nice breaking point in the material. What was in terms of models the difference between the models we saw before the midterm and the models since the midterm? What was before the midterm? What was the first model we saw? Linear regression. KN&N regression.
(03:19) Regression. Regression. Regression. Meaning what? From a machine learning perspective, what does regression mean? It's not like categorical. It's not categorical. So therefore, it is a numeric variable, a quantitative variable. What is it? Which variable, the target, the response, the why variable.
(03:55) Okay? And that's the driving force in the decision between all the different models we're going to use is first ask yourself what response variable what's my target is it numeric or is it categorical and that's going to lead us down a whole different class of models. We started the semester by saying our response variable is numeric and let's talk about ways to think about modeling that. All right.
(04:21) The first model that we've used that we've relied on a lot is linear regression models. Great. I used to teach a whole class on linear regression models. And so linear regression modeling is a type of model for a numeric outcome. And we label it as parametric or nonparametric. Parametric. Why is it parametric? We put structure on that relationship of how the response relates to the predictors. We put mathematical structure on it.
(04:57) We define parameters which gives us that mathematical structure and therefore we call call it parametric. Okay. From a statistical perspective, we say, "Oh, it's parametric because we make a distributional assumption on the data." Whatever. We put this mathematical framework on it. Okay? From a machine learning model perspective, there are the betas that we're trying to estimate.
(05:23) Okay? So, we first lay out the structure and then we estimate it from the data. Non-parametric, what's the difference? We don't put that mathematical structure that mathematical framework to the rel to relating y to the x axis. We let the data speak. Okay, sort of with me. All right. So before the midterm, we talked about a lot of different models. This might not be plugged in.
(05:48) And we had 12 different lectures before the midterm. I did very few of them, but that's okay. Started off with data. talked about visualization did a little bit of pandas and then yes we jumped into linear regression somewhere in here we talked about KN&N I forget when that was okay we did linear regression multiple regression we did polomial regression to me that's all regression there's no differences there okay we said once we get into polomial regression and interaction terms especially we start to see the potential
(06:22) for overfitting all right and so the more predictors we bring into the model, the more likely we are to overfitit to the training data at hand. And so what did we do to handle that? We started doing cross validation to help us decide which predictors to include or which model to use, which model is best for predicting in the future, which of those betas are more reliable, which ones should be thrown away. All right.
(06:55) Once we did that, we also said, well, maybe we don't want to throw away betas. Maybe instead we want to shrink them, regularize them to smaller values because we want them to have less variability depending on what other variables are in that, what other predictors are in the model. And so we did a little bit of shrinkage regularization using ridge and lasso. there.
(07:18) We definitely needed cross validation because we needed cross validation to help us tune the hyperparameter in those models. What's the hyperparameter in those models? The lambda penalty term, right? How much should we shrink our coefficients towards zero? Once we talked about that, we then took a break and said, "All right, these are a whole class of models.
(07:42) From those class of models, we might want to do more inferential type approaches. Statistically speaking, inferences means quantifying uncertainty, talking about making statements about where that beta estimate is along with bounds of uncertainty as a confidence interval. And we also did some hypothesis testing. For some reason, that's left out of this title, but hypothesis testing is here as well.
(08:07) And we did that both through bootstrapping and permutation testing through resampling at first not putting probabilistic modeling on it. And then we layered probabilistic modeling on top of that. I did at least we talked about maximum likelihood estimation. Things were normally distributed in uh linear regression.
(08:27) We added the layer of Beijian inference to it. And then we tidied some things up. talked about highdimensional data and PCA right before the midterm and that's where we were at the midterm. Okay, all of that material basically we said all right that was all useful for when our response variable was numeric.
(08:49) Let's take all of those same methods and apply it to when our response variable is categorical. What changes? question first. Make sure for cross validation we use it to tune hyperparameters. Is that the only uh reason for us to use cross validation? Cross validation. I like this cross validation.
(09:17) What is cross validation useful for? Lots of things. I rarely use cross validation in my line of work. Okay. Why do I rarely use cross validation in my line of work? Because I'm not a machine learner by training. Sometimes I use machine learning models. I'm a statistician by training. What does statistitian usually try to do? Put inferences on the betas. Okay.
(09:41) So there is a particular model or a few models I care about and I want to use all of the data that I have at hand to talk about really honing in on the association between a predictor and a response. I don't care how accurate my model is. I care about how X relates to Y in 90% of my applications. I care if a new treatment is effective. I don't care how effective it is.
(10:06) I mean, yes, I care how effective it is, but I really want to see is this new treatment better than the old treatment. Okay? I'm trying to determine whether its beta is bigger than the old beta to put it in our context. All right? And I'm going to use all the data to do that. There's one model I care about. Okay? I can control for other factors.
(10:24) I can include them in my regression model if I care. All right, I don't use cross validation often, but if I do, it's for predictive purposes, for choosing between different predictive models. And so at a high level, we use cross validation to choose between models. That's all it is. Cross validation is useful to choose between models. Choosing between models we can sort of delineate into a lot of different things.
(10:55) One of those things is tuning a hyperparameter. Tuning the hyperparameter in ridge or lasso. I just think of that as choosing between models. One of my models is all of these predictors, large set of predictors with a lambda of one. That's a model. And the next model is all of those same predictors but with a lambda of two. All right.
(11:24) And so I'm just iterating lambda or changing lambda and saying that's a new model that I'm going to consider. All right. And so this tuning of the lambda in ridge or lasso tuning of a hyperparameter to me is just choosing between models because it is it's really no different. It's just in a very specialized context. Does that answer it? Right. Other questions? All right, back to my question. We have all these methods, all of these 12 lectures, 11 really, talking about models for outcome variables that are numeric.
(11:59) We're taking all of that methodology and porting it into a classification paradigm. What changes? Why did we change things? Why don't we just use linear regression when we have a classification problem? When our response variables classes instead of numeric, what's the simple simplest classification problem? Response variable that's categorical.
(12:31) What's the simplest one? Binary. Binary. Yeah. Zeros and ones. And we could model that pretty easily with a linear regression. There's no harm in doing that really, although you can get weird predictions outside of the bounds of 0 and one. Okay. So, what do we do instead of using linear regression? We use logistic regression, right? What does it do? Instead of fits a line, if it's a sigmoid, an S-shaped curve, that's all it's doing, changing that curve.
(12:58) And that interpretation is very different. We'll jump into that maybe later today. Okay. So, linear regression becomes logistic regression. All these things become logistic regression. Region lasso becomes region lasso for logistic regression. Beijian inference becomes basian inference for logistic regression.
(13:14) It's all the same. It's just under the separate branch of the class of models because our response variables different. Okay. In the classification problem paradigm, it's not just zeros and ones. We could have multiclass problems too. and those multiclass problems. Now, I definitely shouldn't fit linear regressions because order of classes usually don't matter.
(13:36) And so, I have to be a little bit more careful in how I frame everything in the multiclass setting with me. So, we took all this ported it to logistic regression and then we added on top of that what type of models we spent about four or five lectures, maybe three, I don't know, four lectures.
(13:55) No, if we include the Beijian thing, it was about five lectures on logistic regression with the Beijian paradigm on top of it, hierarchical models, things of that. And then where did we go? Then Kevin walked away. Pablo walked in the door. And what did Pablo force down your throats? In a happy way, in a happy way, not forcing. Asked you to digest this stuff.
(14:16) Trees. Decision trees, right? And so we talked about decision trees. from decision trees. We talked about multiple decision trees and combining multiple decision trees through bagging, through random forests, through boosting, through mixture of experts, which is more broad than that too.
(14:40) Okay, so we started off with base decision trees, one tree at a time, and then we talked about ensembling those trees together or ensembling other me models together. Okay. And that's what we had talked about since the midterm. All the sort of logistic aggression stuff. We can get into the details if you want. We had to worry about the multinnomial multiclass setting. We talked about Beijian models and hierarchical models within that.
(15:03) From there, we talked about decision trees with Pavlo. I came back because my dog died earlier to talk about missingness. No, it's not an excuse. It's actually what happened. And then we talked about ensembling. Okay, great. I know it's sad, but these things happen. She was old. She had lived a good life. Done. Review session over. Not quite.
(15:29) All right. I like to think about asking questions of your data. Okay. So, let's make a decision tree of our models. All right. What should be the first question among all the models we've talked about in this class? What should be the first question you ask of your data? You got it. Start here.
(16:06) Why response variable? Ask two questions or ask a question with two possibilities. Are you numeric or are you categorical? All right. And we're going to have similar branches on either one of these first primary branchings. All right. All right. Now, in our class of models for numeric response variable data, we have two big classes of models. I mentioned it already.
(16:40) Linear regression is a type of what class of models? Starts with a P, ends with an arometric. Parametric. So make a decision whether you want a nicely interpretable model that's parametric versus non-parametric. Okay. And within the parametric world, really, we got one. All right. What is our one parametric model that we saw in this class? All we got is linear regression.
(17:26) Now, there's very many flavors of linear regression, but really it's all linear regression, right? What were all the different flavors? Simple, multiple, polomial, interaction terms. That to me is all just like multiple linear regression. Okay. But we have generally here regularized versus unregularized regular old regularized linear regression. Okay. But there's lots of things to consider within this.
(18:05) How to handle your data. uh your predictors can be of any form. Okay. But your decisions might be for predictions. Parametric models are usually most useful for inferences but can be used for predictions as well. All right.
(18:26) What are a class of models when we have nonparametric want to take a nonparametric approach when we have numeric outcomes. What was the first one we saw? K&N. There you go. We got K&N. How does KN&N work? You got to determine your neighbors. And once you got your neighbors in the regression setting, what do you do? You take the average of your neighbors to do the predictions.
(18:56) Okay, there's not a whole lot there other than through plotting predictions to understand how variables relate. Okay, things to consider in KN&N. Should you standardize your predictors? If you don't know, probably you should, but there are reasons why you might not want to. You don't have to standardize your predictors, but without more information, that's probably the default. Okay.
(19:26) When might you not want to standardize your predictors? Well, that's another example. Decision tree. Yeah. Yeah. We'll get back. We'll get to decision tree in a second. When in a KN&N would you not want to standardize your predictors? If you have categorical predictors, that might be helpful to think about. Well, what does it mean to standardize one hot encoded or categorical predictors? I don't know.
(19:51) If they're ordinal, same idea. One example that I often give in KN&N is sometimes your neighborhood, you only want to match people that are of the same class as you. All right? You want to match the closest people within your category. So if you're trying to use both college data and high school data to predict some outcome, you might only want to match among the college people and only want to match among the high school people to determine your neighborhood.
(20:27) So what you do is you standardize all of the other variables, but you overweight make the standard deviation really big I think of that categorical variable. So like the distance there, you make sure you're within that same category. All right? And so instead of making it zero and one in that category, you make it zero and a billion to make sure ensure you match on that category of high school versus college.
(20:56) Right? So sometimes you can rely on that to have a little bit more uh care in how you use K andN. All right. We have lots of other nonparametric models here. You can map this in a lot of different ways, but what really I'll do it differently. What's the other nonparametric model we kind of relied on? Decision tree. Thank you. Decision tree.
(21:33) Okay. What's a decision tree? How do you do a decision tree in a regression problem? You tell SK Learn to do it, right? No. What is under the hood of a decision tree? Cann find your neighbors and you average among your neighbors to do the prediction. What do you do in a decision tree? Split your data based on a threshold into two different regions.
(22:06) Okay? And then average those regions and predict based on the averages among those regions. Kind of with me? Okay. How do you choose that threshold? Choose a threshold event like such that when you split um it'll become most pure by the gene. In a regression problem, what's your loss function? Mean square error for all these models. Essentially mean square error is the driving force.
(22:48) We're trying to minimize mean square error. And we try to minimize mean square error here by picking a threshold that splits your data into two. So that when you calculate the mean square error in one group and the mean square error in the other group region, sorry, I should say one region versus the other region, when you weight them based on sample sizes, you've reduced mean square error the most.
(23:19) Okay, so if you had a scatter plot really it's sums of squares error if that means anything to you. We have an x and a y. This is a regression problem. Linear regression would just fit a line. What does a decision tree do? It doesn't fit a line. What does it fit? A decision tree. Well, what is the decision tree evaluation of the function of how Y relates to X? Right? This is the whole idea.
(23:57) We're trying to determine how Y is related to X in some functional form. And linear regression says that functional form is a line. Decision tree says that functional form is a. Have you thought about it this way before? It's a step function. It's a step function.
(24:28) That's all it's doing is it's fitting a step function, the best step function in a greedy way to determine where those steps are. And so here, where do you think the threshold should be? Well, the threshold could be here. It could be here. It could be here. at every break in between the points, right? Could be a threshold to consider.
(24:47) But visually, where do you think the threshold should be? If this is on a scale of 0 to 100, where do you think the threshold would be set? It's pretty intuitive. 50. Right in the middle, maybe. Why? Because if you set it at 50, is that the middle? 50. I think it's going to be a little less than that. I'm going to say 40, right? Why? Because if I set a threshold at 40, call that t.
(25:19) Now I've split my region into two. Okay, the region to the left. How do I model them? An average, a flat line. Okay, so here is yar for region one. What about in region two? A flat line, right? And so that flat line maybe is like here. This is y bar for region two. And yeah, that's a pretty good choice, I think, for a threshold. And now we just fit a step function.
(25:58) Okay, what's the next splitting? Where do we consider a second split if all we have is this one predictor? Well, let's start actually writing out the tree. What's the tree? It's a tree. But I for is the less than always to the left would make sense, right? So I think this is like x is less than 40. This is x is greater than or equal to 40.
(26:32) Where does equal sign go? Oh, I'm not sure. I'd have to look at the the notation or the documentation. Okay. And then this would say, oh, y bar, let's say this goes from 0 to 10. We would predict y bar equals I don't know what's that three. What's this like six? with me kind of. All right. Where do we split next? Oh, question. Threshold seems kind of excessive. Yeah.
(27:18) How computationally do we define these thresholds? There are some computational efficiencies here. I'm not the expert to ask here, but really what it's doing is it is checking every threshold and then keeps track of what those threshold splits would be um across all of your predictors and it kind of keeps track of the cardality there in a pretty intelligent way if you want to speed it up uh computationally.
(27:36) Okay, but there's probably approximations some sort of like binary type search to find that in a faster way. Okay, this isn't algorithms. Good. takes CS124 maybe they might talk about this and that they should be fun too. Okay, but good question. All right, so what's the next splitting? Where should we split next? We have to look at each of the two sub regions separately.
(28:19) Right? Why? because we've decided already to include region one to be all the observations where x is less than 40. And so now we're going to break up this region into two smaller regions. We're going to break up this region into two smaller regions separately independently from the other region. Okay? And there might be other predictors we might want to consider in each of those sub regions as well.
(28:41) Okay? And it's considering those other predictors independently separately. Okay? Okay. So then we might want to aha I do have some color. Split this region maybe here and it's like this versus this step function. We'd probably split this region like here and it's probably like this versus this with me.
(29:10) And so now we're splitting here based on Oops, that's orange. We'll say x is less than 20. x is greater than or equal to 20. x is less than 60. X is greater than or equal to 60. Okay, we're building a step function one step at a time. That's all we're doing in a decision tree. Okay, that's decision tree for regression. Decision free tree for classification.
(29:42) Same idea except now usually we think about it as like a threedimensional object and splitting into like purity. Instead we have to define our uh metric a little differently. Okay, great. That's a decision tree. And then that decision tree we use for a lot of different types of models. We can do some ensembling of decision trees. And really I kind of think of them in two ways that we ensemble decision trees.
(30:12) We either bag or we boost with me kind of. What's the difference? What's another word for bagging? Stands for bootstrap aggregating. All right, so we're bootstrapping our data, refitting our models to bootstrapped data.
(30:55) We know what bootstrapping is hopefully, right? Sampling the same sample size that you started with with replacement. Okay. And then how do we combine the separate decision trees on those separate bootstrapped data sets? You average the predictions. Okay. So these are averages of complex base trees. complex. How do you make a tree complex? Overfit, high variance, complex. What does that mean? Super deep. Lots of lots of splits, lots of nodes. Okay.
(31:55) How you restrict the depth? There's a lot of ways to do that, but high depth, small number of observations in each leaf, dot dot dot dot dot. Okay, so there's a lot of ways to define that hyperparameter for complexity with me. Okay, what's a specific example of bagging? Or bagging is a specific example of what related type class model? After we learn the bagging model, it's in the notes. What's the next lecture? Random forest.
(32:28) So to me, random forest is the general class of bagging where bagging is a specific subgroup of random forests. Okay, so bagging which includes random forests, a random forest. How is the bagged model a special case of the random forest? You bootstrap your data.
(33:05) And in a random forest, what else do you do? What's that? Aggregate. Aggregate. Well, that's agging in general. But what does random forest layer on top of that? a subset of predictors and we bring randomly selecting a subset of predictors at every split. Okay, so for this toy example on the board, we could bootstrap the data many many times, but we only have one predictor as it's visualized here.
(33:35) And so I can't do a random forest. I can do a bagging model, but I can't do a random forest because there's only one predictor to consider at every single split. Random forests only get you somewhere where you have a predictor more than sorry, a predictor beyond what a bagging model would have. Okay, they're useful for when you have a whole whole whole lot of predictors.
(33:59) How do you incorporate interaction terms into these decision trees and random forests? Interaction terms, how are they incorporated? automatically naturally, right? Because of the creation of these trees, I'm splitting on the left hand side of this tree independently separately from how I'm splitting on the right hand of this tree.
(34:24) There's no reason to think those splits will be the same. And so how one variable determines splits over here is different potentially from how that same variable determines splits over on the right hand side of the tree. How one variable relates to the response is different depending on the value of another predictor.
(34:42) That's an interaction effect. Okay. How is linearity handled or nonlinearity nonlinearity handled? Naturally, automatically because where we're splitting doesn't have to be in the middle. It could be somewhere to the left or somewhere to the right. And you can naturally build a step function that is nonlinear. Okay.
(35:08) How does bagging apply or random force in my mind apply to this idea of a step function? What does it do? What does bagging do to this step function? So this is a single decision tree. What would bagging do? What would bootstrap aggregating do? You get a new collection of data points. Maybe there's two data points here. that observation got sampled twice. Maybe that one got left out. Here's one.
(35:37) Here's one. You know, they're on top of these. But sometimes there's double or triple points. Sometimes the points are left out on that bootstrap aggre uh bootstrapped sample. And so when I rebuild the tree with its own thresholds, those thresholds probably will be a little bit off. Maybe for my second tree, it's at 44. And the splitting maybe will be I'm using green.
(36:01) I'm using blue. Do I should use a different color? Maybe for my second tree it'll be somewhere here and then it'll predict it a little higher over here and maybe a little higher over there. Okay, but we jiggle where those thresholds are with every one of those bootstrap samples.
(36:23) We jiggle the height of that decision tree or that decision tree prediction height, whatever step function height. All right. And what we do is we kind of just blend them through. We average across them. And by jiggling the heights of the step function and the thresholds of the step function, we kind of smooth out that step function.
(36:42) Okay? And that's what bootstrap agg uh aggregating does. And on top of that, random forests essentially just make those thresholds vary more where they're located because sometimes something else gets controlled for before I consider this other variable. Okay, so when you do a random forest, you average over complex base trees.
(37:08) What are you leveraging? This is me as a statistician. I get real excited. It's a simple formula that we're leveraging when we do bagging and random forests to improve the overfitting, reduce the overfitting, reduce the variability. Anybody see this formula before? If you've taken a stat class like 104 or 110, you've seen it.
(38:06) If you've taken an econometrics class, you've seen it. What are we doing? What is this showing? This is the law of large numbers but with covariance between your random variables. Okay, when you average the predictions, think of this as like yhat one and yhat 2. When you average the predictions, the amount of variability of that average depends on the variability of the first tree, the variability of the second tree, and the covariance between the predictions of the two trees.
(38:41) Okay? And hopefully that covariance isn't one. As long as it's something less than one, you're going to start reducing the variability in that tree as a result. Okay? This isn't just for two trees. This is for like 500 trees. Usually, you fit. and you're averaging over 500 trees.
(39:02) But this is the base formula underlying that intuition of we're averaging over many complex models. Therefore, we're trying to reduce the variability because if co-variance between X and Y is less than one, then this will be reducing variability. kind of with me maybe. Right. This is y hat one. This really should be written as like y hat one and y hat two because we're doing predictions from two different trees.
(39:50) We'd rather have the variable. We want to go variance as small as possible to leverage this reduction in variability in our predictions. Okay. And we can't really manufacture that covariance. What is this covariance represent? Every single tree, decision tree from the bootstrapped uh sample data, right? These are our base models, our base learners as they're called.
(40:24) They are predicting based on the structure of those trees and the structure of those trees are going to be very similar. So the predictions will always be very similar but sometimes they're really far off. Okay. However, if we have structure in those trees that are more dissimilar, then we can reduce the covariance in the predictions between those trees. And that's what random forests is getting at.
(40:51) We're reducing the covariance of the structure of one tree base tree and another. Okay, that's what it essentially is leveraging. You don't want to lose error though or accuracy. So you need to make sure all those base trees are similarly accurate. Okay. So that was leveraging random forest and bagging is leveraging the law of large numbers in the presence of coariance or less than as long as the coariance is less than one.
(41:23) We're improving things through an average. We want it as far away from one as possible. Smallest smaller. Ah so these are the same trees. Good question. These are the same trees. So their variance by by structure should be the same. The structure of tree one and the structure of tree two are the same.
(41:49) Varian doesn't it still improve like it doesn't have to be less than one. It just has Oh sorry sorry sorry. Yeah yeah yeah yeah. Good call. Good call. Good call. I said coariance. I'm thinking in terms of correlation. Okay. So covariance is the square roots of standard deviations. Thank you. Thank you. Thank you. correlation. And so correlation, don't forget, relates to co-variance based on being multiplied by their standard deviations.
(42:13) So yeah. Yeah. Yeah. So as long as they're not perfectly correlated, we need that co-variance to be less than the variance. Yeah. Yeah. Yeah. I got you. Modification question. So what I understand is that begging tries to address the the high variance from single decision tree by aggregating many decision trees.
(42:37) Right. But since in bing averaging many decision trees per decisions. Yes. Right. Uh but in bagging we know one tree might be heavily correlated with other trees and using that formula the correlation would be high. Right. So begging does not solve the problem of high variance.
(43:04) Then it it reduces the problem of high variance because the correlation won't be perfect because since we're doing uh bootstrap sampling the correlation will be high like 0.9 but it's not going to be a correlation of one. And so we will slowly reduce that variance when we average over many. And random forest reduces this correlation. So you beat me to the punch.
(43:28) So random forest, you still have positive correlation between all of your trees and how they're structured, but you're hoping that correlation is even less than what it was when you bagged. Yep. If the correlation is less than one, you're saying that the coar less than like so don't forget I screwed up when I wrote this on top. Covariance is equal to the correlation times the standard deviation we'll call it of y1 y 2.
(44:15) This is correlation of y1 and y 2 is the co-variance of y1 with y 2. And so you get from the correlation to the covariance by multiplying by the two separate standard deviations which is just the square root of the variances. And then the one halves kind of cancel not cancel reduce that variability as long as that correlation is less than one.
(44:42) And when I plug this in here, I'm reducing the overall variance of those two. Thank you. Good catch. Sorry for the confusion. Okay. All right. Random forests bagging. Compare that to boosting. What's boosting doing? Completely separate from random force and bagging. It's also a version of ensembling. But what is its approach? What's that? It learns from the errors, right? It learns from the errors of other models.
(45:25) Well done. So what is boosting? It's predicting learning from the errors or residuals of simple trees. Okay? Okay. And so what it's trying to do is one little step at a time trying to reduce the bias of all of those oversimplified underfit models. Okay.
(46:15) So it takes a completely separate approach to improving that prediction reducing bias one little step at a time versus improving the variance the overfitting through averaging. Okay. We had lots of different versions of boosting two big versions. We learned gradient and add a boost. Well done. What's the difference at a high level? Gradient boosting versus add a boosting. High level.
(46:53) Any anybody really know what there's just one quick description for the difference between the two? Gradient boosting is for numeric. Add a boosting is for categorical. Unfortunately, no. Gradient and add a boost can both be used for categorical and for regression problems. Add a boost for regression is a little weird, but you can do it. They take a different approach into handling the errors. All right. What's the difference approach? Add a boosting, add a boy, add a girl.
(47:26) You're given extra weight to the residuals to the observations that have large residuals to those that you're mclassifying. Okay? Whereas gradient boosting, you're just fitting to the residuals directly. Okay? So instead of changing the case weights of those observations, you're off on you're just fitting to the residuals directly one little step at a time.
(47:58) What is fitting to uh it's still fitting to the data but in a uh reweed situation. I have some slides to maybe illustrate that. Okay. Oh, that's good chalk. Oh, well, I did leg day yesterday, so I can't bend down very easily. If you know Alex Young, blame him. Okay.
(48:36) And then all of these models together, we can sort of combine all include these as well. We can ensemble them all together. We kind of learned three different approaches of ensembling the last day of class really quickly. Do you remember what those were? Mixtures of mixture of experts. That was the third one we learned. Yeah, I probably should have said boosting. There's two kinds. Let me wrote that out.
(49:18) There's the gradient version and there's the add a version. These involve weights. These involve residuals directly. Okay. Mixture of experts. Did you catch what the other two we labeled as on the last day of class? For those of you who came, blending and stacking, you got it. Okay.
(50:00) What are we going to learn next semester for those of you who are going to take B? Neural nets. And neural nets essentially are just a generalization of a mixture of experts in a lot of different ways. Okay, is the way I think about it at least. Okay, what's blending? This is the easy one. This is like the intuitive one to me. What's your favorite model class in a regression problem? Pick one.
(50:30) Mine's regression, so I'm going to choose regression as my base model. What's going to be your favorite base model? Decision tree. Great. Decision tree. You fit a decision tree to the data. I fit a regression problem to the data. What do you want to fit? KN&N. All right, we got a CANN coming in. Anybody else have a favorite type of model? Pick one. Rand forest. Great. Adaboost. Great. Lasso. Great.
(51:05) gradient boosting. Great. We got all these different model classes. Okay. And so you fit a model six different ways. Each of those six models gives us predictions. Okay. Some models do well. If there's a linear relationship, the linear regression will pick it up probably best. If there's a nonlinear relationship, maybe the decision tree will get it best.
(51:32) If there's a lot of interactions, maybe the boosting or the random forest model will get it best. Okay. And what we want to do then is from every observation in your data set, in your training data set, you get a yhat. Okay. For each of these models and so I'm the first row in that data set that we fit six separate models. I get a yhat one for model one. I get a yhat two for model two.
(51:55) I get a yhat three for model 3. Dot dot dot. And what I want to do then is put a model on top of that to use those predicted predictions from all these different classes of models as my new X's into this meta model. And so the meta model is pick your favorite model to basically combine the predictions into a new prediction. Okay. So all we're doing is fitting meta models.
(52:25) These two are very similar. Meta modeling the Y hats as predictors and these Y hats come from other models. Okay, what's the difference? This is essentially saying in comparison to bootstrap aggregating. Okay, your meta model in a bootstrapped aggregated model is essentially every decision tree is its own yhat and you can just average the y hats.
(53:07) So my base model is a priority. I'm just going to average those predictions across the 500 base trees. It's not really a learned meta model. It's an enforced meta model. Okay, but maybe it's not smart to do that for all of your observations in all places.
(53:26) Maybe you want to learn from the different structures that you used for all those different base models. It's not very useful when the structure of those base models are all the same. It's useful for when the structure of those base models are quite different. Okay? And to me, stacking and blending is kind of the same. Stacking just uses cross validation in a somewhat smarter way.
(53:50) How did we model YASA filters was it? Oh, so every observation, so imagine your rectangular data set, right? Your rectangular data set has like X1, X2, X3, X4, X5 has a Y, right? Your base model is using X1, X2, X3, X4 to predict Y. And you can just use the predict command to get Yhat. And that's a new variable in your data frame. Just append it to your data frame as Y hat one.
(54:18) Okay. Fit now a decision tree. Y 2 fit that that was the linear regression. Fit a random forest. Yhat 3. New variables you're appending to your data set. And now those Y hats can be predictors in whatever model you want to choose to combine them to predict Y. On a second level, person also pass and justice predicted.
(54:48) Yes. So that's what Pablo was talking about with pass through. And so the pass through is you don't have to only use the Y hats, you can also use the predictors in doing that modeling. It kind of is based on what interpretation you want.
(55:07) So if you want to interpret like saying, oh, what class of model is most important? I'm going to ignore those x's because I can kind of have a waiting function across then my y hats. That's what my meta models estimates is going to be basically reflect. But if all you want to do is just create a best predictor. Yeah.
(55:24) Bring the x's in too. And that can inform some information. Yeah. And the structure there can be very free roaming and how you combine them. Ask what model do we use? What model do you use? Any model that uses those types of yhats. Presumably they're numeric in this regression problem to predict Y is still a numeric outcome. So you can use linear regression. You know you can use random forest for a regression problem. The Y determines what type of model you want.
(55:49) And which model to use depends on what's your goal to improve accuracy or to interpret the results of that meta model. Just like always, do you want a parametric one or do you want a not parametric one? linear regression for like the final model with the wise predictors. Yeah. How would you interpret those coefficients? So the coefficients essentially would tell you of that meta model would basically give you the weightings of each of those Y hats that should be used to predict the actual Y.
(56:28) So each model gets a beta in that meta model linear regression and that beta tells you if it's zero. That's saying that predictor doesn't matter in the context of the other models that were considered. Not needed. Not needed in context of these other models. Doesn't mean it wasn't good.
(56:47) It's just doesn't add anything beyond what the other models already do. So yes, the question is I can just use the y and the x's in a new model. predicted it for those previous. So the question is, won't there be colinearity among your y hats that you're using in your meta model and the x's you're using in your meta model, especially if one of your base models is linear regression and your meta model is also linear regression.
(57:39) Yes, there's a lot of colinearity there. Yes. So that's why the estimates of those betas you have to be worried about. That's why maybe if you have really high colinearity, you don't want to use the same type of model in your base model as your as your meta model. Then we um fit linear linear regression using all the um predicted yaxis as predictors within coefficients.
(58:10) then tell us which of the models um contributes most to the predictions and how is that if so how is that different from you know the um mean square error? So the betas in your meta model will the question is how do what do the betas sort of measure in that meta model? Yeah, the betas can be used as sort of interpreting which of those models, base models that went into it was most influential in the prediction of the actual Y.
(58:41) Note, they're all going to have Y hats. They're all going to be hopefully by themselves positively correlated with the actual Y. Otherwise, your model is crap. You're not going to want to even consider it. And so, you're going to want to somehow then the those betas you'd expect to mostly be positive. If they were all equal, then you're just doing a weighted average.
(59:01) Okay? If they're different, then you can sort of see that they being the betas, you can see which model is relatively stronger. One more question. Your meta model could also be one of the regular models you use. Yeah, your meta model doesn't need to be linear regression. It could be the same class of model that you used as one of your base models.
(59:26) It's totally fine, especially if a lot of your base models are of different family type classes of models. Yes. Okay. Last one. Mixture of experts. How is that different? like the models are like highly specific to certain change. Yeah. At a high level, what it is is the mixture of experts allows for some models in some locations of the actual observed predictors to be better.
(1:00:13) Whereas in other locations in the region of predictors that you have maybe a different model is better. Right? And so it pres uh creates this whole gating modeling as the meta model to talk about which of those models should have more weight in different regions of your predictors. Okay. Okay.
(1:00:44) And then all neural nets on top of that is it says rather than doing this sort of as two stages, let's just do it at one time. The meta model is part of the model. Okay. All right. This is everything from before the midterm sort of that we could apply of course with decision trees and stuff. Take all of this and just copy paste under categorical.
(1:01:11) It's all the exact same except replace linear regression with logistic regression and everything else is the same except the algorithms are quite different for ministry of experts that you like change the weight of each model based on their region of predictor space. Isn't that basically like three then if we just have like a different weight for each model in each region? Well, be careful.
(1:01:38) So, isn't that isn't this mixture of experts sort of like a tree? Now, be careful that the mixture of experts, one of the inputs to it is going to be the actual set of X's. When we're talking about blending and stacking, if your meta model was a decision tree, but all you're bringing in is this Y variable, essentially the Y hats from those other base models, it doesn't allow for that necessarily interpretation.
(1:02:03) Okay. If you also bring in the X's in a clever way, then you can get a blending or stacking model to mimic what the mixture of experts model does using a decision tree or something like that. Yep. Yeah. The inputs to the mixture of X's models is both the X's and all the Y predictors. The Y hats for sure. The Y hats from the base models for sure.
(1:02:31) You're using the predictions of your base models as the predictors in the meta model. But you can if you want bring along with that the axis, the actual predictors themselves. You um see like which region or which location in the predictors that a certain model is better if you're not bringing the axis, right? So for mix of experts, you would have to also bring in the axis.
(1:02:58) So this definitely includes the x's. Yeah. Okay. Logistic regression. How is that different from linear regression? The outcome is categorical. So what do we do with the model differently? These are really big blackboards. We say y = beta 0 plus beta 1 x1 plus beta 2 x2 plus dot dot dot plus some residual.
(1:04:03) And we say those residuals are normally distributed from a probabilistic perspective. And then our loss is mean square error. What is mean square error? You should know this formula. What is mean square error? Should be ingrained in your back of your brain, in the back of your eyeballs. It's a mean. So what does that mean? One over n. Great. It's an average. So it's a mean of a sum.
(1:04:40) An average of a sum, right? And so we're averaging Summing over n things and then dividing by n. A squared something is its mean means 1 / n * a sum squared squared error. What's my error? y ius yhat. There you go. There's your mean square error. There's your loss. Okay. logistic regression. What's your logistic regression saying? I could have combined this two steps of the equation into saying y conditional on my x's is distributed normal x beta sigma squared.
(1:05:45) And so I'm saying y is conditionally distributed on x with a normal mean based on those x's in a linear fashion and a sigma squared. Okay, that expression, what does that mean for logistic regression? Y conditional on my x's follows what distribution? We'll use the sigmoid function eventually.
(1:06:16) But the distribution of y, what variable is y? What measurements can it get? In the simplest case, just zeros and ones, right? Just zeros and ones. And so we find them to be berni or binomial with an n of one distribution right y's normal conditional on x y's berni conditional on x okay and what's the parameter in a berni the probability the p we don't say the probability is linear related we say it's the log odds is linear related which in the end becomes 1 over 1 + e to the negative x beta and when I write it as x beta I'm including at the beginning a one and a beta 0 being multiplied together okay so
(1:07:07) I'm always including the intercept when I write it just simply as x beta okay with me kind of great that's that sigmoidal function the beta's over Here their interpretation a one unitit change in x is that much of a predicted change in y holding the other predictors constant.
(1:07:30) What's the interpretation of a beta here? It's that log odds. So what do we do? We exponentiate the estimated beta to say the change in the odds is multiplicatively or the estimated odds ratio is e to the beta holding the other predictors in the model constant. Okay, don't forget that interpretation as well.
(1:07:55) All right, and so this is kind of the same thing as the analog to these first two. And what's the loss function? What's that? BCE. Is that what you said? BCE. What does that stand for? Binary cross entropy. Yep. What's binary cross entropy? Where does it come from? It's the same thing as over here.
(1:08:33) Where did MSE come from from a pro probabilistic perspective? From the negative log likelihood. And so this was the negative log likelihood. This is the negative log likelihood coming from the Berni distribution. Okay. And what's the negative log likelihood here? I don't know. It's a sum of i= 1 to n. If you want to average them, that's fine. You don't need to look at something like y i log p i + 1 - y i log 1 - pi.
(1:09:24) What's pi? Where's the betas? This is what we're minimizing. Minimizing negative minimizing this expression to solve for our betas. Where are betas in this in the P's? Right? Where how are the betas showing up in the P's through this expression? Right? you to substitute this expression for pi in these two places and then you would just use calculus to solve. But unfortunately there's no closed form solution.
(1:10:16) And since there's no closed form solution, what do we have to do? Use some sort of iterative algorithm, numeric algorithm like gradient descent. You got it? Great questions on that. Okay, so that's porting linear regression into logistic regression. How do we port regularization into that? the same way.
(1:10:49) Meaning on top of this loss function, just add in a penalty term lambda times the sum of the absolute values if it's lasso of your betas or squared betas if it's ridge. Right? No different. No different. Not a loss function. Penalize the magnitude of the betas. Okay. All right. How do we port KN&N into the classification paradigm? Remember K&N, you got to figure out who your neighbors are based on the predictors. That hasn't changed.
(1:11:20) Okay. And then how do you do predictions? You take the majority vote if all you care about is pure classifications. I'm a probabilist, so I care about predicting probabilities, not just zeros and ones. So what do I do? I look at the proportion in my neighborhood. You know if my it's KN&N of five and three say yes and two say no my predicted probability is 6.
(1:11:47) Yes, you can change that to a one if you do pure classification. Okay, great. So okay an end's easy. Not much changes. What about a decision tree? What changes? The whole junk about finding a threshold's the same. Okay, but what's the loss function? Pick your favorite loss function.
(1:12:20) I think there are three different choices that Pablo presented. Do you remember what those three choices were? Genie, entropy, and mclassification rate. Okay, pick your favorite. That could be another hyperparameter you tune on. Okay, that will help determine where to draw the threshold, how to split your regions.
(1:12:44) But then how do you do the predictions? The same thing you do in KN&N. Just go to the leaf in which you're predicting at wherever leaf you go to, you can use that to do a predicted probability based on proportions or a predicted classification based on plurality wins. All right, great. Random forests, same idea. Boosting, same idea. Ensembling, same idea.
(1:13:15) Nothing else changes really at that at that level other than what we already talked about changing. Okay. Confusion matrices. What are confusion matrices? They're confusing. It's those like 2 by two tables. I forget which is the rows and which is the columns. One is the like observed Y and the other is the predicted Y. And then you have the zeros and the ones in that 2 by 2 table if it's just zeros and ones. And you have false positive.
(1:13:55) What's a false positive? You predict it to be a success, a one, but it's actually a zero. And we want to control that rate of false positives and control that rate of false negatives. And we can play around with those false positives and false negative rates by playing around with we call it again a threshold but a different meaning of threshold. 0.
(1:14:20) 5 is the typical saying if you're above 0.5 let's predict you to be a one. If you're below 0.5 let's predict you to be a zero. But we could rebuild the confusion matrix from any old model choosing a threshold of 6 or 7. What happens if you choose a threshold of 6 instead of 0.
(1:14:44) 5? What's going to happen to your false positives? It's going to be harder to predict a positive. You're going to have fewer predicted positives. You're gonna have fewer false positives, but you're gonna have more false negatives. If you bump that threshold above 0.5. Okay, you can choose a new threshold 7.71 72. You can plot the ratio of false positives to one minus false negatives or something like that. And that gives you the ROC curve.
(1:15:11) Last thing I didn't really touch on was missingness. That's on the exam. Three types of missingness. What's the simplest one? Completely at random. What's the middle one? At random mar. What's the hardest one? Mnar. When should you do imputation? All of them. Okay. When is imputation most important? The middle one.
(1:15:45) It's also important for the last one especially, but you're not going to fix everything. Okay, so imputation works well if you use a model. When you impute, when it's missing at random, you can recover that missingness. When it's missing not at random, hopefully you can recover some of that missingness, but not all of it.
(1:16:04) And then when it's missing completely at random, you can just impute whatever you want. Okay, imputing is going to help, but it's not going to help nearly as much. Okay, what model should you use when you do imputation? It depends on the variable that has missingness. If it's categorical, use your class of categorical models. I don't care which one. If it's numeric, use your class of numeric variables. Parametric, nonparametric, I don't care which one.
(1:16:34) Great questions. Yeah, we're kind of out of time, but I can answer questions. I got to skip that whole thing. Hierarchical models to me the idea of Beijian models and hierarchical models are definitely parametric. Okay. And how we've used them, we've used them as extensions of either linear regression or logistic regression. Okay.
(1:17:04) And to me, a hierarchical model, all it's doing is essentially putting a layered structure to the logistic regression model. What adds that layered structure is really kind of where your data is coming in. How your data are clustered is the most common application. Okay. So the example we gave was see someone wearing a Celtic sweatshirt. Uh how basketball shooters were making shots in the NBA.
(1:17:28) Basketball players are making shots in the NBA. And our data were coming in at two levels. We had the individual shot, shot shot byshot data, success, failure there, distance, things like that. But we also had data coming in at the player level. You know, we could look at characteristics of the player.
(1:17:50) And so we set up that hierarchical structure with those two different layers. Okay, it's a lot more complicated than that, but MCMC. What's MCMC? Markov chain Monte Carlo. Why do we use MCMC? In our use case, we do it to sample from a posterior distribution. I mean, you can use it to sample from any distribution.
(1:18:26) Okay, from any distribution that's useful, that's typically hard to get at analytically, meaning writing out the formula to solve from based on formulas. But I can empirically sample from it via this algorithm, this iterative algorithm, MCMC. Okay, where do we use it? Hierarchical models, logistic regression models, the Beijian perspective of it. As long as there's no closed form solution to that posterior distribution.
(1:18:56) Good. Kind of great. Other questions? We still got two minutes. Although the cameraman probably wants to go. Yes, they're both connected to the negative log likelihood. Yes. So that means that MSse is literally just the negative likelihood. Yep.
(1:19:23) MSE is just literally the negative log likelihood of the normal if the only unknown. If you know like there's this unknown sigma squar that comes along with it too. That's ignore that fact. Literally just a negative literally the negative long likelihood of that Bernoli distribution. Yep. Yeah. Um going back to a mixture of experts said so the they ways each model kind of based on like the region sets is best.
(1:19:51) Yes. Um like how do you quantify that? How do you tell like what region a model is good in? How does that actually fit? Yeah, good question. So, mixture of experts, what it's doing is it has this gating network. This isn't even the full uh slide of how it works, but essentially you have all these different give me an X or a set of X's.
(1:20:25) My model from all those different experts, if you will, those different base models will give us a prediction. And then we turn on some of these models based on what the gating network or the gating meta model will tell us to turn on those predictions for various different values that go into that g of x function as an idea.
(1:20:57) So that gating network is basically turning things on potentially zeros and ones. Should it be included depending on the values of the x's that went into the gating network for that model or should it be turned off is the simplest way to think about it. It doesn't have to be on and off. That's what does it have and it takes whatever the best then it estimates based on improving that prediction improving that loss whether or not what that estimated G should be for that model for that set of X's.
(1:21:29) It gets really complicated really quickly. Okay. Quick question about cannon. Y can just separate the uh like the data set into two. Yeah. So KN and if you when I introduced it if you wanted to only match within a category I said you could instead of turning that predictor from zeros to ones to like zeros and a billion to force them to be separated.
(1:22:23) Yes, you could fit a KN&N over here with just the zeros and a KN&N over here with just the ones and that's mathematically going to be equivalent. Okay, the thing is it's just like are you fitting two models or you fitting one model? What's the what's the difference with one model? It's one line of code. Two models, it's two lines of code. But mathematically, it's equivalent. Mathematically, it's equivalent.
(1:22:45) Which of these get affected in their predictions based on whether or not you standardize the predictors? K&N matters greatly whether you standardize the predictors. Linear regression, general linear regression does not matter in how it does the predictions. regularized uh linear or logistic regression does matter whether you standardize the predictors and how they perform in prediction. Which one's better? I don't know.
(1:23:14) But it does lead to different predictions. Okay. Uh decision trees. Does it matter if you standardize the predictors? Not at all. And that's kind of the base of all of these H interpreting the betas does matter if you standardize the predictors but the y hats will always be the same whether you standardize the predictors or not other questions when you standardize the predictors in linear regression uh Let's see simple linear regression.
(1:24:03) Is it does the the um the slope is that just the correlation coefficient? Oh, when you standardize the predictors, yes, is that just the cor correlation coefficient? If the y is also standardized, then the slope will be the correlation coefficient. Yes. Generally, do you standardize both the x's and y's? No. Generally, you only standardize x in most applications.
(1:24:27) Um rarely do you standardize why though there are some reasons why you might last question logistic regression the logistic regression yes uh we use MCMC is it because of the like the like follows the logistic distribution and also because of the prior follows normal distribution ution. So it's like there's no conjugacy.
(1:24:58) So why do we use MCMC in Beijian logistic regression? That's where we introduced MCMC. Why do we have to use it there? Because I don't know the name of the distribution in the posterior. Okay, there's we aren't relying on any conjugacy there. We put the normal distribution prior on the betas and the posterior of the betas. I don't know what distribution that is.
(1:25:26) and I can write down its height, but I don't recognize it as a normal. I don't recognize it as something I know how to sample directly from. And so I have to use some other sampling technique, empirically sampling technique like MCMC to get at it to sample from it. Linear regression, I can write it down. There's a conjugacy there um that I can rely on.
(1:25:52) For this time there are only two options for basian logistical regression are you conjug either we rely on conjugacy or we do MCMC of some sort or some other sampling technique of some sort. In the logistic regression framework there is no conjugacy. So we have to rely on MCMC.
(1:26:17) In linear regression there is conjugacy that we can rely on. All right, we're out of time. Thanks everyone. Sorry I couldn't answer questions for those online. Yay. Thanks for great class. That's all folks. Yay. Should be on ad even though we didn't even really touch them.
(1:26:41) It's just me pulling off things that I think are important to think about before the exam. Great.