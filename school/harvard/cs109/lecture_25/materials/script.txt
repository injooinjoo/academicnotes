(2) 109 day 25 - YouTube
https://www.youtube.com/watch?v=Kya5kTQ4pz4

Transcript:
(00:00) Welcome, welcome. >> You're not wearing mine. >> Sorry. Don't see me. You're wearing two room mics. >> Thank you. Okay. Test Richard. Test test test. Okay. Again, welcome everyone. Uh students here, students online, students everywhere. This is our last lecture. Happy to see you here. Um we're going to finish with a strong lecture.
(00:51) Very good lecture. Good show today. uh a little bit hard at then but uh very relevant for what is happening today with LLMs. All right, settle down. Uh my name is Pablo Protovas and I'm going to be your last lecture for this class. So let's get started. We start again with Switzerland. We are into the Swiss modes this this semester. Okay.
(01:17) So let's just get started. So I have of course four things to do. I'm going to talk about the ensemble methods in general blending stacking a mixture of experts. So let's put the here the perspective what we have been doing is combining models first we combine them with bagging and random forest with by the way we uh well I'll wait for the announcements when everybody comes in.
(01:44) So for blending for bagging and random forest what we did we take a lot of trees and we aggregate them. We're trying to reduce variance by aggregate by adding things up. When we went to boosting slightly different instead of aggregating we went sequentially. One tree was learning from the mistakes of the other one. And we did two cases gradient boosting for regression and adabush for classification.
(02:10) Difference between bag and random forest is very similar. The only difference is that random forest before you split you take a randomly a subset and you choose from those. Okay. Uh today we're going to move into slightly different thing yet we're going to be combining trees. Okay.
(02:31) It's again kind of foresty thing though these methods we're going to be talking today can apply to any kind of models. How we combine models. I think it's extremely relevant for projects. Uh I personally do like these methods a lot because while you're doing oh I run logistic regression I run I don't know SBM I run KN&N I run trees each one of them is doing huh >> run base >> basian sorry I apologize you run base you run hierarchical base double hierarchical base until your brain explodes uh and then you want to combine all these things. Uh this is what you're
(03:14) going to be learning today. Very relevant. I promise you if you pay attention, you learn this, you're going to be using it more often than you think. All right. So let's start with the unseample methods. This is a kind of review of what we have done with bugging and random forest. I put them together in some ways.
(03:34) So we start with some training set. We bootstrap multiple samples. We train model one to model N. We make a prediction. We aggregate the output and here I have my test or validation set. By now you're mature enough to know what is validation and when we use testing. Sometimes we kind of get lazy in our slides or in presentations and in blogs in papers.
(03:58) But by now I hope you understand validation set. We use it to choose model hyperparameters test is to report your final answer. Okay. Uh so for bagging and bootstrapping uh for bagging and random forest uh each sample of the data combined so the base models are the key word here is homogeneous and in bagging we have homogeneous strong models.
(04:28) What do I mean by homogeneous? They're the same type of models, trees, decision trees, right? Uh and they all have the same hybrid parameters. So when we did bagging or random forest, the depth of each tree will be the same. Okay. Yep. The metric we use for um setting up the the purity was the same. We don't just do some trees in genie and some trips in entropy.
(05:03) Right? So in this scenario, the trees are homogeneous. The same trees we do it multiple times and then we aggregate. Maybe it's a question. Why do we want them homogeneous? Why do we use homogeneous when we did back? Because when we want to take the power of aaging out, it takes out the complexity by having the all the same, right? So we don't have some more strong than others because then we have to weight them differently. Okay.
(05:32) Um these trees are in generally strong strong trees for bagging a random forest. We don't do weak learners. We do strong learners. Usually we're trying to get learners that overfeit a little bit and then we remove the variance. We remove the overfitting by aggregation by averaging. Okay. Um so these train models can be trained in parallel.
(05:57) There is no dependence on one to the other. So if you have uh 100 trees or 100 estimators what you can do you can do them on a 100 different threads on your CPU or 100 machines or whatever and then you aggregate them right and we talk about majority or mean as the final answer. Okay. All right. So in boosting we did it slightly different.
(06:22) Uh this is the training set. I go to the model. I make a predictions. I find the residual mistake or residual or mistake. In the case of Adabus, we're looking which points are misclassified and then we wait them higher. One thing that may have missed last week and I use this first 10 minutes a little bit for review while everybody's coming in is that when we reweigh in Adaboost it just one thing I noticed yesterday at the review and was telling everyone I want to tell the whole class is that if you think about it we're not rewe
(06:58) from scratch we notch them if we mclassify a point we notch it up if we classify correctly, we notch it down. So because we have the new weight to be the old weight times something. So if you think about if you have a point that you constantly predicting correctly, the weight slowly slowly will go down.
(07:22) Now if there is a new stamp that makes a mistake, it's not just going to jump up. It's going to go slowly up. It's kind of an exponential average or smoothing, right? All right. So that's that part. So we get the mistakes, we train the next model on the mistakes and we keep going and then we aggregate the output by putting some weight in front of the some learning rate or what we have seen last um Monday for Adabus we can find the exact waiting thing the lambda that can give us the optimal train.
(07:57) All right, that's about boosting. Um, in boosting the trees are still the models are still homogeneous. Every tree will be a weak learner, but it will be the same. They are the same type of medals define the same hybrid parameters. In boosting, we don't mix uh different trees, same trees, we use them together for everything. Okay, every tree here because we use weak learners, we have high bias means they're making all mistake and we try to predict better by combining them, right? Uh the prediction from the base models are through some weighted averaging.
(08:42) weighted here is the lambda I was talking about the learning rate and basically is for classification again we do weighted averaging and for regression weighted sum right okay now one thing to notice in um in boosting this is sequential is not parallel so it tends to be a little bit slower in general because you have to go one after the other because you cannot train the second stamp until you have the result from the first one etc etc All right.
(09:15) So far aggregation and prediction what we have is that we have the input data model one model of course in between we have some bunch of uh bootstrap we make the predictions each one of them get a class and what we do we just sum them up. That's the aggregation without weights and that's kind of bugging and random force. Okay we can also do something slightly different.
(09:41) We can just weight them differently. Each one will be weighted differently and it's kind of adabus but not exactly. Um I have an adabus I thought I put a note there. I apologize. Uh there is a slide I need to unhide it. is not exactly adabus but adabus is a special case of weighted because we have weight but it's not exactly as I do it here I'll unhide it there's a little note that adabus is not exactly a weighted averaging there is a little bit different to that uh in regression again we take the average which is bagging a random forest and this one is a way to
(10:23) do weighting average and I don't think I have yes I don't have gradient boosting because it doesn't fit exactly there Now the question we have and we're starting new now is can we aggregate outputs in a different way and the question is yes and can we use different models as base learns so I'm going now away from the idea of every base model to be homogeneous every the same to something different now I'm going to have every model to be slightly different that's why I say this is great for combining logistic regression, basian inference,
(11:04) uh basian logistic regression, you can combine trees, you can combine neuron networks, anything you can combine the results from each model. Okay. And this is what we're getting into now. So can we use different models? Yes. And okay, so the first method we're going to be talking is blending.
(11:24) So I'm going to talk blending and staging. They're very similar. And then we're going to go to mixture of experts. Okay. So, blending and and and and staging conceptually they're very easy, but when you do it, you have to be careful. So, what I did here, I went and spent a lot of time making very detailed slides. So, at least once you see the whole process how you deal with validation sets and hold out sets and test sets because that's where the complication of blending and staging comes in.
(11:58) All right, ready to rock and roll. All right, so I think most of the people are here. Uh just few announcements. Uh Chris make a post last night or yesterday afternoon summarizing everything that you need to know until the end of the semester because some of you seems to forget that you have a quiz.
(12:20) Some of you forget there is a final. Some of you forgot this milestone and that. And instead of looking around in Canvas to find the calendar and ask questions, uh, Chris kindly just put one post with everything you need to know between yesterday and the end of the semester. Please read it. Um, there is a review session on Thursday.
(12:42) Thursday, Chris, >> there is one tonight at Zoom. Chris, there's one Thursday at SEC. increase. There is one review session on Friday at the science center. Uh huh. >> Coll. And that will be recorded and the and the zoom will be recorded for people they want to see. >> In your best interest to go if you can because then you can ask questions.
(13:06) So >> it is of course always in your best interest to show up right like coming here and see. Uh so these are the reviews. They're going to be office hours next week. regular office hours for our amazing TFS. So, please read that thing to keep track of what's homework three grades were not released. The reason is because grade scope ate all your submissions and u thanks to Greece. Please say thank you.
(13:34) He managed to hack the system and get them all out. I don't know. You have them now, right? Yeah. Okay. So, good. So it it was a lot of work by Chris and Rashi I guess trying to get this going. Thank you Chris. Uh was a panic mode because gray scope ate aid your submission at your grace.
(13:54) I we couldn't figure out what was going but it's taking care. Uh besides that any announcement okay let's go. Um so blending. So what we're going to do, we're going to be train heterogeneous models, different models. So the way it's going to go is the following. So we start with the input data. Okay, here's my input data. And I'm going to train n models.
(14:20) They don't have to be the same. One can be logistic regression, the other could be brand of forest, the other could be boosting. So I have now n predictions. So the idea is I'm going to take these predictions and this is the key here. Let me show you. This is the key idea. The new idea I have something called the meta model and the meta model is going to try to learn from the predictions of the individual other models and then that will sorry and that will of course give me the final prediction uh here. Okay. Yeah. Okay. So, train n
(15:03) models from same data. Different models don't have to be the same. Get the outputs for those models. Pass it to a meta model. And that could be anything. Usually I like it and I'm sure Kevin would like it too is some simple the meta model. I like it to be simple because then I can interpret which model is doing what. Okay.
(15:27) And then finally the final prediction. Okay. Now let's go more step by step. Step one, we start with the input data and we training set. We create a training set. Uh we train a validation set. And this validation set I'm going to be using to fine-tune my individual models. Okay. All right. Then I'm going to create a hold out set which I'm going to use to validate the meta model.
(16:02) Right? So remember I have to use something to validate the individual first layer models and I'm going to need now another data set to validate and fine-tune the meta model. Okay. And then of course the test set. Right? The test set is at the very end of the whole system. Ha. Too many. Yes. Okay. All right. So, let's go. Step two.
(16:29) Step two, we feed the base model on the training set. Right. So, now I'm training the the the the the base models on the training set. So, I have now feeded base model one, feed base model two all the way to n. Okay. And then I have predictions for each one of these base model. I'm going to a little bit slowly just so you don't miss it because we're not going to review this.
(16:58) So this is your chance, right? All right. Let's see what we have done. I take my train model and I predict and then I now I take the validation set and I make predictions on the validation. Okay. Okay. Now I'm going to take the model predictions from the validation set and I'm going to concatenate them. I'm going to put them together. Here they are. And here's the key.
(17:29) I'm also going to take the original X and Y, the original data, and I'm going to put them all together. And this is going to be my new training set for the meta model. Did you get that? By the way, if you I'm not refresh your head because I make some changes this morning in case you got it from before. All right. So, let's see what we did.
(17:58) We took the training data. We train the model and then we take the validation. We make predictions. We took the predictions from the base models. We added the original data X and Y and now I have a new model, a new data set and this is going to be the training set for the meta model. All right, step six. Uh I am also going to take the predictions from the hold out set.
(18:27) Okay, so remember I have three fourth data sets here. I have the training set, the validation and the last third one I'm calling the hold out set and I have a test. Now I'm taking the prediction from the hold out set and I make predictions from the fitted model. So now I'm also going to combine those and I'm going to call it as the validation set and I put X and a Y sorry.
(18:53) And then that's going to become my validation set for the meta model. Okay, very nice slides, right? Very clear. Hope you follow this. Okay, so what we did, we create the train with the training that we use the validation predict from that that will be the training of the meta model and I get the hold outset predict on those that becomes the validation of the mo of the meta model.
(19:25) Okay, so now I'm fine-tuning that. So I have the fitted model and now I'm getting now I'm going to generate predictions from the test set which I'm going to use with the X from there should be the X here. I'm going to take the X from the test set and I'm going to use that as a way to do my combined test set for the whole model for the fitted model the fitted meta model and that will give us the final prediction for testing.
(19:55) Okay, so we have how many steps? A lot of steps. Now this is for blending. Compared to random for blending provides flexibility allow us to combine models of different types. Meta model learns how to best combine diverse model outputs. Okay. And blending models can be interpretable if the meta model as I said before if the meta model is simple like linear regression because then I can see from the coefficients of the meta model will tell me which base model is important and of course I'm using each which of the features are
(20:36) important. Okay, so that's my story by dividing the data sets into the problem with this. One thing you have to consider is the fact that now I'm taking my data set, my training set. I'm going to split it into training, validation, hold out, and test. Right? So we have one extra set that I'm not using actually for prediction on the base model.
(21:02) So if you are short on data, there's going to be a little bit of problem, right? If you're not, go for it, right? And usually we're not, right? All right. Okay, that's just a rhetorical question. Okay, so you can do some augumentation if you run out of Okay, now we're going to go into stacking. And as I said, um, stacking and blending are very similar, except stacking has one thing that Pablo likes a lot, which is it doesn't like validation sets.
(21:38) What do I do like cross validation, right? So staging is blending but with cross validation. Okay? So again I'm going to go slowly and if it gets boring it's okay at least you have a chance to think as I go. So similar to blending staging st similar to blending staging will be trained on heterogeneous models right they don't have to be all the same.
(22:07) It's going to be very similar idea as before. The only difference we're going to do now is going to do cross validation. And it's easy to say let's just do cross validation on uh meta models and everything but I promise you it gets a little bit tricky. So I wanted to at least explain step by step how we do that.
(22:29) And since I painfully did this slides I you have to suffer with me because it's just step by step making all these animations is not easy. All right this I actually stack generalization work by deducing the bias of general blah blah blah. I was reading that last night. I say I don't know what he's talking about. So too many words. So let's just go and do it step by step.
(22:49) So the idea here is step one, we going to first split the data into training set, valid, hold out set and test set. Now notice one thing I don't have here. What I I don't have is a validation set. Why? Because I'm going to do cross validation. Okay. So now I'm going to use that training set on the top here and I'm going to do my faults on this and that we're going to be used for a validation. Okay. Um All right.
(23:20) So we're going to do cross validation. Yes. Okay. All right. This is the last lecture guys. So we need to have a little bit of fun. So step one and we need to split the training data set. And for simplicity or just so I have some concrete I split it into three folds. One, two, three right here. So my training set now becomes three folds.
(23:46) All right. So step two, uh we going to fit the base model with cross validation. Now while we do the validation, um we concatenate the output of the cross valid prediction from each base model in the new data set. Okay, let me see this exactly how it works. So we firstly take fold one on the val from our validation fold right now.
(24:14) So that means we're training on fold two and three. Maybe this is a review for cross validation too. So we take the fold one that's going to be now my validation. So I'm training on fold two and three. Yeah, that will give us a base model one fitted on fault two and three. Okay, model two was treated on again fold two and three all the way to end. Okay, that's for one fold.
(24:45) So we're going to make predictions on fold one. This is my validation prediction from fold one. Okay. Okay. Now I'm going to do uh so this is my predictions from base model one to n from fold one. This is the validation predictions that I'm doing because I'm going to use those later for my meta mode.
(25:12) Now I'm going to take fold two for validation and therefore I'm going to be training on fold one and fold three. Right? So here I'm taking that. So I'm trained on fold one and fold three and I'm validating on this is my train and now I'm validating on fold two on every model I train on fold one and fold three okay that creates a second data set that it comes from the second model we was trained on fold one and fold three all right and then I'm going to do it of course on fold three uh now I'm training on fault one and fault two and I'm validating on
(25:53) fault three and I'm creating the predictions which I'm going to use to create my third data set there. Okay, so far so good. Okay, so now the next one I'm going to put of course the x's and the y's from each fold. I'm attaching them to each one of them. And now of course I'm going to have my new training set for meta model.
(26:18) Same way as before, but now I'm doing fault by fault. And now I have my training set for the meta model. Cool. Next, I am going to make a prediction on the hold out set. But you remember now I have three models. Model one model was trained on fault one and fault two. The second model was throwing on fault one and fault three.
(26:43) And the third model is trained on fault two and three. Right? So if you put it all together now I have the predictions from model one and for each model I have three predictions for each fold right three data sets and then I'm going to aggregate all these things and that becomes the hold outset prediction from base one for two and three and then I'm going to com combine all these together now to make my um come on give it all to me Right.
(27:15) So now I have for each model I have three predictions and all of them combined. Now I have the whole thing for every meta for every base model and that will come model one prediction, model two prediction etc. Now I'm going to combine all these things and that will be my validation for base one and we do the hold out and that will become the validation set for the meta model.
(27:42) So the validation set for meta model comes from the hold out set on the base model and again because I have faults now I have it multiple times so I need to combine them nicely. All right so now we have training set of meta model we train a meta model and we have the final prediction. Okay, now couple of points before I hope this if you haven't got it all now you have the slides you can go step by step but the basic idea now and let me put it in a high level is that we train the base models with the training we validate the base models we take the results of the
(28:26) validation of the base models that will become the training for the meta model why is that because I don't want multiple train on the same thing I want to have independent and then what will happen on so that would be the training and what we have on the whole that the third data set the predictions from that from the base models becomes the validation set for the meta model okay so it's a three-stage thing here which you have to keep in that's what I said is slightly complicated when you do this blending and staging is to think
(29:01) carefully of your what is your training what is your validation for the base model and what is your training and what is your validation for the meta model if you don't do this properly it's going to overfeit guarantee okay all right now a couple of things I wanted to talk about is about sklearn uh if I remember correctly no skarn yes uh so remember one thing I was doing here is I was taking the X and the Y of the training set or the validation set or on the hold out set and I was concatenating with the predictions the
(29:42) wise I'm getting from the models that's not the default thing that's the way I want it that's how I teach it but sklearn is called pass through right know what is called yeah um there is something called the pass through which I think by default is false so if If you do the sklearn and you use one of the stacking and blending models, it will not pass the X either from the training or the validation or the hold out set.
(30:11) It will not pass it to become the training of the meta model. By default, you would not but I highly recommend to pass through to set it to true. Then what you're learning is not only from the predictions but from the original data too. So the meta model is kind of learning which model shall I use conditionally on what's your input which I think is very important and it give much better results.
(30:41) I do not understand why the the default is false but that's the way it is. Okay. All right. Um now how do we combine the output of based models? models output can be different depending on the task. For regression that would be Y and for classification you remember the classification random forest for example will give us a label logistic regression will give us a probability right even if you do multiclass so I have this little thing I said I'm going to take the output from the base model and I'm going to use it as an input if you use let's
(31:19) say random forest or any decision tree the output the default output will be a label Okay, you can treat this as a categorical but that's not the most effective way. So what we like to do is to turn uh the the output of random forest or bagging into a probability and I'll keep doing that so I don't have Kevin raising his hand.
(31:45) Um because what we're going to take is we take for example in the random forest random forest you remember every tree makes a prediction. We're going to turn that into a probability or a score. Let's call it a score just to be on the safe side. So we're going to turn it to a score and that score will be basically the ratio of the majority class. Okay.
(32:05) And that means it's a continuous variable. Then I can nicely use it in my meta model even if it's logistic regression or linear regression whatever I'm going to do depending on the task. All right let's take a breather here. See how we doing. or the whole idea of a lecture is to get the gist of it, not the details.
(32:34) You can go through the details again by yourself. Okay. All right. So, uh I think that's for blending and staging. This is my summary. I like to give you summaries. All right. Now, is it possible to cover different input regions with different learners? Okay, that's the last topic. And I think you want to go and leave but I'm going to hold you for another 20 minutes here before we're done. Okay.
(33:02) So the last thing is became very popular with deepseek and some mistral and this language models is called mixture of experts. Last year when I was teaching that some students came to me said why do we have this last lecture? Why do you do mixture of experts when and then three months later boom the big news came.
(33:23) LLMs are outperforming because they use mixture of experts. So I'm proud to say hey it was in my lectures. Okay. So let's talk about mixture of experts because I think it's getting a lot of popularity and I want to address that a little bit. Right. Okay. What is mixture of expert as opposed to that now the intuition between mixture of expert is a little bit what I said earlier.
(33:43) That's why I like to pass through. I want to learn which expert is the one to use not just depending on their output but depending condition on the very much input. Right? So idea is that some expert will be good in some part of the input domain some of them will be better in some other part of the input domain. Okay.
(34:12) All right. I'm going to start writing the secret code. I should use red. >> Huh? >> Maple. Maple tree is the secret code for today. Can you see it? Can you hear me? Maple tree. The one that makes sugar. Maple tree. All right, let's go on. So consider the case of a phys physical examination and the goal is to have the diagnosis if the patients have any
(35:17) severe disease. Now the patient will go into consultation room and now you have experts. The experts will be the radiologist, maybe the pathologist etc. Right? So so we get the report and then at the end I want to summarize the professional opinions. But depending on the input I may be paying attention more to some of the doctors.
(35:42) For example, if I have an MRI, maybe the radiologist is where I want to put more emphasis than my general practitioner, right? Because that's where the data are, right? So the other example I said is from the business world. Now you have an accountant, you have business development, data analyst, consultant, lawyer, financial expert.
(36:03) Now I want to have depending on what is my question depending what is my input I want to wait these experts different maybe each one of these expert is good for their own thing but maybe not for depending on what I'm asking if I'm going to be asking legal advice maybe I should be listening to the lawyer not to the data analyst with that if I want some question about my data some modeling or some simple thing they relate to the data.
(36:37) I should not be asking uh the financial expert or the consultant for that matter. You should never ask the consultant but I want to go with the data analyst. Right? All right. Now let's simplify this a little bit better. I have this data set and I have two experts, right? So one expert is a linear regression. Let's say now two of the experts are linear regression.
(36:58) So each one of these experts is a linear regressor. Okay. Now fine I can put my two experts and I can just say this expert will specialize in this area and this expert will generalize in this area. Now these are two homogeneous models two linear regressors. Right? But what if I have this? Now I have one expert which is a polomial regressor and the other one is a linear regressor.
(37:23) Right? Now what I can do is sync the data. I'm going to put emphasis on this guy in X smaller than one and I'm going to put emphasis on this guy when X is larger than one. Okay, so that's the beauty of this thing. Sometimes we may need the third expert. Now I have another expert who just averages thing, right? It is not even a a regressor. It's just averaging.
(37:52) the first model we introduced in this class if you remember the averaging. So that's my third expert and now I have three expert each one of them is specialize a particular part of the thing. All right. Now usually we only know we know only the x and the y but when we going to do inference we don't know the y.
(38:20) So they only want to know the X, right? So we cannot decide based on the Y. We can learn which one to do depending on the X and the Y, but at the end of the day, deciding which expert I'm going to use will only depend on the X, of course, right? If I knew the Y, that would be problem solved, right? All right.
(38:45) So we begin the framework similarly as before. So now I have the input data. I have K experts and here's my K experts produce a Y, right? Each one of them produce I do it for regression for for now. I think it's a regression. Yes, just so you see the example. So each expert produces some output. Okay. Now what we did before we have a meta model, but now I'm going to do slightly different.
(39:12) So I have my meta expert there and the final predictions. But now I'm doing the new trick ready suspense. Okay. So what I'm going to do I'm going to create some gating network or gating model which I call G which is going to inform the weights there. Okay. So now I have this gate one, gate 2, gate three, gate four, gate K and that will be used as an input with conjunction with my prediction for the meta model.
(39:51) Okay. So now my final prediction is basically the y times the gate. Okay. And in this case I'm average then it should be one over z there. Okay. Yeah. All right. All right. So, fitting the models involve learning the parameters for each expert. Fine. And now learning and also learning the parameters of the gating network.
(40:22) So I'm going to be learning both of them. I'm going to be learning the individualbased models but at the same time I'm going to be learning the gate how much to pay attention to which expert. Now I make it a little bit simple because otherwise it gets a little bit complicated. So what I do now I said my base models are linear regressors.
(40:46) So I have theta instead of beta. There was too many last night to change them to betas. But thetas are the coefficients of my linear regression is the same as betas. Okay. All right. So now so what I get my output for each of these models is just theta* x. And there's a reason I'm doing the linear regression just to to understand how the derivatives go and everything.
(41:10) But let's go slowly here. So what we have is my my gating network. My gates I want them their gates. They're either open or they're closed. That's why they call gates. So that means it should go from zero which it means closed and gate one it means the gate is fully open. Right? Um anybody here has done neural networks? Raise your hand.
(41:38) This what how many of you have played with neural networks? Come on. You're lying to me. You just don't want to answer. Right. All right. So this anybody who has done neural networks this will look familiar. If not you don't need to know but we're going to cover that in case you dare to take the 109B in great details.
(41:57) This is all we're going to be doing gates and sigmoids. Okay. All right. The other thing is that just to keep things um normalized I'm going to require my gates to be equal to one. The sum of the gates to be equal to one. Right. So this is nicely organized. So this is a model and this model will produce scores. The score will be for gate one score for gate two g score for three three that's not a problem right we can do it anything we want a linear regression whatever you want you make a model the model will say this is given the input
(42:35) this is how much score I give to model one how much I give to model two how much I give to model three etc. Okay. But now I require also to have the sum of the gates to be equal to one. And I require also that the score that score or that should be between zero and one. Have we seen anything like that before? Kevin has. I have.
(43:11) has seen I give you a hint actually I give you the answer when we did logistic regression we have binary classification then we switch it to multiclass classification and we get the one versus rest you remember the one versus the rest for each one of them we're getting a score and then we said hey those scores should be probabilities they should add to one and between 0ero and one.
(43:44) Do you see the similarity? We want the gates to be between 0 and one and add to one. In multiclass logistic regression, we wanted the same thing. And what did we do in the OVR to make sure that they add to one? That's going to be a quiz question, midterm final question. Anyone back there? >> Soft max. Thank you. So, we're going to use soft max.
(44:15) So, soft max means I'm going to take whatever comes out of the gate there. Come on, soft max, come to me. Yeah, soft max. So, I'm going to take whatever comes out of the gate and I'm going to soft max. It's actually I'm going to soft max from the beginning. So, now they add to one, right? Okay. So, this is my thing. E to the EA X divide by the sum.
(44:43) Now what is EA? Theta is the parameters of the gate model. Right? So we have theta which is the parameters of the base model. EA is going to be the parameters of the gate model. Let me explain. Every model comes besides decision trees come with a set of parameters. What are the parameters for linear regression? The beta coefficient.
(45:07) What are the parameters for logistic regression the better? What are the parameters for my gate? The ether. Okay, so think about this very similarly to multinnomial logistic regression. Okay, you just give a bunch of probabilities out of this model. Okay, now here we are. So the the g output is e to the ea x. Now x is the input.
(45:35) If you remember what I'm trying to learn here is to find which model to be active to be open depending on the input. So that's what I'm learning. I'm learning how to turn this on and off depending on the input. And how do I learn that with the parameters it given the data I'm going to have? I'm going to learn the given that if X is small that is my model.
(46:04) If X is medium that is my model. How do I do that? With the I'm going to be adjusting the to learn which model to be more active or less active depending on the input. All right. Now I have this beautiful model. Uh now so now let's write down the loss function because at the end of the day we're not doing basian. So we have to write our loss function. It's the same.
(46:30) Kevin is thinking come on this is the same likelihood negative log line anyway let's just write the loss function and the loss function could be or it makes sense is basically yn is the true value minus what the whole thing is predicting right which is g times the prediction for each of the base model okay and that square that's the me think about it I have the true value and I have the prediction from the whole model which is basically a linear com a weighted linear combination of the predictions from the individual
(47:07) models and I weight them based on this gate. Okay, good. So I'm learning now two things. I'm learning the base models and I'm learning how to combine it and that is my loss function. And that loss function could do it could it? And that's where things get a little bit complicated. And I want you to fasten your seat belt for a minute because we're going for a little bumpy, right? So hold it.
(47:36) Take a deep breath. We'll explain everything. So I put a big X there. I don't like that. And the story goes into Mistra. Mistra is a French company who introduced this mixture of experts for large language models without not big success and they kind of gave up. And the reason is because they use a loss function like this.
(47:59) And the reason this is not a good loss function. I have a sentence which says but this goes against our goal of having specialized experts. Anybody understands that statement? I guess not. You shouldn't. I said what the what on earth are you talking about? So here's my statement. The error term the residual there is y minus yfal.
(48:21) Yfal is the combined with the G's right is the same for every expert if you do the derivatives I don't want to do the derivatives in slides but if you do the derivatives what you find uh experts do not get their own error so every expert will get every error for every point and will be the same and therefore instead of receiving the same coming from the blended in instead they receive the global error they're not getting their own error.
(48:51) And this actually becomes much obvious if I write the derivatives. So what is happening is so every expert is trying to push the final prediction towards the true y and that is a problem because now every expert sees the same error and all of them trying to do the same thing and what happened? They collapse. They all become the same.
(49:18) They say either they all become the same or what happens and very easy to demonstrate if you do this what will happen one expert we somehow dominate and everything else will be zero so all G's will be zero except one or the exactly all the same there's two equilibrium here everything the same or one expert very much get the G1 and everybody else is zero remember they just have to add to one and that's called model collapse expert collapse And that's what is the problem with Mistra. They could not get away from
(49:51) that. They're trying a bunch of tricks until Dipsy came with two ideas. One is important sampling actually and ideas that Kevin has talked during the sampling. Uh but we're not going to get to that. We're going to get that at lecture 23 of 109B or something like that. All right. So now instead of using that loss I'm going to change my loss and I'm going to put the G outside.
(50:20) Now by doing that every expert will now see only some parts of the point. So the derivatives are flowing nicely. So if I'm doing well in this region the derivatives will go there and I'm going to be updating the gate and the model at the same time. Again, it becomes much easier if you see the derivatives, but I'm not actually I do have the derivative. I didn't spare you on that.
(50:44) Sorry about that. So, I have that and I'm going to focus only one point to see that. And then now the derivative, you remember we learning actually let me take a moment here. I have five more minutes to finish this. Remember the way we learning. Learning means feeding. Fitting means minimizing the loss.
(51:03) And there's kind of two ways we learn in this class how to minimize the loss. First one we did algebraic solution to gradients equal to zero that was a linear regression and the special case for uh binary predictors in think but in general in logistic in linear regression the second method we have talked about it is this thing called gradient descent and the gradient descent it tells us I'm going to go in the opposite direction of your gradient now let's write down the gradients to see how they So now I have a gradient with respect to
(51:40) theta because that's how I'm going to be learning these models. So this has g * y minus da da da. Now notice that they're both tied up. The gradient on the on the on the theta and the g are tied up. Meaning that while I'm learning the base model, I depend what the gate is doing, right? And then I can also do the the derivative with respect to ether and then that give you basically the difference between the y times gi.
(52:12) So this way I'm able to actually learn the gates and also the the model at the same time. Now this is I did it for linear regression is much easier. It doesn't have to be linear regression. The same things apply just the derivatives of with respect to theta x will have the derivative of whatever the model is the activation function. All right.
(52:38) So we have the meta model. Now you can also do this other thing and again now we prime you and preparing you I'm getting you ready for neural networks. So after this class you say come on this looks little more complicated than it should be. That's where neural networks come in is just this is the prime for neural networks because what I did I got the base models and then I have a a meta expert.
(53:06) Now what I'm going to do I have the base models and now I'm having multiple meta experts and now I have another expert which is on top of the experts. Now I'm doing hierarchical learning, not the basian hierarchal by hierarchal by staging things up. And that's what a neuron network is all about.
(53:27) Okay? And you're learning from the expert to the expert to the expert to do that. So think about that. First layer is neurons, second layer is the second layer of neurons, etc., etc. And that's it. Ha, I'm done. Okay. So this is the end of the lecture, end of the semester. I'll take one minute to say a few words. Thank you very much. Thank you for coming.
(53:49) Uh, hold on. Hold on. The bad news comes. Before you leave, you're going to have to sign up a paper that is going to be on the doors. Don't leave yet. But before you leave, make sure you see one of our five TFs here. Use one, two, three, four doors. If you leave from here, it's going to be Kevin, but use these four doors. You need to sign up.
(54:11) Your name and Harvard ID. We're going to cross match with the attendance we have today. I've been warning everybody. Anybody who has do that is going to be reported automatically. Now, let's talk about the good things. You're here. Not your fault. Unless you gave uh don't go yet.
(54:32) Make sure that we have the TFS on the place. Are we there? Uh just hold on a second. Um, first I want to thank my teaching team, my collab co-instructors and the TFS. I want to thank you for coming. One thing I want you to get into the habit and I noticed this semester. Can you hold on a second please? I'll appreciate.
(54:55) Uh, one thing I noticed this semester is missing the office hours. And I start doing office hours. More students are start coming to my office hours. and Kevin and Chris. I bet you you're learning so much with an hour and a half or half an hour with one of the TFS than struggling with LLM. Use large language models to answer the small questions.
(55:17) Do not use large language model as a way of learning. It's going to drive you into the rabbit hole because it just gives you pages and pages. Please start using the office hours for all your classes. We are here to help. That's why we're here. an hour and 15 minutes or an hour of lecture is not enough. Things go through you and you're never going to absorb it.
(55:36) There's no way to absorb. Even if I spend the most time and make very clear lectures, there is no way you're going to learn. Okay. So, the final thought is that Kevin and I, Chris and the rest of team, why we doing this teaching besides getting paid is uh we want to see we try and I mean it's honest, it's cliche, but it sounds that's what it is.
(55:58) I want to make a positive impact in your life. Definitely not negative. If nothing else, marginally positive impact in your life. And if I have exceeded that, that's for me. I don't care about key scores. I don't care about anything. I just want to make sure that I have this semester, me, Kevin, Chris, and the TFS, we have make you learn something.
(56:20) I don't care about grades so much. No, I don't care about grades. I don't care about evaluations at all. Uh what I care is that you tell me look I started this semester I wanted to learn I learn and I learned so much. I do not care also if you come very advanced and you just didn't learn anything because you knew everything.
(56:45) I want to know that even if you're advanced you learn a little bit but also if you started very low and you're struggling in this class. Hey that's why you're here. That's why to learn. If you start struggling and now you said okay now I'm getting it not everything I don't expect everybody to be super ninja of data science that's not the way it works we want to make a difference we put in the effort I hope you can see it uh the slides office hours and everything that's our part your part is to learn and I hope I only hope and I you tell me yes send me a
(57:16) message this I learned you don't have to do that but if you give me an indication that I have made some positive delta gradient in the right direction. I'm very happy. All right. Thank you very very much.