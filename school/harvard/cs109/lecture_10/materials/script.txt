109 day10 - YouTube
https://www.youtube.com/watch?v=dj6cvy4NL7Q

Transcript:
(00:04) Great. Any announcements? Any announcements? There is that. Yeah. Yeah. Yeah. Yeah. I just went well everybody. All right, let's get started. Sorry for a little bit of a delayed start here. Good morning. Good morning. Good morning. Happy Monday. How y'all doing? How's pet 2 going? Goodish. Short? Long? Short? Pablo says short. He had no problem doing it. Great.
(00:52) How was the quiz? Yeah, quiz went well. Quiz went well. Generally speaking, quiz went well. You all did a pretty good job. Um, scores will be released within a day or two. Yeah. Yeah. Yeah. Probably tomorrow at some point, if not before class on Wednesday, and we'll talk about um some of the trickier problems in class on Wednesday. So, those are the big announcements.
(01:20) Um, problem set, due tomorrow, problems at 3 release shortly around that time. And then, um, the quiz results will be coming out soon. What come How about next week? What's going on next week? What's due next week? Project stuff. So, we will be releasing project topics and you will have to be selecting a project topic from the list.
(01:46) So, essentially, it's kind of just picking your project, rescoping the pro problem. You don't have to do exactly what the project says, but using the data set or what the proposal that's listed for you is and then who is in your group. And that's kind of a a low bar, a low task, a low ask, I should say. Okay.
(02:10) What else is going on next week? No class on Monday. Yay. See, I get the big applause. I get the big applause. Yeah. Holiday. Holiday holiday on Monday. I mean, there's big pl applause over here in the peanut gallery, too. Great. And what happens the week after that? The midterm. So, I'll talk a little bit about the midterm later today, too.
(02:36) Um, the midterm will have some questions that will be a little bit more challenging, I would say, than the quiz quiz questions. The quiz was meant to just review, recap. The midterm will challenge you a little bit more. Okay, let's reframe ourselves a little bit. Today, we're getting into some new stuff in the second twothirds of it.
(03:04) If you've taken stat 110 and stat 111, it'll just be a lot of review for you. But that's okay. We're going to put it into the context of how we're going to use it in this class going forward. And that is essentially the use of bay bay rule and basian inference and sort of changing our mindset a little bit from what you may have seen from AP stat or introstat or previous stat classes. Okay.
(03:28) So we'll go through a decent amount of time just reviewing the inferential stuff from last time. I went through it unbelievably quickly and so we'll go a little bit slower through those details and then we'll connect that to these. All right, don't forget uh the example we had last time we were trying to predict the selling price of homes in the Harvard Square, Cambridge, Somerville area.
(03:56) And the simplest model we said we looked at the scatter plot. We fit price in thousands of dollars to square footage size of the house and basically just built a simple linear regression model based off of that. And we had our estimates an intercept and a slope. A slope of 0.5898. What was its interpretation? When x changes one unit, that's the expected change in y, right? That's the association.
(04:28) And since Y is in thousands of dollars, that means every extra square footage square foot is associated with roughly $600 higher selling price. Be careful. Doesn't mean it's causal because there could be other confounders uh in that data set that we haven't controlled for. Okay? So it might not just be the size of the house that's uh correlated with that coefficient. Uh we wrote out a population regression model. This is just simple regression for now.
(04:53) And we said based on our estimates, we wanted to say where we think our true intercept and true slope are for the population for if we measured all homes in this area. Okay, so beta 1 reflects the population line for all homes in this geographic region. And then from our sample of 500 or so homes, we got an estimate. Don't forget we have one other parameter in our model in this population model the beta 0 the beta 1 and then the sigma squar and if this is multiple regression we have more betas we have beta 1 beta 2 beta 3 up to beta p or beta k depending on how you ever define the number of
(05:36) predictors what's the difference population versus estimated from a sample of data what's the connection we're thinking of these as realizations s from a sampling random mechanism that created the data from the population. Either we did a random sample or there's some theoretical true population model that's creating our data in a random way.
(06:04) Well, we don't want just point estimates. We're going to want to talk about how this point estimate for the slope, we zoom in on the slope, how that is predictive or descriptive of the true population line. It's estimating the true population line and we know there's uncertainty in that estimation. So, we're not only going to use our estimate for the slope, we're going to build a confidence interval around it.
(06:28) And we talked about two different ways of doing confidence intervals, bootstrap resampling and through formulas. Okay. Either way, in essence, what we're doing is building a distribution for beta 1. Okay. For the true slope or for beta 1 hat technically for the estimate for the slope. That distribution of all potential beta 1 hats.
(06:56) The estimates of the slope should look what shape should have what shape? We call that sampling distribution that reference distribution of our estimator should look approximately normal under the right conditions either through enough samples that we've taken enough observations that we've taken or relying on the fact that we started off with normality.
(07:22) Okay, at a high level. So if we trust normality then we can use probability theory. If we don't trust normality, that's when we use uh the bootstrap resamples approach that Pablo introduced. And I agree it's often better a better choice. Okay. But there's ramifications to that. Whoops. Um so these are called the standard errors.
(07:48) It's based on what is giving us the uncertainty in those estimates depends on the uncertainty in our model is the underlying driving force. So think about your scatter plot. The more spread out your observations are around the line, the more uncertainty there will be in that slope estimate. Get a new sample and your slope could be further off versus if the points are really close to the line, collect a new sample, the slope won't change much.
(08:11) Okay? And so that's what that standard error reflects. And so when we look at that formula, we see all right, its standard error depends on kind of two things. the standard deviation of the residuals, how much uncertainty is around your model, but also this denominator, sum of squares of your X variable. What the heck does that mean? There should be some intuition.
(08:36) What are we measuring here? We're measuring the uncertainty in a slope estimate. How can you improve your estimator? How can you improve that uncertainty? Reduce its variance. And how can we reduce the variance based on this formula sigma squar we can't really control but in the denominator we have some control there how can we improve increase this summation having more data points is the simplest way might be costly but the more observations inherent in that formula it's a sum across your observations. And if you increase your sample size, you're going to decrease your standard error. Should
(09:24) make sense, right? More be more data, the more certain you are of your measurement being close to the true measurement with me. How else can you control it? What else goes into that summation? How far apart your x's are. And so if you are able to control the variability in your X wider ranges, you can actually improve your estimator too. That's hard to control.
(09:51) That's sort of like controlling sigma. Sometimes you have control of your predictor variables. Sometimes you don't. Okay. What happens if you standardize your predictor? Midterm type question. [Music] What happens to this formula if you standardize your predictor? When do you standardize predictors? Sometimes maybe you want to focus on interpretability of the coefficients.
(10:31) Not just interpretability of the coefficients, but comparability across your coefficients. You want to put them on the same scale. And so you control all of that variability naturally in the predictors by putting them on the same scale by standardizing. If you standardize it, what's going to happen to this formula? What's going to happen to the sigma? Then as predictor, does that change the uncertainty in the prediction? The event sigma stays the same.
(11:07) What happens to the denominator, the summation? Well, the probability is going to go down. Your actual X variable probably has more than a standard deviation of one. Depends on the units. Okay. So, now we've just reduced the standard error of beta 1. By standardizing our predictors, we're going to get narrower confidence intervals. We're going to get more significant p values. Fantastic.
(11:36) No, it doesn't happen that way. What changes? What else changes when you standardize the predictor? Xar mean would change. But really, how does that break into a regression? The estimate beta 1 itself changes. Remember that interpretation of that slope estimate is completely different when you standardize your predictor. It's no longer a one unitit change in X.
(12:06) It's now a one standard deviation change in X. And so you're not actually getting narrow. I mean on the face of it, they're narrower confidence intervals, but your confidence intervals, how much they narrow is similar to how much beta 1 moves. It's on the same scale. So if your t statistic was four before standardization, your tstistic is still four after standardization.
(12:30) It doesn't affect the significance of that predictor. Okay, so we just built a little bit of intuition on these formulas. This is kind of just stepping through that same idea uh just in the slides rather than talking through it. Standard errors. Great. Those standard errors are based on the residual standard deviation or the residual variance. we have to estimate it from the data.
(12:55) How do you estimate it? What's our source of variance in our data? Well, that's mean square error. And so mean square error is the average square deviation around the predictions. And we just in the parametric perspective kind of change that estimate ever so slightly controlling for the number of predictors in the model.
(13:20) Right? The more predictors you have, the more correct your estimate will be artificially reducing your mean square error. And so we have to artificially reinflate it. And that's essentially what that formula is doing. If you've heard of adjusted R squar, it's essentially the same thing as adjusted R square.
(13:37) All right, you got your standard error, you got your estimates. Let's put turn it into a confidence interval. How do you do it? You take your estimate plus or minus some critical value from what distribution? a t distribution times that standard error estimate of the slope. Okay, t distribution percentile, t distribution quantile, what value do you typically see? How many standard errors is significant? Roughly two.
(14:06) If you have an infinite sample size, it's going to be something like 1.96. It says it right there. Roughly two 2.01, 2.1, something like that. Okay. What does a t distribution look like? We'll see it in a second. Okay, to make that a little bit more explicit, we've had three different intervals that came with us in regression. We had two different intervals estimating something and we had a another interval predicting something, right? We had an interval for estimating the beta coefficients. We had the interval for estimating the mean of Y
(14:41) conditional on X and we had the prediction interval for predicting a new observation giving X. And we'll get into that a little bit later. So just keep that in mind. Right now we're just talking about estimating the slope, estimating the parameters, not estimating an observation's response variable. All right.
(15:02) In multiple regression, those were the formulas for simple regression. Things get a little bit more complicated. Don't forget your linear algebra that Pablo introduced. We get that xrpose x inverse xrpose y formula just a whole large matrix operation. And then when we calculate the variance, if you showed a little bit of math, I could show and derive the fact that the variance under the right conditions is as such.
(15:28) What is that variance in multiple regression? What is it representing? Some variance of a vector beta. When you talk about the variance of many multiple variables, random variables, what are you actually talking about? A whole variance coariance matrix. Okay. And so this result is a variance coariance matrix as we call it.
(16:00) How many elements are in that matrix? How many betas do we have? How many datas are in a linear regression model? Multiple linear regression model. Simple regression is two. Multiple regression is more than two. It's an intercept. And then your P predictors presumably. Okay. And so beta is a vector of P + 1. Beta estimates. And we're asking for the variance of a P+1 dimensional vector.
(16:34) And as a result, you get a P+1 by P+1 matrix. And in that P+1 by P+1 matrix, the diagonals represent the variances of those coefficients. And then you can just pluck out that diagonal term for the one you care about and take the square root. And that will turn it into its standard error. And then the formula you use to build a confidence interval is just this.
(16:59) Simple enough, right? You're just pulling out that specific standard error out of that matrix. Well, if you can't follow that because you don't haven't taken math 21B or whatever linear algebra course, that's totally fine because Python will do all the work for you, right? Just get Python.
(17:17) And if you're using stats models because you're doing statistical inference, you should. It's going to report all those values for you. Okay, great. relating that to hypothesis test. I'll show you some output and talk about it. Hypothesis testing, what's the difference? Confidence interval is an estimate with bounds of uncertainty.
(17:37) A hypothesis test is there's a specific value you care about and we want to determine if it's a reasonable one. What's the specific value you care about that we usually care about? A slope of zero. So beta 1, beta 2, beta p, we might test any one of those being equal to zero. All right? And so we'd set up our null hypothesis to be that the slope is equal to zero. The alternative is that the slope is non zero. There's no association as the null.
(18:05) There is an association as the alternative. Then we go through our approach of fitting a linear regression model. Whether that's multiple or simple. We collect our data and compute the test statistic. That will be our t test for the slope. And then we calculate our p value based on the sampling distribution under the null hypothesis.
(18:23) and therefore we'll have to figure out whether or not we have a real association or not and state it in words. Okay, so that's kind of the whole process for linear regression. Our null hypothesis there's no relationship between the J predictor and the response.
(18:41) So if it's simple regression, you only got one predictor, but you might care about the third predictor in your model. And that you set that parameter to be zero, the alternative is non zero. And then our test statistic is just going to be that same estimate that we saw a few slides earlier for the slope divided by the standard error. The same estimate that we got a few slides earlier for its standard error.
(19:01) Nothing changes mathematically. We've just now instead of adding plus or minus, we've just now turned it into a ratio. Implicit in this formula is we subtracted off zero because the null hypothesis says that beta 1 on average should be centered at zero. All right.
(19:21) All right. And so we're now measuring how far is our estimate from zero in terms of its standard errors. And then we go to that reference t distribution to make that comparison. Sample your data. Another alternative, if we're not using this probability theory and this t test statistic, we could use a permutation method to perform this operation. And we'll get to it in a bit. Want me to speed up? Oh, okay.
(19:46) You want me to go further with the permutation? We will. Okay, I won't go so fast through with it then. And then once you make that conclusion, you either reject or not based on a p value or some comparison to what you would expect under the null hypothesis. And always, always, always restate your conclusion. Is there evidence of an association or not? Okay, great.
(20:06) That p value is now going to be coming from that reference t distribution. Thank you, Wikipedia. That's where I got the slide from. I didn't have time or didn't have the effort to re uh calculate this in Python. But what we're looking at here is for a t distribution, it is bell-shaped.
(20:30) It's just like a normal distribution, but how is it different than a normal distribution? You're basically giving her less observations distribution, more variance like a penalty for not knowing the actual variance of the population. Let me explain that. So essentially this distribution has fatter tails than a normal distribution has wider tails because instead of knowing the true sigma to divide by in that standardization term we have to estimate it.
(20:54) And by estimating it we're adding more uncertainty into the calculation. And to account for that extra uncertainty we use a reference distribution that's a little bit wider. Okay. And this is essentially what we would expect our bootstrapped intervals to approximate. Okay, here we talk about a p value. We do a two-sided absolute value term here.
(21:19) So when you calculate the p value, you calculate the probability that your tstistic, whether that's through resampling or through your formulas, is greater than or equal to a t test. So the theoretical t distribution, how often is it further in the tails than what we observed in our actual test statistic? Okay.
(21:45) And so as a diagram, you would just draw out the t distribution and you say, "All right, we observed a tstistic of -4. What's more extreme than -4? Anything to the left of it. What's more extreme than -4? Look at the positive side. And anything more extreme than that, because our alternative hypothesis is that there's an association and we don't care what the direction is. It's an association either positive or negative. Either one we're allowing for.
(22:08) It's how we set up the test with me kind of. Okay. And then to do this, you can use the CDF from students t distribution in stats models stats.t.cdf if you actually want to go through the process. Okay. It's probably in numpy as well. All right. So this is the side note on a permutation test. Yes. absolute values.
(22:40) Why do we take the absolute value? Why do we take the absolute value? Why are these absolute value in this formula of p value in this probability? Why are they absolute values? Pause. You weren't paying attention. I tried diff that's okay. Yeah. See what what's the absolute value for because we don't care about which side is it? Because we're performing a two-sided test.
(23:28) We set up our alternative as not equal to zero. We didn't say explicitly we have to have positive terms because we're a physics problem and the only thing we can find are positive coefficients. In real life it could be negative or positive depending on what you control for, what you condition on. Okay, great. All right.
(23:51) So the permutation test, this is essentially the analog to bootstrap resampling except now we're doing permutation resampling when we want to perform hypothesis testing. Slightly different assumption when we do the resampling. Okay, what's a permutation? A reshuffleling, a reordering. All right.
(24:18) And so what we're going to do when we perform a permutation test, not like we did in Bootstrap where we sample our data, what we do is we reshuffle our data. We take our response variable and we shuffle it up and reapply it to the predictor variable. So you're shuffling your Y and keeping the rows of X as they were. What is that invoking? What is that doing to your data set? No relationship. Enforcing the null hypothesis.
(24:53) So instead of bootstrap sampling your data which is doing resampling under the condition of your data, this permutation testing is resampling under the condition of the null hypothesis. And so we reshuffle our data in the simplest case, reshuffle the response variable and then we perform the resampling many many times just like when we did the bootstrapping. Each time you reestimate your model, you get a new beta 1.
(25:24) But that new beta 1 estimate through permutation testing now should be centered where we did it. So the null hypothesis says beta 1 should be zero. And so if you build the reference distribution of all beta 1's, it'll be centered at zero instead of what was estimated in the data. All right? So it's a little bit different.
(25:50) You're resampling under a different condition. All right? So you don't really necessarily want to do a bootstrap when you're doing a hypothesis test because hypothesis tests want you to conserve type one error. We don't want to have bad type one error rates. And by sampling under the null, we restrict that.
(26:14) So essentially, this bootstrap approach is prone under certain conditions to an inflated type one error. If all of your assumptions are met, you're fine. But it's just when the assumptions aren't valid, issues can happen. When you have multi-olinearity, issues can happen. Okay? It can inflate type one error. And so what we do instead is we perform this permutation test, resampling your data.
(26:33) and maybe I'll have an example for you uh on Wednesday. Okay, here's some output. Here's an inference through bootstrapping. Here's our rebootstrapped many, many times confidence interval. What did we do? What does it look like we did? We got an estimated beta value. Looks like it's off, but that's Oh, no, it's not off. Our estimated beta value remember from our data set was like 0.589 or something like that. Okay.
(27:06) And so when we reample our data through bootstrapping and recalculate our beta estimate under the hood, we didn't show the code here. We get a new beta one hat. And it's now going to be bouncing around that 089 value. All right. Sometimes lower, sometimes bigger. And then in this bootstrap sample, we just pull off the middle 95%. What distribution does it look like? bell-shaped bell-shaped like a normal bell-shaped like a t distribution.
(27:38) Okay, what's the difference? Plus all right and then we can also use stats models to do all the work for us. When we perform stats models and get the confidence interval here, what is it doing? Not bootstrapping. It's just using the formulas we saw from a few slides earlier. confidence interval automatically is given to us 0.589. Its confidence interval goes from 0.544 to 636.
(28:04) The confidence interval through bootstrapping goes from 0487 to 0.75. How do you compare them? How do they compare? How do you compare them? You look at them, dummy. But how do they compare? Which one's wider? The bootstrap one. Where are they centered? At the same place, right? Because they're based on the sampling distribution being centered at the point estimate. No problem. But the one for the bootstrap is wider.
(28:38) The one for stats models based on the formula is a little narrower. What explains the difference? Why aren't they the same thing? The assumptions for the formulas may not be met for that formula. Exactly. Right. Now, what are the assumptions? We talked about this right at some point. Kevin's going to the board today.
(29:08) One problem with this lecture hall, our boards aren't the best. We make an assumption on the response variable Y conditional on X is going to follow what distribution a normal. This model says what about the mean of the distribution? What model did we fit? What predictors did we use? It's a blank blank regression model.
(29:50) A implear finish it off. A simple linear regression. Someday I'm going to say something like that and say a bad word. Really make a fool of myself. But the normal distribution has two assumptions. There's a mu parameters I should say. Mu is beta 0 plus beta 1x here.
(30:16) And so we're talking about each of the I observations and then the variance is a sigma squ. Okay. So that's the likelihood based representation of linear regression. Okay. Distributional assumption based regression as a result. We kind of have four assumptions to this model. Remember what they are when we listed them out before. What's the most important one? That's Minneapolis.
(30:47) What's the least important one for those SAS friends out there? Which one do you not have to worry about? Normally there's that. Okay. It matters a little bit, but if your sample size is reasonably large, you're okay. All right. Reasonably large. 50. If you have 50 observations, you can just ignore the normality assumption. Okay, roughly ballpark figures.
(31:16) What are the two in between? You got constant variance and you got independence. I could write this all in one expression to say that my observations are independent for i= 1 up to n. And so now I just wrote out the full model.
(31:42) What are the assumptions of my observations conditional and X are independent? There's one constant variance sigma squared. It doesn't vary depending on your eye. The relationship is linear. And then what's the last one? Uh normality right there. Normality of your observations conditional on your X. Your residuals are what's going to show illustrate those assumptions. And here when we looked at the scatter plot, we saw that the constant variance was clearly violated.
(32:09) I don't have it on me right here, but if you go to last Wednesday's lecture notes, you saw that that variance. There was a clear fanning out around the regression line. And that variability doesn't depend on sample size in its assumption. If the sample size is large, you're not fixing that. Same with independence.
(32:28) If the sample size is large, you're not fixing that. linearity doesn't matter what sample size you have, you're not fixing it. You have to change your model. Okay? And so the bootstrap sample doesn't require that constant variance. Does require independence. It does require linearity. Last two it doesn't require. Okay, great. multiple regression.
(32:58) Those coefficients get estimated based on that hat matrix. Xrpose Xhat XRpose Y. Stats models is doing that underneath the hood. For every coefficient, there's a standard error. It's pulling off that variance coariance matrix.
(33:16) You don't have to worry about the math behind it, but realize that's what's going on under the hood. How does stats models estimate those estimates or get those estimates? Not through any numerical method, not through optimization. It's a closed form solution here both for the variance and the estimates. We got lots of little output here. We see for square footage that estimate is pretty consistent to the sim simple regression model 641 versus 598.
(33:40) Uh with a confidence interval, it really hasn't changed at all with much certainty. And we looked at beds has a negative effect last time, which was surprising to us because beds typically are correlated with large homes. And when you squeeze more beds into a fixed size house, you might actually be decreasing the value of your home. And that's one potential interpretation, though.
(34:05) Watch out for the causal effect. Okay, with me? Great. Game time. So, this game, word of warning, might be a little challenging because I like challenging questions. All right, but I do like volunteers. Whether you get it right or wrong, you can get your free day. Any volunteers? Oh, we got more. Have you volunteered before? A new volunteer? Come on.
(34:45) that hasn't done it before. If nobody else volunteers, it's coming your way. All right, let's do it. The one time you sit in the bottom. I can throw it up to the balcony. Go ahead. State your name. I'm Giovanni. I'm a PhD student. One fun fact. In which department are your PhDs? I'm actually in the committee on the study of religion. Nice. Welcome. We'd love to have you.
(35:11) And my fun fact is I make my own soaps. Make your own soaps. What do you start with? Uh lie and various fats. Various fats. Do you use like goat milk? Ever? Uh that's actually the next one I'm making. Or sheep's milk. I made my first uh tallow one, which is Talk to me afterwards. Talk to me afterwards. Sounds good.
(35:34) All right, game time. All right. What happens to the distribution when B the number of bootstrap samples increases? And I think my uh animations were out of order. Apologies. So what are we looking at here? This is the same plot as before. This is the bootstrap sampling distribution over repeated bootstrap samples.
(35:57) What are the different beta hats we expect to see? All right. What can happen? The distribution becomes more normal. the variance decreases, the resulting confidence intervals become narrower, and the distribution gets smoother. Check all that apply. And let me hear that, you're you're in the game. You got to talk through your thoughts.
(36:21) So, I'm I imagine with a lot of sampling, the um the law of large numbers starts to apply and it begins to look more normal. And I think given that it's already has a bit of a normal shape, both A and D would apply. like maybe those weird spikes towards the right would start to even out. Um I I feel like the variance would not decrease very significantly, but maybe maybe I'm wrong about that.
(36:53) And I don't I feel like we don't actually with the confidence interval. I don't feel like it would inherently become narrower. um like it might slightly but I don't think that's necessarily against statistically meaningful um because otherwise we could arbitrarily take more and more bootstrap samples and have a ridiculously narrow confidence interval. Good job talking through this.
(37:12) So yeah, this is an artificial number B. Your final answer, sorry. Oh, my final answer is A and D. A and D. A and D. Who agrees A and D? A and D is a great answer. who agrees that at least one of A and D is a great answer. Okay, I think this is like technically you're kind of right now that I heard what you talked about.
(37:36) I could kind of agree with what you said because one of these is like debatable. Okay. All right. So, the answer I gave Oh, that's wrong. That's wrong. That's wrong. I changed the order of what was underlying it and that's what did it. That is wrong. I did purposeful because I don't know if I hit it on you or not. That's right. That's right. When you get it wrong, be like, I did it on purpose.
(38:07) There you go. My thinking was the distribution gets smoother. What are you doing when you increase B? you're getting a more precise estimate of the sampling distribution. You're not changing the properties of the true sampling distribution. You're not making it narrow. You're not artificially improving the confidence interval.
(38:33) So, you're getting a more precise, smoother version of the distribution. Now, if you want to argue that smoother distribution is going to look more normal, I totally can can see that by that. Great. So yeah, D for sure and A debatable depending on how you argue. Great, great. Thank you. Thank you. Thank you. Let's give a round of applause. That was a challenging one.
(38:52) How can you affect B and C? What's an artificial way of affecting beings? I mean what's the proper way? How do you actually change the variability? How do you narrow your confidence interval? You increase the underlying sample size your data stays small. Increase n. Increasing b doesn't do it. Okay. Artificially what do you do? What some people accidentally do when they do their bootstrap sample? They don't take n observations with resampling. They take a halfaphazard number. They take two times n accidentally. And that will
(39:38) artificially narrow your confidence interval, but you've essentially don't have independent observations anymore. You've artificially doubled your sample size. Okay, great. Let me hide this again. One more question. Use this output. That's all. Use this output to predict. We have volunteer. Have you volunteered before? Oh, no. I have not. Great. Great. Name? Hi, I'm Trisha.
(40:14) Where you from? California, but I'm visiting from MIT. Why are you MIT student? Great. Trisha. MIT student. One fun fact. Other than you're from California. Um, my favorite color is green. My favorite color is green. Well done. You like it? You and my wife can get along. All right. Use this output to predict with 95 uncertainty the selling price of a home that might be 2860 square feet which might be applicable to one or more of your professors.
(40:44) Okay, options pick all that apply estimate whole bunch of numbers. Here's the output. Okay, off the bat I'm going to eliminate A and B because I think I need to account for the intercept that we find. Okay. Um between C and D I the standard error I see for uh where is it? The 0023 is that value.
(41:15) Um, I know the plus or minus two makes sense for 95% uncertainty, but I'm kind of torn between C and D because I'm not sure if I need to account for that sigma hat squared and then take that the sum of those two. Can I find a friend? Yeah, let's find a friend. Found us. Find an actual friend. I love it. I love it. Um, can I get a clap? All right, let's ask the audience.
(41:47) Ask the audience here dep uh deciding between C and D to do the prediction for a home with 2860 square ft C or D. How many people round of applause think it's C? How many people think it's D? Pretty split here. Pretty split here. Anybody want to give an argument one way or the other? Oh, we got two volunteers. Got stats friends. Hold your thought, Trisha. See who's more convincing. Sounds good.
(42:25) So, it's asking for the um for the for you to predict the the selling price of a single observation. Um so, you're going to have to account for the individual variability. If it was asking for the mean um for like a given a given square footage, then you wouldn't have to account for sigma hat squared. Um but in this case, it would be d for that reason.
(42:48) Oh, I was going to say the same thing. Oh, you were going to say D as well. Yeah, like for example, if there's headrest guess, however you say it, then that means depending on which predict where you choose along the X's, you're going to have different variances.
(43:06) that 0023 is just like basically a mean variance of the observations. All right. Are you convinced? I am. I think the argument about the fact that it's a specific home makes sense. So, I'll go with D. Yeah. With D. Everyone agree? Yeah. Yeah. You got it. You got it. It is D. It is D. Animations are screwy. So, it is D. What is D? It's a prediction interval.
(43:34) Why is this a prediction interval? We hit it on the head. It is estimating with uncertainty the price of a single home. A single home. No matter how precise your estimates are, no matter how many observations you collect, there is going to be that stochcastic noise, that irreducible noise around the model for all a future single observation. And so that variability, that estimate of the variability around the line will never go away under that condition.
(44:13) And so yes, we want to include the uncertainty of a single home. We want to include sigma squar of the residuals. Thank you. Uh when we do this calculation with me all right the opposite if part C is not actually 100% right algebraically but it's a rough estimate. Part C is incorporating the uncertainty of the slope coefficient.
(44:38) it's not properly accounted for and it's also not including the uncertainty of the estimation of the intercept. We should include both terms in part C. This would be the really bad formula for estimating the uncertainty as uh mentioned for the mean of all homes at that uh square footage, right? And so mean of all homes, this would be one formula which again isn't 100% accurate.
(45:03) And then this would be the prediction for a new home. Again, it's not 100% the correct term, but close enough. And then part A is the only other reasonable one we have. And that's including the in this case, this is the confidence interval for the slope. There's indec. [Applause] And so in theory, if this were the standard deviation of this predicted mean, then you have to take that same standard deviation squared plus the variance of the prediction because we're assuming those are two independent sources of variability and the variance is add.
(45:52) That's the last one is that inflated version of corrected version of MS. of all 280 that that will use. So D is predicting where a single observation will lie in the response. C is a hypothetical situation where you're trying to predict where in the population the mean of all homes size 2860 where is the mean of all those homes? Okay. So, it's not predicting all of your observations and averaging them. It's saying conditional on an x of 2860.
(46:44) This is where we think the mean of those homes would be. Okay. Great. All right. So, just a lot of review there. I didn't really step too much into the likelihood, but we're going to review that and then connect that to bay. So, remember what was likelihood last time? It was useful for performing inferences.
(47:09) It is the inverse of a probability calculation. Right? We set up a PDF. We set up a PMF. And instead of saying the probability of X given your parameters, we're now saying what is the likelihood of a choice of a parameter given your data or given your X. And we're flipping that conditional idea. We lose the probability because we weren't careful in how we did it.
(47:35) So we're flipping the PDF or PMF on its head. And so we're talking about thinking about the unknown parameter in terms of the observed data. So if our X's are coming from a normal distribution, the unknown parameters are mu and sigma squared. And so what we're saying is given the data we see, what are the most reasonable, the most likely choice for mu and sigma squar to describe those x's that we actually observed. All right.
(48:00) And so we talk about the likelihood expression in terms of mu and sigma squared given the x's we observe. This is univariat. We're just measuring a single x. And what we're doing is we're taking a product of pdf evaluations. And now we're saying all right we're going to maximize that likelihood with respect to mu and sigma squar. And so we just take partial derivatives and solve.
(48:25) Take the log makes your partial derivative calculation a little easier. Okay with me kind of. Why do we take a product? We assume our assumptions our observations were independent. And so when we talk about the contribution to the likelihood of each individual observation, we can just multiply them together.
(48:57) when we want to look at the joint contribution of all of your observations. All right, multiplying the marginal PDFs together to get the joint PDF, the probability that X1 equals the value it got times the probability that X2 got the value it got, etc., etc., etc. Okay, the example here we talked about just looking at the likelihood versus the log likelihood.
(49:26) We're going to try to maximize choose the mew that maximizes that likelihood function. Our data had three observations. It had a sample mean of six. And whether we're using the likelihood function or the log likelihood function, they both top out at a mu of six. And so we can use the log likelihood instead of the likelihood to aid us to do the optimization because sums are a lot easier to take derivatives and handle numerically. Great.
(49:51) So we maximize that likelihood with respect to mu and sigma squar. With math, we said we did that analytically. Doing what? To maximize a function with respect to unknown parameters. What do you do? Take the derivative, set it equal to zero, have a system of n unknowns and equations and you solve. Okay? Sometimes that evaluates in a closed form solution. Sometimes it doesn't. Linear regression it does.
(50:18) With computers, you do gradient descent. And since it's descent, meaning you're trying to bottom out, you take the descent of the negative log likelihood. Sometimes they call it gradient ascent, but whatever. With me? Great. All right, we did this for regression. We just wrote out the regression model out on the board. Here it is again.
(50:44) The likelihood then contribution of a single observation is just plugging it into the normal distribution underlying it. The joint distribution then is just producting all those individual contributions. To maximize that we take the log. That product becomes a sum. To maximize we take the derivative. There's that negative log likelihood.
(51:07) We're going to try to minimize this negative log likelihood. It's equivalent to minimizing. And there was a typo last time. Notice here we have a minimal function. We're minimizing a sigma in the denominator. But if you ignored the sigma, this is the MSE. If you take the sigma into account, I called it the standardized MSE.
(51:32) Either way, with respect to beta 0 and beta 1, optimizing your choice for the linear terms, it doesn't matter if you incorporate the sigma or not. It's a constant. you can pull it out. And so the equivalent of maximizing likelihood is equivalent to this standard ordinary least squares operator that we've been talking about from the start with linear regression. That's our loss function. That's our natural loss function.
(51:56) Okay, quick review. All right, so let's connect this to Baze. How many of you have heard of Baze rule or B formula before? Hopefully everybody. Great. If I asked you to write it out, could you? Maybe what is Baze rule useful for conditioning. And if you're an AP stat student, you said getting a five on the AP test.
(52:22) That's all you care about, right? No, no thinking there. No, but let's build some intuition, some thinking into this. And so, how do I write out BA's rule? It's a connection between two events A and B when we first learn it.
(52:42) And we say based on conditional probabilities, if we're looking at the probability of A given B, it's the intersection over B. Let's talk about a simple example. In this class, roughly, these are ballpark numbers among the undergrads. Roughly 40% of you are stack concentrators, roughly 60% of you are CS concentrators, and roughly 20% of you are both. Sorry, grad students, I don't know you as well.
(53:07) Okay? and you don't fit this nice little story. Okay. How can you be both? Well, either as a joint or a double. Who would be a double in the two? I don't know. Just trying to avoid the thesis, I guess. But that's okay. All right. So, let's define two events to describe this scenario. What are my two events? Ah, it doesn't spin that way. Darn it. They didn't erase it for me.
(53:45) That's not the magic word of the day. What's A? What's B? What's the secret word? It's coming. It's coming. What's A and B? for this example. A is the event. Sure. A is the event in SAS. SAS B is CS. Let's just call them CS and stat. You don't have to use A's and B's. It's fine if you want to fit it into the formula as seen. We're going to write it as stat and CS. Okay.
(54:31) And what events do we know? What's the probability of stat? What's the proportion of people who are stat concentrators? Yeah, it's given there 40%. 0.4. What's the probability of CS? 60%. More people are CS here. Great. And what's that third thing? It's the intersection of those two things. So that's the probability of stat and CS.
(55:00) Roughly 20% of you are joint concentrators. I have no idea if these numbers are right. I just made them up. Okay. How do we turn that into a conditional sign then? Well, first of all, distort mean exclusive. Disjoint means there's no overlap means there's no intersection. Are these two events disjoint? No. Okay.
(55:27) Great. Three. Are these two events independent? Why is it not a joint? Cuz the intersection isn't probably zero. Okay, great. Are these two events independent? I don't know. No. Great. Move on. How do you know? How do you know they're not independent? What was the assumpt independence? When we did the joint PDF, we said independence is when probability of A and B equals the probability of A times the probability of B. Well, here the probability of stat and CS is 20%.
(56:15) We're asking the question probability of just stat is 40%. The probability of just CS is 60%. And since those two things are not equal, that means they are dependent. Okay. What does it mean in terms in expression and feeling? Does it mean that two events are dependent? Why does it make sense that they're dependent here? Go ahead.
(56:49) Knowing something about provides information about the chances of the other effect. What does that mean? If I know somebody's a CS concentrator, it means, oh, that tells me they're more or less likely to be a stat concentrator. Okay, that probability changes once you have information over on the left. It informs about the chances of the thing on the right.
(57:14) Okay, well, why do we care? because this is related to the idea of conditional probability. We can calculate two different conditional probabilities here really easily. Probability of stat given CS and probability of CS given stat. What's the difference? The numerator right there, it's the same intersection 02.
(57:36) In both these cases, the denominator depends, right? If we're conditioning on stat, we plug in the value 04. If we're conditioning on CS, we're plugging in the value 6. All right? And so you get two different probability calculations. Should you compare those two probability calculations? Why does Kevin ask that question? Because the answer is no.
(58:09) Why shouldn't you compare the 2 over point4 to the 02 over 6? They're expressions of different things. being random. The probability of CS given stat is saying given the fact that I know someone's a stat concentrator, what are the chances they are also CS? The second expression is given the chances I know someone is a CS concentrator, what are the chances they are stat? And so you're asking for an apples to orange comparison. Don't compare those two numbers to each other.
(58:40) Okay, great. So different interpretations talked a little bit about what they represent. To build some intuition, it might make sense to look at a ven diagram. We have A and B. We have stat concentrators in pink. We have CS concentrators in blue. And we have the intersection between the two.
(58:59) And how do you calculate the conditional probability in terms of intersections over what we have here? So if you want B over A, you're dividing by pink. If you want A over B, you're dividing by light blue. Okay? you're looking within a different subgroup. All right, can get really tricky. I'm not going to get into this.
(59:22) How many of you know what the Monty Hall problem is? You all probably have heard of it. What's the Monty Hall problem? Who was Monty Hall? Game show host back from like the 50s or 60s, even before my time. Publish is probably right. Who's this guy? He took over the hosting duties. I never watch show ever. I know he's Wayne Brady It's Wayne Brady  Anyway, so essentially what's the play? There's three pri uh prizes behind three doors. Two are crappy prizes, one's a good prize.
(1:00:03) Monty Hall or Wayne Brady asks uh to choose one of the three doors and then you have the option after Monty Hall reveals one of the other doors has a crappy prize behind it. He says, "Should you switch doors? Do you want to switch doors or do you want to stay on your original door?" If you stay, what's the probability you win? If you switch, what's the probability you win? It's 1/3 if you stay, 2/3 if you switch.
(1:00:28) It's a little anti-intuitive. happy to talk about it afterwards. The way I convince myself is all right, what if there's a hundred doors 99 behind which is the goat so we can make goats milk um soap behind the other one is a nice car. And so if you pick in that hundred door example a losing door, what's the chance at the start you pick a losing door? 99 over 100.
(1:00:59) Monty Hall or Wayne Brady removes the other 98 crappy doors and no matter what you do, if you switch, you're always switching into a winner. And so if you switch in that 100 door example, you have a 99 over 100 chance of winning. If you don't switch, which is a strategy, not a random event. If you switch, you have to pick the winning door at the beginning. And there's a one over 100 chance that you do that. Okay.
(1:01:22) Anyway, again, happy to follow up afterwards. There's a whole New York Times article from about 15 years ago just stepping through it because there's a whole debate in the mathematical and log philosophy world about whether what was the correct calculation and the statisticians and probability people are like what are you talking about it's a simple calculation but you know philosophy people had a different made a different argument. All right.
(1:01:47) Anyway, Baze rule essentially what we're looking at to plug these values in. We could switch from the conditional probability conditioning on one event to the conditional probability of the other event. That's what Baze rule allows us to do. Flip the conditional probability. Okay? So, if you know one condition and you want the other condition, you apply B rule. Here's the simple formula.
(1:02:12) when you know the two marginalss. Here's the more extended formula when you're looking at in the denominator what's called the law of total probability. Okay, going through this quick. All right, an example. This happens all the time in diagnostic testing and this is going to show up again when we get to classification in a few weeks.
(1:02:37) Classification modeling because in classification modeling we care about getting correct predictions. And so either our model is correct or not and it's dependent upon the class that we started off with. And you might want to invert that probability calculation at some point. We can talk about sensitivity and specificity of these tests here. This is trying to predict whether or not someone's pregnant.
(1:03:02) Sorry, if you're pregnant, the medical rule sometimes says you're diseased. If you're not pregnant, we denote it as disease minus. And we have various different probabilities. If you're pregnant, the test will show that you're pregnant 97% of the time. Why isn't it 100%. User error. There's sometimes user error.
(1:03:26) Probability of test minus when you're not pregnant is a little bit higher. It's harder to make that error when you're not pregnant, but it happens at from time to time. And then among those who are taking that pregnancy test is roughly 30%. And so what you can do is apply B rule to figure out the chance of whether or not you're pregnant depending on whether or not you believe 30% is your chances to start.
(1:03:47) And so if you work this all out, you plug those numbers into the longer version of BA rule, you get a probability of 97%. Okay? And this is the probability you as a user of a pregnancy test care about. I test whether or not I'm pregnant, the chance that I'm pregnant becomes it's always zero.
(1:04:12) But in this case, for this person, the chance of being pregnant is roughly 98%. Okay, after getting a positive test, what was their starting probability? 30%. We had the prior belief that they were pregnant of 30%. collect some data, collect some evidence, and we update that probability to 98%. This idea of updating your belief in the chances of something happening is this Beijian perspective on probability.
(1:04:50) Okay, tree diagram can help if you want to work through something like this. You just plug everything in, you find the intersections, and you do all the work. Okay, if you don't like the scary B rule formula and in this class hopefully it's not too scary, then you can work through it as a tree diagram instead. Okay, this is the whole idea. We started off with a probability that someone was pregnant at 30%.
(1:05:13) That was our prior belief. This is called the prior probability. It might not be 30%. For you, you might believe your chances are 50%. How's that going to change the result? you come in with higher probability, you're going to leave with higher probability even if the evidence uh is still consistent. So the probability was updated to 97%. We call that the posterior probability.
(1:05:38) And in Baze inference, we're going to put a whole distribution to these probabilities because we're not just dealing with a yes no question anymore. We're now dealing with a whole random variable oops of possibilities. Okay, this change is basically just updating the probability given the evidence. What's our evidence in real life in this data science world? What is going to be act as our evidence? The data. We're going to update our beliefs in parameters.
(1:06:10) Once we collect some data, we update our model. And that's our connection to Beijian inference. What's today's magic word? You got a long weekend and all of you are also using Apple products probably. Apple ale Apple things you can find on a farm this time of year. Apple. All right. Baze rule for distribution.
(1:06:44) So what we do instead is instead of writing our B rule, the simplest version of Baze rule as updating your belief in A based on what you observe for B. We're now instead going to think of this in terms of parameters of your model given the data you saw. So we're going to start off with a parameter that's going to have a whole distribution.
(1:07:16) Instead of just writing it as a simple probability, it's a probability mass function or probability density function whether or not we think it's condition or continuous or discrete. And so we start off with a P of theta. We collect some data and then we update that probability distribution of theta given what we observe with X the data we obser with me. Okay.
(1:07:43) This simplest version is thought of in terms of hypothesis. You could start off with a prior belief that the null hypothesis is true. collect some data and given the null is true you can know the probability of observing that data then you can update your probability conditional on that fact. Okay, that's one of the simplest applications of B rule in this inferial perspective.
(1:08:09) The more general version if we think of theta this parameter which as a frequentist we would tell you is a fixed value now we're going to put a whole distribution on it. All right. And this is somewhat natural in some settings. And so theta is going to have a whole distribution. We're thinking of theta, a parameter, as continuous.
(1:08:28) Think about the mean distribution. What are its parameters? What are the parameters of a normal distribution? I said mean distribution. Normal distribution. One of them is mean, mu. The other one is variance sigma squared. And so mu as a parameter can have what values in a normal distribution? Anything unbounded. Sigma squared as a parameter.
(1:08:54) What values can it have? Something positive. And so we can put a distributional prior assumption that follows those rules. And that could be our starting spot, our prior distribution on theta, mu and sigma squar. And then we can collect some data from that distribution and update where we think mu and sigma squar are. Okay, we can break this down. We have x is our matrix or vector of observations.
(1:09:21) Just the general x. Make it simple. We have a single variable at this time. Just think about it as a vector of observations. Theta is the vector of parameters. It might just be a single scalar. F ofx given theta that's our likelihood. that is what is the probability or density of observing my x's given the theta I started off with and then we have a marginal PDF of x's essentially we just have to treat this as a normalizing constant with respect to theta because theta is what our goal is this is just a nuisance and so we're going to make this a little bit easier and then what's important here is f of theta is the
(1:09:59) prior distribution we put on that parameter theta and then as a result once we do this updating After collecting our data, now we have a whole posterior distribution on theta. And since we're changing the frequentist idea and with it comes hypothesis testing is a single value of theta reasonable or not, we're now putting a whole distribution to the values of theta.
(1:10:27) Okay, how do we integrate out that b denom? How do we get rid of that denominator? We integrate out the thetas over all choices of thetas based on our prior distribution and that just becomes a normalizing constant. So a lot of times we kind of just ignore it in this formula. We just say we're proportional to the numerator from B rule.
(1:10:46) All right, simple example. We don't have time to work through it, but we'll probably start here next time. You own three coins, a fair one and two biased ones with one with probability P 0.1, one with probability 0.9. I don't know. You're a magician. You reach into your pocket and select one coin at random to flip. Okay, you flip it four times.
(1:11:05) It's three. See, three heads and one tail. And the question is after collecting that data, we want to update our information. When I reach into my hand and pull out one coin, what's my prior probability of each of those three coins? 1/3. There's the distribution on the three possibilities. Okay.
(1:11:31) And then after updating based on evidence, which way will I start to lean? I got three heads and a tail. What is that more in line with? The probability that the true P is 0.9 and less likely to be the probability that P is 0.1. And after collecting data, I will update my evidence and hopefully it's no longer 1/3 1/3.
(1:11:57) It's like maybe 50% that it was the coin with probability 0.9 and maybe it's only 10% with the probability that the coin is 0.1. Okay? And we'll work through this next time. All right? We're working through bay, getting to base, connecting it to regression in a bit, and we're going to be using these in Python. Thanks much. See you on Wednesday.