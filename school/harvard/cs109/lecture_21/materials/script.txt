(4) 109 day 21 - YouTube
https://www.youtube.com/watch?v=1Et1U42nJSw

Transcript:
(00:01) Audio audio test one too and and I share. Okay. All right. All right. Five, four, three, two, one. It's 10:30 a.m. Welcome, welcome. welcome you on all. I am Pablo Protoabas and in case you forgot me. I will be your lecturer today. It's a little bit cold out there.
(00:42) So Kevin and I are assuming you're going to be crawling in as the time goes. I am not going to make an announcement until 10 15 minutes after we start because I notice we all start with 30 40 50 people. Uh today we have about 10 20 30 40 50 people. By 10:45 we about 150. All right. So I'm going to make announcements halfway or a little bit after we start. All right.
(01:08) So today we starting with the Oregon coast. Yuking, it's you. Good picture. Nice photo. Thank you. Now I know your last name. Shiao. Uh so let's recap a little bit where we are in this story. Uh we started with trees simple decision classification tree easy peasy clear. We split at any at any split we decide how to split by looking at every predictor and for each predictor all the unique values or the unique splits we can do or important splits. Okay that was decision trees. If we grow the tree very far, we realize
(01:53) it's going to overfit. So we control for complexity in two ways. One is by controlling the depth of the tree or the complexity of the tree from the small to the large or from shallow to deep and the other one is by letting the tree grow and then prune. All clear? Yeah. Now then we kind of switch gear and we start talking about regression. There's a slight difference.
(02:22) The only difference here is that in classification, every split we do, we're trying to get purity. In regression, every split we do, we're trying to have the points at each region to be as similar as possible. It is purity in its own way by measuring the MSE. Okay, that's all we covered so far.
(02:48) Uh again we're going to control for complexity with stopping criteria or pruning and how we choose which stopping criteria and where to stop is done with usual cross validation. Okay now let me while you everybody's settling it at 3 four minutes after the hour I want to ask a couple of questions. The first question is we were talking about cross validation. You have become expert of answering cross validation.
(03:19) Right? So in classification we're warming up. I haven't start lecturing until everybody comes in. In cross validation in classification when we do cross validation what is that we're looking to decide where to stop or which to use? Let's talk about max depth of the tree. We said we're going to decide on the depth of the tree using cross validation.
(03:47) I want someone to explain to me what were we looking for. Remember what we going to have? We're going to have as the depth of the tree increases the validation error will go down and then it's going to go up. Right? What are we looking in the yaxis for classification? The color of the tree, the smell of the tree, the size of the tree.
(04:20) What are we looking when we do cross validation? A you really killing. It's a correct answer, but the one I don't like is so it's correct answer. AU is one of the metrics that you could use. But what else can we use? AU is perfectly fine as just not my favorite one. But I I take it as a correct answer. You cross validate all the time.
(04:54) When you cross validate, you have to choose based on some metric. A C is one. What else? Put it in a quiz question. Just add it up for me. Yes, you see F1 score. Correct. Alex says accuracy F1 score or A area under the curve or some metric that tells us what we like. Now follow-up question. Why don't I use gene index? Why don't I use BC? Why don't I use entropy? Am I allowed to do cold call changes? No, we are choosing a model. We're doing cross validation. We're using depth three.
(05:59) and Alex says use F1 score accuracy and Lu Yukin Lui said AU all good answers but the question I asked why don't I use gene index why don't I use the entropy the one I use to create my tree why do I abandon it why am I a trader to my gene index what happened because I don't care about gene index what I care is the metric that I'm going to sell my model to.
(06:36) Right? What I care is the model to perform well under and in data well for me is accuracy for Lui is Yukiin Lugi is a U and for Max Alex sorry Alex is F1 score or accuracy it said all good it depends. It depends what you want to do. Right now, switch gear. Everybody's here almost. Yeah. Let's switch gear. When I do regression, what do I use to cross validate? I was going to say I was going to say your full name is your last name. Zoe. Malucas. Malucas. No.
(07:19) What's your last name? Guru. Zo. Go on. MSE why what why do we have now we're using MSE we use MSE to minimize we use MC to cross validate why it's like what people that's what people thank you very much good good answers all right now with that warm up let's do the announcements of the day uh first announcement we know we have quiz this week don't panic it's not the end of the world you're just learning stuff and we validate crossv validate you sort of that's all I don't want this attitude oh my god this quiz it's just eases what we do is relatively easy you study you do fine second
(08:07) announcement uh tomorrow 6:30 you may have seen the post 6:30 at SEC which is on the other side you have to cross the river in this cold weather uh near my office challenge of the day is to find my office space uh there is a in front of it. Uh, we're going to do a review. I will be doing review.
(08:28) Um, that's announcement number two. I won't use slides. I'm just going to go on the on the on the whiteboard. I have a question. Yes. Yes. Yes. Terry, how are we administering? Okay. Uh, Terry, he says, "How we administering the quiz?" Uh, the slightly different from the last one.
(08:59) and the midterm we going on the computers no more paper and killing trees I am an environmentalist I don't believe in killing trees so you're going to do it on it like the practice the same one you're just not allowed to get away from that page and very easy to check that so don't even try uh okay so you just get your red page you do it and that's it it's uh Yes.
(09:24) Is it section? Of course. Yeah. Okay. So, any other question? One page shapes. Let's repeat. One change at section. I don't understand why you're asking me if it's section. Have we done anything outside sections? No. M was at section. No. Huh? The coding part. No, the quiz is just uh multiple choice questions.
(09:55) 30 minute one cheat sheet on on it. Okay, any other questions, Kevin, that I forgot. All right, let's move on on to my big announcement. So, tomorrow we're having a review uh outside my office. Uh I hope there is enough space. Well, we'll see how many you can. Um we can I can accomm
(10:20) odate the quite a lot. 6:30 p.m. I will start sharp at 6:30 that I usually start sharp 6:30 and I'm going to finish by 7:30 8:00 no more long. So do not come at will come at 6:30. I'm just going to go through them uh the material. Uh announcement number three. Um there's a lot of requests for pass fail. Uh I check the policy for pass fail. Graduate students you're not allowed.
(10:42) So don't even bother. Uh, undergrads, you're allowed. And apparently Kevin, I and Chris have no say. So if you make a request, we have to approve it. I don't understand why they make me click if I have to. Any idea, Kevin? You know these things, too. Can we? I read the policy this morning.
(11:06) So at the end of the day, we approve everything. But I want to make few things clear. If you to go pass and fail, it's going to be very hard for me to support you in any ways. Letter of recommendation reference because to me it feels like you're giving up. Uh I got busy. I have midterms. Yes, we all do. For me, I'm oldfashioned. You said you're going to do something.
(11:30) Either you do pass fail at the beginning, not November 17, which or out 10 when you already saw your midterm exam. So a strong correlation for people they ask for past fail with a grade. You don't have to satisfy me. That's fine. I'm just telling you my point of view. Old fashion guy. When I said I'm going to do something, I do it. Even if I don't get the best grade, the rest of you will approve it.
(11:54) Can we start approving as we go? Uh that's totally up to you. I just talked to few of them and already told you it's not only me. I work for or I advise companies when they show me a resume of someone who's take data science or any technical course at Harvard is past fail. I say no because that that means you actually didn't do well. It is past fail at Harvard.
(12:21) Past fail or Harvard means you're really not trying. So that's my point of view. Ignore it. There's nothing personal here. That's what I do. The same with admissions for master's program. When I say you that you tried to do graduate program in data science or something and I saw you took CS 109 and you passed fail, it's a red flag.
(12:41) There is someone else who got B+ or B. I prepare that. All right, that's on that note. The last thing I want to talk before we start talking about class is uh I got a lot of students which came to us to me say I need to do I want to do better. Good. I like that. That's proactive. What shall I do? Kevin mentioned last time, I'm going to mention it again.
(13:08) Our attendance at office hours is less than one person per officer. So go to the office hours. There is a lot to learn from office hours and I'll explain to you why. Because if you go to charge video or LLM and try to study with it, it's going to confuse you more than it. And I tried with couple of students some practice. We put the question of the student we go to charge.
(13:32) It gives you so much and you don't know what to learn and what not to learn. You just ask what is a decision tree? It's three, four, five, six pages long. Okay? And then you start saying, "Oh my god, there is something there. Did Pablo mention that?" No, no, let me read that. And before you know, you're all overwhelmed and you lose your confidence.
(13:52) What is great for LLMs is to answer questions when you know the big picture. And what is good for office hours and especially as that as we've been doing it for infinite amount of time is that we're able to give you what is the big picture how things connect and what is important and what is not. So please if you're still having little difficulty use the office hours is your best bet.
(14:18) My office hours is Monday from 5:00 p.m. to 9:00 a.m. I'm there for 4 hours. So use the office hours. My office hours been used now. I told a lot of people to start coming. Uh Kevin, how many you have? Four last time. Four, four. Kevin, now start getting Chris is one or two. Last time I was looking at him, three or yes. So use our office hours.
(14:45) Um some people are do come to my office hours and can tell you we're learning right? Yeah we are learning. So it is it's not that we're very talented but we've been doing it for a while. So we know how to put the story together how to tell you what is important what's not important how things are connected and you hear me and Kevin and I here we keep connecting things. That's how learnings are. LLMs are fantastic to answer specific questions.
(15:14) LLM in my opinion are really not the best uh media to learn from scratch. Yes, I thought any other questions before I just go into bagging. Good. Why you look all worried? Okay. So, here's what I'm going to cover today is only three things. The motivation for Maggie, the details of bagging and out of bag error.
(15:44) three things I kept it short because I know there's quiz uh again this is the important things the rest you get to your LLM all right let's start with the motivation uh so we have seen trees decisional regression trees and we know that there's also the dangers of underfeeding or overfeeding and we control that by controlling the depth of the tree the problem is decision Ision trees make this very vertical and horizontal splits.
(16:19) So if you have a very kind of complicated decision boundary, let's imagine you have a circle or some shape like that. In order for me to make a nice decision boundary to go around, I have to make many splits. If I make many splits, what will happen? Overfeeding. Right? Now, if I control and I don't allow many splits, I'm going to have underfeeding.
(16:40) So though decision trees are nice, easy to understand, interpretable, easy to compute, they don't perform as well. So let's look at this example here. I have depth one, depth two, and depth three. Depth one, I'm making a split like that. Okay, which is by the way weird split to make, but anyways, so that's the first split. But then I do another split. So I have now two splits and these are the two decisions.
(17:07) One will be red and one will be green. Now at depth four now I can get the capture of the idea. But as you can see I just make a square which is not actually what happens to the data. I'm still under fitting. Let's increase it. Now I go to depth 20. I'm start getting the right shape but and eventually over fit.
(17:27) But now this is kind of a circular shape. Right. So it seems with some number of splits I can get the circle right. But if you have more and more complex decision boundaries, that means I have to make too many splits and if I do too many splits, I overfeit. Okay, so that's a little bit of a problem.
(17:44) So what shall we do? So underfeed overfeed. What shall we do? Okay, I'm sorry. This is one more thing. So we have two ways to control for overfeitting. Two approaches. One, we control the depth of the tree and the other is to let the tree grow and we shrink it by pruning. Right. All right. All right. So, I'm going to go straight to back.
(18:26) Those slides are what I was saying just so we don't need to talk about it again. So what is bugging? Okay. So the example that I'm going to get is the following. You go to the doctor and you get an opinion. Okay. So let's say you have some in this case we have some MRI scans. I get to to the doctor. I get an opinion. You take the opinion and do what? Let's think about it. Life experience.
(18:55) you go to another doctor, right? I mean, especially if something serious, at least most of us, you don't just go to one doctor and you get an opinion and you walk away, right? You want a second opinion, right? And that's exactly the idea here. So, what we want to do is the following.
(19:20) We go to the we take the scans, we go to the first specialist and we go to multiple doctors and each doctor hopefully has a different perspective. And now what I'm going to do, let's say I went to three doctors, uh, and two doctors said yes, you need an operation, one doctor says no, then you go with a yes. If it's the if if if all of them say no, you're happy. If all of them say yes, you follow the thing.
(19:45) If it's one or two, you decide on the pluradit. Right? So here is the idea. So I go I get my scans. I go to different doctor. Hospital A, hospital B, hospital C. Uh the doctors in hospital A tells me yes or no. In hospital B, yes or no. And the other way around and all of them give you an opinion, right? All right. So then what we do? We just put them all together. So if it's yes, no, yes, no.
(20:12) I'm gonna in this case I say no because the majority say no. Is that idea clear? Should be clear. We do that every time all the time. In order to do something you usually don't want one opinion. You want multiple opinions. Okay. So here is the point that before I move on I want you to think if every any doctor or any model is overfitting that means it's going to be sensitive to your data right what is the way to avoid that is by maybe use multiple opinions and the hope is that the error that each model
(20:52) introduces will wash out and statist istics work in this beautiful way and actually it's a it's a nice story to tell because the first time someone was doing OS ordinarily square it was actually an astronom took it later and he was trying to predict the orbits of uh I believe Jupiter planets and at that time the idea is that you I don't know you have six unknowns it's a six orital elements for one of the planets so you needed six measurements Right? Actually six. Yes, there six unknowns, three positions, three velocities, six unknown. So I need
(21:30) three six equations. So I need actually three measurements because I have velocity and position. So that was the idea. I have n unknowns. I have n equations. I should be able to solve it. Right? So that's where things got a little bit stuck. They couldn't solve these equations because they're very tricky.
(21:50) And someone said, well, what if we make more measurements? And the people say, "Ooh, more measurements that oversp specifies the problem." So then Gaus of course formalize it by saying, "Look, if I measure the error and I have six measurements, I have certain error. If I do 20 measurements, the error actually will go down because now I'm using the the numbers, right?" And it was a a paradigm shift in some ways. It was late 1800s if I'm not mistaken.
(22:21) This was kind of a paradigm shift in the idea that if you add more and more measurements or more and more models, what will happen is your error will not go up. It will go down. For all you, it seems like a very comfortable idea. You don't sleep to question. But think about it. In late 1800s, adding more measurements, you adding more error. So they're expecting it actually to do worse.
(22:43) But the idea that error is random and the idea that error will wash away. It was one of the first ideas and I would call that the birth of statistics. Yep. All right. So back to our case. So we have now three doctors, three models. We combine them to get one.
(23:10) So the idea here is that if I do this I may be able to reduce the variance reduce the fact that every model has its own little overfitting okay so let's look at that in details so I have some training data set I train in this case n models and I have n predictions so what I will be doing is just final prediction for classification will be the plurality the majority for regression will be the average right all good that should be clear so what I do I now I have n models and each model makes a prediction and if it's a regression I average the prediction if it's a classification I take the
(23:47) majority okay that should be straightforward so so the final prediction always will be some accuracy or something like that all So there's many different ways of doing this ensemble learning or a sample thing. The first one is bagging which I'm going to talk to today. The next one should be random forest which is not in my list for some reason.
(24:18) Uh it should be bagging, random forest, boosting, stacking and blending and mixture of experts. We will cover all of them. Okay, these are all based on the same idea that if I have one tree, one model, I can do so much. If I have many of them, depending how I combine them, how I train them, I can do better.
(24:38) Okay, so we're going to start with the simplest one, which is bugging. And on Wednesday, I'm talk I'm going to be talking about random forest. Then we're going to boosting two ways of doing boosting. And then the last lecture, we're going to be talking about stacking, blending, and mixture of experts. So all of the unsampled methods for you in the next two four five lectures. Okay.
(25:03) So in this lecture we will we focus on back. Now the question is wait how do these doctors and models learn? What data set do they see? Right in this particular lecture today our models 1 2 3 4 5 and models will keep them the same. decision trees, okay, or regression trees, okay? In a two weeks, we start mixing models, mixing architecture. But for now, let's try with the simplest thing. I have n models.
(25:36) Each one of them is a decision tree. Each of them has the same architecture. The only difference now is what? I have different training data. And the question is, and you should know the answer. I want to train n models and I want to have n training data set. But wait, I only have one training set. What shall I do? Split the training set. Not a wrong answer.
(26:15) Uh but that means every model sees a smaller training set. Okay. So it will work. But now you don't give your models the chance to learn because now the training set is smaller for each one of what's the other method we learn bootstrapping. Can Yes. Good. Bootstrapping. Of course, we're going to do the same thing. So here it is. Bootstrapping. Okay.
(26:46) Now the moment you're all waiting is that what's the big deal about bugging? Here it is. So we have bootstrap bootstrap data sets. Yep. If we bootstrap we create different data sets and each data set will be used to train one decision tree. And now I have an ensemble of decision trees that I can aggregate. Correct? All right. So what is bootstrapping? Come on. They know this.
(27:14) But let's remind you what bootstrapping is. I have the data set here. MRI scans for five patient. I have label. It tells me that's what I'm going to be training. I get five in this case three u new data says bootstrap data set. You remember bootstrap is with replacement.
(27:37) So some of them will appear twice some of them will be out and then I'm going to be take that and okay here we are. So we're going to combine and sample a bootstrapping together. So bootstrapping data train different trees aggregate. So bootstrap are you ready? Plus aggregating if you put them together what you get. Okay. So is B A G G I N G. Uh, I know with my New Jersey accent maybe you don't see the difference between a bag and a bag because these things are a little bit tricky for me. Uh, but this is bagging with an A, not bag with a U.
(28:24) Okay. U, I think I'm going to let Kevin say the difference because I'm having difficulty with these two words. But you got the idea. This is B A G G I N G which is bugging which is bootstrap aggregation. Isn't it brilliant? Yeah. All right. So that's it. This was introduced in 1996 uh for bootstrap aggregating. So you get your data, you created different training sets by bootstrapping.
(28:53) Um the other idea of splitting is not actually wrong. It's just suboptimal. Yeah. And then we get three three and then we do prediction X. That's it. End of the lecture. I'm done. I have nothing else to talk about. Uh so for classification we do that. Uh so let's summarize bootstrap we generate multiple samples.
(29:19) So training via bootstrap aggregate for a given input we output the average of all the trees. Any questions about because that's the basic idea. I'm going to talk of course a few more things but this idea should be extremely straightforward. Yeah I got thumbs up. One student gave me thumbs up. Okay. Are we good? back top VIP section. Yep. All right. All right. So now let's look at this in a little bit different way.
(29:47) So what we have here is I have my data set. I bootstrap of course and I created a bunch of different models and what I'm overplotting here is the different decision areas for 10 different models. Right? Okay. So what do we see here for max step two? You notice that these boundaries or the areas don't change too much for max step five they change a little max step 100 it has changed quite a lot okay so if I go and do that for now 100 bootstraps now if you start looking at here what you see is that you start getting the details without actually varying too much the reason is because
(30:31) when we wash it out Sorry. When we sum them up, we reduce the variance. So we have a lot of trees. Each tree here is a depth 100. Each one of them has a depth 100 tree will have high variance. You remember overfeitting, high variance, low bias. That thing the same applies here and we saw it before.
(30:57) But now what I do, I get a lot of trees that overfeeding. But I aggregate them. So you can see that the overfeeding is kind of grayish areas but because we don't overfeed always the same way that will go away. Okay, that's the kind of the intuition I want you to get. Right? So the error that each tree is making the overfeeding that each deep tree is getting is going to be different from the other one.
(31:25) So now if I aggregate those those errors will go away on average. Okay, this concept is so I do it here for multiple 150. All right. Now, the advantage of bootstrapping bagging is the following. Remember when I was doing a single decision tree, I said I need to go deep in order to get complex decision boundaries.
(31:54) But I was kind of constrained by the fact that I was over 50 feet. Now with bagging what I can do is I can go deeper. Okay? Because if I can go I'm I'm allowed to I can afford to go deeper because some of the overfeeding will go a way. Okay? So that's the main idea. Now in some blogs, some LLM, some books will tell you that in bagging you go all the way deep and you averageaging.
(32:21) I don't think it works like in practice. We still have to control for the depth of the tree. Okay. In principle, I said if you do, you can go all the way deep. I think this is for Kevin's kind of data set. You have 1,00 so. So the depth doesn't go very high. But if you Huh. Yes. big data. Uh but in general practice, we still have to control for the depth of the tree.
(32:51) We still have to control for the complexity of the tree. Even though we can go deeper than single decision tree, you don't get it totally for free. So what do we should do? How do we control our complexity? Cross validation. Excellent. And we do the same here, of course. All right. So, as we say, this is the advantage. Uh bagging will allow us to have high expressiveness expressiveness because I can go deeper.
(33:24) Uh and it's going to have lower variance because I am aggregating I'm averaging out. Okay. So, uh see this is a a visual of a bugging for classification. This is for diabetics uh diabetic uh uh data set. Do you see who make that Kevin? This is Rahul. Yes.
(33:52) So uh what we have here is four bootstrap four data sets and we trained four trees and you can see the first tree has glucose at the top some split then blood pressure BMI glucose blood pressure glucose diabetic and then here you can see what I want you to focus is at the leaf nodes they're very different from each other why because at the leaf nodes is where the overfeeding happens right so by having different ones.
(34:18) Now let's say I get one new data point. I may end up in this one. We said glucose. This we said diabetes pedigree function and this PMI and this age. So you see now I have different opinions for what causes diabetic diabetes and therefore if you average them out the hope is that you get the right answer. Okay. All right.
(34:43) So let's see for uh regression how would it look just so you see the visual um this is for one bootstrap sample I fit one regression tree right this is a regression I don't remember what's the depth but about four three four then I do another one right I bootstrap again I get a new data set and I train a new tree and that's how it looks like let's do Do it again. Three times. Well, four times.
(35:15) Well, let's put it all together. Okay. So, this is a bunch of the the thin lines is a bunch of individual decision trees on bootstrap samples. The thick line is average. Now, I don't know if you can see, but it smoons out some of the jiggly zigzag behavior. And that's how you smooth it out. And I think for regression for me it's easier to see that if you do this aggregation, this averaging, you average some of the zigzag that you get out. Good point to make.
(35:50) We're talking about bagging with decision trees. Can I do bagging for linear regression? Yes, the answer is yes, you could do that. It's a way for avoiding overfeeding. is a regularization but why did then Kevin and I told you Kevin's false no is uh because you know we're trying to give you by double check so you can channel so we try to not overload at the beginning we do but the same idea of aggregation this way can apply to linear regression logistic regression neural networks deep neural networks entries just neuron will be too expensive. So we
(36:36) avoid doing but this is a it's a one way of regularization. Why do I call it regularization all of a sudden? Because we're trying to avoid overfeitting. Anything we do to our algorithm to avoid overfitting, I'm calling it regularization. Okay, cool. Any questions? Now the drawbacks that we have for bagging is the following.
(37:12) You remember I start selling you decision trees under one couple of selling points. I stood here in front of you and said trees are interpretable. Good, right? But now they're not so much anymore because I used to say, "Hey, you are going to the party because your homework is easy.
(37:37) This is my decision tree, right? Or this is a lemon because the width is so much and the depth is so much." Those are interpretable models. But here, what shall I say? Most of the trees say this, but I don't have the same kind of logical flow anymore because I have too many trees, right? H that's no good. All right.
(37:57) So what are we going to do about that? Uh we're going to use permutation importance and MDI which stands mean decrease in impurity which I'm going to postpone until Wednesday because it applies to random forest. So I'm not going to talk about it now. I'm going to talk on Wednesday. Okay. So but there's a way to deal with that. is not as nice and elegant as oh is a tree is a lemon because it has with that and hide that or is a a kiwi because it's a furry on the outside or something like that.
(38:27) Okay, so we're going to we're going to look at it a little bit later. But now how about underfeitting or overfeitting? So I said there's two drawbacks. One interpretability the other is underfeitting overfeeding. Meaning that the same issues appear on bagging as they appear on single decision trees. There's a dangers of overfeitting and there's a dangers of underfeitting.
(38:53) We have to control the complexity or something to avoid underfeitting and to avoid overfeitting right and for single decision trees we did it with stopping criteria and pruning. So we're going to try to do the same thing. So what I'm saying here is that the number of bootstrap the number that you can get away for eliminating every single um overfilling aspect of this. So let's control again for depth or something.
(39:26) So I think this is what I'm going to do here. So the first thing I want to do is the stopping condition. So in H3 still uses a stopping condition. max depth, minimum sample per leaf, etc. This behaves is just like a single tree. Let's look at it. The depth of the tree or the complexity in the x-axis, the error in the y-axis, training error will be going down as we make the tree more and more complex. It's a training error.
(39:53) Of course, it will go down. Validation at one point is going to start going up. And somewhere here is my my tree that I want. Okay, you've seen this before. You've done this before. This should not be surprised, right? We control for complex. You can do the same for pruning. But now let's do one more thing.
(40:18) Now this story will continue until you reach my age, which is the following. You get your model, you make it more complex, more sophisticated. But every time you do that, you add also some complexity. No free lunch. In this case, we added trees, right? we got a an ensemble. So yes, we reduce variance. Yes, we reduce overfeed and we get more expressiveness.
(40:44) But at what cost? The cost is that now we have one more thing to control which is what is the thing we added between decision and bugging? What is what we did? We just a good a bunch of trees, right? So what is the question? What is what we introduce now that we need to control? Huh? Number of trees of course right we're talking about forest the forest five trees 100 trees bagging is an aggregation of many trees.
(41:25) So we have to control for that. Controlling means I have to decide what is the number of trees in skarn is called number of estimators. And so one thing that you have to be aware is that the number of estimators if you think about it is not controlling the complexity meaning that if I put more trees does it reduce the complexity or increase the complexity? None. what it does it reduces the variance. Okay.
(41:58) So it's not like the maximum depth or something that you make it larger you start overfeitting and it's not like K andN which is the smaller the K more the complex is not controlling complex it is controlling variance. So let's look at it here. So as I increase the number of estimators which is the number of trees and I use the number of estimators because that's what sklearn uses but I have it here number of trees right that means the number of bootstraps the that thing right so as I increase the number of estimators what will happen is the
(42:35) training error is start going up Why does it make sense? Should it go up or did I make a mistake? Now, you have to think this is at a fixed depth of the tree. I'm not changing the depth of the tree. Well, if I have one tree and overfeit, so the training error should be somewhere.
(43:08) If I now aggregate, now the overfeitting goes down. I am overfeitting less. So, it's going to go a little bit up. Okay, it's unavoidable. This will depend, of course, on the max depth you have. The level that you go up from here to here, it will depend how much you overfeed at one tree. If you're overfeeding a lot, then it's going to go much up.
(43:35) If you're not overfeeding already, so you have already optimized for the max step, you may see this being very flat. Okay, let me repeat this. The fact that the training error will go up is because now when I aggregate I'm washing or I'm removing some of the overfeitting, right? So if because I average them, right? So if your tree is the for number of estimators here, the depth of the tree it's very high. So that means I'm overfeitting a lot for one tree. Then this is going to go up.
(44:08) If the if the single tree max depth is already fine-tuned not to overfeit, it's going to go very slow. Sometimes you don't even going up. Okay? It's just going to go very slowly up. Now, let's look at the validation error. Oh, look at this beauty. Look at this horse. This beautiful.
(44:34) Look at how much I reduce the error on my validation by increasing the number of estimators. That's the key. The training you lose a little bit but we don't care. But look at the validation. It went down. Boom. I'm reducing a lot of variance from the team. Yes. What happens when it overs and when it's how when it unders what happens? Uh the question is your name? Okay. Katie. Yeah. Katie asked a very good question is the following.
(45:06) This will go up if you're overfeeding for a single tree. What will happen if your single tree is underfeeding? And if it's underfeeding and average, I think it's going to stay very much constant. You underfeed, you underfeed, you underfeed. I don't have I don't think it's going to go up. Yeah. Huh? Yeah. I mean, it could go up. They could go out a little bit, go down, but it's more stable because you underfeed.
(45:33) If you get 10 trees that underfeed and you add them together, it's still going to underfeit, right? Yeah. Good question. No. Yes. So, previously in your slide, you say backing drives the benefit of high expressive, which means we increase these deeper trees. But why we still need to control the the deeps of the Okay, so the question is um I said bagging will allow us for more expressiveness meaning deeper tree and the question why do you still have to control it? That's the question right? Yeah. So as I mentioned you can't get
(46:12) everything for free meaning that if I go too expressive right I'm still going to overfeit right? So in principle I said you can go all the way to full tree and then you had a lot of bootstraps and worse the overfeeding away right but it doesn't work like that because a if the tree is very deep then your bootstraps how many trees you have to do you now you pay computation cost plus after a while your bootstrap samples are correlated so you're not gaining much so practically speaking I do I'm allowed I'm allowed it does work to have deeper trees but can go all
(46:56) the way to the full tree right okay all right so now this is overfeitting and underf for decision tree versus bagging I'm going to skip that for now I'll come back to it later is to compare decision trees and bagging how they behave differently uh you can see that in the bagging that my underfeeding would go away faster. Okay. All right.
(47:22) So this uh kind of summarize everything for this part. I have one more section to go but not so much. What are the limitation of a single decision tree models and how does bagging address this limitation? Let's actually instead of me reading it, I'll leave it there for I'm going to play a game.
(47:46) I'm going to leave it there for 15 seconds, 10 seconds for you to think and then I'll show it to you. Right? You just better if you try to think it. It shouldn't be so difficult because we just talk about it and then it gives me a time to pace around and look at you and make you feel uncomfortable. All right. So the answer I have single decision trees particular deep ones are prone to overfeitting and may have high variance bagging addresses this limitation by creating an assemble multiple decisions you train on different bootstrap samples of the date by averaging the prediction of these
(48:16) trees. Bagging reduces variance in MIG. Explain the concept of bootstrapping and it significance of bugging. Come on. I don't even Yes. recycling technique. Boom. Good. Next one. How does bagging leverage and sample learning to improve prediction accuracy. By the way, these are not trick questions as you can see. I'm just repeating what I said in SL.
(49:05) It's just good to think it and put it there. And people told me they like this that I summarize everything. you know what I'm expecting you to learn, you get some thing to go forward. Okay. So, bagging utilizes sample learning by combining the predictions of multiple trees. That's the end of the story.
(49:22) The rest is words. It's basically just combining by averaging or get a plurality. All right. Next one. Describe the aggregation process in bagging both. Come on. Um what are the advantage of bugging over single decision tree models? which is basically um prediction accuracies by reducing variance and and overfitting ah reducing overfeed improve generalization and robustness.
(50:03) Um okay so these are there it's just getting you to start thinking and I said it bagging also random forest also any of these are sample methods they lose this beautiful interpretable thing that you can just reason you can write the reasons of why a decision was made and we're going to have to address okay all right so I think that's I have a lot today Okay.
(50:31) All right. So, this is the cliffhanger for the next one. So, I'll be talking about cross validation. You like cross validation. But when we do cross validation, we use grid search, right? Even though you use sklearn to do it, we just try every single possible scenario.
(50:59) Right? Now as we get the model more complex let's see what are the hyperparameters I have in bagging let's think about what are they number one is the number of trees correct number two is some complexity measure the max the depth of the tree right number three which stopping condition to use and number four. What's the number four? Yes. What is the fourth one? It's hidden.
(51:39) There's one more kind of genet that's a choice you have to make, right? Okay. So, four now do cross validation from this. You need to choose genie entropy or mclassification. That's one axis. One, two, three. Now I go to the other axis which was um already forgot uh which stopping condition.
(52:10) For each one of them maximum depth, minimum lift notes da da da. That's another axis. Then for each one of them I have maximum depth. One, two, three, four, five. Minimum lift do. Now I'm adding another dimension which is the bootstrapping. Good. Now you see the problem. Let's say you have three for genie or whatever the the criteria.
(52:36) Let's say we do four for u stopping conditions and then you do for each one of them you do 10 and for the bootstraps you do another 10. 10 x 10 x 4* now I'm already about thousand right actually 2,000 so I need to to I need to feed 2,00 3,000 tricks and compare and find out which one I like and remember book is cross validation do I do it k times you chart so that means you're going to be waiting here for a few hours all right now with that in mind let's talk about the Cool new idea to avoid some of this cost. I'll go back. Did you like my cliffhanger?
(53:21) Let's go for a commercial break. Oh, I thought you going to put some blood What am I shooting? Should be Where the hell is it? Okay, part two. Now we are in Mexico.
(54:36) Tina Capaceta Brian. Brian, I'm telling you, this been the partner for the last three years. Whoever's picture is there usually is not in class. Uh, so I'm not surprised. So we're gonna be talking about out of bag error. Here is in Mexico where magic happens. So let's start with a simple example to explain that.
(55:01) I'm going to finish exactly on time. Watch me. So here's a simple example. So remember we starting with bootstrap. Okay. So here's my data set. This only thing I have. I have a in my data set. I have a cow, a cat, a horse, a dog, and a rat. Okay. Now, I'm going to do bootstrapping with replacement. You know that, right? So, these my three ones.
(55:28) The first one has dog, cat, dog, horse, horse, and dog. And you say, "Wait, why is horse twice?" Because I do with replacement. Yep. And the second one, I have rat, rat, cow, dog, and cow. And the third one, dog, rat, horse, cat, and rat. I like the first one. It has no rats. All right. So, the first one does not have rat or a cow. The second one.
(55:59) Did I miss something or didn't? Oh, okay. There's a slide. What am I missing? It doesn't matter. So, the point is the first one when I bootstrap some of the samples were left out. Okay. So in this case the rat and the cow are not included. This one the horse is not included and this one something is not included or maybe nothing is not included. Okay.
(56:29) All right. So let's look at that. So the out of bag estimate is a method to determine the prediction error while being trained. Okay. So the idea is very simple but I'm going to go slowly so you get the whole idea closely.
(56:50) So what I'm going to do I'm going to use these things that have not been in my training as a way to do validation. Okay. So let me step you do step by step. Uh so here is my original data set. I said I'm going to go a little bit slowly. So I have some x and some y and I have some values of x and some values of y. X1 to Xn, Y1 to Y. Yep. So the first thing I do, I do my bootstrap sample and I have X1, Y1, X3, etc. Remember is with replacement.
(57:22) So I'm going to use that to build a tree. Remember we take the bootstrap sample, we built a tree. No. Yeah. So now let's look at the data. And I have red the ones they use and the green that not be used. X2 and Y2 are not used. X4 and Y4 are not used. Ta light bulbs. You see this pop this model this tree is trained on the red data not on the green one.
(57:52) Remember when we validate we validate on a data set that has not been used for training. Boom. Okay. Now I'm going to carry this a little more. So second sample again I do my bootstrap. I train a second tree. The green points there I'm not being used for that tree. Mhm. Okay. Now what I'm going to do I'm going to go the other way around and pay attention to this is the trickiest part of the lecture today.
(58:22) Not so tricky but you have to pay attention. I'm going to take one data point in my data set and I'm going to ask the question which trees have not use that data point in the trade. Yeah. So these for example for for point X I Yi any of these points these that the trees that they have not used that data point in the train.
(58:55) So what shall I do? Make prediction for that one. And that will be your So I make predictions. Identify observation train model have not seen. Get the prediction for this observation from the model. And now I'm going to do the majority classification for that and the average for regression. And that will be let's look at it.
(59:26) So for point I the point error the point is the average of all the trees that have not use that point to predict right. So it's like a validation. So the total pointwise out of bag error is the number right that gives me one or two right zero and one right for regression I'm averaging the predictions from trees that they have not used this data point to train you all falling asleep come on pay attention this is a little bit tricky but you have to pay attention so I'm taking my point I look at it which trees have not used it these ones give me the predictions you that you haven't seen me
(1:00:13) and I averaged and then the final error of course will be the the MSE in this case and this is mclassification error right so I get the prediction from the trees that have not used this point that give me the answer and now I calculate the MSE or the classification error all right so I'm going to be doing this for all of them right so now what I I average over all the points the error the mclassification error and that will be my mclassification error and for regression it's just the average of MS of the let's repeat it for each point there's some trees I have not used
(1:00:56) that point for training I'm evaluating it I take the average or the aggregation or the majority I use that as the prediction and calculate with that error And I do it over everything. Now notice something. I didn't have to use cross validation. I didn't have to split my data into multiple things.
(1:01:20) What did I do? The same data as I go through I said bootstrap. Okay, this I have not used. Let's predict on those and we'll go on. Right. So this called out of back error. And so given a training set and an assembled method, we compute the out of back error one for each point X.
(1:01:41) In the training set, we average the predicted output to do only use the B trees with bootstrap training set exclude this point. We compute the error for this average prediction. We call this the pointwise out of back error and we average the pointwise out of back error over the full training set. Uh, now actually before I show that, let's stop for a second to see if we have any questions.
(1:02:07) Did you digest it? 50%. 30%. 0%. No. Yes. And uh how do we find we actually let me turn it around because it's important. I take for each point the trees that have not used that point and I make prediction for those I have arrange them and that's my prediction the validation prediction for that point and then I do it for every point and I then I take the whole thing together. Okay. It's the same thing but just I just turn it around in order to be able to do any other questions.
(1:02:58) Yeah. Um yeah. So what is the point that said if we're going to use sample work points to evaluation that people use the test set to select models okay let me besides the joke let me explain oh that So we know why we're doing the training data, right? To to fit for the parameters, the betas for linear regression, the betas for logistic regression and the splits here. Right? That's my training then.
(1:03:48) Now the question is why do we need validation set at all? Why don't we use the tra the testing anyway? And Chris is about to kill himself. I'm telling you, why do I need validation and why do I need testic? Andrea, you move from there. No. Um goodness and I said to the accurate model the validation use test the goodness of the hybrid parameter and the test for the goodness of the overall final model. Good. Great.
(1:04:47) Let me repeat it in my own words. It's correct what Andrew said. We use the validation to select among models or decide on the hybrid parameters and we use the testing just for the final evaluation of the selected model. Right. Yep. And I said you remember this if you use your test set to select the hybrid parameter or the model is there's a special place in in hell for people they do this don't use your test set to select your model use your validation to select your model I said it many times okay three things training date to find the parameters validation to select hyperparameters or
(1:05:30) select different choices different models test is just The last thing you do just to tell this is my final model. This is the performance. Let's get one more thing straight. Why do I use cross validation? What's wrong with one validation set? Yeah, you do. Very subjective to the randomness of displaying your data.
(1:05:58) So you try a lot a lot of models, what you're going to end up you're going to find one model that works just for that data set. Good. Any other questions before I wrap up for today? Think about this as a review for your quiz. NECA based on validation question is that good question is is that okay this out of bag plays the role of validation do we still have test data yes we the answer is yes so we start with your data you do train test split like skarn does for you so there you don't have to do any shenanigans You put the test site on the side. Don't
(1:06:46) touch it. Put it on the vault until the very end. You use out of bag error as a way to select model as a way to validate to decide is it genex or is it that is it what's the depth of my tree? What is the number of estimators? All these things, right? That's what you out of back which substitute the validation error.
(1:07:10) So you don't have to do that and you save time and you save data. But the test you wanted good more questions. Okay. So by the way just don't raise your hand because I don't have a fly eyes. So I get to see where you um the question I'm going to refresh and just give me thumbs up if I got it. There is a scenario that I showed before the rat, the cow and the horse that some of the points will be not used less time or more time right so it's not balancing good question anybody has an insight to that what happened what do you think it will Should I worry or should I not?
(1:08:31) Would you worry? No. But you should make a So here's your PhD depends on it. Now you have to make a guess there. What would you do? Nobody has an insight. Welcome anybody any idea? So if what Andre is saying look bootstrap will have some natural imbalance in the samples. What will I do about that? Nothing. Large numbers take care of it.
(1:09:24) If we're worry about that, bootstrap will be totally useless. Actually, sampling from a population will be totally useless. Right? So, for example, if I want to see the po the height of the average student, the height of a student Harvard and I use you guys as my sample. Of course, there's a chance that I get this particular part of the population.
(1:09:49) uh should we like control about that in principle? Yes. But in general, we just hope that the large number will take care of it. Yep. Yeah. So that's actually something that happens a lot. Randomness is your friend in many cases. It helps you wash out overfeeding uh things like that, biasing in your data and things like that. And it's like missing at random as Kevin said.
(1:10:15) All right. Next question. Um, okay. So, this is the this is a little bit tricky and I don't want you to get too wrapped up about that. Uh, but I wanted to point out and we can discuss this more in office hours is this is to go beyond the obvious thing right is the question is is out of back better than cross validation.
(1:10:46) The answer it is yes because in cross validation the data are used the ones you validate even though you average are used in some of the training set. So there is a little bit of danger of leakage there. And this plot this paper show that in general the tests in bagging are much more robust than tests with uh I mean with cross validation than ah OB is much more robust than cross validation.
(1:11:15) I will leave that as good to know. I won't ask in the quiz uh because it's actually quite complicated to go down the rabbit path and think about it. But it is true that if you think about it in cross validation every data set has been used for training even though it's not the same time there may be some information that they leak from there to the other. So OB in general is better. All right.
(1:11:41) So we talk about the drawbacks of so interpreability is going to be still something we worry and I want to leave you with a cliffhanger again which is the following. I want you to look at this and I want you to look at it with a critical point of view and tell me if you see any problem. This is bagging.
(1:12:04) We're happy. We're going to take care of the interperability. We took care of our feeding. We took care of everything. But look at this. Predict it. This is your model. And you think what is wrong with this? And if you don't know, that's okay. I'll talk to you on Wednesday, but you have to come on Wednesday to find the answer.
(1:12:37) Yes, John. Right. Oh my god, that's good. Like I mean I don't know what you mean, right? But that's not the That's fine. The notes are different. The notes are different like in each level in each level. Actually, this exactly the opposite that I'm worried about. First, you guys be quiet. All right. I'm not going to tell you.
(1:13:18) I give you one more minute. By the way, the the tree of the day is birch. I need to write it. Uh while you're thinking, actually, yeah, birch tree is the tree of the day. By the way, coming to class, you're supposed to be here most of the time. Showing up 5 minutes before does not count. And as I say, I know.
(1:14:06) So don't count for your rates at the end. All right. Birch B I R C H. Right. Correct. Yeah. Beautiful tree. Especially the All right. So to finish this class, uh the thing I wanted you to see and you haven't seen it is that the top note for each tree is the same. And that's no good. It means the trees are correlated.
(1:14:38) And as I said, for next Wednesday on the three mysteries unveiled, can't trees even truly be independent? The secrets unraveled. Tune in and I lock the enigma only at the Wednesday lecture. I'll tell you the whole thing on Wednesday. Thank you guys.