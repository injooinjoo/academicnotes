109 day 12 - YouTube
https://www.youtube.com/watch?v=G9dw2IA3gH0

Transcript:
(00:01) We got another minute. I'm just testing. Talks amongst yourselves. Just pro just practice problems. Announcements. Just Good morning data scientists. Welcome to another edition of 109A 109A 209A E109A. Uh, a couple announcements to get you started. Don't forget project stuff.
(01:10) What's happening with projects right now? What's due when? By the end of the week, pick your projects with your group members and submit that. So, it's pretty straightforward. Not a whole lot going on. shouldn't be too much of a task, but finding those project partners is important. Okay.
(01:41) What else is on your mind with regards to this course? Midterm, right? Midterm. When is it? Next week in section in section. And it will be the entire section. It will be roughly twice as many multiplechoice problems and a few extra fill-in- thelank short answer problems. So, it'll be a little bit more than twice as long as the quiz. 2.2. We'll say version 2.2.
(02:13) Oh, the answer key to the last quiz. Thank you. Yes, we'll make sure that's posted. Yep. Answer key to the last quiz. And then we'll also have extra practice problems too. Yeah. So there will definitely be practice questions, practice problems. Uh look for that to be released on Friday. Sort of like an ed quiz uh for for practice for review.
(02:38) As far as practice coding, let me come back to you on that. But the midterm itself in class in section 75 minutes, you get how many sheets of cheat sheets? Yeah, that's what I thought. Two cheat sheets for the midterm, right? You got one for the quiz, you get two for the midterm, front and back. And then there's a take-home coding portion.
(03:02) No AI, no uh um LLM for that, but you can sort of use our notes however you want. Okay. The coding portion is out of class, after class. Uh, and they get how long to do that? 24 hours, three hours. Did we determine that? There's a there's a window in which there's a window and then there's three hours. Two hours.
(03:27) So, we'll let you know what the details are. It's either two hours to do the coding or possibly probably I think for the final it's different, but two hours. is expected to be two hours for the coding. Expect it to be two hours for the coding. Yeah, I probably should give Chris a microphone.
(03:50) Don't expect it to actually need two hours, but you are given two hours in case you have any stumbles. homework. Like two weeks after that, so homework 3 is not due for a while. Yeah. Yeah. Homework three. If you look at Canvas on the due date, it's like November 4th or something. Yeah. Yeah. Yeah. So, we expect you to be working on homework 3, to have started homework 3 because homework 3 is covering a lot of the material on the midterm and if you haven't even touched it, you're at a disadvantage when the midterm comes around.
(04:33) Okay? So, we expect you to work on it, read through it, but realize all the details of tidying up all of your answers, doing all the coding is not due until a week and a half or so after the midterm is is over. Will there be a practice for the coding? Again, we'll see what we can do for a practice for the coding.
(04:55) We'll see what No promises. No promises. What are you going to be expected to be tested on with the coding? What you did on on homeworks, you know, what you did in sections. Yeah, homework is your best test. Homework and the section material are your best tests. expect to maybe do something with like fitting models some sort of there will be no web scraping. Yeah.
(05:26) Yeah. Yeah. I would definitely say there's going to be no web scraping because I wouldn't be able to pass that. Yeah, there's a calendar on Canvas for dates. All right. So, we'll get into all these details. Look for an announcement on Canvas and Ed. So all these little details just hold off. Just realize there's a coding portion. Take home two hours after the midterm itself. That's in class 75 minutes.
(05:52) Closed book, but two cheat sheets. That's conceptual in nature. Lots of multiple choice and a few short answer. The coding is coding. Don't expect it to have to take you two hours, but we give you that window in case you get tripped up. Okay, we'll talk more about that next week. Last question. topics. All topics through today. All topics through today.
(06:17) And so I'll lead us to today's lecture notes. So we're going to tie up the ideas with phase today. We're going to talk a little bit about big data a little bit more than we've already talked about it. Talk uh automated way of handling big data situations and that is PCA. And then we'll talk a little bit about review.
(06:40) And you can think about that review as just a little bit of review before the midterm. Okay? And so this is kind of the end of the material that will be tested on the midterm after today's lecture. All the stuff next week which is now doing classification modeling is not going to be on the midterm. Though you know stuff in that will be useful conceptually and of course we'll be on the final exam.
(07:06) Okay. Murmurmer murmur. Baze. What is baze? All about that baze. What is baze? Who is baze? Why is baze? How do we baze? Who's baze? Thomas bae. Who was he? I don't know. Some person who did genetics and stuff, you know, and that's where he used the Baze formula a bunch of times. But it's based on a formula. And how do we use it? We use it in inference.
(07:49) We use it to update our belief in an unknown parameter for your model, for your population. And so we say, oh, we think the mean of a normal distribution going into the problem is somewhere around 20, maybe between 10 and 30. We collect some data and based on the likelihood of seeing that data, now we can kind of hone in on that parameter not being between 10 and 30, but maybe closer to Xbar, but we're going to put a whole distribution around that, and it's going to be pulled a little bit based on our prior information. Okay. When we go through
(08:24) this whole process, we get a entire posterior distribution for our set of parameters in our model. In linear regression, what are our parameters in our model? What are our parameters in linear regression? If you don't know this, you're going to be in trouble for the midterm. Parameters in linear regression. Say it. Just say it out loud. betas mostly.
(08:53) How many of those do we got? P + one of those. P being the number of predictors plus one because we're always going to have the intercept unless it's a weird situation. Okay, so P + one beta parameters and a sigma squared, a variance term or sigma a standard deviation term. Okay. And so those are the parameters we put prior information on, prior distribution, prior belief on, we collect our data and then we get a posterior joint distribution of all those parameters. Okay, it's ugly.
(09:26) It's complicated. Okay, how do you look at a whole joint distribution? It's difficult to conceive of what that would look like, especially when it's multi-dimensional. So what do we do instead? Instead of just thinking about that whole distribution, we start thinking about simulating that posterior distribution. And this is just a rough framework for how to do that. We'll implement it after the midterm.
(09:50) Okay, we're coming back to bay stuff later. All right. So in bay we're going to have to do a lot of computation because we get this multi- uh multiparameter distribution and inferences are going to be pretty tough to develop and calculate under those settings. When we have nice well-known posterior distributions that have names like a normal distribution or multivariable normal distribution were in great shape. Okay, we had this normal gamma distribution when we were dealing with linear regression and that's not
(10:20) easy to handle. But when those things are closed form, figuring out the posterior mean, posterior median, posterior mode, they're all easy to do. Okay, you know a normal distribution, where's the mean, median, and mode all in the same place where mu is. Okay, as an example. But if it's not well known because your prior is not a conjugate, what does it mean to be conjugate? What's a conjugate prior distribution mean? The posterior follows the same family of distributions as the prior. So if you started off putting a prior distribution
(10:59) on your parameter to be the mean based on a normal distribution, you collect your data. As a result, your posterior that you're going to perform your inferences with is going to also be normal.
(11:17) Okay? or if you start off with a beta distribution, which we'll get to after the midterm, you have a binomial distribution with your data and then you have a beta distribution in the end. Okay? So, we'll talk about that beta binomial distribution, too. Great. But if it's not well known because it's not conjugate or not of closed form, we're going to have to kind of use a different approach to doing that. And that's where we're going to use these numerical methods.
(11:36) We're going to do the simulation and do computational uh performance to do cal uh calculate inferences mainly those credible intervals and simulating from that distribution is the way to go. Lots of ways to simulate from a distribution. Uh the posterior might have closed form but it's tough to think about uh writing it down in closed form solution.
(12:00) And a lot of times we can break this distribution into a conditional and a marginal. And that's generally the idea of what the normal gamma distribution was. Okay, give me a gamma distribution on the inverse for the variance. And then once I know that, then I know the normal distribution conditional on that. And so if you know that closed form solution to the posterior, you can first sample your gamma and then conditional on your gamma sample, you can then sample the normal distribution.
(12:32) You just need to know how to update from the prior to the posterior given the data. Okay. So once you figure out how to simulate and we'll talk about other ways to simulate that posterior distribution, we want to sometimes summarize that whole distribution. uh with specific values and determining the posterior mean a lot of times is easy when you simulate.
(12:57) What do you do? You collect thousands if not tens of thousands of realizations from that posterior distribution for the unknown parameter and then you just calculate a sample average to estimate that mean of the entire distribution. Right? Simulate from a distribution to estimate a mean. Calculate the sample mean. That's all we're doing here.
(13:17) Okay, but if we're trying to calculate the posterior mode, things are a little trickier. I don't know what the mode, the top peak of that smooth distribution is going to be very easily from an empirical sample of data with me. So, how do we do that? What's a method we could use to do that? You could build a histogram and you could eyeball it.
(13:50) All right, but what would be a more computational way to do that? What would you do to a histogram to try to figure out where the peak of that histogram is? Remember, histograms are prone to sort of choice of bins, where they're located, things like that. What can we do instead? No ideas. What can you do to a histogram? sample more data.
(14:23) The more you sample the data, what do you get in the histogram? More certainty in where that distribution lies. Okay? Why? Because you're trying to smooth out the bins in the histogram. How else can you smooth out the bins in a histogram? Something called a kernel density estimate. So we'll come to defining what that kernel density estimate is.
(14:50) Essentially fitting a curve to that empirical histogram that you get from your sample data. Okay. So eventually we'll get to talking about what those kernel densities are. And finding the mode is something called bump hunting uh has been called in the past as well. All right. So that's difficult. Calculating a mean of a sample of data is easy.
(15:09) Calculating a mode of a whole distribution based on a sample is tough. Calculating a credible interval. Is that easy or tough? That's going to be easy. How do you do it? We've done it a billion times. Not a billion times. What is essentially this analogous to? Where have we calculated interval from empirical estimates before? do a percentile percentile method just like we did when we did bootstrapping. It's the exact same idea.
(15:47) Just calculate the desired percentiles or quantiles from your sample of estimates, your theta, your mew. That's what these represent. And it's just like you're doing a confidence interval from a bootstrap data. take your second half two and a half percentile and your 97.5 percentile on these estimates or posterior uh empirical estimates of theta.
(16:13) Okay, lots of ways to do it as I mentioned with this normal gamma model. What do we do? Well, we know the mean distribution conditional on sigma squared is going to follow a normal and we know that the inverse of that sigma squared is going to follow a gamma conditional on your variables x's. they have closed forms.
(16:30) So how do we go about simulating this? Well, you first simulate from that gamma distribution. You now know its variables, its parameters. And so you can use something like numpy or sci to do that for you. Once you sample the sigma squared, then you can then plug that in to the formula for mu, which is conditionally no non- sigma squared.
(16:55) And it has a closed form solution and you just sample from the normal. Now you get pair-wise an estimate for sigma squar and an estimate for mu. Okay, they're joint distribution and you can plot them however you want. You can look at contour plots, things of that nature along with estimating the uh credible intervals. If things aren't that easy, there's other approaches.
(17:16) You may have heard of MCMC before, Markoff chain, Monte Carlo, and it's just several different algorithms to kind of simulate in our cases our use cases from these posterior distributions, especially when they're joint and they're not closed form. There's a lot of examples in the ways to do this. All right, we can do this through things like adaptive rejection, through Gibb sampling, and Metropolis Hastings.
(17:41) Which one we use depends on what we know about the posterior distribution. I'm not going to go too far into these. Eventually, we might come back to it. Just know at a high level, what are we trying to do? We have this posterior distribution that we don't know its closed form solution to.
(17:59) But we can estimate the height of that distribution, no matter what values of those parameters we're given. All right? And so given that height of that distribution or at least proportional to that height of the distribution, we want to be able to sample from it. Okay? in a repetitive in uh sequential way. So this general idea of doing uh first adaptive rejection sampling is sort of equivalent to thinking about throwing darts at a dart board. Okay.
(18:33) So if you know what your distribution generally looks like and you know your data in this case are uniform. We don't have to always use the uniform case are uniform on the unit circle and you're throwing darts at the xy plane. You can sample from x, sample from y independently. And then every time the dart falls inside the circle, you keep that as an actual result.
(19:01) Every time the dart throw falls outside that circle, you throw it away and repeat the process again. All right? And so we have this whole posterior distribution. And what we're doing is throwing darts at it. And if the dart falls out the range of reasonable values when we have multiple parameters, we throw it away.
(19:20) And when it falls within the range of reasonable values, we keep it or we keep it with a certain amount of probability. Okay, that's the general idea of this adaptive rejection sampling. Another approach is this idea of Gibb sampling. This is when we know things conditionally. If we know one parameter conditional on another, we might know theta 1 conditional on theta 2 and theta 3.
(19:44) Think of this in terms of like beta 0 given beta 1 and sigma squar in a linear regression model etc. Then what you can do is just start off with initial values maybe your hyperparameters the means of your hyperdistributions and then you choose a new value for one of those parameters given the sigma 2 and sigma 3. Then since you know sigma 2 given or sorry theta 2 given the other two parameters you adaptively reiterate through this process over and over again until you come to after some burn-in period you get some joint realizations in your three parameters. Okay, the general idea here is you start off with
(20:19) an initialization. You do some burn-in period to sort of let the system work and then eventually you're going to come to a distribution in the posterior that is following the general format or the general shape of the dis posterior distribution you care about.
(20:41) All right, again this is just highle ideas of sequentially updating where you think the posterior distribution is given the previous iteration. We'll come back to this and actually talk about how to implement these ideas. The Metropolis Hastings approach is a similar thing that has a little bit more uh difficult technique uh specifically. All right, so here's the idea of a metropolis hasting.
(21:04) The general idea is you have a posterior distribution. All right, let's imagine this as our height of our posterior distribution and we want to sample from it. And we're going to do this repetitively, one step at a time, sequentially. All right? And so once you're given a particular place that you sample from in this distribution, what you basically say is I want to now sample a second point from this distribution and I'm going to step in a direction of that second point.
(21:40) and we decide to accept that step in a direction either left or right based on the relative heights to the distribution. So if we started off with a really high place and we're stepping to a really low place, we're going to step into that new tail part of the distribution with very low probability. As a result, we're going to end up in the tail of the prob probability distribution with really low probability. Matching that intuition.
(22:10) If we are stepping into a higher probability region in that distribution or higher density region, we're actually going to always step in that direction. And as a result, if you iterate this process, we're going to end up building the distribution that has a shape similar to this, similar to the distribution that we are stepping based off of.
(22:32) Okay, at a high level, what is Metropolis Hastings doing? It's stepping from one place in the distribution to the other with the probability of that step proportional to the ratio of the heights of the curve at those two places. All right. And now imagine this is just one dimensional. Imagine you have hundreds of dimensional parameter space.
(22:54) It's really difficult to imagine exactly what the shape of the whole distribution is. But if you have just two places you're comparing now computationally, it's pretty easy. All right. And now we have an algorithm of a way to step through that distribution very simply. Question. If we're shape, okay, so what's the difference between this and a histogram or frequentist? approach. So now it's a completely different question.
(23:48) The whole question here when we're sampling from a posterior is I don't know what the distribution should look like. All right. I just know how to evaluate how high the curve is given an XYZ theta 1 theta 2 theta 3 set of parameters. Okay. But I don't know if it's a normal. I don't know if it's a gamma. I have no idea what the family shape will look like. All right. It might be multimodal.
(24:12) It might be two-dimensional with a peak here and a peak over there. I know how to evaluate how high the curve is given that I'm in this location, but I have no idea what the general shape will look like because it's not a very easily written down mathematical format. Okay? Or at least I'm not a well-known one.
(24:33) And so what we're going to do is then sample trying to start building a histogram empirically on that three-dimensional space, if you will. All right? And then this is just a means of building a histogram, not a means of trying to smooth out the histogram. We're just trying to get empirical uh random samples from that posterior distribution. So it's a different goal from what we had before with smoothing out the KD.
(24:58) We can still layer that on top, right? So you are given a prior distribution. You're given a likelihood and mathematically you can combine them together to build a posterior but it might not have a nice mathematical frame framework and so I don't know how to really put a normal distribution to collect data from it.
(25:24) I don't know how to calculate the mean because it doesn't integrate well and so this is just a way of numerically building out what that posterior distribution would look like. It's a computational method. All right, we'll come back to this after the midterm, but this is just giving you a sense that where we're headed in the Beijian framework is that it becomes computationally more intensive because we have to start doing some simulations here. Great meme of the day posterior.
(25:56) So essentially starting off up to the homore frequentist and once you become a little bit more uh evolved, you become a homoasian. Great. and prior versus posterior always fun. Okay, so that's sort of just tying up the loose ends to doing baze inference. I think of Beijian modeling, Beijian inference as an alternative to a lot of these classic methods that we think of as frequentist when we want to build confidence intervals and hypothesis tests to infer about specific things, parameters in our underlying models versus building a whole bian framework
(26:34) which has a lot of nice more complicated and allows for more flexibility in our modeling approach. Okay, they're both parametric in ways. Let's take an aside now. So, we're going to shift gears away from doing inference. We'll come back to it when we do the review here in a little bit. And we're going to start a new topic that we've hinted at before.
(26:58) And this is just thinking about dealing with big data and high dimensionality. One way to do that is through this principal components analysis. So this is going to be a new method, a new algorithm that we're going to just sort of hit the highlights of just to talk about all right when you deal with this highdimensional setting.
(27:18) What are some tools we can use to handle it? All right, so first off, what are big data and highdimensionality? What does that mean? We've talked about it briefly in this class. What do we mean by high dimensionality? Let's go back to our data sets. So when we think about data sets, the simplest form of a data set, it has rows and columns, right? And so big data can come in one of two or three ways in the simplest case.
(27:48) Either you have a lot of rows or you have a lot of columns or you have a lot of both. All right? And the issues they create are a little bit different depending on the setting. So when we talk about big data, that's kind of what we're talking about. Now Pablo's joked that when I talk about big data as a statistician, I think of n as being in the hundreds because I can allow allow for things like central limit theorem generally to behave properly in that size of a sample in the hundreds.
(28:17) However, big data in the tech world generally means you're dealing with millions of obs observations or even billions of observations. That view of big data is very different. Big data depends on whether you have a lot of observations or a lot of predictors. They both can be referred to as dealing with big data. Let's think about this for a second.
(28:36) What are some issues when we're trying to fit just a linear regression model or a lasso model or a basian model? What are some issues that it can occur when n is big? Let's imagine we're dealing with millions of observations or billions of observations. If n is really big but p isn't too big, what issues occur? Issues.
(29:02) Is it a problem? Is everything fixed when n is big? Say that again. There could be a lot of noise. There could be a lot of still uh stochastic n irreducible uncertainty volatility in your data. you're never going to get rid of that even with a large n. So computationally things start to be a little bit uh slower when n is enormous when you're dealing with billions of observations.
(29:44) Just calculating an average or calculating a median can take some time. So you're talking about computational cost as one of the big issues here. without you could have some issues with independence when n in your sample is sort of leading to the pi uh population size. We're not worried about finite population size for this this class.
(30:11) But yeah, that's another issue that can occur. The big thing is computationally things will slow down computationally and just having a big N doesn't fix everything. All right. What are some issues when P is big? When the number of columns is big but n isn't big as big as p is what can occur? Somebody else overfitting. That's right.
(30:37) We saw that when we have a lot of predictors, we start to overfit our model to the data. All right. Mathematically, what happens when p is too big? Not just overfitting in your estimation. When you try to invert a matrix, what happens? Or go ahead, something else. Collinearity related to overfitting. There's risk of colinearity.
(31:03) And what happens when you have perfect colinearity? You have a matrix that you can't invert. All right? You try to take calculate in ordinary le squares xrpose x inverse. That XRpose X matrix you're trying to invert only holds if your columns are linearly independent. And if you have perfect colinearity among your predictors, that means that matrix is linearly dependent.
(31:28) Okay? All right. And we'll get into that as well. And when N and P are both big, you kind of have the worst of both worlds. Okay? You're trying to do computations that are going to take forever. All right.
(31:44) We talked a little bit about when n is big, doing estimation can be slow, especially when things aren't of closed form. We're not just going to talk about issues. We like to talk about fixes. What's a way to fix that? How can you solve the issue of too big of an N? It's really simple. More computers. Yeah. Put more money into the system. All right. Let's think the other way. If we don't have all the money in the world and don't have the biggest fanciest graphics card, yeah, we can just simply randomly sample some of our data rather than relying on all millions of observations.
(32:20) Just take a subsample. And sometimes even just taking 10% of your data or less to perform your training is totally fine. I mean, if you have millions of observations, who cares if it's 10 million or 100 million? there's enough data there if you only have uh dozens or hundreds of parameters.
(32:41) Okay, statistical inference, another little note, isn't really important when you start to think about very big n. Why not? If you have millions of observations, why don't I care about performing hypothesis tests? Why don't I comp really care much about doing confidence intervals? What happens to confidence intervals as n goes up? They get squeezed. And so they get smaller and smaller, narrower and narrower.
(33:19) And then you get a confidence interval that has ends up being essentially a point estimate. And so involving those uncertainties of your estimates or doing hypothesis testing, everything seems to be statistically significant when n is big. All right? And so when we're talking about really big, we're talking about hundreds of thousands, millions, or even billions of observations.
(33:38) They don't perform t tests anymore. Okay? And being big doesn't solve everything. Not only are there some computational issues, it doesn't solve the issue of bias. So if you are collecting your data in some biased way, collecting more of your biased data doesn't fix anything.
(33:59) In fact, there are some results that actually show that it can worsen the results when your data are biased. All right, don't believe me. Jali Mung, one of our professors in the stat department, has a nice little YouTube video here kind of explaining the issue of sampling a lot of data when your data are biased to begin with. It actually exacerbates issues.
(34:19) Okay, a lot of technical jargon there, so just watch out. Okay, so banana is big. It doesn't solve everything. It makes things slower. What happens when P is big? We talked a little bit about it. Some issues that arise. Matrices can't be inverted. That's an issue in OS. Multiolinearity is likely to be present. Models are susceptible to overfitting. These are all kind of related ideas.
(34:40) Okay. P approaches N or is even bigger. This is generally speak uh what is talked about when you talk about the curse of high dimensionality. When you have a lot of predictors, you have high dimensionality in your predictor set. When does this occur? Anybody come up to a situation when they've actually had to deal with the highdimensional setting in practice? When can it occur with our modeling approaches? When can you have a lot of predictors? We can artificially make lots of predictors. How can we artificially make lots of predictors?
(35:25) rather than just sampling random things from a normal distribution. Of course, you can make crappy predictors that way. Polomial regression. Exactly. Right. So, if you look at a 20th order polomial and you have not just one predictor to start with, but a 100 predictors to start with.
(35:44) Now, you got 2,000 predictors as a set. How else can we layer more dimensionality on top of that? interaction terms. Interaction terms. Yeah. So interaction terms will make it even larger too. If you start off with P inter or P predictors that are main effects, there's P choose two two-way interaction terms. There's also three-way interaction terms. There's interaction terms between a polomial for X1 and another polomial for X2.
(36:10) And so you can imagine that set being becoming extremely large very quickly. So when you're performing polomial regression when your predictors are uh in this case or you're dealing with a lot of interaction terms the place where I see it a lot is what we measure are genomic markers in the world of medicine sometimes.
(36:29) So you take a whole genomic or genetic scan of someone you get tens of thousands of genomic markers if not hundreds of thousands of genomic markers and afterwards you're trying to fit that to about a hundred or even a thousand people. too many predictors for the number of observations you have. And in this case also, it can occur when you start doing NLP and you're looking at English words and interactions between them, how they appear in a text.
(37:02) How many English words are there in in our dictionary? Anybody have any sense of the order of magnitude of the lexicon of the English language? What was that? About a 100. I use about 100. 2,000. It's in the tens of thousands. In the tens of thousands. So 12,000. There you go. Sounds good. Sounds good. All right. So what can occur is what's called unidentifiability.
(37:30) So when you have perfect mult multiolinearity, we don't know how to invert that matrix. And so when you start to fit a linear regression model in this case, it's a question of what happens. So one of the issues here is if you have at least n minus one predictors then you're going to be guaranteed to have that unidentifiable situation.
(37:49) We actually have and you can think of this as having extra parameters in your model that your data are not able to estimate. The remaining predictors coefficient actually can be estimated to be anything and they just essentially two of them kind of can balance each other out. So if the first predictor is estimated to be five, the second predictor could be estimated to be neg five. It's beta parameter.
(38:14) If the first one was 10, the second one would be negative 10. And so there's an infinite set of number of pred parameter estimates you can get. All right, there's some general approaches to handling this. You could drop a predictor, but a question is all right, in skarn, how does it handle the situation? What's the easiest way to create perfect colinearity in your set of predictors? If you wanted to physically do it, how can you make perfect colinearity? Yeah, just double your predictor set, right? Just take one predictor, copy it over, and now you have X1 and X2 that are exact same thing. By definition,
(38:54) those are collinear, perfectly correlated predictors. How does SKLearn handle that situation? How would you handle that situation in practice? Well, what one thing you could do, what R does, R says, all right, let's estimate the first one and then the second one that's collinear to it, let's just say it's unidentifiable, let's just drop it from the estimates. Okay, sklearn does other things.
(39:20) All right, sklearn doesn't do it that way. KSK learn what it does. If we're back to this predicting price from the square foot of homes in the Cambridge Somerville area, if we fit the model to predict price from square footage, we got a slope estimate. If we fit a predictor model that has a double copy of square footage and refit the model, what sklearn does is it essentially takes that estimate and splits it in two.
(39:55) Is that a reasonable thing to do? If you're not aware of this situation, it can cause problems. If you're trying to interpret 294 in an inferential way, that's a problem. Trying to interpret it, that's a problem. You can't change one variable without changing the other. However, from a predictive perspective, this is a ve very reasonable thing to do because in a new future observation, you might see these two variables be different in some way.
(40:35) And we want to contribute that predictive power equally from our training set to those two variables predictors in that training set. All right. And so this is essentially averaging that predictive power across the two predictors. A reasonable thing to do if you're trying to think about predicting in the future. Okay. But it's good to know how skarn handles these edge cases. All right.
(40:55) High dimensionality. We're going to be talking about for now the situation when P the number of predictors is large. When P the number of predictors is large, how could we handle it? What tools have we used so far to help us handle a high number of predictors? Lasso. Lasso is a simple way to sort of handle that. Let's penalize ridge or lasso.
(41:23) Penalize those parameters to account for some overfitting. Let's penalize them closer to zero because we're thinking we're overfitting to the too many predictors we have. We're going to talk about more specific ways to handle it and that is what PCA is here with lots of interaction or polomial terms.
(41:44) We're going to have that design x matrix that's too highdimensional in order to in uh uh invert. When we talk about the feature space, as Pablo mentioned a few uh weeks ago, it's going to be dominated by empty space. Our resulting feature space is what's deemed as sparse. Meaning we have lots of predictors and we have observations within that highdimensional space that when you're measuring them are really far away.
(42:09) And so then when you're trying to estimate the true functional form over that highdimensional space, if your observations are really far away, it's really hard to estimate what that function would look like when it's far away from two really actually observed observations. Okay, from a conceptual idea, what that means is strange things can happen in this setting.
(42:35) So let's just look at visually speaking what happens when uh dimensionality gets high. You can think of this geometrically. All right. And the geometric perspective here is just kind of thinking about the ratio of in this three-dimensional object of in this case the area or volume compared to the surface area of that dimensional object. All right.
(42:59) And so when we think two-dimensionally versus threedimensionally, we can talk about the volume of a sphere versus the volume of a cube. Our observations are lying in the space. And we just want to think about how much of that cubic space does that sphere take up or how much of that square does that circle take up. Proportionally the area here.
(43:37) What is this ratio of the circle to the square? How much area of the unit squared does the circle take up that's inscribed inside of it? If this area has radius one, what's the area of that circle? Pi. 3.14159 2653589 7932 3846264 33 38327950. Anyway, the square that goes around that circle, how much area does that have? Not one but four. Right. All right.
(44:22) And so what's the ratio circle to square? 78%. 79% something like that. Okay. You can play the same game in a unit cube versus the unit sphere. What proportion does the sphere take up inside the cube? How does it compare to 78 or 79%. It's much smaller. All right? And so when we're talking about distance in high dimension and we're talking about volume and area in high dimensions that ratio when you have a fixed number of observations represented by this circle within the square or the sphere within the cube gets smaller and smaller. All right? So that ratio of how the
(45:14) sphere compares to the cube depending on your dimensions drops prohibitively as those dimensions go up. Another way to think about this is if we just talk about randomly sampling from normal distributions.
(45:34) If we just randomly sample from two separate independent normal distributions and we just measure uklidian distance from them. If those two normal distributions basically are being sampled with one dimension, then the average distance between any two points is pretty small. If we randomly sample from a normal distribution with 45 observations or sorry a thousand different dimensions to it, the average distance between any two data points gets further and further apart. All right.
(46:02) So this high issue of high dimensionality just means proportionally or relatively speaking your dimensions start to get further apart, right? And so estimating some sort of function over those points that are really far apart which is what this goal of estimation is is going to be really difficult. Okay, that's related to the idea of overfitting as well.
(46:28) This general idea of poor generalization leads to the idea of overfitting. If our predictors are highly correlated, there's going to be redundancy in our data set of predictors. Therefore, we're going to have overfitting as as well. All right, so here's a simple geometric representative representation of the approach we're going to take with PCM.
(46:45) All right, we're going to start off with two different predictors, X1 and X2. They have a certain amount of variability, and it's saying, all right, which of these two predictors do we want to use to predict some other response variable? We're doing data reduction with our predictor set.
(47:05) All right? And we want to reduce the size of the number of predictors we have that contains as much of the information of them as possible. All right? If they have different variances, if they have different spreads, probably what you want to do is use the predictor that has represents the largest spread, the largest variability, right? And so if you were going to use one of these two predictors, you probably would choose X1 as your predictor to predict the response. Now, this is pretty cooked. We might want to scale things. That's going to put everything on the same scale. All right.
(47:37) Here, when I created X1 and X2, how were they related? This is synthetic data. X1 and X2, what's the correlation between them? horizontal line correlation is zero. Okay, that's not the case in real life. Very rarely are you going to have two predictors that are uncorrelated. Generally, what's going to happen is you're going to have two predictors that have correlation.
(48:08) What does that look like in threedimensional space or two dimensional space? That means you have this scatter plot that follows a line, right? We could estimate a correlation to these two predictors. Correlation probably something like 7.8. It's positive. All right.
(48:26) And then what we're going to say is, all right, let's just imagine there's just not two predictors, but there's hundreds or thousands of predictors that are all correlated. What we might want to do instead of using all 1,000 or hundreds of predictors, we might want to boil them down into their uh the essence component parts. And that's what PCA does. It's saying let's take what we started off with as two predictors and boil them down into their simplest form.
(48:55) Let's combine them to make one predictor to represent both of them. All right. How would you go about using these two predictors to build a new predictor, a single predictor that you're then going to use in a future prediction model for a separate response? What could you do given this picture? How could you combine X1 and X2? You could pick X1.
(49:28) You could pick X2 or you could pick some combination of those two. All right. What would be a reasonable best combination of X1, X2 to represent this two-dimensional scatter plot? Maybe the average of X1 and X2. Okay, that might be an issue. If they're perfectly collinear and they're negatively related, you're going to get zeros potentially. You could do some sort of regression to kind of fit some sort of line to this scatter plot.
(50:04) And then for every x1 and every x2, you can then create an x3 that is some linear combination of x1 and x2. might work out to be just the average. Okay, in the simplest case. All right. Well, that's essentially what we're going to do. We have X1 and X2 and we want to combine them. They have redundant information because they're correlated and we want to boil them down to the base case that represents those two variables that were measured separately into one measured engineered feature.
(50:31) And that's the goal of PCA. Essentially what it does is it takes and breaks down this scatter plot into directional vectors. One that is in the direction of highest variance seen in this scatter plot. And then once you get that directional vector, it takes an orthonormal vector and puts it into the direction that is orthogonal to it.
(50:56) All right? And so essentially what it does is it takes this X1 and X2 original scale and does a linear transformation into a Z1 and Z2 scale where Z1 goes down the spine of that correlation and Z2 is everything left over. Okay, with two dimensions probably doesn't really make much sense, but you can imagine this extending to hundreds if not thousands of dimensions. with me kind of.
(51:29) Okay, so that's kind of the goal we're playing here or the game we're playing here. We want to come up with a way of doing a transformation from X1 and X2 dimensions doing some sort of matrix transformation into Z1 and Z2 which where Z1 has the most information about your original predictors X1 and X2 and then Z2 is what remains.
(51:54) So what's the best way of choosing which predictors you're going to combine? So we're going to do this in an automated algorithmic way and that's what PCA is going to do. So if two dimensions you can imagine this geometrically very easily in three dimensions you have to pick the Z direction carefully.
(52:11) All right and it's depends on the data you have and the automated way of doing that is this idea of PCA. All right. How many of you have taken a linear algebra class before? All right. In your linear algebra class, do they ever talk about PCA? If they didn't, it's your loss. Okay. What is PCA? Anybody here of IGEN vectors and IGEN values before? They probably talked about IGEN vectors and IGEN values, right? Did they give you a context for igen vectors and IGEN values? If they did, it was probably PCA. If they didn't, it's a shame because it should have been PCA. What
(52:50) does IGEN value and IGEN vector decomposition do? It takes your matrix of vectors that might not be linearly independent and does a transformation into a matrix space that creates vectors that are linearly independent aka orthogonal to each other with ordering with a best vector, a second best vector, a third best vector. And that's exactly what PCA is doing.
(53:22) If you didn't follow that because you don't know linear algebra, it's totally fine. It's the geometric interpretation of that. Okay, it's taking this two-dimensional space and finding a vector that fits along the spine. Okay, so that's kind of the framework here. We have our original set of vectors, our x's, and what we're trying to do is perform some linear transformation of our x vectors, and we're trying to create a new set of z vectors that result that are better representation that are now ordered from best to worst. This is only in your predictor space.
(53:58) Okay? So, your x's, let's say we have p variables, these are columns in your original data matrix. All right? and we're going to transform them linearly into a resulting Z set of vectors and they can have as many Z vectors as you originally started off with X vectors. Okay, what's the goal of doing this? To reduce our dimensionality rather than halfhazardly removing some of your predictors to start with.
(54:27) Now we're transforming into a space that has order into those set of vectors. We have a best, we have a second best up to a worst. And so what we can do is just pair down our resulting dimension of vectors just for throwing out the worst ones. How many should you keep? What kind? Depends on the setting with me kind of with that framework, right? And this is sort of the mathematical way of combining the two.
(54:55) And then what you do is you use these Z vectors and you fit a linear regression model to it. Instead of keeping all P vectors, you create a subset of those Z vectors that you then use in your model. And the reason we're using Z instead of X is because we have an automated way of ordering which is best to represent your predictor set.
(55:19) This is a transformation of your predictors only, completely separate from your response variable. So that's our framework. We're going to create this set of Z vectors that hopefully captures the highest amount of variation. We're doing this linear transformation rotation of our uh set of axes to sort of have a smaller dimensional set so that we don't have a bad time when we start fitting models.
(55:44) So we first create this what's called first principal component that's along the spine of our predictors and then what remains our second permit principal component will be the best of the variability of the predict predictors that remain around that first principal component. All right.
(56:03) And so what you can think of this as doing essentially is doing that data reduction in a way. It's trying to find the single var uh vector that represents the data in the proper geometric way. The animation shows the projections onto several candidate vectors. That's what this red represents.
(56:24) And essentially what you're doing is you're trying to find the vector that minimizes the orthogonal distance to it for all the data points in your data set. That's what this igen value vector decomposition is doing. All right, don't worry. There's a whole lot of linear algebra that tells you how to use this, how to perform this. But don't worry, in skarn, there's an automatic way of doing it.
(56:48) Okay, it's within the decomposition class. There's something called PCA. All right, so we just kind of need to know how to use it. So what I create as an example here, this is again that Cambridge homes data set, Cambridge Somerville Homes. We have a set of five predictors. Those five predictors I call my X matrix. What we do is we fit a PCA projection of that transformation.
(57:12) So we fit our PCAS to X and then we transform our X based on that fit and create a new X matrix. Okay. So what did we do? The PCA tells us how to do the transformation and then we apply that transformation to the data vector X to create this new X matrix that we can then use in regression. All right.
(57:38) And so as a result, this PCA X is essentially just going to be then what we called the new matrix sets. And so what we have as a result are our different PCA components are W's which reflect the linear combinations of all of the separate predictors that we started off with.
(58:04) So this PCA component vector 1 2 3 4 5 tells you that this is essentially the linear transformation of our original five vectors that would get us from X to what's called PCAX. All right. So we linear transform square foot, beds, baths, lot size, and distance based on this function to create the new X matrix most important column. All right, the interpretation here is that these W elements, there's five of them.
(58:41) We're linearly transforming each of those five variables in our original data matrix X. What are the components contra uh comprising it? Well, you can see based on the multiplication, the two variables that uh contribute the most to that first matrix are square foot and beds. Why is it reasonable that that's where our spine of our original matrix goes to in this PCA transformation? Those two variables are highly correlated.
(59:12) those two various variables had high variance and they are essentially saying that that first vector that our original X matrix is comprised of is mostly what's important is to hold on to square root of foot and bed some combination of them. Okay, but it's a contribution of all five vectors into it. Once that first vector is defined, then orthogonal to that first vector, we have a linear combination of other variables.
(59:38) And it looks like some combination of baths and lot size defines that second vector best. All right. And you can imagine down the line there's going to be five different component. And then essentially what you also get is you get the variance in your original predictor space that each of these components comprise. Right? So we're going to have five separate components.
(1:00:03) The first component vector comprises over 90% of the variability in our original predictor space. All right, just with a single variable, we can boil down our data set, our predictors into a single variable with 90% of that original uncertainty. And it just sort of boils down breaks down further from that. All right.
(1:00:22) So we're trying to transform our original predictor space into its component parts. That first variable holds 90% of the information. We can go continue on this idea of doing this transformation. Uh an alternative thing is trying to find a lowdimensional linear surface that is closest to the data points.
(1:00:46) Just like we're trying to angle that uh vector that matches that two-dimensional scatter plot, we can think of the three-dimensional aspect of it is we have a plane that we're trying to angle into the direction of our data points as well. Okay. And higher dimension holds as well. All right. So once you figure out how to do uh PCA, there's two use cases really in this class for using PCA.
(1:01:06) High dimensions we want to boil down to its key component parts. two things. One is we're going to use PCA for visualization and the other is we can use PCA to build our separate regression model. All right, when we do PCA for visualization, quite honestly, this is how I use PCA all the time.
(1:01:26) As essentially, I have a data set where I have hundreds of predictors and I just want to visualize what's going on. Is there any capability in actually using these predictors to predict the response? I perform PCA. I take the first two components, put them on X and Y, and plot the data on top of that.
(1:01:46) And I just can now say, all right, is there a difference somewhere to predict my response based on these two variables that I just pulled out of my predictor space. So that's essentially what this is doing essentially in high dimensions. When you have lots of X variables and you just want to visualize it, pull off the first two components of PCA and just use those as your X and Y in your scatter plot.
(1:02:10) Here what we have is we have as an example sort of illustrating where we're headed in this class is to do classification. Imagine the setting where we have several different uh measurements on different penguin species. These different penguin species, there are three of them. They're the Adelaide, the chinstrap, and magentu. And we want to be able to predict which individual penguin comes from each of these different classes, types of penguins.
(1:02:41) We are measuring four different things, their bill length, their bill depth, their flipper length, and their body mass. And we're trying to boil down these measurements on these penguins into their raw component parts in order to visualize what's going on. Okay, looking at the scatter plot, you can uh kind of get a general sense of how these penguins are different.
(1:03:02) All right, these genu penguins sort of are separate in most of these dimensions from the Adelaide and the ginstrap. However, you might come to some sort of component parts where we look at flipper length and bill length that separate them pretty well, all three groups.
(1:03:23) What we want to do is not just look at the X's we were given, but come up with some combination of X's that separate these even better. That's what PCA is hopefully going to do. So, what we do is we take these four measurements, build our PCA model, and then basically project those three different groups onto the PCA components that were contrived of those four different measurements.
(1:03:44) And we're hoping here that our three different subgroups of penguins are well separated here. Reasonably that seems to be the case. Okay. Why is this an improvement? Why do we care about doing this? In some settings, you have these genomic markers or you might have lots of predictors that you're trying to predict some response to and you're just trying to get a sense for a feel in your data through visualization.
(1:04:15) Is there any reasonable uh opportunity to actually perform predictions here? What we can do is actually break this down even further into males and females. If we were trying to predict into six subclasses, things would even get a little bit more challenging. All right. So, not only does it uh is it useful for predicting penguins and doing a classification there, it can occur as well when we start talking about imaging data.
(1:04:42) When we deal with imaging data, a lot of times what we're going to end up doing is pixelating the images to represent those different uh objects or different images that those objects might represent. So the fashion emnest data set is a well-known one where there's 10 different categories of clothing that are essentially boiled down to grayscale pixels entering uh resulting in 784 different features.
(1:05:09) And so what comes in is the grayscale measurement for each of the 784 pixels defining that image. All right, X matrix has 784 features. And here are some examples. You have t-shirts, trousers, pullovers, dress code, etc. Each of these images is measured on a gray scale. So every pixel, black, white, or some gray in between.
(1:05:38) Now you can imagine looking at them visually it's really easy to see the difference between them. But the question is mathematically how can you make that assessment? Well, you have 784 data points. And what you do is you just do a PCA component uh breakdown. And so you get a first PCA component, a second PCA component of all those gray grayscale pixel measurements. After doing so, you can then try to get a sense of how your data falls into these PCA components.
(1:06:10) You're not guaranteed that you're going to be able to separate your categories, your response variables through this PCA de uh decomposition. Mainly your PCA is ignorant of the response. Okay, it's done just using the predictors simply so that you can plot them on an XY scale. What you'll notice then every image class is depicted as a different color.
(1:06:37) And so what you're doing is you're projecting the top two principal components simply just to visualize the data set. While there is a lot of overlap, a lot of these categories, these types are starting to be grouped together. And just be aware PCA never actually saw those category labels. It's just done based on those grayscale pixels.
(1:06:58) Seems to do a reasonable job just visual visually uh look thinking about how we're separating those different classes uh in this case. All right, we're just about out of time. We got about 10 minutes. Falls in the air. Today's keyword is pumpkin. Today's keyword is pumpkin. Secret word. All right.
(1:07:29) So, one of the use cases is just to help us visualize things that are in high dimension to put on two-dimensional visualization space. The other use I mentioned already is this idea of doing PCA for regression. One way to do variable selection is to perform a PCA decomposition and then start to fit a model to those de uh component vectors.
(1:07:52) What you do is you fit a set of predictors that is now going to be some subset of your original set of predictors and now you've reduced the dimensionality. Hopefully you've gotten rid of that potential for perfect multiolinearity. The big thing is how do you decide on a value of M? And so once you perform your PCA decomposition starting off with zero components up to one component all the way down what you're seeing here is the amount of variance that is explained for every PCA component.
(1:08:26) And you see this general idea here of the diminishing returns where the first component explains in this example about 7% of the variability in your predictor set and every time you use an extra component it go gets a little bit worse and worse. And so at some point you just say all right enough vectors is enough. Here we say maybe once we get to a point here this idea of an elbow method gives us a sense of all right let's draw the line here when we get to 20 components we see that that diminishing return starts to tail off a lot you can also make the judgment based on the total cumulative
(1:09:04) variance explained number of components as it increases that starts to increase go up and in this case if we wanted to explain 90% of our variability in our hundreds of predictors we would only need 53 of them. Depends on the setting, depends on your data set, how many of these components you want to use.
(1:09:24) And once you have the components, the number of components selected, then you can start fitting different models uh with that as our new class of predictors. There's a whole lot of math trying to help you interpret what the results of this PCRs are. In this class, we're not going to get too much into it. Okay.
(1:09:43) The general idea is a way to reduce the dimensionality of your data set in an automated way. So to reduce the uh amount of overfitting. Okay. Lots of notes. Cons. There's nothing known about the response variable when you create your principal components. You might go through this process to reduce your data set and as a result your first two components which contrive a lot or are composed of a lot of your predictor space may not actually be predictive of your response.
(1:10:15) Your first principal component is not guaranteed to be the best predictor of the response variable. They're created ignorant of the response. The interpretation of the coefficients when you fit a regression model to your PCA components is pretty much lost. you're now having some transformation of all of your predictors in that first variable. And I have no idea what that really means in context.
(1:10:38) And it's never really going to improve the predictive power of a model. It's just going to make it a little bit easier, less cumbersome to use. It has a lot of uh pros. It's going to reduce that cursive dimensionality, reduce the effect of overfitting. It's going to allow us to visualize predictors on a much smaller set of your predictors.
(1:10:58) Um and it does like I said reduces that multi-olinearity which is related to computational time. This idea of PCA is just a first glimpse into the idea of how to handle dimensionality highdimensionality situations. An automated way to handle that is through PCA. Okay, that's a lot to say. We have some review here.
(1:11:22) Like I said, we only have a few minutes, but let's hit uh the first thing here of review. And we'll get into it again on Monday and section has some of this review as well. What do we know about hypothesis testing? Hypothesis testing in five minutes for those of you who've taken a stat class. What is hypothesis testing? dependable type of correlation.
(1:12:05) There you go. I like that second word in correlation. So, we have a response variable Y in the regression setting, a predictor variable X, and we're trying to determine whether there's a real relationship between Y and X. We're trying to test whether that slope is different from zero.
(1:12:23) It's a formal way to do that. Okay, we build two competing worlds. We say this data could have been generated under the world with no association or we this data was created under the world of an association. The slope is zero versus the slope is non zero. Okay, that is a statement about truth that the underlying model the beta for it is either zero or not.
(1:12:50) We build a null hypothesis beta equals 0 and an alternative hypothesis beta equals 1 or beta is non zero. All right, that is the true beta, the population beta, the theoretical beta. What do we do? We collect data and get an estimate from our scatter plot. And then based on our scatter plot, we basically collect our data and compute that test statistic for that slope.
(1:13:25) What letter do we give that test statistic? What distribution do we think it follows? That t test statistic. We talked about this before in skarn. In stats models, it will automatically do this. We'll step through it real quickly here. We then use that value of that t test and its related p value to either reject or not reject that null hypothesis of no association or not. And then we try to conclude in the context of the problem.
(1:13:50) Okay, let's go through this once. Oh, calculating the p value some details to it. Let's go through this once for the stats model example. Here's the relationship of how square footage relates to selling price of these homes. Here's the scatter plot. What do you notice? Do things look linear? Yeah, reasonably well. Okay.
(1:14:16) Do you think there's a real association in the model that created these data? Yeah, there's probably a real association. We assume our slope is going to be significantly different from zero. What about our other assumptions? What can you say about the other assumptions? Independence of prices of homes. Are they independent? Probably not. What about normality? Hard to tell from the scatter plot.
(1:14:36) What about constant variance? Seems pretty well violated. All right, what do we do anyway? We ignore that fact. We fit a regression model in stats models and we get an estimated intercept. We get an estimated slope. This is our beta 1 hat. And based on our beta 1 hat, we want to see if that's evidence that the true slope is non zero.
(1:14:58) Okay, we have 500 and some obser observations and we want to see if we have significant evidence statistically in the data to say that the true slope is non zero. Of course, it's non zero. So, we state our hypothesis. The true slope is zero or not. We collect our data. It's going to be a t test. We say for now we trust our assumptions. We sample our data, fit our model, get our estimates. Our estimated slope is 0.589.
(1:15:23) It has a formula. Our estimated standard error for that slope is 023. You take the ratio of those two and you get a t statistic of 25.2. That t statistic of 25.2 is our test statistic that we think follows a t distribution with n minus2 degrees of freedom. Okay.
(1:15:47) And so we fit uh based on that t distribution, we try to find the p value, which is the probability of any old t distribution with 590 degrees of freedom of being further out in the tail than our observed value of 25.2 or further. What are the chances of that? A t distribution looks a lot like a normal. We're essentially 25 standard errors away.
(1:16:05) That's pretty darn far, right? We're way out in the tail. That probability uh Python reports as zero. really it's less than 0001. Okay. At any reasonable type one error, we usually compare it to 0.05. Since our p value is low, HO must go. We reject the null hypothesis. Okay. Restated. That basically says there's evidence in our data to suggest that housing prices are truly associated with the size of a home. Okay. Using our output from stats models to perform a hypothesis test.
(1:16:37) What assumptions went into this? linearity, normality, constant variance, and independence. That constant variance is clearly violated. So, what do we do instead? The permutation test. And we'll start there next time. Okay, great. Pumpkin is our keyword. Enter that in for your attendance today. See you on Monday.